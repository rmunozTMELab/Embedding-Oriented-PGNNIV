{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7a48a0a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys \n",
    "\n",
    "current_directory = os.getcwd()\n",
    "models_directory = os.path.abspath(os.path.join(current_directory, '..'))\n",
    "sys.path.append(models_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "71461cff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "import json\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import itertools\n",
    "\n",
    "import matplotlib.colors as mcolors\n",
    "import matplotlib.ticker as ticker\n",
    "from matplotlib.ticker import ScalarFormatter\n",
    "from mpl_toolkits.axes_grid1.inset_locator import inset_axes\n",
    "from matplotlib.ticker import LogLocator, LogFormatter\n",
    "from matplotlib.lines import Line2D\n",
    "\n",
    "# Imports de la libreria propia\n",
    "from vecopsciml.kernels.derivative import DerivativeKernels\n",
    "from vecopsciml.utils import TensOps\n",
    "from vecopsciml.operators import zero_order as zo\n",
    "from vecopsciml.algebra import zero_order as azo\n",
    "\n",
    "# Imports de las funciones creadas para este programa\n",
    "from utils.folders import create_folder\n",
    "from utils.load_data import load_data\n",
    "from utils.checkpoints import load_results\n",
    "from utils.fourier_base import compute_fourier_base\n",
    "\n",
    "from vecopsciml.operators.zero_order import Mx, My"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "440063e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from architectures.pgnniv_baseline import PGNNIVBaseline\n",
    "from architectures.pgnniv_fourier import PGNNIVFourier\n",
    "from architectures.pgnniv_pod import PGNNIVPOD\n",
    "from architectures.pgnniv_decoder import PGNNIVAutoencoder\n",
    "from architectures.autoencoder import Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4658e960",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataframe_a_latex(df, nombre_archivo=None, index=False, caption=None, label=None):\n",
    "    \"\"\"\n",
    "    Convierte un DataFrame en una tabla LaTeX.\n",
    "    \n",
    "    Args:\n",
    "        df (pd.DataFrame): DataFrame a convertir.\n",
    "        nombre_archivo (str): Ruta para guardar el archivo .tex (opcional).\n",
    "        index (bool): Si se incluye o no el índice.\n",
    "        caption (str): Título de la tabla (opcional).\n",
    "        label (str): Etiqueta de la tabla para referencia cruzada (opcional).\n",
    "    \n",
    "    Returns:\n",
    "        str: La cadena en formato LaTeX.\n",
    "    \"\"\"\n",
    "    latex_str = df.to_latex(index=index, caption=caption, label=label, escape=False)\n",
    "\n",
    "    if nombre_archivo:\n",
    "        with open(nombre_archivo, 'w', encoding='utf-8') as f:\n",
    "            f.write(latex_str)\n",
    "    \n",
    "    return latex_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4d61ce4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully loaded from: /home/rmunoz/Escritorio/rmunozTMELab/Physically-Guided-Machine-Learning/data/non_linear_10_0/non_linear_10_0.pkl\n"
     ]
    }
   ],
   "source": [
    "data_name = 'non_linear_10_0'\n",
    "\n",
    "ROOT_PATH = os.path.abspath(os.path.join(os.getcwd(), \"../\"))\n",
    "DATA_PATH = os.path.join(ROOT_PATH, r'data/', data_name, data_name) + '.pkl'\n",
    "\n",
    "dataset = load_data(DATA_PATH)\n",
    "dx = dataset['x_step_size']\n",
    "dy = dataset['y_step_size']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a101e6f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def relative_error(validation, prediction, dx=dx, dy=dy):\n",
    "    \n",
    "    prediction_error = np.sqrt((np.trapz(np.trapz((validation - prediction)**2, dx=dy), dx=dx) /\n",
    "                                np.trapz(np.trapz((validation)**2, dx=dy), dx=dx)))\n",
    "\n",
    "    return prediction_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4c8a0587",
   "metadata": {},
   "outputs": [],
   "source": [
    "def smooth_curve(data, window_size=1000):\n",
    "    window = np.ones(window_size) / window_size\n",
    "    return np.convolve(data, window, mode='valid')\n",
    "\n",
    "def cm_to_in(cm):\n",
    "    return cm * 0.393701\n",
    "\n",
    "def normalize_list(lst):\n",
    "    max_value = np.max(lst)\n",
    "    return [x / max_value for x in lst]\n",
    "\n",
    "linewidth = 1.5  \n",
    "title_fontsize = 14  \n",
    "label_fontsize = 14  \n",
    "legend_fontsize = 12 \n",
    "tick_fontsize = 11  \n",
    "\n",
    "# plt.rc('text', usetex=True)\n",
    "plt.rc('font', family='serif')\n",
    "\n",
    "posX = cm_to_in(10) # posición de la esquina inferior izquierda de la imagen en X\n",
    "posY = cm_to_in(10) # posición de la esquina inferior izquierda de la imagen en Y\n",
    "width = cm_to_in(12)  # ancho de la imagen\n",
    "height = cm_to_in(8) # alto de la imagen\n",
    "\n",
    "color = [0.1, 0, 0.8]  # triplete RGB, valores entre 0 y 1\n",
    "subplot_adjust_left = cm_to_in(0.15)\n",
    "subplot_adjust_bottom = cm_to_in(0.15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "63edea20",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7e665132",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "baseline\n",
      "Data successfully loaded from: /home/rmunoz/Escritorio/rmunozTMELab/Physically-Guided-Machine-Learning/data/non_linear_10_0/non_linear_10_0.pkl\n",
      "Data successfully loaded from: /home/rmunoz/Escritorio/rmunozTMELab/Physically-Guided-Machine-Learning/data/non_linear_10_0/non_linear_10_0.pkl\n",
      "Data successfully loaded from: /home/rmunoz/Escritorio/rmunozTMELab/Physically-Guided-Machine-Learning/data/non_linear_10_0/non_linear_10_0.pkl\n",
      "Data successfully loaded from: /home/rmunoz/Escritorio/rmunozTMELab/Physically-Guided-Machine-Learning/data/non_linear_10_1/non_linear_10_1.pkl\n",
      "Data successfully loaded from: /home/rmunoz/Escritorio/rmunozTMELab/Physically-Guided-Machine-Learning/data/non_linear_10_1/non_linear_10_1.pkl\n",
      "Data successfully loaded from: /home/rmunoz/Escritorio/rmunozTMELab/Physically-Guided-Machine-Learning/data/non_linear_10_1/non_linear_10_1.pkl\n",
      "Data successfully loaded from: /home/rmunoz/Escritorio/rmunozTMELab/Physically-Guided-Machine-Learning/data/non_linear_10_5/non_linear_10_5.pkl\n",
      "Data successfully loaded from: /home/rmunoz/Escritorio/rmunozTMELab/Physically-Guided-Machine-Learning/data/non_linear_10_5/non_linear_10_5.pkl\n",
      "Data successfully loaded from: /home/rmunoz/Escritorio/rmunozTMELab/Physically-Guided-Machine-Learning/data/non_linear_10_5/non_linear_10_5.pkl\n",
      "Data successfully loaded from: /home/rmunoz/Escritorio/rmunozTMELab/Physically-Guided-Machine-Learning/data/non_linear_100_0/non_linear_100_0.pkl\n",
      "Data successfully loaded from: /home/rmunoz/Escritorio/rmunozTMELab/Physically-Guided-Machine-Learning/data/non_linear_100_0/non_linear_100_0.pkl\n",
      "Data successfully loaded from: /home/rmunoz/Escritorio/rmunozTMELab/Physically-Guided-Machine-Learning/data/non_linear_100_0/non_linear_100_0.pkl\n",
      "Data successfully loaded from: /home/rmunoz/Escritorio/rmunozTMELab/Physically-Guided-Machine-Learning/data/non_linear_100_1/non_linear_100_1.pkl\n",
      "Data successfully loaded from: /home/rmunoz/Escritorio/rmunozTMELab/Physically-Guided-Machine-Learning/data/non_linear_100_1/non_linear_100_1.pkl\n",
      "Data successfully loaded from: /home/rmunoz/Escritorio/rmunozTMELab/Physically-Guided-Machine-Learning/data/non_linear_100_1/non_linear_100_1.pkl\n",
      "Data successfully loaded from: /home/rmunoz/Escritorio/rmunozTMELab/Physically-Guided-Machine-Learning/data/non_linear_100_5/non_linear_100_5.pkl\n",
      "Data successfully loaded from: /home/rmunoz/Escritorio/rmunozTMELab/Physically-Guided-Machine-Learning/data/non_linear_100_5/non_linear_100_5.pkl\n",
      "Data successfully loaded from: /home/rmunoz/Escritorio/rmunozTMELab/Physically-Guided-Machine-Learning/data/non_linear_100_5/non_linear_100_5.pkl\n",
      "Data successfully loaded from: /home/rmunoz/Escritorio/rmunozTMELab/Physically-Guided-Machine-Learning/data/non_linear_1000_0/non_linear_1000_0.pkl\n",
      "Data successfully loaded from: /home/rmunoz/Escritorio/rmunozTMELab/Physically-Guided-Machine-Learning/data/non_linear_1000_0/non_linear_1000_0.pkl\n",
      "Data successfully loaded from: /home/rmunoz/Escritorio/rmunozTMELab/Physically-Guided-Machine-Learning/data/non_linear_1000_0/non_linear_1000_0.pkl\n",
      "Data successfully loaded from: /home/rmunoz/Escritorio/rmunozTMELab/Physically-Guided-Machine-Learning/data/non_linear_1000_1/non_linear_1000_1.pkl\n",
      "Data successfully loaded from: /home/rmunoz/Escritorio/rmunozTMELab/Physically-Guided-Machine-Learning/data/non_linear_1000_1/non_linear_1000_1.pkl\n",
      "Data successfully loaded from: /home/rmunoz/Escritorio/rmunozTMELab/Physically-Guided-Machine-Learning/data/non_linear_1000_1/non_linear_1000_1.pkl\n",
      "Data successfully loaded from: /home/rmunoz/Escritorio/rmunozTMELab/Physically-Guided-Machine-Learning/data/non_linear_1000_5/non_linear_1000_5.pkl\n",
      "Data successfully loaded from: /home/rmunoz/Escritorio/rmunozTMELab/Physically-Guided-Machine-Learning/data/non_linear_1000_5/non_linear_1000_5.pkl\n",
      "Data successfully loaded from: /home/rmunoz/Escritorio/rmunozTMELab/Physically-Guided-Machine-Learning/data/non_linear_1000_5/non_linear_1000_5.pkl\n",
      "baseline\n",
      "                  Hyperparameters    time       eQ1       eQ2       eQ3  \\\n",
      "N_data Sigma Mode                                                         \n",
      "10     0     5               4396   25.93  7.56e-02  8.47e-02  9.38e-02   \n",
      "             10              4501   25.49  8.07e-02  9.33e-02  1.06e-01   \n",
      "             50              5341   25.69  7.62e-02  8.70e-02  9.78e-02   \n",
      "       1     5               4396   29.09  7.11e-02  7.32e-02  7.52e-02   \n",
      "             10              4501   30.15  7.55e-02  8.17e-02  8.78e-02   \n",
      "             50              5341   26.43  7.45e-02  8.52e-02  9.60e-02   \n",
      "       5     5               4396   27.64  1.11e-01  1.11e-01  1.12e-01   \n",
      "             10              4501   29.52  1.10e-01  1.16e-01  1.22e-01   \n",
      "             50              5341   26.47  1.07e-01  1.14e-01  1.20e-01   \n",
      "100    0     5               4396   28.76  1.39e-03  4.00e-03  9.54e-03   \n",
      "             10              4501   29.45  1.10e-03  3.03e-03  1.00e-02   \n",
      "             50              5341   29.02  3.04e-03  6.89e-03  1.23e-02   \n",
      "       1     5               4396   29.24  1.61e-02  2.32e-02  3.17e-02   \n",
      "             10              4501   29.39  1.88e-02  2.27e-02  5.72e-02   \n",
      "             50              5341   30.13  1.97e-02  2.80e-02  3.66e-02   \n",
      "       5     5               4396   30.11  1.19e-01  1.41e-01  1.92e-01   \n",
      "             10              4501   27.16  1.17e-01  1.47e-01  1.96e-01   \n",
      "             50              5341   27.39  1.15e-01  1.36e-01  1.79e-01   \n",
      "1000   0     5               4396  254.74  1.05e-04  1.44e-04  2.61e-04   \n",
      "             10              4501  253.72  1.14e-04  1.54e-04  2.52e-04   \n",
      "             50              5341  251.97  9.65e-05  1.37e-04  2.48e-04   \n",
      "       1     5               4396  254.20  1.18e-02  1.26e-02  1.36e-02   \n",
      "             10              4501  254.48  1.17e-02  1.26e-02  1.35e-02   \n",
      "             50              5341  253.50  1.17e-02  1.27e-02  1.36e-02   \n",
      "       5     5               4396  229.03  5.94e-02  6.51e-02  7.13e-02   \n",
      "             10              4501  222.71  5.88e-02  6.37e-02  6.81e-02   \n",
      "             50              5341  220.34  5.84e-02  6.36e-02  6.91e-02   \n",
      "\n",
      "                         eK  \n",
      "N_data Sigma Mode            \n",
      "10     0     5     2.53e-02  \n",
      "             10    2.51e-02  \n",
      "             50    3.39e-02  \n",
      "       1     5     4.76e-01  \n",
      "             10    4.74e-01  \n",
      "             50    5.14e-01  \n",
      "       5     5     9.98e-01  \n",
      "             10    9.99e-01  \n",
      "             50    9.95e-01  \n",
      "100    0     5     3.33e-02  \n",
      "             10    2.98e-02  \n",
      "             50    3.35e-02  \n",
      "       1     5     2.96e-01  \n",
      "             10    3.30e-01  \n",
      "             50    2.15e-01  \n",
      "       5     5     9.60e-01  \n",
      "             10    9.81e-01  \n",
      "             50    9.68e-01  \n",
      "1000   0     5     2.61e-02  \n",
      "             10    2.26e-02  \n",
      "             50    2.55e-02  \n",
      "       1     5     3.11e-02  \n",
      "             10    4.23e-02  \n",
      "             50    4.44e-02  \n",
      "       5     5     8.18e-01  \n",
      "             10    8.69e-01  \n",
      "             50    9.07e-01  \n",
      "Tiempo actual en segundos: 13220.221088409424\n",
      "Tiempo actual en minutos: 220.33701814015706\n",
      "\n",
      "\n",
      "fourier\n",
      "Data successfully loaded from: /home/rmunoz/Escritorio/rmunozTMELab/Physically-Guided-Machine-Learning/data/non_linear_10_0/non_linear_10_0.pkl\n",
      "Data successfully loaded from: /home/rmunoz/Escritorio/rmunozTMELab/Physically-Guided-Machine-Learning/data/non_linear_10_0/non_linear_10_0.pkl\n",
      "Data successfully loaded from: /home/rmunoz/Escritorio/rmunozTMELab/Physically-Guided-Machine-Learning/data/non_linear_10_0/non_linear_10_0.pkl\n",
      "Data successfully loaded from: /home/rmunoz/Escritorio/rmunozTMELab/Physically-Guided-Machine-Learning/data/non_linear_10_1/non_linear_10_1.pkl\n",
      "Data successfully loaded from: /home/rmunoz/Escritorio/rmunozTMELab/Physically-Guided-Machine-Learning/data/non_linear_10_1/non_linear_10_1.pkl\n",
      "Data successfully loaded from: /home/rmunoz/Escritorio/rmunozTMELab/Physically-Guided-Machine-Learning/data/non_linear_10_1/non_linear_10_1.pkl\n",
      "Data successfully loaded from: /home/rmunoz/Escritorio/rmunozTMELab/Physically-Guided-Machine-Learning/data/non_linear_10_5/non_linear_10_5.pkl\n",
      "Data successfully loaded from: /home/rmunoz/Escritorio/rmunozTMELab/Physically-Guided-Machine-Learning/data/non_linear_10_5/non_linear_10_5.pkl\n",
      "Data successfully loaded from: /home/rmunoz/Escritorio/rmunozTMELab/Physically-Guided-Machine-Learning/data/non_linear_10_5/non_linear_10_5.pkl\n",
      "Data successfully loaded from: /home/rmunoz/Escritorio/rmunozTMELab/Physically-Guided-Machine-Learning/data/non_linear_100_0/non_linear_100_0.pkl\n",
      "Data successfully loaded from: /home/rmunoz/Escritorio/rmunozTMELab/Physically-Guided-Machine-Learning/data/non_linear_100_0/non_linear_100_0.pkl\n",
      "Data successfully loaded from: /home/rmunoz/Escritorio/rmunozTMELab/Physically-Guided-Machine-Learning/data/non_linear_100_0/non_linear_100_0.pkl\n",
      "Data successfully loaded from: /home/rmunoz/Escritorio/rmunozTMELab/Physically-Guided-Machine-Learning/data/non_linear_100_1/non_linear_100_1.pkl\n",
      "Data successfully loaded from: /home/rmunoz/Escritorio/rmunozTMELab/Physically-Guided-Machine-Learning/data/non_linear_100_1/non_linear_100_1.pkl\n",
      "Data successfully loaded from: /home/rmunoz/Escritorio/rmunozTMELab/Physically-Guided-Machine-Learning/data/non_linear_100_1/non_linear_100_1.pkl\n",
      "Data successfully loaded from: /home/rmunoz/Escritorio/rmunozTMELab/Physically-Guided-Machine-Learning/data/non_linear_100_5/non_linear_100_5.pkl\n",
      "Data successfully loaded from: /home/rmunoz/Escritorio/rmunozTMELab/Physically-Guided-Machine-Learning/data/non_linear_100_5/non_linear_100_5.pkl\n",
      "Data successfully loaded from: /home/rmunoz/Escritorio/rmunozTMELab/Physically-Guided-Machine-Learning/data/non_linear_100_5/non_linear_100_5.pkl\n",
      "Data successfully loaded from: /home/rmunoz/Escritorio/rmunozTMELab/Physically-Guided-Machine-Learning/data/non_linear_1000_0/non_linear_1000_0.pkl\n",
      "Data successfully loaded from: /home/rmunoz/Escritorio/rmunozTMELab/Physically-Guided-Machine-Learning/data/non_linear_1000_0/non_linear_1000_0.pkl\n",
      "Data successfully loaded from: /home/rmunoz/Escritorio/rmunozTMELab/Physically-Guided-Machine-Learning/data/non_linear_1000_0/non_linear_1000_0.pkl\n",
      "Data successfully loaded from: /home/rmunoz/Escritorio/rmunozTMELab/Physically-Guided-Machine-Learning/data/non_linear_1000_1/non_linear_1000_1.pkl\n",
      "Data successfully loaded from: /home/rmunoz/Escritorio/rmunozTMELab/Physically-Guided-Machine-Learning/data/non_linear_1000_1/non_linear_1000_1.pkl\n",
      "Data successfully loaded from: /home/rmunoz/Escritorio/rmunozTMELab/Physically-Guided-Machine-Learning/data/non_linear_1000_1/non_linear_1000_1.pkl\n",
      "Data successfully loaded from: /home/rmunoz/Escritorio/rmunozTMELab/Physically-Guided-Machine-Learning/data/non_linear_1000_5/non_linear_1000_5.pkl\n",
      "Data successfully loaded from: /home/rmunoz/Escritorio/rmunozTMELab/Physically-Guided-Machine-Learning/data/non_linear_1000_5/non_linear_1000_5.pkl\n",
      "Data successfully loaded from: /home/rmunoz/Escritorio/rmunozTMELab/Physically-Guided-Machine-Learning/data/non_linear_1000_5/non_linear_1000_5.pkl\n",
      "fourier\n",
      "                  Hyperparameters    time       eQ1       eQ2       eQ3  \\\n",
      "N_data Sigma Mode                                                         \n",
      "10     0     5               2016   30.39  5.68e-02  6.21e-02  6.73e-02   \n",
      "             10              2071   30.77  3.81e-02  4.85e-02  5.89e-02   \n",
      "             50              2511   26.61  3.19e-02  4.22e-02  5.24e-02   \n",
      "       1     5               2016   27.41  6.08e-02  6.65e-02  7.21e-02   \n",
      "             10              2071   27.99  4.39e-02  5.25e-02  6.12e-02   \n",
      "             50              2511   27.94  5.43e-02  6.06e-02  6.69e-02   \n",
      "       5     5               2016   28.12  7.54e-02  7.90e-02  8.27e-02   \n",
      "             10              2071   28.34  6.88e-02  7.55e-02  8.22e-02   \n",
      "             50              2511   21.56  1.83e-01  2.65e-01  3.47e-01   \n",
      "100    0     5               2016   28.27  4.52e-02  6.42e-02  9.38e-02   \n",
      "             10              2071   28.20  7.85e-03  1.94e-02  3.09e-02   \n",
      "             50              2511   28.29  1.52e-03  2.65e-03  1.26e-02   \n",
      "       1     5               2016   28.76  4.91e-02  6.62e-02  9.46e-02   \n",
      "             10              2071   28.60  1.68e-02  2.48e-02  3.14e-02   \n",
      "             50              2511   28.75  1.29e-02  1.52e-02  1.65e-02   \n",
      "       5     5               2016   28.75  7.73e-02  9.08e-02  1.17e-01   \n",
      "             10              2071   28.56  6.50e-02  6.97e-02  7.39e-02   \n",
      "             50              2511   28.59  6.21e-02  6.71e-02  7.32e-02   \n",
      "1000   0     5               2016  243.97  4.12e-02  5.28e-02  7.77e-02   \n",
      "             10              2071  245.19  6.51e-03  1.22e-02  1.95e-02   \n",
      "             50              2511  245.04  3.21e-04  4.76e-04  8.49e-04   \n",
      "       1     5               2016  245.13  4.26e-02  5.35e-02  7.91e-02   \n",
      "             10              2071  243.58  1.40e-02  1.78e-02  2.37e-02   \n",
      "             50              2511  244.93  1.16e-02  1.24e-02  1.34e-02   \n",
      "       5     5               2016  243.26  7.13e-02  8.32e-02  1.04e-01   \n",
      "             10              2071  234.74  5.93e-02  6.44e-02  7.06e-02   \n",
      "             50              2511  212.66  5.78e-02  6.21e-02  6.68e-02   \n",
      "\n",
      "                         eK  \n",
      "N_data Sigma Mode            \n",
      "10     0     5     5.45e-01  \n",
      "             10    2.32e+00  \n",
      "             50    2.26e-01  \n",
      "       1     5     6.68e-01  \n",
      "             10    3.27e-01  \n",
      "             50    9.09e-01  \n",
      "       5     5     1.41e+00  \n",
      "             10    4.36e-01  \n",
      "             50    9.44e-01  \n",
      "100    0     5     1.28e+00  \n",
      "             10    8.83e-01  \n",
      "             50    7.82e-02  \n",
      "       1     5     8.41e-01  \n",
      "             10    5.62e-01  \n",
      "             50    8.44e-02  \n",
      "       5     5     6.46e-01  \n",
      "             10    7.76e-01  \n",
      "             50    5.75e-01  \n",
      "1000   0     5     9.41e-01  \n",
      "             10    4.15e-01  \n",
      "             50    4.03e-02  \n",
      "       1     5     6.75e-01  \n",
      "             10    7.13e-01  \n",
      "             50    3.60e-02  \n",
      "       5     5     6.54e-01  \n",
      "             10    5.09e-01  \n",
      "             50    4.98e-01  \n",
      "Tiempo actual en segundos: 12759.64788389206\n",
      "Tiempo actual en minutos: 212.66079806486766\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "N = [10, 100, 1000] \n",
    "R = [0, 1, 5]\n",
    "n_modes = [5, 10, 50]\n",
    "models = ['baseline', 'POD', 'autoencoder', 'fourier']\n",
    "models = ['baseline', 'fourier']\n",
    "\n",
    "for model_i in models:\n",
    "\n",
    "    print(model_i)\n",
    "\n",
    "    combinations = list(itertools.product(N, R, n_modes))\n",
    "    n_data_vals, ruido_vals, modos_vals = zip(*combinations)\n",
    "\n",
    "    # Create the errors table for each model\n",
    "    multi_index = pd.MultiIndex.from_arrays([n_data_vals, ruido_vals, modos_vals], names=[\"N_data\", \"Sigma\", \"Mode\"])\n",
    "    error_table = pd.DataFrame(index=multi_index, columns=[\"Hyperparameters\", \"time\", \"eQ1\", \"eQ2\", \"eQ3\", \"eK\"])\n",
    "\n",
    "    for n_i in N:\n",
    "        for r_i in R:\n",
    "            for mode_i in n_modes:\n",
    "                \n",
    "                data_name = f'non_linear_{n_i}_{r_i}'\n",
    "                model_name = f'{model_i}_model_{mode_i}'\n",
    "                \n",
    "                ROOT_PATH = os.path.abspath(os.path.join(os.getcwd(), \"../\"))\n",
    "                DATA_PATH = os.path.join(ROOT_PATH, r'data/', data_name, data_name) + '.pkl'\n",
    "                RESULTS_FOLDER_PATH = os.path.join(ROOT_PATH, r'results/', data_name)\n",
    "\n",
    "\n",
    "                MODEL_RESULTS_PATH = os.path.join(ROOT_PATH, r'results/', data_name, model_name)\n",
    "\n",
    "                dataset = load_data(DATA_PATH)\n",
    "\n",
    "                # Train data splitting in train/test\n",
    "                X = torch.tensor(dataset['X_train'], dtype=torch.float32).unsqueeze(1)\n",
    "                y = torch.tensor(dataset['y_train'], dtype=torch.float32).unsqueeze(1)\n",
    "                K = torch.tensor(dataset['k_train'], dtype=torch.float32).unsqueeze(1)\n",
    "                f = torch.tensor(dataset['f_train'], dtype=torch.float32).unsqueeze(1)\n",
    "\n",
    "                X_train, X_test, y_train, y_test, K_train, K_test, f_train, f_test = train_test_split(X, y, K, f, test_size=0.3, random_state=42)\n",
    "\n",
    "                # Data processing and adequacy with our TensOps library\n",
    "                X_train = X_train.to(DEVICE)\n",
    "                X_test = X_test.to(DEVICE)\n",
    "\n",
    "                y_train = TensOps(y_train.to(DEVICE).requires_grad_(True), space_dimension=2, contravariance=0, covariance=0)\n",
    "                y_test = TensOps(y_test.to(DEVICE).requires_grad_(True), space_dimension=2, contravariance=0, covariance=0)\n",
    "\n",
    "                K_train = TensOps(K_train.to(DEVICE).requires_grad_(True), space_dimension=2, contravariance=0, covariance=0)\n",
    "                K_test = TensOps(K_test.to(DEVICE).requires_grad_(True), space_dimension=2, contravariance=0, covariance=0)\n",
    "\n",
    "                f_train = TensOps(f_train.to(DEVICE).requires_grad_(True), space_dimension=2, contravariance=0, covariance=0)\n",
    "                f_test = TensOps(f_test.to(DEVICE).requires_grad_(True), space_dimension=2, contravariance=0, covariance=0)\n",
    "\n",
    "                # Loading and processing validation data\n",
    "                X_val = torch.tensor(dataset['X_val'], dtype=torch.float32).unsqueeze(1)\n",
    "                y_val = TensOps(torch.tensor(dataset['y_val'], dtype=torch.float32, requires_grad=True).unsqueeze(1), space_dimension=2, contravariance=0, covariance=0)\n",
    "                K_val = TensOps(torch.tensor(dataset['k_val'], dtype=torch.float32, requires_grad=True).unsqueeze(1), space_dimension=2, contravariance=0, covariance=0)\n",
    "                f_val = TensOps(torch.tensor(dataset['f_val'], dtype=torch.float32, requires_grad=True).unsqueeze(1), space_dimension=2, contravariance=0, covariance=0)\n",
    "\n",
    "                # Predictive network architecture\n",
    "                input_shape = X_train[0].shape\n",
    "                predictive_layers = [20, 10, mode_i, 10, 20]\n",
    "                predictive_output = y_train.values[0].shape\n",
    "\n",
    "                # Explanatory network architecture\n",
    "                explanatory_input = Mx(My(y_train)).values[0].shape\n",
    "                explanatory_layers = [10]\n",
    "                explanatory_output = Mx(My(f_train)).values[0].shape\n",
    "\n",
    "                # Other parameters\n",
    "                n_filters_explanatory = 5\n",
    "\n",
    "                if model_i == 'baseline':\n",
    "\n",
    "                    try:\n",
    "                        model = PGNNIVBaseline(input_shape, predictive_layers, predictive_output, explanatory_input, explanatory_layers, explanatory_output, n_filters_explanatory).to(DEVICE)\n",
    "                        optimizer = torch.optim.Adam(model.parameters(), lr=1e-5)\n",
    "                        model, optimizer, lists = load_results(model, optimizer, MODEL_RESULTS_PATH, map_location=DEVICE)\n",
    "\n",
    "                        time = np.cumsum(lists['time_list'])[-1]\n",
    "\n",
    "                        hyperparameters = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "                    except:\n",
    "                        pass\n",
    "\n",
    "                elif model_i == 'POD':\n",
    "                    try:\n",
    "                        if X_train.shape[0] < mode_i:\n",
    "                            continue\n",
    "\n",
    "                        U_train, S_train, Vt_train = torch.linalg.svd(y_train.values.detach().squeeze().to('cpu').view(y_train.values.detach().shape[0], -1), full_matrices=False)\n",
    "                        U_reduced_train = U_train[:, :mode_i]\n",
    "                        S_reduced_train = S_train[:mode_i]\n",
    "                        Vt_reduced_train = Vt_train[:mode_i, :]\n",
    "                        POD_base = Vt_reduced_train.to(DEVICE)\n",
    "\n",
    "                        model = PGNNIVPOD(input_shape, predictive_layers, POD_base, predictive_output, explanatory_input, explanatory_layers, explanatory_output, n_filters_explanatory).to(DEVICE)\n",
    "                        optimizer = torch.optim.Adam(model.parameters(), lr=1e-5)\n",
    "                        model, optimizer, lists = load_results(model, optimizer, MODEL_RESULTS_PATH, map_location=DEVICE)\n",
    "\n",
    "                        with open(os.path.join(MODEL_RESULTS_PATH, \"time.txt\"), \"r\") as f:\n",
    "                            time_pod = float(f.read().strip())  # Usa float o int según lo que necesites\n",
    "\n",
    "                        time = np.cumsum(lists['time_list'])[-1] + time_pod\n",
    "\n",
    "                        hyperparameters = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "                    except:\n",
    "                        pass\n",
    "\n",
    "                elif model_i == 'fourier':\n",
    "                    try:\n",
    "\n",
    "                        X_mesh = torch.tensor(dataset['X_mesh'])\n",
    "                        Y_mesh = torch.tensor(dataset['Y_mesh'])\n",
    "\n",
    "                        base = compute_fourier_base(mode_i, X_mesh, Y_mesh)\n",
    "\n",
    "                        \n",
    "\n",
    "                        model = PGNNIVFourier(input_shape, predictive_layers, base, predictive_output, explanatory_input, explanatory_layers, explanatory_output, n_filters_explanatory, device=DEVICE).to(DEVICE)\n",
    "                        optimizer = torch.optim.Adam(model.parameters(), lr=1e-5)\n",
    "                        model, optimizer, lists = load_results(model, optimizer, MODEL_RESULTS_PATH, map_location=DEVICE)\n",
    "\n",
    "                        time = np.cumsum(lists['time_list'])[-1]\n",
    "\n",
    "                        hyperparameters = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "                    \n",
    "                    except:\n",
    "                        pass\n",
    "\n",
    "                elif model_i == 'autoencoder':\n",
    "                    try:\n",
    "                        MODEL_RESULTS_AE_PATH = os.path.join(ROOT_PATH, r'results/', data_name, model_name) + '_AE'\n",
    "                        MODEL_RESULTS_PGNNIV_PATH = os.path.join(ROOT_PATH, r'results/', data_name, model_name) + '_NN'\n",
    "\n",
    "                        autoencoder_input_shape = y_train.values[0].shape\n",
    "                        latent_space_dim = [20, 10, mode_i, 10, 20]\n",
    "                        autoencoder_output_shape = y_train.values[0].shape\n",
    "\n",
    "                        autoencoder = Autoencoder(autoencoder_input_shape, latent_space_dim, autoencoder_output_shape).to(DEVICE)\n",
    "                        optimizer = torch.optim.Adam(autoencoder.parameters(), lr=1e-4)\n",
    "\n",
    "                        autoencoder, optimizer, lists = load_results(autoencoder, optimizer, MODEL_RESULTS_AE_PATH, map_location=torch.device('cpu'))\n",
    "\n",
    "                        hyperparameters_ae = sum(p.numel() for p in autoencoder.parameters() if p.requires_grad)\n",
    "\n",
    "                        time_ae = np.cumsum(lists['time_list'])[-1]\n",
    "\n",
    "                        pretrained_encoder = autoencoder.encoder\n",
    "                        pretrained_decoder = autoencoder.decoder\n",
    "\n",
    "                        for param in pretrained_decoder.parameters():\n",
    "                            param.requires_grad = False\n",
    "\n",
    "                        pgnniv_model = PGNNIVAutoencoder(input_shape, predictive_layers, pretrained_decoder, predictive_output, explanatory_input,\n",
    "                                                        explanatory_layers, explanatory_output, n_filters_explanatory).to(DEVICE)\n",
    "                        optimizer = torch.optim.Adam(pgnniv_model.parameters(), lr=1e-4)\n",
    "\n",
    "                        model, optimizer, lists = load_results(pgnniv_model, optimizer, MODEL_RESULTS_PGNNIV_PATH, map_location=torch.device('cpu'))\n",
    "                        optimizer = torch.optim.Adam(model.parameters(), lr=1e-5)\n",
    "                        model, optimizer, lists = load_results(model, optimizer, MODEL_RESULTS_PGNNIV_PATH, map_location=DEVICE)\n",
    "\n",
    "                        time = time_ae + np.cumsum(lists['time_list'])[-1]\n",
    "\n",
    "                        hyperparameters = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "                    except:\n",
    "                        pass\n",
    "\n",
    "                u_train = y_val.values.detach().numpy() \n",
    "                u_predicted_train = model(X_val)[0].detach().numpy() \n",
    "                er_u_train = relative_error(u_train, u_predicted_train).flatten()\n",
    "\n",
    "                erQ1_u = np.percentile(er_u_train, 25)\n",
    "                erQ2_u = np.percentile(er_u_train, 50)\n",
    "                erQ3_u = np.percentile(er_u_train, 75)\n",
    "\n",
    "                u_min = u_train.flatten().min()\n",
    "                u_max = u_train.flatten().max()\n",
    "                steps = 1000\n",
    "                u_for_validating = torch.linspace(u_min, u_max, steps=steps).view(steps, 1, 1, 1)\n",
    "                K_for_validating = (u_for_validating*(1-u_for_validating)).detach().cpu().numpy()\n",
    "                K_predicted_for_validating = model.explanatory(u_for_validating.to(DEVICE)).detach().cpu().numpy()\n",
    "                diff_squared = (K_predicted_for_validating - K_for_validating) ** 2\n",
    "                true_squared = K_for_validating ** 2\n",
    "                u_vals = u_for_validating.numpy().flatten()\n",
    "                numerator = np.sqrt(np.trapz(diff_squared.flatten(), u_vals))\n",
    "                denominator = np.sqrt(np.trapz(true_squared.flatten(), u_vals))\n",
    "\n",
    "                er_K_train = numerator / denominator\n",
    "\n",
    "\n",
    "                # K_train_ = Mx(My(K_train)).values.detach().numpy() \n",
    "                # K_predicted_train = model(X_train)[1].detach().numpy()\n",
    "                # er_K_train = er_sum(K_train_, K_predicted_train)\n",
    "\n",
    "                tiempo_minutos = time / 60\n",
    "\n",
    "                idx = (n_i, r_i, mode_i)\n",
    "                error_table.loc[idx, \"Hyperparameters\"] = hyperparameters\n",
    "                error_table.loc[idx, \"time\"] = f\"{tiempo_minutos:.2f}\"\n",
    "                error_table.loc[idx, \"eQ1\"] = f\"{erQ1_u:.2e}\"\n",
    "                error_table.loc[idx, \"eQ2\"] = f\"{erQ2_u:.2e}\"\n",
    "                error_table.loc[idx, \"eQ3\"] = f\"{erQ3_u:.2e}\"\n",
    "                error_table.loc[idx, \"eK\"] = f\"{er_K_train:.2e}\"\n",
    "                \n",
    "\n",
    "    print(model_i)\n",
    "    print(error_table)\n",
    "    tiempo_segundos = time\n",
    "    tiempo_minutos = tiempo_segundos / 60\n",
    "    print(f\"Tiempo actual en segundos: {tiempo_segundos}\")\n",
    "    print(f\"Tiempo actual en minutos: {tiempo_minutos}\")\n",
    "    print(\"\\n\")\n",
    "                \n",
    "\n",
    "    tabla_latex = dataframe_a_latex(error_table, \n",
    "                                    nombre_archivo=os.path.join(os.getcwd(), \"error_tables\", f\"error_{model_i}\"), \n",
    "                                    index=multi_index,\n",
    "                                    caption=\"Tabla con formato LaTeX\",\n",
    "                                    label=\"tab:formateada\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fe92d7d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb69349c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SciML_test_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
