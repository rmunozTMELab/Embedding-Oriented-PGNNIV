{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7a48a0a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys \n",
    "\n",
    "current_directory = os.getcwd()\n",
    "models_directory = os.path.abspath(os.path.join(current_directory, '..'))\n",
    "sys.path.append(models_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "71461cff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "import json\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import itertools\n",
    "\n",
    "import matplotlib.colors as mcolors\n",
    "import matplotlib.ticker as ticker\n",
    "from matplotlib.ticker import ScalarFormatter\n",
    "from mpl_toolkits.axes_grid1.inset_locator import inset_axes\n",
    "from matplotlib.ticker import LogLocator, LogFormatter\n",
    "from matplotlib.lines import Line2D\n",
    "\n",
    "# Imports de la libreria propia\n",
    "from vecopsciml.kernels.derivative import DerivativeKernels\n",
    "from vecopsciml.utils import TensOps\n",
    "from vecopsciml.operators import zero_order as zo\n",
    "from vecopsciml.algebra import zero_order as azo\n",
    "\n",
    "# Imports de las funciones creadas para este programa\n",
    "from utils.folders import create_folder\n",
    "from utils.load_data import load_data\n",
    "from utils.checkpoints import load_results\n",
    "from utils.fourier_base import compute_fourier_base\n",
    "\n",
    "from vecopsciml.operators.zero_order import Mx, My"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "440063e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from architectures.pgnniv_baseline import PGNNIVBaseline\n",
    "from architectures.pgnniv_fourier import PGNNIVFourier\n",
    "from architectures.pgnniv_pod import PGNNIVPOD\n",
    "from architectures.pgnniv_decoder import PGNNIVAutoencoder\n",
    "from architectures.autoencoder import Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4658e960",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataframe_a_latex(df, nombre_archivo=None, index=False, caption=None, label=None):\n",
    "    \"\"\"\n",
    "    Convierte un DataFrame en una tabla LaTeX.\n",
    "    \n",
    "    Args:\n",
    "        df (pd.DataFrame): DataFrame a convertir.\n",
    "        nombre_archivo (str): Ruta para guardar el archivo .tex (opcional).\n",
    "        index (bool): Si se incluye o no el índice.\n",
    "        caption (str): Título de la tabla (opcional).\n",
    "        label (str): Etiqueta de la tabla para referencia cruzada (opcional).\n",
    "    \n",
    "    Returns:\n",
    "        str: La cadena en formato LaTeX.\n",
    "    \"\"\"\n",
    "    latex_str = df.to_latex(index=index, caption=caption, label=label, escape=False)\n",
    "\n",
    "    if nombre_archivo:\n",
    "        with open(nombre_archivo, 'w', encoding='utf-8') as f:\n",
    "            f.write(latex_str)\n",
    "    \n",
    "    return latex_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4d61ce4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully loaded from: /home/rmunoz/Escritorio/rmunozTMELab/Physically-Guided-Machine-Learning/data/non_linear_10_0/non_linear_10_0.pkl\n"
     ]
    }
   ],
   "source": [
    "data_name = 'non_linear_10_0'\n",
    "\n",
    "ROOT_PATH = os.path.abspath(os.path.join(os.getcwd(), \"../\"))\n",
    "DATA_PATH = os.path.join(ROOT_PATH, r'data/', data_name, data_name) + '.pkl'\n",
    "\n",
    "dataset = load_data(DATA_PATH)\n",
    "dx = dataset['x_step_size']\n",
    "dy = dataset['y_step_size']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a101e6f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def relative_error(validation, prediction, dx=dx, dy=dy):\n",
    "    \n",
    "    prediction_error = np.sqrt((np.trapz(np.trapz((validation - prediction)**2, dx=dy), dx=dx) /\n",
    "                                np.trapz(np.trapz((validation)**2, dx=dy), dx=dx)))\n",
    "\n",
    "    return prediction_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4c8a0587",
   "metadata": {},
   "outputs": [],
   "source": [
    "def smooth_curve(data, window_size=1000):\n",
    "    window = np.ones(window_size) / window_size\n",
    "    return np.convolve(data, window, mode='valid')\n",
    "\n",
    "def cm_to_in(cm):\n",
    "    return cm * 0.393701\n",
    "\n",
    "def normalize_list(lst):\n",
    "    max_value = np.max(lst)\n",
    "    return [x / max_value for x in lst]\n",
    "\n",
    "linewidth = 1.5  \n",
    "title_fontsize = 14  \n",
    "label_fontsize = 14  \n",
    "legend_fontsize = 12 \n",
    "tick_fontsize = 11  \n",
    "\n",
    "# plt.rc('text', usetex=True)\n",
    "plt.rc('font', family='serif')\n",
    "\n",
    "posX = cm_to_in(10) # posición de la esquina inferior izquierda de la imagen en X\n",
    "posY = cm_to_in(10) # posición de la esquina inferior izquierda de la imagen en Y\n",
    "width = cm_to_in(12)  # ancho de la imagen\n",
    "height = cm_to_in(8) # alto de la imagen\n",
    "\n",
    "color = [0.1, 0, 0.8]  # triplete RGB, valores entre 0 y 1\n",
    "subplot_adjust_left = cm_to_in(0.15)\n",
    "subplot_adjust_bottom = cm_to_in(0.15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "63edea20",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "56e28bd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# N = [10, 100, 1000] \n",
    "# R = [0, 1, 5]\n",
    "# n_modes = [5, 10, 50]\n",
    "# models = ['baseline', 'POD', 'fourier','autoencoder']\n",
    "\n",
    "# combinations = list(itertools.product(N, models, n_modes))\n",
    "# n_data_vals, ruido_vals, modos_vals = zip(*combinations)\n",
    "\n",
    "# # Create the errors table for each model\n",
    "# multi_index = pd.MultiIndex.from_arrays([n_data_vals, ruido_vals, modos_vals], names=[\"N_data\", \"Model\", \"Mode\"])\n",
    "# error_table = pd.DataFrame(index=multi_index, columns=[\"time\", \"eQ1\", \"eQ2\", \"eQ3\", \"eK\"])\n",
    "\n",
    "# for model_i in models:\n",
    "#     for n_i in N:\n",
    "#         for r_i in R:\n",
    "#             for mode_i in n_modes:\n",
    "\n",
    "#                 idx = (n_i, model_i, mode_i)\n",
    "                \n",
    "#                 data_name = f'non_linear_{n_i}_{0}'\n",
    "#                 model_name = f'{model_i}_model_{mode_i}'\n",
    "                \n",
    "#                 ROOT_PATH = os.path.abspath(os.path.join(os.getcwd(), \"../\"))\n",
    "#                 DATA_PATH = os.path.join(ROOT_PATH, r'data/', data_name, data_name) + '.pkl'\n",
    "#                 RESULTS_FOLDER_PATH = os.path.join(ROOT_PATH, r'results/', data_name)\n",
    "\n",
    "\n",
    "#                 MODEL_RESULTS_PATH = os.path.join(ROOT_PATH, r'results/', data_name, model_name)\n",
    "\n",
    "#                 dataset = load_data(DATA_PATH)\n",
    "\n",
    "#                 # Train data splitting in train/test\n",
    "#                 X = torch.tensor(dataset['X_train'], dtype=torch.float32).unsqueeze(1)\n",
    "#                 y = torch.tensor(dataset['y_train'], dtype=torch.float32).unsqueeze(1)\n",
    "#                 K = torch.tensor(dataset['k_train'], dtype=torch.float32).unsqueeze(1)\n",
    "#                 f = torch.tensor(dataset['f_train'], dtype=torch.float32).unsqueeze(1)\n",
    "\n",
    "#                 X_train, X_test, y_train, y_test, K_train, K_test, f_train, f_test = train_test_split(X, y, K, f, test_size=0.3, random_state=42)\n",
    "\n",
    "#                 # Data processing and adequacy with our TensOps library\n",
    "#                 X_train = X_train.to(DEVICE)\n",
    "#                 X_test = X_test.to(DEVICE)\n",
    "\n",
    "#                 y_train = TensOps(y_train.to(DEVICE).requires_grad_(True), space_dimension=2, contravariance=0, covariance=0)\n",
    "#                 y_test = TensOps(y_test.to(DEVICE).requires_grad_(True), space_dimension=2, contravariance=0, covariance=0)\n",
    "\n",
    "#                 K_train = TensOps(K_train.to(DEVICE).requires_grad_(True), space_dimension=2, contravariance=0, covariance=0)\n",
    "#                 K_test = TensOps(K_test.to(DEVICE).requires_grad_(True), space_dimension=2, contravariance=0, covariance=0)\n",
    "\n",
    "#                 f_train = TensOps(f_train.to(DEVICE).requires_grad_(True), space_dimension=2, contravariance=0, covariance=0)\n",
    "#                 f_test = TensOps(f_test.to(DEVICE).requires_grad_(True), space_dimension=2, contravariance=0, covariance=0)\n",
    "\n",
    "#                 # Loading and processing validation data\n",
    "#                 X_val = torch.tensor(dataset['X_val'], dtype=torch.float32).unsqueeze(1)\n",
    "#                 y_val = TensOps(torch.tensor(dataset['y_val'], dtype=torch.float32, requires_grad=True).unsqueeze(1), space_dimension=2, contravariance=0, covariance=0)\n",
    "#                 K_val = TensOps(torch.tensor(dataset['k_val'], dtype=torch.float32, requires_grad=True).unsqueeze(1), space_dimension=2, contravariance=0, covariance=0)\n",
    "#                 f_val = TensOps(torch.tensor(dataset['f_val'], dtype=torch.float32, requires_grad=True).unsqueeze(1), space_dimension=2, contravariance=0, covariance=0)\n",
    "\n",
    "#                 # Predictive network architecture\n",
    "#                 input_shape = X_train[0].shape\n",
    "#                 predictive_layers = [20, 10, mode_i, 10, 20]\n",
    "#                 predictive_output = y_train.values[0].shape\n",
    "\n",
    "#                 # Explanatory network architecture\n",
    "#                 explanatory_input = Mx(My(y_train)).values[0].shape\n",
    "#                 explanatory_layers = [10]\n",
    "#                 explanatory_output = Mx(My(f_train)).values[0].shape\n",
    "\n",
    "#                 # Other parameters\n",
    "#                 n_filters_explanatory = 5\n",
    "\n",
    "#                 if model_i == 'baseline':\n",
    "\n",
    "#                     try:\n",
    "#                         model = PGNNIVBaseline(input_shape, predictive_layers, predictive_output, explanatory_input, explanatory_layers, explanatory_output, n_filters_explanatory).to(DEVICE)\n",
    "#                         optimizer = torch.optim.Adam(model.parameters(), lr=1e-5)\n",
    "#                         model, lists = load_results(model, optimizer, MODEL_RESULTS_PATH, map_location=DEVICE)\n",
    "\n",
    "#                         time = np.cumsum(lists['time_list'])[-1]\n",
    "\n",
    "#                         hyperparameters = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "#                     except:\n",
    "#                         pass\n",
    "\n",
    "#                 elif model_i == 'POD':\n",
    "#                     try:\n",
    "#                         if X_train.shape[0] < mode_i:\n",
    "#                             continue\n",
    "\n",
    "#                         U_train, S_train, Vt_train = torch.linalg.svd(y_train.values.detach().squeeze().to('cpu').view(y_train.values.detach().shape[0], -1), full_matrices=False)\n",
    "#                         U_reduced_train = U_train[:, :mode_i]\n",
    "#                         S_reduced_train = S_train[:mode_i]\n",
    "#                         Vt_reduced_train = Vt_train[:mode_i, :]\n",
    "#                         POD_base = Vt_reduced_train.to(DEVICE)\n",
    "\n",
    "#                         model = PGNNIVPOD(input_shape, predictive_layers, POD_base, predictive_output, explanatory_input, explanatory_layers, explanatory_output, n_filters_explanatory).to(DEVICE)\n",
    "#                         optimizer = torch.optim.Adam(model.parameters(), lr=1e-5)\n",
    "#                         model, lists = load_results(model, optimizer, MODEL_RESULTS_PATH, map_location=DEVICE)\n",
    "\n",
    "#                         with open(os.path.join(MODEL_RESULTS_PATH, \"time.txt\"), \"r\") as f:\n",
    "#                             time_pod = float(f.read().strip())  # Usa float o int según lo que necesites\n",
    "\n",
    "#                         time = np.cumsum(lists['time_list'])[-1] + time_pod\n",
    "\n",
    "#                         hyperparameters = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "#                     except:\n",
    "#                         pass\n",
    "\n",
    "#                 elif model_i == 'fourier':\n",
    "#                     try:\n",
    "\n",
    "#                         X_mesh = torch.tensor(dataset['X_mesh'])\n",
    "#                         Y_mesh = torch.tensor(dataset['Y_mesh'])\n",
    "\n",
    "#                         base = compute_fourier_base(mode_i, X_mesh, Y_mesh)\n",
    "\n",
    "                        \n",
    "\n",
    "#                         model = PGNNIVFourier(input_shape, predictive_layers, base, predictive_output, explanatory_input, explanatory_layers, explanatory_output, n_filters_explanatory, device=DEVICE).to(DEVICE)\n",
    "#                         optimizer = torch.optim.Adam(model.parameters(), lr=1e-5)\n",
    "#                         model, lists = load_results(model, optimizer, MODEL_RESULTS_PATH, map_location=DEVICE)\n",
    "\n",
    "#                         time = np.cumsum(lists['time_list'])[-1]\n",
    "\n",
    "#                         hyperparameters = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "                    \n",
    "#                     except:\n",
    "#                         pass\n",
    "\n",
    "#                 elif model_i == 'autoencoder':\n",
    "#                     try:\n",
    "#                         MODEL_RESULTS_AE_PATH = os.path.join(ROOT_PATH, r'results/', data_name, model_name) + '_AE'\n",
    "#                         MODEL_RESULTS_PGNNIV_PATH = os.path.join(ROOT_PATH, r'results/', data_name, model_name) + '_NN'\n",
    "\n",
    "#                         autoencoder_input_shape = y_train.values[0].shape\n",
    "#                         latent_space_dim = [20, 10, mode_i, 10, 20]\n",
    "#                         autoencoder_output_shape = y_train.values[0].shape\n",
    "\n",
    "#                         autoencoder = Autoencoder(autoencoder_input_shape, latent_space_dim, autoencoder_output_shape).to(DEVICE)\n",
    "#                         optimizer = torch.optim.Adam(autoencoder.parameters(), lr=1e-4)\n",
    "\n",
    "#                         autoencoder, lists = load_results(autoencoder, optimizer, MODEL_RESULTS_AE_PATH, map_location=torch.device('cpu'))\n",
    "\n",
    "#                         hyperparameters_ae = sum(p.numel() for p in autoencoder.parameters() if p.requires_grad)\n",
    "\n",
    "#                         time_ae = np.cumsum(lists['time_list'])[-1]\n",
    "\n",
    "#                         pretrained_encoder = autoencoder.encoder\n",
    "#                         pretrained_decoder = autoencoder.decoder\n",
    "\n",
    "#                         for param in pretrained_decoder.parameters():\n",
    "#                             param.requires_grad = False\n",
    "\n",
    "#                         pgnniv_model = PGNNIVAutoencoder(input_shape, predictive_layers, pretrained_decoder, predictive_output, explanatory_input,\n",
    "#                                                         explanatory_layers, explanatory_output, n_filters_explanatory).to(DEVICE)\n",
    "#                         optimizer = torch.optim.Adam(pgnniv_model.parameters(), lr=1e-4)\n",
    "\n",
    "#                         model, lists = load_results(pgnniv_model, optimizer, MODEL_RESULTS_PGNNIV_PATH, map_location=torch.device('cpu'))\n",
    "#                         optimizer = torch.optim.Adam(model.parameters(), lr=1e-5)\n",
    "#                         model, lists = load_results(model, optimizer, MODEL_RESULTS_PGNNIV_PATH, map_location=DEVICE)\n",
    "\n",
    "#                         time = time_ae + np.cumsum(lists['time_list'])[-1]\n",
    "\n",
    "#                         hyperparameters = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "#                     except:\n",
    "#                         pass\n",
    "\n",
    "#                 u_train = y_val.values.detach().numpy() \n",
    "#                 u_predicted_train = model(X_val)[0].detach().numpy() \n",
    "#                 er_u_train = relative_error(u_train, u_predicted_train).flatten()\n",
    "\n",
    "#                 erQ1_u = np.percentile(er_u_train, 25)\n",
    "#                 erQ2_u = np.percentile(er_u_train, 50)\n",
    "#                 erQ3_u = np.percentile(er_u_train, 75)\n",
    "\n",
    "#                 u_min = u_train.flatten().min()\n",
    "#                 u_max = u_train.flatten().max()\n",
    "#                 steps = 1000\n",
    "#                 u_for_validating = torch.linspace(u_min, u_max, steps=steps).view(steps, 1, 1, 1)\n",
    "#                 K_for_validating = (u_for_validating*(1-u_for_validating)).detach().cpu().numpy()\n",
    "#                 K_predicted_for_validating = model.explanatory(u_for_validating.to(DEVICE)).detach().cpu().numpy()\n",
    "#                 diff_squared = (K_predicted_for_validating - K_for_validating) ** 2\n",
    "#                 true_squared = K_for_validating ** 2\n",
    "#                 u_vals = u_for_validating.numpy().flatten()\n",
    "#                 numerator = np.sqrt(np.trapz(diff_squared.flatten(), u_vals))\n",
    "#                 denominator = np.sqrt(np.trapz(true_squared.flatten(), u_vals))\n",
    "\n",
    "#                 er_K_train = numerator / denominator\n",
    "\n",
    "\n",
    "#                 # K_train_ = Mx(My(K_train)).values.detach().numpy() \n",
    "#                 # K_predicted_train = model(X_train)[1].detach().numpy()\n",
    "#                 # er_K_train = er_sum(K_train_, K_predicted_train)\n",
    "\n",
    "#                 tiempo_minutos = time / 60\n",
    "                \n",
    "#                 error_table.loc[idx, \"time\"] = f\"{tiempo_minutos:.2f}\"\n",
    "#                 error_table.loc[idx, \"eQ1\"] = f\"{erQ1_u:.2e}\"\n",
    "#                 error_table.loc[idx, \"eQ2\"] = f\"{erQ2_u:.2e}\"\n",
    "#                 error_table.loc[idx, \"eQ3\"] = f\"{erQ3_u:.2e}\"\n",
    "#                 error_table.loc[idx, \"eK\"] = f\"{er_K_train:.2e}\"\n",
    "                \n",
    "\n",
    "#     print(model_i)\n",
    "#     print(error_table)\n",
    "#     tiempo_segundos = time\n",
    "#     tiempo_minutos = tiempo_segundos / 60\n",
    "#     print(f\"Tiempo actual en segundos: {tiempo_segundos}\")\n",
    "#     print(f\"Tiempo actual en minutos: {tiempo_minutos}\")\n",
    "#     print(\"\\n\")\n",
    "                    \n",
    "\n",
    "#     tabla_latex = dataframe_a_latex(error_table, \n",
    "#                                     nombre_archivo=os.path.join(os.getcwd(), \"error_tables\", f\"error_{model_i}\"), \n",
    "#                                     index=multi_index,\n",
    "#                                     caption=\"Tabla con formato LaTeX\",\n",
    "#                                     label=\"tab:formateada\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7e665132",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "baseline\n",
      "Data successfully loaded from: /home/rmunoz/Escritorio/rmunozTMELab/Physically-Guided-Machine-Learning/data/non_linear_10_0/non_linear_10_0.pkl\n",
      "Data successfully loaded from: /home/rmunoz/Escritorio/rmunozTMELab/Physically-Guided-Machine-Learning/data/non_linear_10_0/non_linear_10_0.pkl\n",
      "Data successfully loaded from: /home/rmunoz/Escritorio/rmunozTMELab/Physically-Guided-Machine-Learning/data/non_linear_10_0/non_linear_10_0.pkl\n",
      "Data successfully loaded from: /home/rmunoz/Escritorio/rmunozTMELab/Physically-Guided-Machine-Learning/data/non_linear_10_1/non_linear_10_1.pkl\n",
      "Data successfully loaded from: /home/rmunoz/Escritorio/rmunozTMELab/Physically-Guided-Machine-Learning/data/non_linear_10_1/non_linear_10_1.pkl\n",
      "Data successfully loaded from: /home/rmunoz/Escritorio/rmunozTMELab/Physically-Guided-Machine-Learning/data/non_linear_10_1/non_linear_10_1.pkl\n",
      "Data successfully loaded from: /home/rmunoz/Escritorio/rmunozTMELab/Physically-Guided-Machine-Learning/data/non_linear_10_5/non_linear_10_5.pkl\n",
      "Data successfully loaded from: /home/rmunoz/Escritorio/rmunozTMELab/Physically-Guided-Machine-Learning/data/non_linear_10_5/non_linear_10_5.pkl\n",
      "Data successfully loaded from: /home/rmunoz/Escritorio/rmunozTMELab/Physically-Guided-Machine-Learning/data/non_linear_10_5/non_linear_10_5.pkl\n",
      "Data successfully loaded from: /home/rmunoz/Escritorio/rmunozTMELab/Physically-Guided-Machine-Learning/data/non_linear_100_0/non_linear_100_0.pkl\n",
      "Data successfully loaded from: /home/rmunoz/Escritorio/rmunozTMELab/Physically-Guided-Machine-Learning/data/non_linear_100_0/non_linear_100_0.pkl\n",
      "Data successfully loaded from: /home/rmunoz/Escritorio/rmunozTMELab/Physically-Guided-Machine-Learning/data/non_linear_100_0/non_linear_100_0.pkl\n",
      "Data successfully loaded from: /home/rmunoz/Escritorio/rmunozTMELab/Physically-Guided-Machine-Learning/data/non_linear_100_1/non_linear_100_1.pkl\n",
      "Data successfully loaded from: /home/rmunoz/Escritorio/rmunozTMELab/Physically-Guided-Machine-Learning/data/non_linear_100_1/non_linear_100_1.pkl\n",
      "Data successfully loaded from: /home/rmunoz/Escritorio/rmunozTMELab/Physically-Guided-Machine-Learning/data/non_linear_100_1/non_linear_100_1.pkl\n",
      "Data successfully loaded from: /home/rmunoz/Escritorio/rmunozTMELab/Physically-Guided-Machine-Learning/data/non_linear_100_5/non_linear_100_5.pkl\n",
      "Data successfully loaded from: /home/rmunoz/Escritorio/rmunozTMELab/Physically-Guided-Machine-Learning/data/non_linear_100_5/non_linear_100_5.pkl\n",
      "Data successfully loaded from: /home/rmunoz/Escritorio/rmunozTMELab/Physically-Guided-Machine-Learning/data/non_linear_100_5/non_linear_100_5.pkl\n",
      "Data successfully loaded from: /home/rmunoz/Escritorio/rmunozTMELab/Physically-Guided-Machine-Learning/data/non_linear_1000_0/non_linear_1000_0.pkl\n",
      "Data successfully loaded from: /home/rmunoz/Escritorio/rmunozTMELab/Physically-Guided-Machine-Learning/data/non_linear_1000_0/non_linear_1000_0.pkl\n",
      "Data successfully loaded from: /home/rmunoz/Escritorio/rmunozTMELab/Physically-Guided-Machine-Learning/data/non_linear_1000_0/non_linear_1000_0.pkl\n",
      "Data successfully loaded from: /home/rmunoz/Escritorio/rmunozTMELab/Physically-Guided-Machine-Learning/data/non_linear_1000_1/non_linear_1000_1.pkl\n",
      "Data successfully loaded from: /home/rmunoz/Escritorio/rmunozTMELab/Physically-Guided-Machine-Learning/data/non_linear_1000_1/non_linear_1000_1.pkl\n",
      "Data successfully loaded from: /home/rmunoz/Escritorio/rmunozTMELab/Physically-Guided-Machine-Learning/data/non_linear_1000_1/non_linear_1000_1.pkl\n",
      "Data successfully loaded from: /home/rmunoz/Escritorio/rmunozTMELab/Physically-Guided-Machine-Learning/data/non_linear_1000_5/non_linear_1000_5.pkl\n",
      "Data successfully loaded from: /home/rmunoz/Escritorio/rmunozTMELab/Physically-Guided-Machine-Learning/data/non_linear_1000_5/non_linear_1000_5.pkl\n",
      "Data successfully loaded from: /home/rmunoz/Escritorio/rmunozTMELab/Physically-Guided-Machine-Learning/data/non_linear_1000_5/non_linear_1000_5.pkl\n",
      "baseline\n",
      "                        eQ1       eQ2       eQ3        eK\n",
      "N_data Sigma Mode                                        \n",
      "10     0     5     8.13e-02  9.08e-02  1.00e-01  1.98e-02\n",
      "             10    7.57e-02  8.60e-02  9.62e-02  2.68e-02\n",
      "             50    7.51e-02  8.53e-02  9.55e-02  2.67e-02\n",
      "       1     5     1.48e-01  1.79e-01  2.10e-01  5.22e-01\n",
      "             10    7.66e-02  8.84e-02  1.00e-01  5.68e-01\n",
      "             50    7.72e-02  8.43e-02  9.14e-02  4.81e-01\n",
      "       5     5     1.03e-01  1.11e-01  1.19e-01  9.98e-01\n",
      "             10    1.16e-01  1.19e-01  1.22e-01  9.96e-01\n",
      "             50    1.04e-01  1.09e-01  1.14e-01  9.96e-01\n",
      "100    0     5     1.18e-03  2.99e-03  9.95e-03  4.00e-02\n",
      "             10    2.24e-03  4.29e-03  9.88e-03  9.64e-02\n",
      "             50    2.08e-03  6.78e-03  1.62e-02  4.08e-02\n",
      "       1     5     1.65e-02  2.36e-02  5.28e-02  3.33e-01\n",
      "             10    1.79e-02  2.46e-02  3.55e-02  3.25e-01\n",
      "             50    1.71e-02  2.13e-02  3.30e-02  3.57e-01\n",
      "       5     5     9.54e-02  1.18e-01  1.91e-01  9.72e-01\n",
      "             10    1.51e-01  1.82e-01  2.08e-01  9.72e-01\n",
      "             50    1.09e-01  1.37e-01  2.11e-01  1.04e+00\n",
      "1000   0     5     1.13e-04  1.60e-04  2.87e-04  3.55e-02\n",
      "             10    1.93e-04  2.33e-04  3.35e-04  2.88e-02\n",
      "             50    1.44e-04  1.82e-04  2.88e-04  2.39e-02\n",
      "       1     5     1.17e-02  1.26e-02  1.35e-02  1.19e-01\n",
      "             10    1.17e-02  1.26e-02  1.35e-02  4.26e-02\n",
      "             50    1.17e-02  1.26e-02  1.35e-02  3.08e-02\n",
      "       5     5     5.95e-02  6.57e-02  7.21e-02  8.75e-01\n",
      "             10    5.87e-02  6.43e-02  7.08e-02  8.43e-01\n",
      "             50    5.85e-02  6.41e-02  6.98e-02  9.27e-01\n",
      "Tiempo actual en segundos: 13904.736866950989\n",
      "Tiempo actual en minutos: 231.74561444918314\n",
      "\n",
      "\n",
      "autoencoder\n",
      "Data successfully loaded from: /home/rmunoz/Escritorio/rmunozTMELab/Physically-Guided-Machine-Learning/data/non_linear_10_0/non_linear_10_0.pkl\n",
      "Data successfully loaded from: /home/rmunoz/Escritorio/rmunozTMELab/Physically-Guided-Machine-Learning/data/non_linear_10_0/non_linear_10_0.pkl\n",
      "Data successfully loaded from: /home/rmunoz/Escritorio/rmunozTMELab/Physically-Guided-Machine-Learning/data/non_linear_10_0/non_linear_10_0.pkl\n",
      "Data successfully loaded from: /home/rmunoz/Escritorio/rmunozTMELab/Physically-Guided-Machine-Learning/data/non_linear_10_1/non_linear_10_1.pkl\n",
      "Data successfully loaded from: /home/rmunoz/Escritorio/rmunozTMELab/Physically-Guided-Machine-Learning/data/non_linear_10_1/non_linear_10_1.pkl\n",
      "Data successfully loaded from: /home/rmunoz/Escritorio/rmunozTMELab/Physically-Guided-Machine-Learning/data/non_linear_10_1/non_linear_10_1.pkl\n",
      "Data successfully loaded from: /home/rmunoz/Escritorio/rmunozTMELab/Physically-Guided-Machine-Learning/data/non_linear_10_5/non_linear_10_5.pkl\n",
      "Data successfully loaded from: /home/rmunoz/Escritorio/rmunozTMELab/Physically-Guided-Machine-Learning/data/non_linear_10_5/non_linear_10_5.pkl\n",
      "Data successfully loaded from: /home/rmunoz/Escritorio/rmunozTMELab/Physically-Guided-Machine-Learning/data/non_linear_10_5/non_linear_10_5.pkl\n",
      "Data successfully loaded from: /home/rmunoz/Escritorio/rmunozTMELab/Physically-Guided-Machine-Learning/data/non_linear_100_0/non_linear_100_0.pkl\n",
      "Data successfully loaded from: /home/rmunoz/Escritorio/rmunozTMELab/Physically-Guided-Machine-Learning/data/non_linear_100_0/non_linear_100_0.pkl\n",
      "Data successfully loaded from: /home/rmunoz/Escritorio/rmunozTMELab/Physically-Guided-Machine-Learning/data/non_linear_100_0/non_linear_100_0.pkl\n",
      "Data successfully loaded from: /home/rmunoz/Escritorio/rmunozTMELab/Physically-Guided-Machine-Learning/data/non_linear_100_1/non_linear_100_1.pkl\n",
      "Data successfully loaded from: /home/rmunoz/Escritorio/rmunozTMELab/Physically-Guided-Machine-Learning/data/non_linear_100_1/non_linear_100_1.pkl\n",
      "Data successfully loaded from: /home/rmunoz/Escritorio/rmunozTMELab/Physically-Guided-Machine-Learning/data/non_linear_100_1/non_linear_100_1.pkl\n",
      "Data successfully loaded from: /home/rmunoz/Escritorio/rmunozTMELab/Physically-Guided-Machine-Learning/data/non_linear_100_5/non_linear_100_5.pkl\n",
      "Data successfully loaded from: /home/rmunoz/Escritorio/rmunozTMELab/Physically-Guided-Machine-Learning/data/non_linear_100_5/non_linear_100_5.pkl\n",
      "Data successfully loaded from: /home/rmunoz/Escritorio/rmunozTMELab/Physically-Guided-Machine-Learning/data/non_linear_100_5/non_linear_100_5.pkl\n",
      "Data successfully loaded from: /home/rmunoz/Escritorio/rmunozTMELab/Physically-Guided-Machine-Learning/data/non_linear_1000_0/non_linear_1000_0.pkl\n",
      "Data successfully loaded from: /home/rmunoz/Escritorio/rmunozTMELab/Physically-Guided-Machine-Learning/data/non_linear_1000_0/non_linear_1000_0.pkl\n",
      "Data successfully loaded from: /home/rmunoz/Escritorio/rmunozTMELab/Physically-Guided-Machine-Learning/data/non_linear_1000_0/non_linear_1000_0.pkl\n",
      "Data successfully loaded from: /home/rmunoz/Escritorio/rmunozTMELab/Physically-Guided-Machine-Learning/data/non_linear_1000_1/non_linear_1000_1.pkl\n",
      "Data successfully loaded from: /home/rmunoz/Escritorio/rmunozTMELab/Physically-Guided-Machine-Learning/data/non_linear_1000_1/non_linear_1000_1.pkl\n",
      "Data successfully loaded from: /home/rmunoz/Escritorio/rmunozTMELab/Physically-Guided-Machine-Learning/data/non_linear_1000_1/non_linear_1000_1.pkl\n",
      "Data successfully loaded from: /home/rmunoz/Escritorio/rmunozTMELab/Physically-Guided-Machine-Learning/data/non_linear_1000_5/non_linear_1000_5.pkl\n",
      "Data successfully loaded from: /home/rmunoz/Escritorio/rmunozTMELab/Physically-Guided-Machine-Learning/data/non_linear_1000_5/non_linear_1000_5.pkl\n",
      "Data successfully loaded from: /home/rmunoz/Escritorio/rmunozTMELab/Physically-Guided-Machine-Learning/data/non_linear_1000_5/non_linear_1000_5.pkl\n",
      "autoencoder\n",
      "                        eQ1       eQ2       eQ3        eK\n",
      "N_data Sigma Mode                                        \n",
      "10     0     5     7.37e-02  1.00e-01  1.27e-01  5.09e+00\n",
      "             10    9.13e-02  1.04e-01  1.17e-01  9.91e-01\n",
      "             50    1.00e-01  1.19e-01  1.37e-01  1.11e+00\n",
      "       1     5     8.89e-02  1.12e-01  1.34e-01  1.24e+00\n",
      "             10    8.83e-02  1.00e-01  1.12e-01  4.97e-01\n",
      "             50    1.07e-01  1.23e-01  1.40e-01  1.14e+00\n",
      "       5     5     1.10e-01  1.19e-01  1.28e-01  1.40e+00\n",
      "             10    1.08e-01  1.14e-01  1.21e-01  1.81e+00\n",
      "             50    1.05e-01  1.15e-01  1.24e-01  7.11e-01\n",
      "100    0     5     4.91e-03  8.03e-03  7.18e-02  2.18e-01\n",
      "             10    6.96e-03  1.59e-02  4.42e-02  6.00e-02\n",
      "             50    7.45e-03  1.28e-02  3.17e-02  9.11e-02\n",
      "       1     5     1.85e-02  2.16e-02  9.95e-02  7.42e-01\n",
      "             10    1.67e-02  2.56e-02  5.70e-02  1.03e+00\n",
      "             50    2.40e-02  4.47e-02  7.32e-02  1.21e+00\n",
      "       5     5     9.88e-02  1.25e-01  1.72e-01  7.36e-01\n",
      "             10    1.07e-01  1.59e-01  2.04e-01  5.97e-01\n",
      "             50    8.40e-02  1.01e-01  1.69e-01  7.07e-01\n",
      "1000   0     5     5.09e-04  6.41e-04  1.21e-03  1.82e-02\n",
      "             10    3.23e-04  5.35e-04  1.01e-03  1.92e-02\n",
      "             50    3.75e-04  5.29e-04  1.00e-03  4.09e-02\n",
      "       1     5     1.19e-02  1.27e-02  1.39e-02  8.79e-02\n",
      "             10    1.19e-02  1.28e-02  1.37e-02  9.97e-02\n",
      "             50    1.18e-02  1.28e-02  1.40e-02  1.25e-01\n",
      "       5     5     6.26e-02  6.87e-02  7.61e-02  8.52e-01\n",
      "             10    6.36e-02  6.85e-02  7.83e-02  9.77e-01\n",
      "             50    6.38e-02  7.02e-02  7.90e-02  7.92e-01\n",
      "Tiempo actual en segundos: 7974.59924530983\n",
      "Tiempo actual en minutos: 132.9099874218305\n",
      "\n",
      "\n",
      "fourier\n",
      "Data successfully loaded from: /home/rmunoz/Escritorio/rmunozTMELab/Physically-Guided-Machine-Learning/data/non_linear_10_0/non_linear_10_0.pkl\n",
      "Data successfully loaded from: /home/rmunoz/Escritorio/rmunozTMELab/Physically-Guided-Machine-Learning/data/non_linear_10_0/non_linear_10_0.pkl\n",
      "Data successfully loaded from: /home/rmunoz/Escritorio/rmunozTMELab/Physically-Guided-Machine-Learning/data/non_linear_10_0/non_linear_10_0.pkl\n",
      "Data successfully loaded from: /home/rmunoz/Escritorio/rmunozTMELab/Physically-Guided-Machine-Learning/data/non_linear_10_1/non_linear_10_1.pkl\n",
      "Data successfully loaded from: /home/rmunoz/Escritorio/rmunozTMELab/Physically-Guided-Machine-Learning/data/non_linear_10_1/non_linear_10_1.pkl\n",
      "Data successfully loaded from: /home/rmunoz/Escritorio/rmunozTMELab/Physically-Guided-Machine-Learning/data/non_linear_10_1/non_linear_10_1.pkl\n",
      "Data successfully loaded from: /home/rmunoz/Escritorio/rmunozTMELab/Physically-Guided-Machine-Learning/data/non_linear_10_5/non_linear_10_5.pkl\n",
      "Data successfully loaded from: /home/rmunoz/Escritorio/rmunozTMELab/Physically-Guided-Machine-Learning/data/non_linear_10_5/non_linear_10_5.pkl\n",
      "Data successfully loaded from: /home/rmunoz/Escritorio/rmunozTMELab/Physically-Guided-Machine-Learning/data/non_linear_10_5/non_linear_10_5.pkl\n",
      "Data successfully loaded from: /home/rmunoz/Escritorio/rmunozTMELab/Physically-Guided-Machine-Learning/data/non_linear_100_0/non_linear_100_0.pkl\n",
      "Data successfully loaded from: /home/rmunoz/Escritorio/rmunozTMELab/Physically-Guided-Machine-Learning/data/non_linear_100_0/non_linear_100_0.pkl\n",
      "Data successfully loaded from: /home/rmunoz/Escritorio/rmunozTMELab/Physically-Guided-Machine-Learning/data/non_linear_100_0/non_linear_100_0.pkl\n",
      "Data successfully loaded from: /home/rmunoz/Escritorio/rmunozTMELab/Physically-Guided-Machine-Learning/data/non_linear_100_1/non_linear_100_1.pkl\n",
      "Data successfully loaded from: /home/rmunoz/Escritorio/rmunozTMELab/Physically-Guided-Machine-Learning/data/non_linear_100_1/non_linear_100_1.pkl\n",
      "Data successfully loaded from: /home/rmunoz/Escritorio/rmunozTMELab/Physically-Guided-Machine-Learning/data/non_linear_100_1/non_linear_100_1.pkl\n",
      "Data successfully loaded from: /home/rmunoz/Escritorio/rmunozTMELab/Physically-Guided-Machine-Learning/data/non_linear_100_5/non_linear_100_5.pkl\n",
      "Data successfully loaded from: /home/rmunoz/Escritorio/rmunozTMELab/Physically-Guided-Machine-Learning/data/non_linear_100_5/non_linear_100_5.pkl\n",
      "Data successfully loaded from: /home/rmunoz/Escritorio/rmunozTMELab/Physically-Guided-Machine-Learning/data/non_linear_100_5/non_linear_100_5.pkl\n",
      "Data successfully loaded from: /home/rmunoz/Escritorio/rmunozTMELab/Physically-Guided-Machine-Learning/data/non_linear_1000_0/non_linear_1000_0.pkl\n",
      "Data successfully loaded from: /home/rmunoz/Escritorio/rmunozTMELab/Physically-Guided-Machine-Learning/data/non_linear_1000_0/non_linear_1000_0.pkl\n",
      "Data successfully loaded from: /home/rmunoz/Escritorio/rmunozTMELab/Physically-Guided-Machine-Learning/data/non_linear_1000_0/non_linear_1000_0.pkl\n",
      "Data successfully loaded from: /home/rmunoz/Escritorio/rmunozTMELab/Physically-Guided-Machine-Learning/data/non_linear_1000_1/non_linear_1000_1.pkl\n",
      "Data successfully loaded from: /home/rmunoz/Escritorio/rmunozTMELab/Physically-Guided-Machine-Learning/data/non_linear_1000_1/non_linear_1000_1.pkl\n",
      "Data successfully loaded from: /home/rmunoz/Escritorio/rmunozTMELab/Physically-Guided-Machine-Learning/data/non_linear_1000_1/non_linear_1000_1.pkl\n",
      "Data successfully loaded from: /home/rmunoz/Escritorio/rmunozTMELab/Physically-Guided-Machine-Learning/data/non_linear_1000_5/non_linear_1000_5.pkl\n",
      "Data successfully loaded from: /home/rmunoz/Escritorio/rmunozTMELab/Physically-Guided-Machine-Learning/data/non_linear_1000_5/non_linear_1000_5.pkl\n",
      "Data successfully loaded from: /home/rmunoz/Escritorio/rmunozTMELab/Physically-Guided-Machine-Learning/data/non_linear_1000_5/non_linear_1000_5.pkl\n",
      "fourier\n",
      "                        eQ1       eQ2       eQ3        eK\n",
      "N_data Sigma Mode                                        \n",
      "10     0     5     5.27e-02  5.83e-02  6.40e-02  2.04e+00\n",
      "             10    3.63e-02  4.09e-02  4.56e-02  7.59e-01\n",
      "             50    6.57e-02  6.93e-02  7.29e-02  1.15e-01\n",
      "       1     5     5.25e-02  5.85e-02  6.45e-02  8.53e-01\n",
      "             10    3.85e-02  4.76e-02  5.67e-02  3.32e-01\n",
      "             50    4.88e-02  4.92e-02  4.95e-02  1.77e-01\n",
      "       5     5     7.61e-02  8.14e-02  8.67e-02  1.11e+00\n",
      "             10    7.95e-02  8.72e-02  9.48e-02  7.84e-01\n",
      "             50    1.17e-01  1.45e-01  1.74e-01  9.57e-01\n",
      "100    0     5     4.56e-02  6.43e-02  9.38e-02  1.03e+00\n",
      "             10    1.19e-02  2.04e-02  2.75e-02  3.92e-01\n",
      "             50    1.88e-03  2.72e-03  9.91e-03  8.60e-02\n",
      "       1     5     4.83e-02  6.42e-02  9.47e-02  8.15e-01\n",
      "             10    1.86e-02  2.50e-02  3.12e-02  4.56e-01\n",
      "             50    1.31e-02  1.58e-02  2.04e-02  5.09e-02\n",
      "       5     5     7.89e-02  9.04e-02  1.17e-01  6.85e-01\n",
      "             10    6.54e-02  7.11e-02  7.63e-02  6.35e-01\n",
      "             50    6.48e-02  6.76e-02  7.39e-02  4.69e-01\n",
      "1000   0     5     4.12e-02  5.28e-02  7.77e-02  7.05e-01\n",
      "             10    6.53e-03  1.22e-02  1.94e-02  5.47e-01\n",
      "             50    3.10e-04  4.75e-04  7.33e-04  4.65e-02\n",
      "       1     5     4.26e-02  5.37e-02  7.92e-02  9.82e-01\n",
      "             10    1.40e-02  1.77e-02  2.37e-02  1.12e+00\n",
      "             50    1.16e-02  1.24e-02  1.34e-02  3.37e-02\n",
      "       5     5     7.14e-02  8.20e-02  1.04e-01  8.98e-01\n",
      "             10    5.95e-02  6.40e-02  7.11e-02  4.72e-01\n",
      "             50    5.78e-02  6.25e-02  6.67e-02  5.22e-01\n",
      "Tiempo actual en segundos: 12874.379319906235\n",
      "Tiempo actual en minutos: 214.5729886651039\n",
      "\n",
      "\n",
      "POD\n",
      "Data successfully loaded from: /home/rmunoz/Escritorio/rmunozTMELab/Physically-Guided-Machine-Learning/data/non_linear_10_0/non_linear_10_0.pkl\n",
      "Data successfully loaded from: /home/rmunoz/Escritorio/rmunozTMELab/Physically-Guided-Machine-Learning/data/non_linear_10_0/non_linear_10_0.pkl\n",
      "Data successfully loaded from: /home/rmunoz/Escritorio/rmunozTMELab/Physically-Guided-Machine-Learning/data/non_linear_10_0/non_linear_10_0.pkl\n",
      "Data successfully loaded from: /home/rmunoz/Escritorio/rmunozTMELab/Physically-Guided-Machine-Learning/data/non_linear_10_1/non_linear_10_1.pkl\n",
      "Data successfully loaded from: /home/rmunoz/Escritorio/rmunozTMELab/Physically-Guided-Machine-Learning/data/non_linear_10_1/non_linear_10_1.pkl\n",
      "Data successfully loaded from: /home/rmunoz/Escritorio/rmunozTMELab/Physically-Guided-Machine-Learning/data/non_linear_10_1/non_linear_10_1.pkl\n",
      "Data successfully loaded from: /home/rmunoz/Escritorio/rmunozTMELab/Physically-Guided-Machine-Learning/data/non_linear_10_5/non_linear_10_5.pkl\n",
      "Data successfully loaded from: /home/rmunoz/Escritorio/rmunozTMELab/Physically-Guided-Machine-Learning/data/non_linear_10_5/non_linear_10_5.pkl\n",
      "Data successfully loaded from: /home/rmunoz/Escritorio/rmunozTMELab/Physically-Guided-Machine-Learning/data/non_linear_10_5/non_linear_10_5.pkl\n",
      "Data successfully loaded from: /home/rmunoz/Escritorio/rmunozTMELab/Physically-Guided-Machine-Learning/data/non_linear_100_0/non_linear_100_0.pkl\n",
      "Data successfully loaded from: /home/rmunoz/Escritorio/rmunozTMELab/Physically-Guided-Machine-Learning/data/non_linear_100_0/non_linear_100_0.pkl\n",
      "Data successfully loaded from: /home/rmunoz/Escritorio/rmunozTMELab/Physically-Guided-Machine-Learning/data/non_linear_100_0/non_linear_100_0.pkl\n",
      "Data successfully loaded from: /home/rmunoz/Escritorio/rmunozTMELab/Physically-Guided-Machine-Learning/data/non_linear_100_1/non_linear_100_1.pkl\n",
      "Data successfully loaded from: /home/rmunoz/Escritorio/rmunozTMELab/Physically-Guided-Machine-Learning/data/non_linear_100_1/non_linear_100_1.pkl\n",
      "Data successfully loaded from: /home/rmunoz/Escritorio/rmunozTMELab/Physically-Guided-Machine-Learning/data/non_linear_100_1/non_linear_100_1.pkl\n",
      "Data successfully loaded from: /home/rmunoz/Escritorio/rmunozTMELab/Physically-Guided-Machine-Learning/data/non_linear_100_5/non_linear_100_5.pkl\n",
      "Data successfully loaded from: /home/rmunoz/Escritorio/rmunozTMELab/Physically-Guided-Machine-Learning/data/non_linear_100_5/non_linear_100_5.pkl\n",
      "Data successfully loaded from: /home/rmunoz/Escritorio/rmunozTMELab/Physically-Guided-Machine-Learning/data/non_linear_100_5/non_linear_100_5.pkl\n",
      "Data successfully loaded from: /home/rmunoz/Escritorio/rmunozTMELab/Physically-Guided-Machine-Learning/data/non_linear_1000_0/non_linear_1000_0.pkl\n",
      "Data successfully loaded from: /home/rmunoz/Escritorio/rmunozTMELab/Physically-Guided-Machine-Learning/data/non_linear_1000_0/non_linear_1000_0.pkl\n",
      "Data successfully loaded from: /home/rmunoz/Escritorio/rmunozTMELab/Physically-Guided-Machine-Learning/data/non_linear_1000_0/non_linear_1000_0.pkl\n",
      "Data successfully loaded from: /home/rmunoz/Escritorio/rmunozTMELab/Physically-Guided-Machine-Learning/data/non_linear_1000_1/non_linear_1000_1.pkl\n",
      "Data successfully loaded from: /home/rmunoz/Escritorio/rmunozTMELab/Physically-Guided-Machine-Learning/data/non_linear_1000_1/non_linear_1000_1.pkl\n",
      "Data successfully loaded from: /home/rmunoz/Escritorio/rmunozTMELab/Physically-Guided-Machine-Learning/data/non_linear_1000_1/non_linear_1000_1.pkl\n",
      "Data successfully loaded from: /home/rmunoz/Escritorio/rmunozTMELab/Physically-Guided-Machine-Learning/data/non_linear_1000_5/non_linear_1000_5.pkl\n",
      "Data successfully loaded from: /home/rmunoz/Escritorio/rmunozTMELab/Physically-Guided-Machine-Learning/data/non_linear_1000_5/non_linear_1000_5.pkl\n",
      "Data successfully loaded from: /home/rmunoz/Escritorio/rmunozTMELab/Physically-Guided-Machine-Learning/data/non_linear_1000_5/non_linear_1000_5.pkl\n",
      "POD\n",
      "                        eQ1       eQ2       eQ3        eK\n",
      "N_data Sigma Mode                                        \n",
      "10     0     5     3.32e-02  4.32e-02  5.32e-02  1.91e-02\n",
      "             10         NaN       NaN       NaN       NaN\n",
      "             50         NaN       NaN       NaN       NaN\n",
      "       1     5     4.50e-02  5.16e-02  5.81e-02  5.17e-01\n",
      "             10         NaN       NaN       NaN       NaN\n",
      "             50         NaN       NaN       NaN       NaN\n",
      "       5     5     8.59e-02  9.33e-02  1.01e-01  8.75e-01\n",
      "             10         NaN       NaN       NaN       NaN\n",
      "             50         NaN       NaN       NaN       NaN\n",
      "100    0     5     2.49e-03  3.69e-03  1.03e-02  4.06e-02\n",
      "             10    1.25e-03  3.11e-03  9.27e-03  9.41e-02\n",
      "             50    2.09e-03  4.66e-03  1.06e-02  4.45e-02\n",
      "       1     5     1.40e-02  1.76e-02  2.26e-02  2.03e-01\n",
      "             10    1.42e-02  1.75e-02  2.42e-02  3.84e-01\n",
      "             50    1.37e-02  1.69e-02  2.36e-02  2.18e-01\n",
      "       5     5     6.38e-02  6.83e-02  7.23e-02  5.98e-01\n",
      "             10    6.49e-02  7.72e-02  8.45e-02  9.63e-01\n",
      "             50    6.40e-02  7.53e-02  8.10e-02  9.69e-01\n",
      "1000   0     5     8.34e-04  1.22e-03  1.98e-03  4.36e-02\n",
      "             10    1.86e-04  2.46e-04  3.86e-04  2.70e-02\n",
      "             50    1.54e-04  2.04e-04  3.24e-04  2.38e-02\n",
      "       1     5     1.16e-02  1.26e-02  1.36e-02  1.02e-01\n",
      "             10    1.16e-02  1.24e-02  1.33e-02  1.40e-01\n",
      "             50    1.15e-02  1.23e-02  1.35e-02  9.11e-02\n",
      "       5     5     5.90e-02  6.45e-02  7.02e-02  6.05e-01\n",
      "             10    5.90e-02  6.37e-02  6.99e-02  8.43e-01\n",
      "             50    5.77e-02  6.24e-02  6.73e-02  6.56e-01\n",
      "Tiempo actual en segundos: 12715.074188947678\n",
      "Tiempo actual en minutos: 211.91790314912797\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "N = [10, 100, 1000] \n",
    "R = [0, 1, 5]\n",
    "n_modes = [5, 10, 50]\n",
    "models = ['baseline', 'autoencoder', 'fourier', 'POD']\n",
    "# models = ['autoencoder']\n",
    "\n",
    "for model_i in models:\n",
    "\n",
    "    print(model_i)\n",
    "\n",
    "    combinations = list(itertools.product(N, R, n_modes))\n",
    "    n_data_vals, ruido_vals, modos_vals = zip(*combinations)\n",
    "\n",
    "    # Create the errors table for each model\n",
    "    multi_index = pd.MultiIndex.from_arrays([n_data_vals, ruido_vals, modos_vals], names=[\"N_data\", \"Sigma\", \"Mode\"])\n",
    "    error_table = pd.DataFrame(index=multi_index, columns=[\"eQ1\", \"eQ2\", \"eQ3\", \"eK\"])\n",
    "\n",
    "    for n_i in N:\n",
    "        for r_i in R:\n",
    "            for mode_i in n_modes:\n",
    "                \n",
    "                data_name = f'non_linear_{n_i}_{r_i}'\n",
    "                model_name = f'{model_i}_model_{mode_i}'\n",
    "                \n",
    "                ROOT_PATH = os.path.abspath(os.path.join(os.getcwd(), \"../\"))\n",
    "                DATA_PATH = os.path.join(ROOT_PATH, r'data/', data_name, data_name) + '.pkl'\n",
    "                RESULTS_FOLDER_PATH = os.path.join(ROOT_PATH, r'results/', data_name)\n",
    "\n",
    "\n",
    "                MODEL_RESULTS_PATH = os.path.join(ROOT_PATH, r'results/', data_name, model_name)\n",
    "\n",
    "                dataset = load_data(DATA_PATH)\n",
    "\n",
    "                # Train data splitting in train/test\n",
    "                X = torch.tensor(dataset['X_train'], dtype=torch.float32).unsqueeze(1)\n",
    "                y = torch.tensor(dataset['y_train'], dtype=torch.float32).unsqueeze(1)\n",
    "                K = torch.tensor(dataset['k_train'], dtype=torch.float32).unsqueeze(1)\n",
    "                f = torch.tensor(dataset['f_train'], dtype=torch.float32).unsqueeze(1)\n",
    "\n",
    "                X_train, X_test, y_train, y_test, K_train, K_test, f_train, f_test = train_test_split(X, y, K, f, test_size=0.3, random_state=42)\n",
    "\n",
    "                # Data processing and adequacy with our TensOps library\n",
    "                X_train = X_train.to(DEVICE)\n",
    "                X_test = X_test.to(DEVICE)\n",
    "\n",
    "                y_train = TensOps(y_train.to(DEVICE).requires_grad_(True), space_dimension=2, contravariance=0, covariance=0)\n",
    "                y_test = TensOps(y_test.to(DEVICE).requires_grad_(True), space_dimension=2, contravariance=0, covariance=0)\n",
    "\n",
    "                K_train = TensOps(K_train.to(DEVICE).requires_grad_(True), space_dimension=2, contravariance=0, covariance=0)\n",
    "                K_test = TensOps(K_test.to(DEVICE).requires_grad_(True), space_dimension=2, contravariance=0, covariance=0)\n",
    "\n",
    "                f_train = TensOps(f_train.to(DEVICE).requires_grad_(True), space_dimension=2, contravariance=0, covariance=0)\n",
    "                f_test = TensOps(f_test.to(DEVICE).requires_grad_(True), space_dimension=2, contravariance=0, covariance=0)\n",
    "\n",
    "                # Loading and processing validation data\n",
    "                X_val = torch.tensor(dataset['X_val'], dtype=torch.float32).unsqueeze(1)\n",
    "                y_val = TensOps(torch.tensor(dataset['y_val'], dtype=torch.float32, requires_grad=True).unsqueeze(1), space_dimension=2, contravariance=0, covariance=0)\n",
    "                K_val = TensOps(torch.tensor(dataset['k_val'], dtype=torch.float32, requires_grad=True).unsqueeze(1), space_dimension=2, contravariance=0, covariance=0)\n",
    "                f_val = TensOps(torch.tensor(dataset['f_val'], dtype=torch.float32, requires_grad=True).unsqueeze(1), space_dimension=2, contravariance=0, covariance=0)\n",
    "\n",
    "                # Predictive network architecture\n",
    "                input_shape = X_train[0].shape\n",
    "                predictive_layers = [20, 10, mode_i, 10, 20]\n",
    "                predictive_output = y_train.values[0].shape\n",
    "\n",
    "                # Explanatory network architecture\n",
    "                explanatory_input = Mx(My(y_train)).values[0].shape\n",
    "                explanatory_layers = [10]\n",
    "                explanatory_output = Mx(My(f_train)).values[0].shape\n",
    "\n",
    "                # Other parameters\n",
    "                n_filters_explanatory = 5\n",
    "\n",
    "                if model_i == 'baseline':\n",
    "\n",
    "                    try:\n",
    "                        model = PGNNIVBaseline(input_shape, predictive_layers, predictive_output, explanatory_input, explanatory_layers, explanatory_output, n_filters_explanatory).to(DEVICE)\n",
    "                        optimizer = torch.optim.Adam(model.parameters(), lr=1e-5)\n",
    "                        model, lists = load_results(model, optimizer, MODEL_RESULTS_PATH, map_location=DEVICE)\n",
    "\n",
    "                        time = np.cumsum(lists['time_list'])[-1]\n",
    "\n",
    "                        hyperparameters = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "                    except:\n",
    "                        pass\n",
    "\n",
    "                elif model_i == 'POD':\n",
    "                    try:\n",
    "                        if X_train.shape[0] < mode_i:\n",
    "                            continue\n",
    "\n",
    "                        U_train, S_train, Vt_train = torch.linalg.svd(y_train.values.detach().squeeze().to('cpu').view(y_train.values.detach().shape[0], -1), full_matrices=False)\n",
    "                        U_reduced_train = U_train[:, :mode_i]\n",
    "                        S_reduced_train = S_train[:mode_i]\n",
    "                        Vt_reduced_train = Vt_train[:mode_i, :]\n",
    "                        POD_base = Vt_reduced_train.to(DEVICE)\n",
    "\n",
    "                        model = PGNNIVPOD(input_shape, predictive_layers, POD_base, predictive_output, explanatory_input, explanatory_layers, explanatory_output, n_filters_explanatory).to(DEVICE)\n",
    "                        optimizer = torch.optim.Adam(model.parameters(), lr=1e-5)\n",
    "                        model, lists = load_results(model, optimizer, MODEL_RESULTS_PATH, map_location=DEVICE)\n",
    "\n",
    "                        with open(os.path.join(MODEL_RESULTS_PATH, \"time.txt\"), \"r\") as f:\n",
    "                            time_pod = float(f.read().strip())  # Usa float o int según lo que necesites\n",
    "\n",
    "                        time = np.cumsum(lists['time_list'])[-1] + time_pod\n",
    "\n",
    "                        hyperparameters = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "                    except:\n",
    "                        pass\n",
    "\n",
    "                elif model_i == 'fourier':\n",
    "                    try:\n",
    "\n",
    "                        X_mesh = torch.tensor(dataset['X_mesh'])\n",
    "                        Y_mesh = torch.tensor(dataset['Y_mesh'])\n",
    "\n",
    "                        base = compute_fourier_base(mode_i, X_mesh, Y_mesh)\n",
    "\n",
    "                        \n",
    "\n",
    "                        model = PGNNIVFourier(input_shape, predictive_layers, base, predictive_output, explanatory_input, explanatory_layers, explanatory_output, n_filters_explanatory, device=DEVICE).to(DEVICE)\n",
    "                        optimizer = torch.optim.Adam(model.parameters(), lr=1e-5)\n",
    "                        model, lists = load_results(model, optimizer, MODEL_RESULTS_PATH, map_location=DEVICE)\n",
    "\n",
    "                        time = np.cumsum(lists['time_list'])[-1]\n",
    "\n",
    "                        hyperparameters = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "                    \n",
    "                    except:\n",
    "                        pass\n",
    "\n",
    "                elif model_i == 'autoencoder':\n",
    "                    try:\n",
    "                        MODEL_RESULTS_AE_PATH = os.path.join(ROOT_PATH, r'results/', data_name, model_name) + '_AE'\n",
    "                        MODEL_RESULTS_PGNNIV_PATH = os.path.join(ROOT_PATH, r'results/', data_name, model_name) + '_NN'\n",
    "\n",
    "                        autoencoder_input_shape = y_train.values[0].shape\n",
    "                        latent_space_dim = [20, 10, mode_i, 10, 20]\n",
    "                        autoencoder_output_shape = y_train.values[0].shape\n",
    "\n",
    "                        autoencoder = Autoencoder(autoencoder_input_shape, latent_space_dim, autoencoder_output_shape).to(DEVICE)\n",
    "                        optimizer = torch.optim.Adam(autoencoder.parameters(), lr=1e-4)\n",
    "\n",
    "                        autoencoder, lists = load_results(autoencoder, optimizer, MODEL_RESULTS_AE_PATH, map_location=torch.device('cpu'))\n",
    "\n",
    "                        hyperparameters_ae = sum(p.numel() for p in autoencoder.parameters() if p.requires_grad)\n",
    "\n",
    "                        time_ae = np.cumsum(lists['time_list'])[-1]\n",
    "\n",
    "                        pretrained_encoder = autoencoder.encoder\n",
    "                        pretrained_decoder = autoencoder.decoder\n",
    "\n",
    "                        for param in pretrained_decoder.parameters():\n",
    "                            param.requires_grad = False\n",
    "\n",
    "                        pgnniv_model = PGNNIVAutoencoder(input_shape, predictive_layers, pretrained_decoder, predictive_output, explanatory_input,\n",
    "                                                        explanatory_layers, explanatory_output, n_filters_explanatory).to(DEVICE)\n",
    "                        optimizer = torch.optim.Adam(pgnniv_model.parameters(), lr=1e-4)\n",
    "\n",
    "                        model, lists = load_results(pgnniv_model, optimizer, MODEL_RESULTS_PGNNIV_PATH, map_location=torch.device('cpu'))\n",
    "                        optimizer = torch.optim.Adam(model.parameters(), lr=1e-5)\n",
    "                        model, lists = load_results(model, optimizer, MODEL_RESULTS_PGNNIV_PATH, map_location=DEVICE)\n",
    "\n",
    "                        time = time_ae + np.cumsum(lists['time_list'])[-1]\n",
    "\n",
    "                        hyperparameters = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "                    except:\n",
    "                        pass\n",
    "\n",
    "                u_train = y_val.values.detach().numpy() \n",
    "                u_predicted_train = model(X_val)[0].detach().numpy() \n",
    "                er_u_train = relative_error(u_train, u_predicted_train).flatten()\n",
    "\n",
    "                erQ1_u = np.percentile(er_u_train, 25)\n",
    "                erQ2_u = np.percentile(er_u_train, 50)\n",
    "                erQ3_u = np.percentile(er_u_train, 75)\n",
    "\n",
    "                u_min = u_train.flatten().min()\n",
    "                u_max = u_train.flatten().max()\n",
    "                steps = 1000\n",
    "                u_for_validating = torch.linspace(u_min, u_max, steps=steps).view(steps, 1, 1, 1)\n",
    "                K_for_validating = (u_for_validating*(1-u_for_validating)).detach().cpu().numpy()\n",
    "                K_predicted_for_validating = model.explanatory(u_for_validating.to(DEVICE)).detach().cpu().numpy()\n",
    "                diff_squared = (K_predicted_for_validating - K_for_validating) ** 2\n",
    "                true_squared = K_for_validating ** 2\n",
    "                u_vals = u_for_validating.numpy().flatten()\n",
    "                numerator = np.sqrt(np.trapz(diff_squared.flatten(), u_vals))\n",
    "                denominator = np.sqrt(np.trapz(true_squared.flatten(), u_vals))\n",
    "\n",
    "                er_K_train = numerator / denominator\n",
    "\n",
    "\n",
    "                # K_train_ = Mx(My(K_train)).values.detach().numpy() \n",
    "                # K_predicted_train = model(X_train)[1].detach().numpy()\n",
    "                # er_K_train = er_sum(K_train_, K_predicted_train)\n",
    "                tiempo_minutos = time / 60\n",
    "\n",
    "                idx = (n_i, r_i, mode_i)\n",
    "                # error_table.loc[idx, \"Hyperparameters\"] = hyperparameters\n",
    "                # error_table.loc[idx, \"time\"] = f\"{tiempo_minutos:.2f}\"\n",
    "                error_table.loc[idx, \"eQ1\"] = f\"{erQ1_u:.2e}\"\n",
    "                error_table.loc[idx, \"eQ2\"] = f\"{erQ2_u:.2e}\"\n",
    "                error_table.loc[idx, \"eQ3\"] = f\"{erQ3_u:.2e}\"\n",
    "                error_table.loc[idx, \"eK\"] = f\"{er_K_train:.2e}\"\n",
    "                \n",
    "\n",
    "    print(model_i)\n",
    "    print(error_table)\n",
    "    tiempo_segundos = time\n",
    "    tiempo_minutos = tiempo_segundos / 60\n",
    "    print(f\"Tiempo actual en segundos: {tiempo_segundos}\")\n",
    "    print(f\"Tiempo actual en minutos: {tiempo_minutos}\")\n",
    "    print(\"\\n\")\n",
    "                \n",
    "\n",
    "    tabla_latex = dataframe_a_latex(error_table, \n",
    "                                    nombre_archivo=os.path.join(os.getcwd(), \"error_tables\", f\"error_{model_i}\"), \n",
    "                                    index=multi_index,\n",
    "                                    caption=\"Tabla con formato LaTeX\",\n",
    "                                    label=\"tab:formateada\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "219f6715",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>eQ1</th>\n",
       "      <th>eQ2</th>\n",
       "      <th>eQ3</th>\n",
       "      <th>eK</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>N_data</th>\n",
       "      <th>Sigma</th>\n",
       "      <th>Mode</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"9\" valign=\"top\">10</th>\n",
       "      <th rowspan=\"3\" valign=\"top\">0</th>\n",
       "      <th>5</th>\n",
       "      <td>3.32e-02</td>\n",
       "      <td>4.32e-02</td>\n",
       "      <td>5.32e-02</td>\n",
       "      <td>1.91e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">1</th>\n",
       "      <th>5</th>\n",
       "      <td>4.50e-02</td>\n",
       "      <td>5.16e-02</td>\n",
       "      <td>5.81e-02</td>\n",
       "      <td>5.17e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">5</th>\n",
       "      <th>5</th>\n",
       "      <td>8.59e-02</td>\n",
       "      <td>9.33e-02</td>\n",
       "      <td>1.01e-01</td>\n",
       "      <td>8.75e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"9\" valign=\"top\">100</th>\n",
       "      <th rowspan=\"3\" valign=\"top\">0</th>\n",
       "      <th>5</th>\n",
       "      <td>2.49e-03</td>\n",
       "      <td>3.69e-03</td>\n",
       "      <td>1.03e-02</td>\n",
       "      <td>4.06e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1.25e-03</td>\n",
       "      <td>3.11e-03</td>\n",
       "      <td>9.27e-03</td>\n",
       "      <td>9.41e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>2.09e-03</td>\n",
       "      <td>4.66e-03</td>\n",
       "      <td>1.06e-02</td>\n",
       "      <td>4.45e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">1</th>\n",
       "      <th>5</th>\n",
       "      <td>1.40e-02</td>\n",
       "      <td>1.76e-02</td>\n",
       "      <td>2.26e-02</td>\n",
       "      <td>2.03e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1.42e-02</td>\n",
       "      <td>1.75e-02</td>\n",
       "      <td>2.42e-02</td>\n",
       "      <td>3.84e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>1.37e-02</td>\n",
       "      <td>1.69e-02</td>\n",
       "      <td>2.36e-02</td>\n",
       "      <td>2.18e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">5</th>\n",
       "      <th>5</th>\n",
       "      <td>6.38e-02</td>\n",
       "      <td>6.83e-02</td>\n",
       "      <td>7.23e-02</td>\n",
       "      <td>5.98e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>6.49e-02</td>\n",
       "      <td>7.72e-02</td>\n",
       "      <td>8.45e-02</td>\n",
       "      <td>9.63e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>6.40e-02</td>\n",
       "      <td>7.53e-02</td>\n",
       "      <td>8.10e-02</td>\n",
       "      <td>9.69e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"9\" valign=\"top\">1000</th>\n",
       "      <th rowspan=\"3\" valign=\"top\">0</th>\n",
       "      <th>5</th>\n",
       "      <td>8.34e-04</td>\n",
       "      <td>1.22e-03</td>\n",
       "      <td>1.98e-03</td>\n",
       "      <td>4.36e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1.86e-04</td>\n",
       "      <td>2.46e-04</td>\n",
       "      <td>3.86e-04</td>\n",
       "      <td>2.70e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>1.54e-04</td>\n",
       "      <td>2.04e-04</td>\n",
       "      <td>3.24e-04</td>\n",
       "      <td>2.38e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">1</th>\n",
       "      <th>5</th>\n",
       "      <td>1.16e-02</td>\n",
       "      <td>1.26e-02</td>\n",
       "      <td>1.36e-02</td>\n",
       "      <td>1.02e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1.16e-02</td>\n",
       "      <td>1.24e-02</td>\n",
       "      <td>1.33e-02</td>\n",
       "      <td>1.40e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>1.15e-02</td>\n",
       "      <td>1.23e-02</td>\n",
       "      <td>1.35e-02</td>\n",
       "      <td>9.11e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">5</th>\n",
       "      <th>5</th>\n",
       "      <td>5.90e-02</td>\n",
       "      <td>6.45e-02</td>\n",
       "      <td>7.02e-02</td>\n",
       "      <td>6.05e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>5.90e-02</td>\n",
       "      <td>6.37e-02</td>\n",
       "      <td>6.99e-02</td>\n",
       "      <td>8.43e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>5.77e-02</td>\n",
       "      <td>6.24e-02</td>\n",
       "      <td>6.73e-02</td>\n",
       "      <td>6.56e-01</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        eQ1       eQ2       eQ3        eK\n",
       "N_data Sigma Mode                                        \n",
       "10     0     5     3.32e-02  4.32e-02  5.32e-02  1.91e-02\n",
       "             10         NaN       NaN       NaN       NaN\n",
       "             50         NaN       NaN       NaN       NaN\n",
       "       1     5     4.50e-02  5.16e-02  5.81e-02  5.17e-01\n",
       "             10         NaN       NaN       NaN       NaN\n",
       "             50         NaN       NaN       NaN       NaN\n",
       "       5     5     8.59e-02  9.33e-02  1.01e-01  8.75e-01\n",
       "             10         NaN       NaN       NaN       NaN\n",
       "             50         NaN       NaN       NaN       NaN\n",
       "100    0     5     2.49e-03  3.69e-03  1.03e-02  4.06e-02\n",
       "             10    1.25e-03  3.11e-03  9.27e-03  9.41e-02\n",
       "             50    2.09e-03  4.66e-03  1.06e-02  4.45e-02\n",
       "       1     5     1.40e-02  1.76e-02  2.26e-02  2.03e-01\n",
       "             10    1.42e-02  1.75e-02  2.42e-02  3.84e-01\n",
       "             50    1.37e-02  1.69e-02  2.36e-02  2.18e-01\n",
       "       5     5     6.38e-02  6.83e-02  7.23e-02  5.98e-01\n",
       "             10    6.49e-02  7.72e-02  8.45e-02  9.63e-01\n",
       "             50    6.40e-02  7.53e-02  8.10e-02  9.69e-01\n",
       "1000   0     5     8.34e-04  1.22e-03  1.98e-03  4.36e-02\n",
       "             10    1.86e-04  2.46e-04  3.86e-04  2.70e-02\n",
       "             50    1.54e-04  2.04e-04  3.24e-04  2.38e-02\n",
       "       1     5     1.16e-02  1.26e-02  1.36e-02  1.02e-01\n",
       "             10    1.16e-02  1.24e-02  1.33e-02  1.40e-01\n",
       "             50    1.15e-02  1.23e-02  1.35e-02  9.11e-02\n",
       "       5     5     5.90e-02  6.45e-02  7.02e-02  6.05e-01\n",
       "             10    5.90e-02  6.37e-02  6.99e-02  8.43e-01\n",
       "             50    5.77e-02  6.24e-02  6.73e-02  6.56e-01"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "error_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4fe92d7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "baseline\n",
      "Data successfully loaded from: /home/rmunoz/Escritorio/rmunozTMELab/Physically-Guided-Machine-Learning/data/non_linear_10_0/non_linear_10_0.pkl\n",
      "Data successfully loaded from: /home/rmunoz/Escritorio/rmunozTMELab/Physically-Guided-Machine-Learning/data/non_linear_10_0/non_linear_10_0.pkl\n",
      "Data successfully loaded from: /home/rmunoz/Escritorio/rmunozTMELab/Physically-Guided-Machine-Learning/data/non_linear_10_0/non_linear_10_0.pkl\n",
      "Data successfully loaded from: /home/rmunoz/Escritorio/rmunozTMELab/Physically-Guided-Machine-Learning/data/non_linear_10_1/non_linear_10_1.pkl\n",
      "Data successfully loaded from: /home/rmunoz/Escritorio/rmunozTMELab/Physically-Guided-Machine-Learning/data/non_linear_10_1/non_linear_10_1.pkl\n",
      "Data successfully loaded from: /home/rmunoz/Escritorio/rmunozTMELab/Physically-Guided-Machine-Learning/data/non_linear_10_1/non_linear_10_1.pkl\n",
      "Data successfully loaded from: /home/rmunoz/Escritorio/rmunozTMELab/Physically-Guided-Machine-Learning/data/non_linear_10_5/non_linear_10_5.pkl\n",
      "Data successfully loaded from: /home/rmunoz/Escritorio/rmunozTMELab/Physically-Guided-Machine-Learning/data/non_linear_10_5/non_linear_10_5.pkl\n",
      "Data successfully loaded from: /home/rmunoz/Escritorio/rmunozTMELab/Physically-Guided-Machine-Learning/data/non_linear_10_5/non_linear_10_5.pkl\n",
      "Data successfully loaded from: /home/rmunoz/Escritorio/rmunozTMELab/Physically-Guided-Machine-Learning/data/non_linear_100_0/non_linear_100_0.pkl\n",
      "Data successfully loaded from: /home/rmunoz/Escritorio/rmunozTMELab/Physically-Guided-Machine-Learning/data/non_linear_100_0/non_linear_100_0.pkl\n",
      "Data successfully loaded from: /home/rmunoz/Escritorio/rmunozTMELab/Physically-Guided-Machine-Learning/data/non_linear_100_0/non_linear_100_0.pkl\n",
      "Data successfully loaded from: /home/rmunoz/Escritorio/rmunozTMELab/Physically-Guided-Machine-Learning/data/non_linear_100_1/non_linear_100_1.pkl\n",
      "Data successfully loaded from: /home/rmunoz/Escritorio/rmunozTMELab/Physically-Guided-Machine-Learning/data/non_linear_100_1/non_linear_100_1.pkl\n",
      "Data successfully loaded from: /home/rmunoz/Escritorio/rmunozTMELab/Physically-Guided-Machine-Learning/data/non_linear_100_1/non_linear_100_1.pkl\n",
      "Data successfully loaded from: /home/rmunoz/Escritorio/rmunozTMELab/Physically-Guided-Machine-Learning/data/non_linear_100_5/non_linear_100_5.pkl\n",
      "Data successfully loaded from: /home/rmunoz/Escritorio/rmunozTMELab/Physically-Guided-Machine-Learning/data/non_linear_100_5/non_linear_100_5.pkl\n",
      "Data successfully loaded from: /home/rmunoz/Escritorio/rmunozTMELab/Physically-Guided-Machine-Learning/data/non_linear_100_5/non_linear_100_5.pkl\n",
      "Data successfully loaded from: /home/rmunoz/Escritorio/rmunozTMELab/Physically-Guided-Machine-Learning/data/non_linear_1000_0/non_linear_1000_0.pkl\n",
      "Data successfully loaded from: /home/rmunoz/Escritorio/rmunozTMELab/Physically-Guided-Machine-Learning/data/non_linear_1000_0/non_linear_1000_0.pkl\n",
      "Data successfully loaded from: /home/rmunoz/Escritorio/rmunozTMELab/Physically-Guided-Machine-Learning/data/non_linear_1000_0/non_linear_1000_0.pkl\n",
      "Data successfully loaded from: /home/rmunoz/Escritorio/rmunozTMELab/Physically-Guided-Machine-Learning/data/non_linear_1000_1/non_linear_1000_1.pkl\n",
      "Data successfully loaded from: /home/rmunoz/Escritorio/rmunozTMELab/Physically-Guided-Machine-Learning/data/non_linear_1000_1/non_linear_1000_1.pkl\n",
      "Data successfully loaded from: /home/rmunoz/Escritorio/rmunozTMELab/Physically-Guided-Machine-Learning/data/non_linear_1000_1/non_linear_1000_1.pkl\n",
      "Data successfully loaded from: /home/rmunoz/Escritorio/rmunozTMELab/Physically-Guided-Machine-Learning/data/non_linear_1000_5/non_linear_1000_5.pkl\n",
      "Data successfully loaded from: /home/rmunoz/Escritorio/rmunozTMELab/Physically-Guided-Machine-Learning/data/non_linear_1000_5/non_linear_1000_5.pkl\n",
      "Data successfully loaded from: /home/rmunoz/Escritorio/rmunozTMELab/Physically-Guided-Machine-Learning/data/non_linear_1000_5/non_linear_1000_5.pkl\n",
      "POD\n",
      "Data successfully loaded from: /home/rmunoz/Escritorio/rmunozTMELab/Physically-Guided-Machine-Learning/data/non_linear_10_0/non_linear_10_0.pkl\n",
      "Data successfully loaded from: /home/rmunoz/Escritorio/rmunozTMELab/Physically-Guided-Machine-Learning/data/non_linear_10_0/non_linear_10_0.pkl\n",
      "Data successfully loaded from: /home/rmunoz/Escritorio/rmunozTMELab/Physically-Guided-Machine-Learning/data/non_linear_10_0/non_linear_10_0.pkl\n",
      "Data successfully loaded from: /home/rmunoz/Escritorio/rmunozTMELab/Physically-Guided-Machine-Learning/data/non_linear_10_1/non_linear_10_1.pkl\n",
      "Data successfully loaded from: /home/rmunoz/Escritorio/rmunozTMELab/Physically-Guided-Machine-Learning/data/non_linear_10_1/non_linear_10_1.pkl\n",
      "Data successfully loaded from: /home/rmunoz/Escritorio/rmunozTMELab/Physically-Guided-Machine-Learning/data/non_linear_10_1/non_linear_10_1.pkl\n",
      "Data successfully loaded from: /home/rmunoz/Escritorio/rmunozTMELab/Physically-Guided-Machine-Learning/data/non_linear_10_5/non_linear_10_5.pkl\n",
      "Data successfully loaded from: /home/rmunoz/Escritorio/rmunozTMELab/Physically-Guided-Machine-Learning/data/non_linear_10_5/non_linear_10_5.pkl\n",
      "Data successfully loaded from: /home/rmunoz/Escritorio/rmunozTMELab/Physically-Guided-Machine-Learning/data/non_linear_10_5/non_linear_10_5.pkl\n",
      "Data successfully loaded from: /home/rmunoz/Escritorio/rmunozTMELab/Physically-Guided-Machine-Learning/data/non_linear_100_0/non_linear_100_0.pkl\n",
      "Data successfully loaded from: /home/rmunoz/Escritorio/rmunozTMELab/Physically-Guided-Machine-Learning/data/non_linear_100_0/non_linear_100_0.pkl\n",
      "Data successfully loaded from: /home/rmunoz/Escritorio/rmunozTMELab/Physically-Guided-Machine-Learning/data/non_linear_100_0/non_linear_100_0.pkl\n",
      "Data successfully loaded from: /home/rmunoz/Escritorio/rmunozTMELab/Physically-Guided-Machine-Learning/data/non_linear_100_1/non_linear_100_1.pkl\n",
      "Data successfully loaded from: /home/rmunoz/Escritorio/rmunozTMELab/Physically-Guided-Machine-Learning/data/non_linear_100_1/non_linear_100_1.pkl\n",
      "Data successfully loaded from: /home/rmunoz/Escritorio/rmunozTMELab/Physically-Guided-Machine-Learning/data/non_linear_100_1/non_linear_100_1.pkl\n",
      "Data successfully loaded from: /home/rmunoz/Escritorio/rmunozTMELab/Physically-Guided-Machine-Learning/data/non_linear_100_5/non_linear_100_5.pkl\n",
      "Data successfully loaded from: /home/rmunoz/Escritorio/rmunozTMELab/Physically-Guided-Machine-Learning/data/non_linear_100_5/non_linear_100_5.pkl\n",
      "Data successfully loaded from: /home/rmunoz/Escritorio/rmunozTMELab/Physically-Guided-Machine-Learning/data/non_linear_100_5/non_linear_100_5.pkl\n",
      "Data successfully loaded from: /home/rmunoz/Escritorio/rmunozTMELab/Physically-Guided-Machine-Learning/data/non_linear_1000_0/non_linear_1000_0.pkl\n",
      "Data successfully loaded from: /home/rmunoz/Escritorio/rmunozTMELab/Physically-Guided-Machine-Learning/data/non_linear_1000_0/non_linear_1000_0.pkl\n",
      "Data successfully loaded from: /home/rmunoz/Escritorio/rmunozTMELab/Physically-Guided-Machine-Learning/data/non_linear_1000_0/non_linear_1000_0.pkl\n",
      "Data successfully loaded from: /home/rmunoz/Escritorio/rmunozTMELab/Physically-Guided-Machine-Learning/data/non_linear_1000_1/non_linear_1000_1.pkl\n",
      "Data successfully loaded from: /home/rmunoz/Escritorio/rmunozTMELab/Physically-Guided-Machine-Learning/data/non_linear_1000_1/non_linear_1000_1.pkl\n",
      "Data successfully loaded from: /home/rmunoz/Escritorio/rmunozTMELab/Physically-Guided-Machine-Learning/data/non_linear_1000_1/non_linear_1000_1.pkl\n",
      "Data successfully loaded from: /home/rmunoz/Escritorio/rmunozTMELab/Physically-Guided-Machine-Learning/data/non_linear_1000_5/non_linear_1000_5.pkl\n",
      "Data successfully loaded from: /home/rmunoz/Escritorio/rmunozTMELab/Physically-Guided-Machine-Learning/data/non_linear_1000_5/non_linear_1000_5.pkl\n",
      "Data successfully loaded from: /home/rmunoz/Escritorio/rmunozTMELab/Physically-Guided-Machine-Learning/data/non_linear_1000_5/non_linear_1000_5.pkl\n",
      "autoencoder\n",
      "Data successfully loaded from: /home/rmunoz/Escritorio/rmunozTMELab/Physically-Guided-Machine-Learning/data/non_linear_10_0/non_linear_10_0.pkl\n",
      "Data successfully loaded from: /home/rmunoz/Escritorio/rmunozTMELab/Physically-Guided-Machine-Learning/data/non_linear_10_0/non_linear_10_0.pkl\n",
      "Data successfully loaded from: /home/rmunoz/Escritorio/rmunozTMELab/Physically-Guided-Machine-Learning/data/non_linear_10_0/non_linear_10_0.pkl\n",
      "Data successfully loaded from: /home/rmunoz/Escritorio/rmunozTMELab/Physically-Guided-Machine-Learning/data/non_linear_10_1/non_linear_10_1.pkl\n",
      "Data successfully loaded from: /home/rmunoz/Escritorio/rmunozTMELab/Physically-Guided-Machine-Learning/data/non_linear_10_1/non_linear_10_1.pkl\n",
      "Data successfully loaded from: /home/rmunoz/Escritorio/rmunozTMELab/Physically-Guided-Machine-Learning/data/non_linear_10_1/non_linear_10_1.pkl\n",
      "Data successfully loaded from: /home/rmunoz/Escritorio/rmunozTMELab/Physically-Guided-Machine-Learning/data/non_linear_10_5/non_linear_10_5.pkl\n",
      "Data successfully loaded from: /home/rmunoz/Escritorio/rmunozTMELab/Physically-Guided-Machine-Learning/data/non_linear_10_5/non_linear_10_5.pkl\n",
      "Data successfully loaded from: /home/rmunoz/Escritorio/rmunozTMELab/Physically-Guided-Machine-Learning/data/non_linear_10_5/non_linear_10_5.pkl\n",
      "Data successfully loaded from: /home/rmunoz/Escritorio/rmunozTMELab/Physically-Guided-Machine-Learning/data/non_linear_100_0/non_linear_100_0.pkl\n",
      "Data successfully loaded from: /home/rmunoz/Escritorio/rmunozTMELab/Physically-Guided-Machine-Learning/data/non_linear_100_0/non_linear_100_0.pkl\n",
      "Data successfully loaded from: /home/rmunoz/Escritorio/rmunozTMELab/Physically-Guided-Machine-Learning/data/non_linear_100_0/non_linear_100_0.pkl\n",
      "Data successfully loaded from: /home/rmunoz/Escritorio/rmunozTMELab/Physically-Guided-Machine-Learning/data/non_linear_100_1/non_linear_100_1.pkl\n",
      "Data successfully loaded from: /home/rmunoz/Escritorio/rmunozTMELab/Physically-Guided-Machine-Learning/data/non_linear_100_1/non_linear_100_1.pkl\n",
      "Data successfully loaded from: /home/rmunoz/Escritorio/rmunozTMELab/Physically-Guided-Machine-Learning/data/non_linear_100_1/non_linear_100_1.pkl\n",
      "Data successfully loaded from: /home/rmunoz/Escritorio/rmunozTMELab/Physically-Guided-Machine-Learning/data/non_linear_100_5/non_linear_100_5.pkl\n",
      "Data successfully loaded from: /home/rmunoz/Escritorio/rmunozTMELab/Physically-Guided-Machine-Learning/data/non_linear_100_5/non_linear_100_5.pkl\n",
      "Data successfully loaded from: /home/rmunoz/Escritorio/rmunozTMELab/Physically-Guided-Machine-Learning/data/non_linear_100_5/non_linear_100_5.pkl\n",
      "Data successfully loaded from: /home/rmunoz/Escritorio/rmunozTMELab/Physically-Guided-Machine-Learning/data/non_linear_1000_0/non_linear_1000_0.pkl\n",
      "Data successfully loaded from: /home/rmunoz/Escritorio/rmunozTMELab/Physically-Guided-Machine-Learning/data/non_linear_1000_0/non_linear_1000_0.pkl\n",
      "Data successfully loaded from: /home/rmunoz/Escritorio/rmunozTMELab/Physically-Guided-Machine-Learning/data/non_linear_1000_0/non_linear_1000_0.pkl\n",
      "Data successfully loaded from: /home/rmunoz/Escritorio/rmunozTMELab/Physically-Guided-Machine-Learning/data/non_linear_1000_1/non_linear_1000_1.pkl\n",
      "Data successfully loaded from: /home/rmunoz/Escritorio/rmunozTMELab/Physically-Guided-Machine-Learning/data/non_linear_1000_1/non_linear_1000_1.pkl\n",
      "Data successfully loaded from: /home/rmunoz/Escritorio/rmunozTMELab/Physically-Guided-Machine-Learning/data/non_linear_1000_1/non_linear_1000_1.pkl\n",
      "Data successfully loaded from: /home/rmunoz/Escritorio/rmunozTMELab/Physically-Guided-Machine-Learning/data/non_linear_1000_5/non_linear_1000_5.pkl\n",
      "Data successfully loaded from: /home/rmunoz/Escritorio/rmunozTMELab/Physically-Guided-Machine-Learning/data/non_linear_1000_5/non_linear_1000_5.pkl\n",
      "Data successfully loaded from: /home/rmunoz/Escritorio/rmunozTMELab/Physically-Guided-Machine-Learning/data/non_linear_1000_5/non_linear_1000_5.pkl\n",
      "fourier\n",
      "Data successfully loaded from: /home/rmunoz/Escritorio/rmunozTMELab/Physically-Guided-Machine-Learning/data/non_linear_10_0/non_linear_10_0.pkl\n",
      "Data successfully loaded from: /home/rmunoz/Escritorio/rmunozTMELab/Physically-Guided-Machine-Learning/data/non_linear_10_0/non_linear_10_0.pkl\n",
      "Data successfully loaded from: /home/rmunoz/Escritorio/rmunozTMELab/Physically-Guided-Machine-Learning/data/non_linear_10_0/non_linear_10_0.pkl\n",
      "Data successfully loaded from: /home/rmunoz/Escritorio/rmunozTMELab/Physically-Guided-Machine-Learning/data/non_linear_10_1/non_linear_10_1.pkl\n",
      "Data successfully loaded from: /home/rmunoz/Escritorio/rmunozTMELab/Physically-Guided-Machine-Learning/data/non_linear_10_1/non_linear_10_1.pkl\n",
      "Data successfully loaded from: /home/rmunoz/Escritorio/rmunozTMELab/Physically-Guided-Machine-Learning/data/non_linear_10_1/non_linear_10_1.pkl\n",
      "Data successfully loaded from: /home/rmunoz/Escritorio/rmunozTMELab/Physically-Guided-Machine-Learning/data/non_linear_10_5/non_linear_10_5.pkl\n",
      "Data successfully loaded from: /home/rmunoz/Escritorio/rmunozTMELab/Physically-Guided-Machine-Learning/data/non_linear_10_5/non_linear_10_5.pkl\n",
      "Data successfully loaded from: /home/rmunoz/Escritorio/rmunozTMELab/Physically-Guided-Machine-Learning/data/non_linear_10_5/non_linear_10_5.pkl\n",
      "Data successfully loaded from: /home/rmunoz/Escritorio/rmunozTMELab/Physically-Guided-Machine-Learning/data/non_linear_100_0/non_linear_100_0.pkl\n",
      "Data successfully loaded from: /home/rmunoz/Escritorio/rmunozTMELab/Physically-Guided-Machine-Learning/data/non_linear_100_0/non_linear_100_0.pkl\n",
      "Data successfully loaded from: /home/rmunoz/Escritorio/rmunozTMELab/Physically-Guided-Machine-Learning/data/non_linear_100_0/non_linear_100_0.pkl\n",
      "Data successfully loaded from: /home/rmunoz/Escritorio/rmunozTMELab/Physically-Guided-Machine-Learning/data/non_linear_100_1/non_linear_100_1.pkl\n",
      "Data successfully loaded from: /home/rmunoz/Escritorio/rmunozTMELab/Physically-Guided-Machine-Learning/data/non_linear_100_1/non_linear_100_1.pkl\n",
      "Data successfully loaded from: /home/rmunoz/Escritorio/rmunozTMELab/Physically-Guided-Machine-Learning/data/non_linear_100_1/non_linear_100_1.pkl\n",
      "Data successfully loaded from: /home/rmunoz/Escritorio/rmunozTMELab/Physically-Guided-Machine-Learning/data/non_linear_100_5/non_linear_100_5.pkl\n",
      "Data successfully loaded from: /home/rmunoz/Escritorio/rmunozTMELab/Physically-Guided-Machine-Learning/data/non_linear_100_5/non_linear_100_5.pkl\n",
      "Data successfully loaded from: /home/rmunoz/Escritorio/rmunozTMELab/Physically-Guided-Machine-Learning/data/non_linear_100_5/non_linear_100_5.pkl\n",
      "Data successfully loaded from: /home/rmunoz/Escritorio/rmunozTMELab/Physically-Guided-Machine-Learning/data/non_linear_1000_0/non_linear_1000_0.pkl\n",
      "Data successfully loaded from: /home/rmunoz/Escritorio/rmunozTMELab/Physically-Guided-Machine-Learning/data/non_linear_1000_0/non_linear_1000_0.pkl\n",
      "Data successfully loaded from: /home/rmunoz/Escritorio/rmunozTMELab/Physically-Guided-Machine-Learning/data/non_linear_1000_0/non_linear_1000_0.pkl\n",
      "Data successfully loaded from: /home/rmunoz/Escritorio/rmunozTMELab/Physically-Guided-Machine-Learning/data/non_linear_1000_1/non_linear_1000_1.pkl\n",
      "Data successfully loaded from: /home/rmunoz/Escritorio/rmunozTMELab/Physically-Guided-Machine-Learning/data/non_linear_1000_1/non_linear_1000_1.pkl\n",
      "Data successfully loaded from: /home/rmunoz/Escritorio/rmunozTMELab/Physically-Guided-Machine-Learning/data/non_linear_1000_1/non_linear_1000_1.pkl\n",
      "Data successfully loaded from: /home/rmunoz/Escritorio/rmunozTMELab/Physically-Guided-Machine-Learning/data/non_linear_1000_5/non_linear_1000_5.pkl\n",
      "Data successfully loaded from: /home/rmunoz/Escritorio/rmunozTMELab/Physically-Guided-Machine-Learning/data/non_linear_1000_5/non_linear_1000_5.pkl\n",
      "Data successfully loaded from: /home/rmunoz/Escritorio/rmunozTMELab/Physically-Guided-Machine-Learning/data/non_linear_1000_5/non_linear_1000_5.pkl\n",
      "fourier\n",
      "                  fourier   POD autoencoder\n",
      "N_data Sigma Mode                          \n",
      "10     0     5       0.94  0.93        1.05\n",
      "             10      0.95   nan        1.06\n",
      "             50      0.95   nan        1.03\n",
      "       1     5       0.92  0.91        1.03\n",
      "             10      0.92   nan        1.01\n",
      "             50      0.95   nan        1.03\n",
      "       5     5       0.95  0.90        1.04\n",
      "             10      0.94   nan        1.05\n",
      "             50      0.96   nan        1.05\n",
      "100    0     5       0.94  0.92        1.03\n",
      "             10      0.95  0.91        1.02\n",
      "             50      0.93  0.93        1.00\n",
      "       1     5       0.94  0.94        1.05\n",
      "             10      0.95  0.95        1.05\n",
      "             50      0.93  0.92        1.04\n",
      "       5     5       0.96  0.92        1.04\n",
      "             10      0.99  0.93        1.05\n",
      "             50      0.95  0.92        1.03\n",
      "1000   0     5       0.95  0.91        0.58\n",
      "             10      0.94  0.91        0.58\n",
      "             50      0.92  0.89        0.57\n",
      "       1     5       0.91  0.88        0.57\n",
      "             10      0.95  0.89        0.56\n",
      "             50      0.96  0.90        0.57\n",
      "       5     5       0.92  0.90        0.57\n",
      "             10      0.91  0.89        0.56\n",
      "             50      0.93  0.91        0.57\n",
      "Tiempo actual en segundos: 12874.379319906235\n",
      "Tiempo actual en minutos: 214.5729886651039\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "N = [10, 100, 1000] \n",
    "R = [0, 1, 5]\n",
    "n_modes = [5, 10, 50]\n",
    "models = ['baseline', 'POD', 'autoencoder', 'fourier']\n",
    "\n",
    "# Create the errors table for each model\n",
    "multi_index = pd.MultiIndex.from_arrays([n_data_vals, ruido_vals, modos_vals], names=[\"N_data\", \"Sigma\", \"Mode\"])\n",
    "error_table = pd.DataFrame(index=multi_index, columns=[\"baseline\", \"fourier\", \"POD\", \"autoencoder\"])\n",
    "\n",
    "for model_i in models:\n",
    "\n",
    "    print(model_i)\n",
    "\n",
    "    combinations = list(itertools.product(N, R, n_modes))\n",
    "    n_data_vals, ruido_vals, modos_vals = zip(*combinations)\n",
    "\n",
    "\n",
    "\n",
    "    for n_i in N:\n",
    "        for r_i in R:\n",
    "            for mode_i in n_modes:\n",
    "                \n",
    "                data_name = f'non_linear_{n_i}_{r_i}'\n",
    "                model_name = f'{model_i}_model_{mode_i}'\n",
    "                \n",
    "                ROOT_PATH = os.path.abspath(os.path.join(os.getcwd(), \"../\"))\n",
    "                DATA_PATH = os.path.join(ROOT_PATH, r'data/', data_name, data_name) + '.pkl'\n",
    "                RESULTS_FOLDER_PATH = os.path.join(ROOT_PATH, r'results/', data_name)\n",
    "\n",
    "                MODEL_RESULTS_PATH = os.path.join(ROOT_PATH, r'results/', data_name, model_name)\n",
    "\n",
    "                dataset = load_data(DATA_PATH)\n",
    "\n",
    "                # Train data splitting in train/test\n",
    "                X = torch.tensor(dataset['X_train'], dtype=torch.float32).unsqueeze(1)\n",
    "                y = torch.tensor(dataset['y_train'], dtype=torch.float32).unsqueeze(1)\n",
    "                K = torch.tensor(dataset['k_train'], dtype=torch.float32).unsqueeze(1)\n",
    "                f = torch.tensor(dataset['f_train'], dtype=torch.float32).unsqueeze(1)\n",
    "\n",
    "                X_train, X_test, y_train, y_test, K_train, K_test, f_train, f_test = train_test_split(X, y, K, f, test_size=0.3, random_state=42)\n",
    "\n",
    "                # Data processing and adequacy with our TensOps library\n",
    "                X_train = X_train.to(DEVICE)\n",
    "                X_test = X_test.to(DEVICE)\n",
    "\n",
    "                y_train = TensOps(y_train.to(DEVICE).requires_grad_(True), space_dimension=2, contravariance=0, covariance=0)\n",
    "                y_test = TensOps(y_test.to(DEVICE).requires_grad_(True), space_dimension=2, contravariance=0, covariance=0)\n",
    "\n",
    "                K_train = TensOps(K_train.to(DEVICE).requires_grad_(True), space_dimension=2, contravariance=0, covariance=0)\n",
    "                K_test = TensOps(K_test.to(DEVICE).requires_grad_(True), space_dimension=2, contravariance=0, covariance=0)\n",
    "\n",
    "                f_train = TensOps(f_train.to(DEVICE).requires_grad_(True), space_dimension=2, contravariance=0, covariance=0)\n",
    "                f_test = TensOps(f_test.to(DEVICE).requires_grad_(True), space_dimension=2, contravariance=0, covariance=0)\n",
    "\n",
    "                # Loading and processing validation data\n",
    "                X_val = torch.tensor(dataset['X_val'], dtype=torch.float32).unsqueeze(1)\n",
    "                y_val = TensOps(torch.tensor(dataset['y_val'], dtype=torch.float32, requires_grad=True).unsqueeze(1), space_dimension=2, contravariance=0, covariance=0)\n",
    "                K_val = TensOps(torch.tensor(dataset['k_val'], dtype=torch.float32, requires_grad=True).unsqueeze(1), space_dimension=2, contravariance=0, covariance=0)\n",
    "                f_val = TensOps(torch.tensor(dataset['f_val'], dtype=torch.float32, requires_grad=True).unsqueeze(1), space_dimension=2, contravariance=0, covariance=0)\n",
    "\n",
    "                # Predictive network architecture\n",
    "                input_shape = X_train[0].shape\n",
    "                predictive_layers = [20, 10, mode_i, 10, 20]\n",
    "                predictive_output = y_train.values[0].shape\n",
    "\n",
    "                # Explanatory network architecture\n",
    "                explanatory_input = Mx(My(y_train)).values[0].shape\n",
    "                explanatory_layers = [10]\n",
    "                explanatory_output = Mx(My(f_train)).values[0].shape\n",
    "\n",
    "                # Other parameters\n",
    "                n_filters_explanatory = 5\n",
    "\n",
    "                if model_i == 'baseline':\n",
    "\n",
    "                    try:\n",
    "                        model = PGNNIVBaseline(input_shape, predictive_layers, predictive_output, explanatory_input, explanatory_layers, explanatory_output, n_filters_explanatory).to(DEVICE)\n",
    "                        optimizer = torch.optim.Adam(model.parameters(), lr=1e-5)\n",
    "                        model, lists = load_results(model, optimizer, MODEL_RESULTS_PATH, map_location=DEVICE)\n",
    "\n",
    "                        time = np.cumsum(lists['time_list'])[-1]\n",
    "\n",
    "                        hyperparameters = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "                    except:\n",
    "                        pass\n",
    "\n",
    "                elif model_i == 'POD':\n",
    "                    try:\n",
    "                        if X_train.shape[0] < mode_i:\n",
    "                            continue\n",
    "\n",
    "                        U_train, S_train, Vt_train = torch.linalg.svd(y_train.values.detach().squeeze().to('cpu').view(y_train.values.detach().shape[0], -1), full_matrices=False)\n",
    "                        U_reduced_train = U_train[:, :mode_i]\n",
    "                        S_reduced_train = S_train[:mode_i]\n",
    "                        Vt_reduced_train = Vt_train[:mode_i, :]\n",
    "                        POD_base = Vt_reduced_train.to(DEVICE)\n",
    "\n",
    "                        model = PGNNIVPOD(input_shape, predictive_layers, POD_base, predictive_output, explanatory_input, explanatory_layers, explanatory_output, n_filters_explanatory).to(DEVICE)\n",
    "                        optimizer = torch.optim.Adam(model.parameters(), lr=1e-5)\n",
    "                        model, lists = load_results(model, optimizer, MODEL_RESULTS_PATH, map_location=DEVICE)\n",
    "\n",
    "                        with open(os.path.join(MODEL_RESULTS_PATH, \"time.txt\"), \"r\") as f:\n",
    "                            time_pod = float(f.read().strip())  # Usa float o int según lo que necesites\n",
    "\n",
    "                        time = np.cumsum(lists['time_list'])[-1] + time_pod\n",
    "\n",
    "                        hyperparameters = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "                    except:\n",
    "                        pass\n",
    "\n",
    "                elif model_i == 'fourier':\n",
    "                    try:\n",
    "\n",
    "                        X_mesh = torch.tensor(dataset['X_mesh'])\n",
    "                        Y_mesh = torch.tensor(dataset['Y_mesh'])\n",
    "\n",
    "                        base = compute_fourier_base(mode_i, X_mesh, Y_mesh)\n",
    "\n",
    "                        \n",
    "\n",
    "                        model = PGNNIVFourier(input_shape, predictive_layers, base, predictive_output, explanatory_input, explanatory_layers, explanatory_output, n_filters_explanatory, device=DEVICE).to(DEVICE)\n",
    "                        optimizer = torch.optim.Adam(model.parameters(), lr=1e-5)\n",
    "                        model, lists = load_results(model, optimizer, MODEL_RESULTS_PATH, map_location=DEVICE)\n",
    "\n",
    "                        time = np.cumsum(lists['time_list'])[-1]\n",
    "\n",
    "                        hyperparameters = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "                    \n",
    "                    except:\n",
    "                        pass\n",
    "\n",
    "                elif model_i == 'autoencoder':\n",
    "                    try:\n",
    "                        MODEL_RESULTS_AE_PATH = os.path.join(ROOT_PATH, r'results/', data_name, model_name) + '_AE'\n",
    "                        MODEL_RESULTS_PGNNIV_PATH = os.path.join(ROOT_PATH, r'results/', data_name, model_name) + '_NN'\n",
    "\n",
    "                        autoencoder_input_shape = y_train.values[0].shape\n",
    "                        latent_space_dim = [20, 10, mode_i, 10, 20]\n",
    "                        autoencoder_output_shape = y_train.values[0].shape\n",
    "\n",
    "                        autoencoder = Autoencoder(autoencoder_input_shape, latent_space_dim, autoencoder_output_shape).to(DEVICE)\n",
    "                        optimizer = torch.optim.Adam(autoencoder.parameters(), lr=1e-4)\n",
    "\n",
    "                        autoencoder, lists = load_results(autoencoder, optimizer, MODEL_RESULTS_AE_PATH, map_location=torch.device('cpu'))\n",
    "\n",
    "                        hyperparameters_ae = sum(p.numel() for p in autoencoder.parameters() if p.requires_grad)\n",
    "\n",
    "                        time_ae = np.cumsum(lists['time_list'])[-1]\n",
    "\n",
    "                        pretrained_encoder = autoencoder.encoder\n",
    "                        pretrained_decoder = autoencoder.decoder\n",
    "\n",
    "                        for param in pretrained_decoder.parameters():\n",
    "                            param.requires_grad = False\n",
    "\n",
    "                        pgnniv_model = PGNNIVAutoencoder(input_shape, predictive_layers, pretrained_decoder, predictive_output, explanatory_input,\n",
    "                                                        explanatory_layers, explanatory_output, n_filters_explanatory).to(DEVICE)\n",
    "                        optimizer = torch.optim.Adam(pgnniv_model.parameters(), lr=1e-4)\n",
    "\n",
    "                        model, lists = load_results(pgnniv_model, optimizer, MODEL_RESULTS_PGNNIV_PATH, map_location=torch.device('cpu'))\n",
    "                        optimizer = torch.optim.Adam(model.parameters(), lr=1e-5)\n",
    "                        model, lists = load_results(model, optimizer, MODEL_RESULTS_PGNNIV_PATH, map_location=DEVICE)\n",
    "\n",
    "                        time = time_ae + np.cumsum(lists['time_list'])[-1]\n",
    "\n",
    "                        hyperparameters = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "                    except:\n",
    "                        pass\n",
    "\n",
    "                u_train = y_val.values.detach().numpy() \n",
    "                u_predicted_train = model(X_val)[0].detach().numpy() \n",
    "                er_u_train = relative_error(u_train, u_predicted_train).flatten()\n",
    "\n",
    "                erQ1_u = np.percentile(er_u_train, 25)\n",
    "                erQ2_u = np.percentile(er_u_train, 50)\n",
    "                erQ3_u = np.percentile(er_u_train, 75)\n",
    "\n",
    "                u_min = u_train.flatten().min()\n",
    "                u_max = u_train.flatten().max()\n",
    "                steps = 1000\n",
    "                u_for_validating = torch.linspace(u_min, u_max, steps=steps).view(steps, 1, 1, 1)\n",
    "                K_for_validating = (u_for_validating*(1-u_for_validating)).detach().cpu().numpy()\n",
    "                K_predicted_for_validating = model.explanatory(u_for_validating.to(DEVICE)).detach().cpu().numpy()\n",
    "                diff_squared = (K_predicted_for_validating - K_for_validating) ** 2\n",
    "                true_squared = K_for_validating ** 2\n",
    "                u_vals = u_for_validating.numpy().flatten()\n",
    "                numerator = np.sqrt(np.trapz(diff_squared.flatten(), u_vals))\n",
    "                denominator = np.sqrt(np.trapz(true_squared.flatten(), u_vals))\n",
    "\n",
    "                er_K_train = numerator / denominator\n",
    "\n",
    "\n",
    "                # K_train_ = Mx(My(K_train)).values.detach().numpy() \n",
    "                # K_predicted_train = model(X_train)[1].detach().numpy()\n",
    "                # er_K_train = er_sum(K_train_, K_predicted_train)\n",
    "                tiempo_minutos = time / 60\n",
    "\n",
    "                idx = (n_i, r_i, mode_i)\n",
    "                # error_table.loc[idx, \"Hyperparameters\"] = hyperparameters\n",
    "                # error_table.loc[idx, \"time\"] = f\"{tiempo_minutos:.2f}\"\n",
    "                error_table.loc[(n_i, r_i, mode_i), model_i] = tiempo_minutos\n",
    "\n",
    "\n",
    "error_table[\"fourier\"] = (error_table[\"fourier\"] / error_table[\"baseline\"]).map(\"{:.2f}\".format)\n",
    "error_table[\"POD\"] = (error_table[\"POD\"] / error_table[\"baseline\"]).map(\"{:.2f}\".format)\n",
    "error_table[\"autoencoder\"] = (error_table[\"autoencoder\"] / error_table[\"baseline\"]).map(\"{:.2f}\".format)\n",
    "\n",
    "\n",
    "error_table = error_table.drop(columns=\"baseline\")\n",
    "\n",
    "print(model_i)\n",
    "print(error_table)\n",
    "tiempo_segundos = time\n",
    "tiempo_minutos = tiempo_segundos / 60\n",
    "print(f\"Tiempo actual en segundos: {tiempo_segundos}\")\n",
    "print(f\"Tiempo actual en minutos: {tiempo_minutos}\")\n",
    "print(\"\\n\")\n",
    "\n",
    "            \n",
    "\n",
    "tabla_latex = dataframe_a_latex(error_table, \n",
    "                                nombre_archivo=os.path.join(os.getcwd(), \"error_tables\", f\"time_prop\"), \n",
    "                                index=multi_index,\n",
    "                                caption=\"Tabla con formato LaTeX\",\n",
    "                                label=\"tab:formateada\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3cb2242b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th>N_data</th>\n",
       "      <th colspan=\"9\" halign=\"left\">10</th>\n",
       "      <th colspan=\"3\" halign=\"left\">100</th>\n",
       "      <th colspan=\"9\" halign=\"left\">1000</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Sigma</th>\n",
       "      <th colspan=\"3\" halign=\"left\">0</th>\n",
       "      <th colspan=\"3\" halign=\"left\">1</th>\n",
       "      <th colspan=\"3\" halign=\"left\">5</th>\n",
       "      <th>0</th>\n",
       "      <th>...</th>\n",
       "      <th>5</th>\n",
       "      <th colspan=\"3\" halign=\"left\">0</th>\n",
       "      <th colspan=\"3\" halign=\"left\">1</th>\n",
       "      <th colspan=\"3\" halign=\"left\">5</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Mode</th>\n",
       "      <th>5</th>\n",
       "      <th>10</th>\n",
       "      <th>50</th>\n",
       "      <th>5</th>\n",
       "      <th>10</th>\n",
       "      <th>50</th>\n",
       "      <th>5</th>\n",
       "      <th>10</th>\n",
       "      <th>50</th>\n",
       "      <th>5</th>\n",
       "      <th>...</th>\n",
       "      <th>50</th>\n",
       "      <th>5</th>\n",
       "      <th>10</th>\n",
       "      <th>50</th>\n",
       "      <th>5</th>\n",
       "      <th>10</th>\n",
       "      <th>50</th>\n",
       "      <th>5</th>\n",
       "      <th>10</th>\n",
       "      <th>50</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>fourier</th>\n",
       "      <td>0.94</td>\n",
       "      <td>0.95</td>\n",
       "      <td>0.95</td>\n",
       "      <td>0.92</td>\n",
       "      <td>0.92</td>\n",
       "      <td>0.95</td>\n",
       "      <td>0.95</td>\n",
       "      <td>0.94</td>\n",
       "      <td>0.96</td>\n",
       "      <td>0.94</td>\n",
       "      <td>...</td>\n",
       "      <td>0.95</td>\n",
       "      <td>0.95</td>\n",
       "      <td>0.94</td>\n",
       "      <td>0.92</td>\n",
       "      <td>0.91</td>\n",
       "      <td>0.95</td>\n",
       "      <td>0.96</td>\n",
       "      <td>0.92</td>\n",
       "      <td>0.91</td>\n",
       "      <td>0.93</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>POD</th>\n",
       "      <td>0.93</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.91</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.90</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.92</td>\n",
       "      <td>...</td>\n",
       "      <td>0.92</td>\n",
       "      <td>0.91</td>\n",
       "      <td>0.91</td>\n",
       "      <td>0.89</td>\n",
       "      <td>0.88</td>\n",
       "      <td>0.89</td>\n",
       "      <td>0.90</td>\n",
       "      <td>0.90</td>\n",
       "      <td>0.89</td>\n",
       "      <td>0.91</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>autoencoder</th>\n",
       "      <td>1.05</td>\n",
       "      <td>1.06</td>\n",
       "      <td>1.03</td>\n",
       "      <td>1.03</td>\n",
       "      <td>1.01</td>\n",
       "      <td>1.03</td>\n",
       "      <td>1.04</td>\n",
       "      <td>1.05</td>\n",
       "      <td>1.05</td>\n",
       "      <td>1.03</td>\n",
       "      <td>...</td>\n",
       "      <td>1.03</td>\n",
       "      <td>0.58</td>\n",
       "      <td>0.58</td>\n",
       "      <td>0.57</td>\n",
       "      <td>0.57</td>\n",
       "      <td>0.56</td>\n",
       "      <td>0.57</td>\n",
       "      <td>0.57</td>\n",
       "      <td>0.56</td>\n",
       "      <td>0.57</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 27 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "N_data       10                                                    100   ...  \\\n",
       "Sigma           0                 1                 5                 0  ...   \n",
       "Mode           5     10    50    5     10    50    5     10    50    5   ...   \n",
       "fourier      0.94  0.95  0.95  0.92  0.92  0.95  0.95  0.94  0.96  0.94  ...   \n",
       "POD          0.93   nan   nan  0.91   nan   nan  0.90   nan   nan  0.92  ...   \n",
       "autoencoder  1.05  1.06  1.03  1.03  1.01  1.03  1.04  1.05  1.05  1.03  ...   \n",
       "\n",
       "N_data             1000                                                  \n",
       "Sigma           5     0                 1                 5              \n",
       "Mode           50    5     10    50    5     10    50    5     10    50  \n",
       "fourier      0.95  0.95  0.94  0.92  0.91  0.95  0.96  0.92  0.91  0.93  \n",
       "POD          0.92  0.91  0.91  0.89  0.88  0.89  0.90  0.90  0.89  0.91  \n",
       "autoencoder  1.03  0.58  0.58  0.57  0.57  0.56  0.57  0.57  0.56  0.57  \n",
       "\n",
       "[3 rows x 27 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "error_table.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f08927e9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SciML_test_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
