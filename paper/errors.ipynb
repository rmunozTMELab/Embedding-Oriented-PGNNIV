{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7a48a0a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys \n",
    "\n",
    "current_directory = os.getcwd()\n",
    "models_directory = os.path.abspath(os.path.join(current_directory, '..'))\n",
    "sys.path.append(models_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "71461cff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "import json\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import itertools\n",
    "\n",
    "import matplotlib.colors as mcolors\n",
    "import matplotlib.ticker as ticker\n",
    "from matplotlib.ticker import ScalarFormatter\n",
    "from mpl_toolkits.axes_grid1.inset_locator import inset_axes\n",
    "from matplotlib.ticker import LogLocator, LogFormatter\n",
    "from matplotlib.lines import Line2D\n",
    "\n",
    "# Imports de la libreria propia\n",
    "from vecopsciml.kernels.derivative import DerivativeKernels\n",
    "from vecopsciml.utils import TensOps\n",
    "from vecopsciml.operators import zero_order as zo\n",
    "from vecopsciml.algebra import zero_order as azo\n",
    "\n",
    "# Imports de las funciones creadas para este programa\n",
    "from utils.folders import create_folder\n",
    "from utils.load_data import load_data\n",
    "from utils.checkpoints import load_results\n",
    "from utils.fourier_base import compute_fourier_base\n",
    "\n",
    "from vecopsciml.operators.zero_order import Mx, My"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "440063e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from architectures.pgnniv_baseline import PGNNIVBaseline\n",
    "from architectures.pgnniv_fourier import PGNNIVFourier\n",
    "from architectures.pgnniv_pod import PGNNIVPOD\n",
    "from architectures.pgnniv_decoder import PGNNIVAutoencoder\n",
    "from architectures.autoencoder import Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4658e960",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataframe_a_latex(df, nombre_archivo=None, index=False, caption=None, label=None):\n",
    "    \"\"\"\n",
    "    Convierte un DataFrame en una tabla LaTeX.\n",
    "    \n",
    "    Args:\n",
    "        df (pd.DataFrame): DataFrame a convertir.\n",
    "        nombre_archivo (str): Ruta para guardar el archivo .tex (opcional).\n",
    "        index (bool): Si se incluye o no el índice.\n",
    "        caption (str): Título de la tabla (opcional).\n",
    "        label (str): Etiqueta de la tabla para referencia cruzada (opcional).\n",
    "    \n",
    "    Returns:\n",
    "        str: La cadena en formato LaTeX.\n",
    "    \"\"\"\n",
    "    latex_str = df.to_latex(index=index, caption=caption, label=label, escape=False)\n",
    "\n",
    "    if nombre_archivo:\n",
    "        with open(nombre_archivo, 'w', encoding='utf-8') as f:\n",
    "            f.write(latex_str)\n",
    "    \n",
    "    return latex_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4d61ce4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully loaded from: /home/rmunoz/Escritorio/rmunozTMELab/Physically-Guided-Machine-Learning/data/non_linear_10_0/non_linear_10_0.pkl\n"
     ]
    }
   ],
   "source": [
    "data_name = 'non_linear_10_0'\n",
    "\n",
    "ROOT_PATH = os.path.abspath(os.path.join(os.getcwd(), \"../\"))\n",
    "DATA_PATH = os.path.join(ROOT_PATH, r'data/', data_name, data_name) + '.pkl'\n",
    "\n",
    "dataset = load_data(DATA_PATH)\n",
    "dx = dataset['x_step_size']\n",
    "dy = dataset['y_step_size']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a101e6f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def relative_error(validation, prediction, dx=dx, dy=dy):\n",
    "    \n",
    "    prediction_error = np.sqrt((np.trapz(np.trapz((validation - prediction)**2, dx=dy), dx=dx) /\n",
    "                                np.trapz(np.trapz((validation)**2, dx=dy), dx=dx)))\n",
    "\n",
    "    return prediction_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4c8a0587",
   "metadata": {},
   "outputs": [],
   "source": [
    "def smooth_curve(data, window_size=1000):\n",
    "    window = np.ones(window_size) / window_size\n",
    "    return np.convolve(data, window, mode='valid')\n",
    "\n",
    "def cm_to_in(cm):\n",
    "    return cm * 0.393701\n",
    "\n",
    "def normalize_list(lst):\n",
    "    max_value = np.max(lst)\n",
    "    return [x / max_value for x in lst]\n",
    "\n",
    "linewidth = 1.5  \n",
    "title_fontsize = 14  \n",
    "label_fontsize = 14  \n",
    "legend_fontsize = 12 \n",
    "tick_fontsize = 11  \n",
    "\n",
    "# plt.rc('text', usetex=True)\n",
    "plt.rc('font', family='serif')\n",
    "\n",
    "posX = cm_to_in(10) # posición de la esquina inferior izquierda de la imagen en X\n",
    "posY = cm_to_in(10) # posición de la esquina inferior izquierda de la imagen en Y\n",
    "width = cm_to_in(12)  # ancho de la imagen\n",
    "height = cm_to_in(8) # alto de la imagen\n",
    "\n",
    "color = [0.1, 0, 0.8]  # triplete RGB, valores entre 0 y 1\n",
    "subplot_adjust_left = cm_to_in(0.15)\n",
    "subplot_adjust_bottom = cm_to_in(0.15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "63edea20",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "56e28bd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# N = [10, 100, 1000] \n",
    "# R = [0, 1, 5]\n",
    "# n_modes = [5, 10, 50]\n",
    "# models = ['baseline', 'POD', 'fourier','autoencoder']\n",
    "\n",
    "# combinations = list(itertools.product(N, models, n_modes))\n",
    "# n_data_vals, ruido_vals, modos_vals = zip(*combinations)\n",
    "\n",
    "# # Create the errors table for each model\n",
    "# multi_index = pd.MultiIndex.from_arrays([n_data_vals, ruido_vals, modos_vals], names=[\"N_data\", \"Model\", \"Mode\"])\n",
    "# error_table = pd.DataFrame(index=multi_index, columns=[\"time\", \"eQ1\", \"eQ2\", \"eQ3\", \"eK\"])\n",
    "\n",
    "# for model_i in models:\n",
    "#     for n_i in N:\n",
    "#         for r_i in R:\n",
    "#             for mode_i in n_modes:\n",
    "\n",
    "#                 idx = (n_i, model_i, mode_i)\n",
    "                \n",
    "#                 data_name = f'non_linear_{n_i}_{0}'\n",
    "#                 model_name = f'{model_i}_model_{mode_i}'\n",
    "                \n",
    "#                 ROOT_PATH = os.path.abspath(os.path.join(os.getcwd(), \"../\"))\n",
    "#                 DATA_PATH = os.path.join(ROOT_PATH, r'data/', data_name, data_name) + '.pkl'\n",
    "#                 RESULTS_FOLDER_PATH = os.path.join(ROOT_PATH, r'results/', data_name)\n",
    "\n",
    "\n",
    "#                 MODEL_RESULTS_PATH = os.path.join(ROOT_PATH, r'results/', data_name, model_name)\n",
    "\n",
    "#                 dataset = load_data(DATA_PATH)\n",
    "\n",
    "#                 # Train data splitting in train/test\n",
    "#                 X = torch.tensor(dataset['X_train'], dtype=torch.float32).unsqueeze(1)\n",
    "#                 y = torch.tensor(dataset['y_train'], dtype=torch.float32).unsqueeze(1)\n",
    "#                 K = torch.tensor(dataset['k_train'], dtype=torch.float32).unsqueeze(1)\n",
    "#                 f = torch.tensor(dataset['f_train'], dtype=torch.float32).unsqueeze(1)\n",
    "\n",
    "#                 X_train, X_test, y_train, y_test, K_train, K_test, f_train, f_test = train_test_split(X, y, K, f, test_size=0.3, random_state=42)\n",
    "\n",
    "#                 # Data processing and adequacy with our TensOps library\n",
    "#                 X_train = X_train.to(DEVICE)\n",
    "#                 X_test = X_test.to(DEVICE)\n",
    "\n",
    "#                 y_train = TensOps(y_train.to(DEVICE).requires_grad_(True), space_dimension=2, contravariance=0, covariance=0)\n",
    "#                 y_test = TensOps(y_test.to(DEVICE).requires_grad_(True), space_dimension=2, contravariance=0, covariance=0)\n",
    "\n",
    "#                 K_train = TensOps(K_train.to(DEVICE).requires_grad_(True), space_dimension=2, contravariance=0, covariance=0)\n",
    "#                 K_test = TensOps(K_test.to(DEVICE).requires_grad_(True), space_dimension=2, contravariance=0, covariance=0)\n",
    "\n",
    "#                 f_train = TensOps(f_train.to(DEVICE).requires_grad_(True), space_dimension=2, contravariance=0, covariance=0)\n",
    "#                 f_test = TensOps(f_test.to(DEVICE).requires_grad_(True), space_dimension=2, contravariance=0, covariance=0)\n",
    "\n",
    "#                 # Loading and processing validation data\n",
    "#                 X_val = torch.tensor(dataset['X_val'], dtype=torch.float32).unsqueeze(1)\n",
    "#                 y_val = TensOps(torch.tensor(dataset['y_val'], dtype=torch.float32, requires_grad=True).unsqueeze(1), space_dimension=2, contravariance=0, covariance=0)\n",
    "#                 K_val = TensOps(torch.tensor(dataset['k_val'], dtype=torch.float32, requires_grad=True).unsqueeze(1), space_dimension=2, contravariance=0, covariance=0)\n",
    "#                 f_val = TensOps(torch.tensor(dataset['f_val'], dtype=torch.float32, requires_grad=True).unsqueeze(1), space_dimension=2, contravariance=0, covariance=0)\n",
    "\n",
    "#                 # Predictive network architecture\n",
    "#                 input_shape = X_train[0].shape\n",
    "#                 predictive_layers = [20, 10, mode_i, 10, 20]\n",
    "#                 predictive_output = y_train.values[0].shape\n",
    "\n",
    "#                 # Explanatory network architecture\n",
    "#                 explanatory_input = Mx(My(y_train)).values[0].shape\n",
    "#                 explanatory_layers = [10]\n",
    "#                 explanatory_output = Mx(My(f_train)).values[0].shape\n",
    "\n",
    "#                 # Other parameters\n",
    "#                 n_filters_explanatory = 5\n",
    "\n",
    "#                 if model_i == 'baseline':\n",
    "\n",
    "#                     try:\n",
    "#                         model = PGNNIVBaseline(input_shape, predictive_layers, predictive_output, explanatory_input, explanatory_layers, explanatory_output, n_filters_explanatory).to(DEVICE)\n",
    "#                         optimizer = torch.optim.Adam(model.parameters(), lr=1e-5)\n",
    "#                         model, lists = load_results(model, optimizer, MODEL_RESULTS_PATH, map_location=DEVICE)\n",
    "\n",
    "#                         time = np.cumsum(lists['time_list'])[-1]\n",
    "\n",
    "#                         hyperparameters = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "#                     except:\n",
    "#                         pass\n",
    "\n",
    "#                 elif model_i == 'POD':\n",
    "#                     try:\n",
    "#                         if X_train.shape[0] < mode_i:\n",
    "#                             continue\n",
    "\n",
    "#                         U_train, S_train, Vt_train = torch.linalg.svd(y_train.values.detach().squeeze().to('cpu').view(y_train.values.detach().shape[0], -1), full_matrices=False)\n",
    "#                         U_reduced_train = U_train[:, :mode_i]\n",
    "#                         S_reduced_train = S_train[:mode_i]\n",
    "#                         Vt_reduced_train = Vt_train[:mode_i, :]\n",
    "#                         POD_base = Vt_reduced_train.to(DEVICE)\n",
    "\n",
    "#                         model = PGNNIVPOD(input_shape, predictive_layers, POD_base, predictive_output, explanatory_input, explanatory_layers, explanatory_output, n_filters_explanatory).to(DEVICE)\n",
    "#                         optimizer = torch.optim.Adam(model.parameters(), lr=1e-5)\n",
    "#                         model, lists = load_results(model, optimizer, MODEL_RESULTS_PATH, map_location=DEVICE)\n",
    "\n",
    "#                         with open(os.path.join(MODEL_RESULTS_PATH, \"time.txt\"), \"r\") as f:\n",
    "#                             time_pod = float(f.read().strip())  # Usa float o int según lo que necesites\n",
    "\n",
    "#                         time = np.cumsum(lists['time_list'])[-1] + time_pod\n",
    "\n",
    "#                         hyperparameters = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "#                     except:\n",
    "#                         pass\n",
    "\n",
    "#                 elif model_i == 'fourier':\n",
    "#                     try:\n",
    "\n",
    "#                         X_mesh = torch.tensor(dataset['X_mesh'])\n",
    "#                         Y_mesh = torch.tensor(dataset['Y_mesh'])\n",
    "\n",
    "#                         base = compute_fourier_base(mode_i, X_mesh, Y_mesh)\n",
    "\n",
    "                        \n",
    "\n",
    "#                         model = PGNNIVFourier(input_shape, predictive_layers, base, predictive_output, explanatory_input, explanatory_layers, explanatory_output, n_filters_explanatory, device=DEVICE).to(DEVICE)\n",
    "#                         optimizer = torch.optim.Adam(model.parameters(), lr=1e-5)\n",
    "#                         model, lists = load_results(model, optimizer, MODEL_RESULTS_PATH, map_location=DEVICE)\n",
    "\n",
    "#                         time = np.cumsum(lists['time_list'])[-1]\n",
    "\n",
    "#                         hyperparameters = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "                    \n",
    "#                     except:\n",
    "#                         pass\n",
    "\n",
    "#                 elif model_i == 'autoencoder':\n",
    "#                     try:\n",
    "#                         MODEL_RESULTS_AE_PATH = os.path.join(ROOT_PATH, r'results/', data_name, model_name) + '_AE'\n",
    "#                         MODEL_RESULTS_PGNNIV_PATH = os.path.join(ROOT_PATH, r'results/', data_name, model_name) + '_NN'\n",
    "\n",
    "#                         autoencoder_input_shape = y_train.values[0].shape\n",
    "#                         latent_space_dim = [20, 10, mode_i, 10, 20]\n",
    "#                         autoencoder_output_shape = y_train.values[0].shape\n",
    "\n",
    "#                         autoencoder = Autoencoder(autoencoder_input_shape, latent_space_dim, autoencoder_output_shape).to(DEVICE)\n",
    "#                         optimizer = torch.optim.Adam(autoencoder.parameters(), lr=1e-4)\n",
    "\n",
    "#                         autoencoder, lists = load_results(autoencoder, optimizer, MODEL_RESULTS_AE_PATH, map_location=torch.device('cpu'))\n",
    "\n",
    "#                         hyperparameters_ae = sum(p.numel() for p in autoencoder.parameters() if p.requires_grad)\n",
    "\n",
    "#                         time_ae = np.cumsum(lists['time_list'])[-1]\n",
    "\n",
    "#                         pretrained_encoder = autoencoder.encoder\n",
    "#                         pretrained_decoder = autoencoder.decoder\n",
    "\n",
    "#                         for param in pretrained_decoder.parameters():\n",
    "#                             param.requires_grad = False\n",
    "\n",
    "#                         pgnniv_model = PGNNIVAutoencoder(input_shape, predictive_layers, pretrained_decoder, predictive_output, explanatory_input,\n",
    "#                                                         explanatory_layers, explanatory_output, n_filters_explanatory).to(DEVICE)\n",
    "#                         optimizer = torch.optim.Adam(pgnniv_model.parameters(), lr=1e-4)\n",
    "\n",
    "#                         model, lists = load_results(pgnniv_model, optimizer, MODEL_RESULTS_PGNNIV_PATH, map_location=torch.device('cpu'))\n",
    "#                         optimizer = torch.optim.Adam(model.parameters(), lr=1e-5)\n",
    "#                         model, lists = load_results(model, optimizer, MODEL_RESULTS_PGNNIV_PATH, map_location=DEVICE)\n",
    "\n",
    "#                         time = time_ae + np.cumsum(lists['time_list'])[-1]\n",
    "\n",
    "#                         hyperparameters = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "#                     except:\n",
    "#                         pass\n",
    "\n",
    "#                 u_train = y_val.values.detach().numpy() \n",
    "#                 u_predicted_train = model(X_val)[0].detach().numpy() \n",
    "#                 er_u_train = relative_error(u_train, u_predicted_train).flatten()\n",
    "\n",
    "#                 erQ1_u = np.percentile(er_u_train, 25)\n",
    "#                 erQ2_u = np.percentile(er_u_train, 50)\n",
    "#                 erQ3_u = np.percentile(er_u_train, 75)\n",
    "\n",
    "#                 u_min = u_train.flatten().min()\n",
    "#                 u_max = u_train.flatten().max()\n",
    "#                 steps = 1000\n",
    "#                 u_for_validating = torch.linspace(u_min, u_max, steps=steps).view(steps, 1, 1, 1)\n",
    "#                 K_for_validating = (u_for_validating*(1-u_for_validating)).detach().cpu().numpy()\n",
    "#                 K_predicted_for_validating = model.explanatory(u_for_validating.to(DEVICE)).detach().cpu().numpy()\n",
    "#                 diff_squared = (K_predicted_for_validating - K_for_validating) ** 2\n",
    "#                 true_squared = K_for_validating ** 2\n",
    "#                 u_vals = u_for_validating.numpy().flatten()\n",
    "#                 numerator = np.sqrt(np.trapz(diff_squared.flatten(), u_vals))\n",
    "#                 denominator = np.sqrt(np.trapz(true_squared.flatten(), u_vals))\n",
    "\n",
    "#                 er_K_train = numerator / denominator\n",
    "\n",
    "\n",
    "#                 # K_train_ = Mx(My(K_train)).values.detach().numpy() \n",
    "#                 # K_predicted_train = model(X_train)[1].detach().numpy()\n",
    "#                 # er_K_train = er_sum(K_train_, K_predicted_train)\n",
    "\n",
    "#                 tiempo_minutos = time / 60\n",
    "                \n",
    "#                 error_table.loc[idx, \"time\"] = f\"{tiempo_minutos:.2f}\"\n",
    "#                 error_table.loc[idx, \"eQ1\"] = f\"{erQ1_u:.2e}\"\n",
    "#                 error_table.loc[idx, \"eQ2\"] = f\"{erQ2_u:.2e}\"\n",
    "#                 error_table.loc[idx, \"eQ3\"] = f\"{erQ3_u:.2e}\"\n",
    "#                 error_table.loc[idx, \"eK\"] = f\"{er_K_train:.2e}\"\n",
    "                \n",
    "\n",
    "#     print(model_i)\n",
    "#     print(error_table)\n",
    "#     tiempo_segundos = time\n",
    "#     tiempo_minutos = tiempo_segundos / 60\n",
    "#     print(f\"Tiempo actual en segundos: {tiempo_segundos}\")\n",
    "#     print(f\"Tiempo actual en minutos: {tiempo_minutos}\")\n",
    "#     print(\"\\n\")\n",
    "                    \n",
    "\n",
    "#     tabla_latex = dataframe_a_latex(error_table, \n",
    "#                                     nombre_archivo=os.path.join(os.getcwd(), \"error_tables\", f\"error_{model_i}\"), \n",
    "#                                     index=multi_index,\n",
    "#                                     caption=\"Tabla con formato LaTeX\",\n",
    "#                                     label=\"tab:formateada\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e665132",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "baseline\n",
      "Data successfully loaded from: /home/rmunoz/Escritorio/rmunozTMELab/Physically-Guided-Machine-Learning/data/non_linear_10_0/non_linear_10_0.pkl\n",
      "Data successfully loaded from: /home/rmunoz/Escritorio/rmunozTMELab/Physically-Guided-Machine-Learning/data/non_linear_10_0/non_linear_10_0.pkl\n",
      "Data successfully loaded from: /home/rmunoz/Escritorio/rmunozTMELab/Physically-Guided-Machine-Learning/data/non_linear_10_0/non_linear_10_0.pkl\n",
      "Data successfully loaded from: /home/rmunoz/Escritorio/rmunozTMELab/Physically-Guided-Machine-Learning/data/non_linear_10_1/non_linear_10_1.pkl\n",
      "Data successfully loaded from: /home/rmunoz/Escritorio/rmunozTMELab/Physically-Guided-Machine-Learning/data/non_linear_10_1/non_linear_10_1.pkl\n",
      "Data successfully loaded from: /home/rmunoz/Escritorio/rmunozTMELab/Physically-Guided-Machine-Learning/data/non_linear_10_1/non_linear_10_1.pkl\n",
      "Data successfully loaded from: /home/rmunoz/Escritorio/rmunozTMELab/Physically-Guided-Machine-Learning/data/non_linear_10_5/non_linear_10_5.pkl\n",
      "Data successfully loaded from: /home/rmunoz/Escritorio/rmunozTMELab/Physically-Guided-Machine-Learning/data/non_linear_10_5/non_linear_10_5.pkl\n",
      "Data successfully loaded from: /home/rmunoz/Escritorio/rmunozTMELab/Physically-Guided-Machine-Learning/data/non_linear_10_5/non_linear_10_5.pkl\n",
      "Data successfully loaded from: /home/rmunoz/Escritorio/rmunozTMELab/Physically-Guided-Machine-Learning/data/non_linear_100_0/non_linear_100_0.pkl\n",
      "Data successfully loaded from: /home/rmunoz/Escritorio/rmunozTMELab/Physically-Guided-Machine-Learning/data/non_linear_100_0/non_linear_100_0.pkl\n",
      "Data successfully loaded from: /home/rmunoz/Escritorio/rmunozTMELab/Physically-Guided-Machine-Learning/data/non_linear_100_0/non_linear_100_0.pkl\n",
      "Data successfully loaded from: /home/rmunoz/Escritorio/rmunozTMELab/Physically-Guided-Machine-Learning/data/non_linear_100_1/non_linear_100_1.pkl\n",
      "Data successfully loaded from: /home/rmunoz/Escritorio/rmunozTMELab/Physically-Guided-Machine-Learning/data/non_linear_100_1/non_linear_100_1.pkl\n",
      "Data successfully loaded from: /home/rmunoz/Escritorio/rmunozTMELab/Physically-Guided-Machine-Learning/data/non_linear_100_1/non_linear_100_1.pkl\n",
      "Data successfully loaded from: /home/rmunoz/Escritorio/rmunozTMELab/Physically-Guided-Machine-Learning/data/non_linear_100_5/non_linear_100_5.pkl\n",
      "Data successfully loaded from: /home/rmunoz/Escritorio/rmunozTMELab/Physically-Guided-Machine-Learning/data/non_linear_100_5/non_linear_100_5.pkl\n",
      "Data successfully loaded from: /home/rmunoz/Escritorio/rmunozTMELab/Physically-Guided-Machine-Learning/data/non_linear_100_5/non_linear_100_5.pkl\n",
      "Data successfully loaded from: /home/rmunoz/Escritorio/rmunozTMELab/Physically-Guided-Machine-Learning/data/non_linear_1000_0/non_linear_1000_0.pkl\n",
      "Data successfully loaded from: /home/rmunoz/Escritorio/rmunozTMELab/Physically-Guided-Machine-Learning/data/non_linear_1000_0/non_linear_1000_0.pkl\n",
      "Data successfully loaded from: /home/rmunoz/Escritorio/rmunozTMELab/Physically-Guided-Machine-Learning/data/non_linear_1000_0/non_linear_1000_0.pkl\n",
      "Data successfully loaded from: /home/rmunoz/Escritorio/rmunozTMELab/Physically-Guided-Machine-Learning/data/non_linear_1000_1/non_linear_1000_1.pkl\n",
      "Data successfully loaded from: /home/rmunoz/Escritorio/rmunozTMELab/Physically-Guided-Machine-Learning/data/non_linear_1000_1/non_linear_1000_1.pkl\n",
      "Data successfully loaded from: /home/rmunoz/Escritorio/rmunozTMELab/Physically-Guided-Machine-Learning/data/non_linear_1000_1/non_linear_1000_1.pkl\n",
      "Data successfully loaded from: /home/rmunoz/Escritorio/rmunozTMELab/Physically-Guided-Machine-Learning/data/non_linear_1000_5/non_linear_1000_5.pkl\n",
      "Data successfully loaded from: /home/rmunoz/Escritorio/rmunozTMELab/Physically-Guided-Machine-Learning/data/non_linear_1000_5/non_linear_1000_5.pkl\n",
      "Data successfully loaded from: /home/rmunoz/Escritorio/rmunozTMELab/Physically-Guided-Machine-Learning/data/non_linear_1000_5/non_linear_1000_5.pkl\n",
      "baseline\n",
      "                        time       eQ1       eQ2       eQ3        eK\n",
      "N_data Sigma Mode                                                   \n",
      "10     0     5     1.028e-02  8.13e-02  9.08e-02  1.00e-01  1.98e-02\n",
      "             10    1.061e-02  7.57e-02  8.60e-02  9.62e-02  2.68e-02\n",
      "             50    1.041e-02  7.51e-02  8.53e-02  9.55e-02  2.67e-02\n",
      "       1     5     1.036e-02  8.74e-02  9.72e-02  1.07e-01  5.22e-01\n",
      "             10    1.065e-02  7.27e-02  9.61e-02  1.20e-01  4.78e-01\n",
      "             50    1.055e-02  7.00e-02  9.73e-02  1.25e-01  2.52e+00\n",
      "       5     5     1.039e-02  1.00e-01  1.09e-01  1.17e-01  9.98e-01\n",
      "             10    1.053e-02  9.61e-02  1.06e-01  1.17e-01  9.95e-01\n",
      "             50    1.042e-02  1.04e-01  1.11e-01  1.19e-01  1.64e+00\n",
      "100    0     5     1.048e-02  1.18e-03  2.99e-03  9.95e-03  4.00e-02\n",
      "             10    1.057e-02  2.24e-03  4.29e-03  9.88e-03  9.64e-02\n",
      "             50    1.032e-02  2.08e-03  6.78e-03  1.62e-02  4.08e-02\n",
      "       1     5     1.068e-02  1.69e-02  2.07e-02  2.97e-02  3.74e-01\n",
      "             10    1.047e-02  1.79e-02  2.45e-02  3.40e-02  3.18e-01\n",
      "             50    1.056e-02  1.70e-02  2.21e-02  3.45e-02  3.50e-01\n",
      "       5     5     9.770e-03  1.17e-01  1.42e-01  1.71e-01  9.69e-01\n",
      "             10    1.035e-02  1.36e-01  1.81e-01  2.40e-01  9.70e-01\n",
      "             50    1.040e-02  1.08e-01  1.27e-01  1.81e-01  9.70e-01\n",
      "1000   0     5     8.854e-02  1.13e-04  1.60e-04  2.87e-04  3.55e-02\n",
      "             10    8.891e-02  1.93e-04  2.33e-04  3.35e-04  2.88e-02\n",
      "             50    8.881e-02  1.44e-04  1.82e-04  2.88e-04  2.39e-02\n",
      "       1     5     8.918e-02  1.17e-02  1.26e-02  1.36e-02  1.19e-01\n",
      "             10    8.894e-02  1.16e-02  1.25e-02  1.35e-02  7.91e-02\n",
      "             50    8.917e-02  1.17e-02  1.26e-02  1.35e-02  4.18e-02\n",
      "       5     5     9.064e-02  5.92e-02  6.49e-02  7.13e-02  9.09e-01\n",
      "             10    8.870e-02  5.91e-02  6.49e-02  7.12e-02  9.54e-01\n",
      "             50    8.958e-02  5.87e-02  6.50e-02  7.04e-02  7.93e-01\n",
      "Tiempo actual en segundos: 0.08957615963833383\n",
      "Tiempo actual en minutos: 0.0014929359939722306\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "N = [10, 100, 1000] \n",
    "R = [0, 1, 5]\n",
    "n_modes = [5, 10, 50]\n",
    "models = ['baseline', 'POD', 'autoencoder', 'fourier']\n",
    "models = ['autoencoder']\n",
    "\n",
    "for model_i in models:\n",
    "\n",
    "    print(model_i)\n",
    "\n",
    "    combinations = list(itertools.product(N, R, n_modes))\n",
    "    n_data_vals, ruido_vals, modos_vals = zip(*combinations)\n",
    "\n",
    "    # Create the errors table for each model\n",
    "    multi_index = pd.MultiIndex.from_arrays([n_data_vals, ruido_vals, modos_vals], names=[\"N_data\", \"Sigma\", \"Mode\"])\n",
    "    error_table = pd.DataFrame(index=multi_index, columns=[\"time\", \"eQ1\", \"eQ2\", \"eQ3\", \"eK\"])\n",
    "\n",
    "    for n_i in N:\n",
    "        for r_i in R:\n",
    "            for mode_i in n_modes:\n",
    "                \n",
    "                data_name = f'non_linear_{n_i}_{r_i}'\n",
    "                model_name = f'{model_i}_model_{mode_i}'\n",
    "                \n",
    "                ROOT_PATH = os.path.abspath(os.path.join(os.getcwd(), \"../\"))\n",
    "                DATA_PATH = os.path.join(ROOT_PATH, r'data/', data_name, data_name) + '.pkl'\n",
    "                RESULTS_FOLDER_PATH = os.path.join(ROOT_PATH, r'results/', data_name)\n",
    "\n",
    "\n",
    "                MODEL_RESULTS_PATH = os.path.join(ROOT_PATH, r'results/', data_name, model_name)\n",
    "\n",
    "                dataset = load_data(DATA_PATH)\n",
    "\n",
    "                # Train data splitting in train/test\n",
    "                X = torch.tensor(dataset['X_train'], dtype=torch.float32).unsqueeze(1)\n",
    "                y = torch.tensor(dataset['y_train'], dtype=torch.float32).unsqueeze(1)\n",
    "                K = torch.tensor(dataset['k_train'], dtype=torch.float32).unsqueeze(1)\n",
    "                f = torch.tensor(dataset['f_train'], dtype=torch.float32).unsqueeze(1)\n",
    "\n",
    "                X_train, X_test, y_train, y_test, K_train, K_test, f_train, f_test = train_test_split(X, y, K, f, test_size=0.3, random_state=42)\n",
    "\n",
    "                # Data processing and adequacy with our TensOps library\n",
    "                X_train = X_train.to(DEVICE)\n",
    "                X_test = X_test.to(DEVICE)\n",
    "\n",
    "                y_train = TensOps(y_train.to(DEVICE).requires_grad_(True), space_dimension=2, contravariance=0, covariance=0)\n",
    "                y_test = TensOps(y_test.to(DEVICE).requires_grad_(True), space_dimension=2, contravariance=0, covariance=0)\n",
    "\n",
    "                K_train = TensOps(K_train.to(DEVICE).requires_grad_(True), space_dimension=2, contravariance=0, covariance=0)\n",
    "                K_test = TensOps(K_test.to(DEVICE).requires_grad_(True), space_dimension=2, contravariance=0, covariance=0)\n",
    "\n",
    "                f_train = TensOps(f_train.to(DEVICE).requires_grad_(True), space_dimension=2, contravariance=0, covariance=0)\n",
    "                f_test = TensOps(f_test.to(DEVICE).requires_grad_(True), space_dimension=2, contravariance=0, covariance=0)\n",
    "\n",
    "                # Loading and processing validation data\n",
    "                X_val = torch.tensor(dataset['X_val'], dtype=torch.float32).unsqueeze(1)\n",
    "                y_val = TensOps(torch.tensor(dataset['y_val'], dtype=torch.float32, requires_grad=True).unsqueeze(1), space_dimension=2, contravariance=0, covariance=0)\n",
    "                K_val = TensOps(torch.tensor(dataset['k_val'], dtype=torch.float32, requires_grad=True).unsqueeze(1), space_dimension=2, contravariance=0, covariance=0)\n",
    "                f_val = TensOps(torch.tensor(dataset['f_val'], dtype=torch.float32, requires_grad=True).unsqueeze(1), space_dimension=2, contravariance=0, covariance=0)\n",
    "\n",
    "                # Predictive network architecture\n",
    "                input_shape = X_train[0].shape\n",
    "                predictive_layers = [20, 10, mode_i, 10, 20]\n",
    "                predictive_output = y_train.values[0].shape\n",
    "\n",
    "                # Explanatory network architecture\n",
    "                explanatory_input = Mx(My(y_train)).values[0].shape\n",
    "                explanatory_layers = [10]\n",
    "                explanatory_output = Mx(My(f_train)).values[0].shape\n",
    "\n",
    "                # Other parameters\n",
    "                n_filters_explanatory = 5\n",
    "\n",
    "                if model_i == 'baseline':\n",
    "\n",
    "                    try:\n",
    "                        model = PGNNIVBaseline(input_shape, predictive_layers, predictive_output, explanatory_input, explanatory_layers, explanatory_output, n_filters_explanatory).to(DEVICE)\n",
    "                        optimizer = torch.optim.Adam(model.parameters(), lr=1e-5)\n",
    "                        model, lists = load_results(model, optimizer, MODEL_RESULTS_PATH, map_location=DEVICE)\n",
    "\n",
    "                        time = np.cumsum(lists['time_list'])[-1]\n",
    "\n",
    "                        hyperparameters = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "                    except:\n",
    "                        pass\n",
    "\n",
    "                elif model_i == 'POD':\n",
    "                    try:\n",
    "                        if X_train.shape[0] < mode_i:\n",
    "                            continue\n",
    "\n",
    "                        U_train, S_train, Vt_train = torch.linalg.svd(y_train.values.detach().squeeze().to('cpu').view(y_train.values.detach().shape[0], -1), full_matrices=False)\n",
    "                        U_reduced_train = U_train[:, :mode_i]\n",
    "                        S_reduced_train = S_train[:mode_i]\n",
    "                        Vt_reduced_train = Vt_train[:mode_i, :]\n",
    "                        POD_base = Vt_reduced_train.to(DEVICE)\n",
    "\n",
    "                        model = PGNNIVPOD(input_shape, predictive_layers, POD_base, predictive_output, explanatory_input, explanatory_layers, explanatory_output, n_filters_explanatory).to(DEVICE)\n",
    "                        optimizer = torch.optim.Adam(model.parameters(), lr=1e-5)\n",
    "                        model, lists = load_results(model, optimizer, MODEL_RESULTS_PATH, map_location=DEVICE)\n",
    "\n",
    "                        with open(os.path.join(MODEL_RESULTS_PATH, \"time.txt\"), \"r\") as f:\n",
    "                            time_pod = float(f.read().strip())  # Usa float o int según lo que necesites\n",
    "\n",
    "                        time = np.cumsum(lists['time_list'])[-1] + time_pod\n",
    "\n",
    "                        hyperparameters = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "                    except:\n",
    "                        pass\n",
    "\n",
    "                elif model_i == 'fourier':\n",
    "                    try:\n",
    "\n",
    "                        X_mesh = torch.tensor(dataset['X_mesh'])\n",
    "                        Y_mesh = torch.tensor(dataset['Y_mesh'])\n",
    "\n",
    "                        base = compute_fourier_base(mode_i, X_mesh, Y_mesh)\n",
    "\n",
    "                        \n",
    "\n",
    "                        model = PGNNIVFourier(input_shape, predictive_layers, base, predictive_output, explanatory_input, explanatory_layers, explanatory_output, n_filters_explanatory, device=DEVICE).to(DEVICE)\n",
    "                        optimizer = torch.optim.Adam(model.parameters(), lr=1e-5)\n",
    "                        model, lists = load_results(model, optimizer, MODEL_RESULTS_PATH, map_location=DEVICE)\n",
    "\n",
    "                        time = np.cumsum(lists['time_list'])[-1]\n",
    "\n",
    "                        hyperparameters = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "                    \n",
    "                    except:\n",
    "                        pass\n",
    "\n",
    "                elif model_i == 'autoencoder':\n",
    "                    try:\n",
    "                        MODEL_RESULTS_AE_PATH = os.path.join(ROOT_PATH, r'results/', data_name, model_name) + '_AE'\n",
    "                        MODEL_RESULTS_PGNNIV_PATH = os.path.join(ROOT_PATH, r'results/', data_name, model_name) + '_NN'\n",
    "\n",
    "                        autoencoder_input_shape = y_train.values[0].shape\n",
    "                        latent_space_dim = [20, 10, mode_i, 10, 20]\n",
    "                        autoencoder_output_shape = y_train.values[0].shape\n",
    "\n",
    "                        autoencoder = Autoencoder(autoencoder_input_shape, latent_space_dim, autoencoder_output_shape).to(DEVICE)\n",
    "                        optimizer = torch.optim.Adam(autoencoder.parameters(), lr=1e-4)\n",
    "\n",
    "                        autoencoder, lists = load_results(autoencoder, optimizer, MODEL_RESULTS_AE_PATH, map_location=torch.device('cpu'))\n",
    "\n",
    "                        hyperparameters_ae = sum(p.numel() for p in autoencoder.parameters() if p.requires_grad)\n",
    "\n",
    "                        time_ae = np.cumsum(lists['time_list'])[-1]\n",
    "\n",
    "                        pretrained_encoder = autoencoder.encoder\n",
    "                        pretrained_decoder = autoencoder.decoder\n",
    "\n",
    "                        for param in pretrained_decoder.parameters():\n",
    "                            param.requires_grad = False\n",
    "\n",
    "                        pgnniv_model = PGNNIVAutoencoder(input_shape, predictive_layers, pretrained_decoder, predictive_output, explanatory_input,\n",
    "                                                        explanatory_layers, explanatory_output, n_filters_explanatory).to(DEVICE)\n",
    "                        optimizer = torch.optim.Adam(pgnniv_model.parameters(), lr=1e-4)\n",
    "\n",
    "                        model, lists = load_results(pgnniv_model, optimizer, MODEL_RESULTS_PGNNIV_PATH, map_location=torch.device('cpu'))\n",
    "                        optimizer = torch.optim.Adam(model.parameters(), lr=1e-5)\n",
    "                        model, lists = load_results(model, optimizer, MODEL_RESULTS_PGNNIV_PATH, map_location=DEVICE)\n",
    "\n",
    "                        time = time_ae + np.cumsum(lists['time_list'])[-1]\n",
    "\n",
    "                        hyperparameters = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "                    except:\n",
    "                        pass\n",
    "\n",
    "                u_train = y_val.values.detach().numpy() \n",
    "                u_predicted_train = model(X_val)[0].detach().numpy() \n",
    "                er_u_train = relative_error(u_train, u_predicted_train).flatten()\n",
    "\n",
    "                erQ1_u = np.percentile(er_u_train, 25)\n",
    "                erQ2_u = np.percentile(er_u_train, 50)\n",
    "                erQ3_u = np.percentile(er_u_train, 75)\n",
    "\n",
    "                u_min = u_train.flatten().min()\n",
    "                u_max = u_train.flatten().max()\n",
    "                steps = 1000\n",
    "                u_for_validating = torch.linspace(u_min, u_max, steps=steps).view(steps, 1, 1, 1)\n",
    "                K_for_validating = (u_for_validating*(1-u_for_validating)).detach().cpu().numpy()\n",
    "                K_predicted_for_validating = model.explanatory(u_for_validating.to(DEVICE)).detach().cpu().numpy()\n",
    "                diff_squared = (K_predicted_for_validating - K_for_validating) ** 2\n",
    "                true_squared = K_for_validating ** 2\n",
    "                u_vals = u_for_validating.numpy().flatten()\n",
    "                numerator = np.sqrt(np.trapz(diff_squared.flatten(), u_vals))\n",
    "                denominator = np.sqrt(np.trapz(true_squared.flatten(), u_vals))\n",
    "\n",
    "                er_K_train = numerator / denominator\n",
    "\n",
    "\n",
    "                # K_train_ = Mx(My(K_train)).values.detach().numpy() \n",
    "                # K_predicted_train = model(X_train)[1].detach().numpy()\n",
    "                # er_K_train = er_sum(K_train_, K_predicted_train)\n",
    "                tiempo_minutos = time / 60\n",
    "\n",
    "                idx = (n_i, r_i, mode_i)\n",
    "                # error_table.loc[idx, \"Hyperparameters\"] = hyperparameters\n",
    "                error_table.loc[idx, \"time\"] = f\"{tiempo_minutos:.2f}\"\n",
    "                error_table.loc[idx, \"eQ1\"] = f\"{erQ1_u:.2e}\"\n",
    "                error_table.loc[idx, \"eQ2\"] = f\"{erQ2_u:.2e}\"\n",
    "                error_table.loc[idx, \"eQ3\"] = f\"{erQ3_u:.2e}\"\n",
    "                error_table.loc[idx, \"eK\"] = f\"{er_K_train:.2e}\"\n",
    "                \n",
    "\n",
    "    print(model_i)\n",
    "    print(error_table)\n",
    "    tiempo_segundos = time\n",
    "    tiempo_minutos = tiempo_segundos / 60\n",
    "    print(f\"Tiempo actual en segundos: {tiempo_segundos}\")\n",
    "    print(f\"Tiempo actual en minutos: {tiempo_minutos}\")\n",
    "    print(\"\\n\")\n",
    "                \n",
    "\n",
    "    tabla_latex = dataframe_a_latex(error_table, \n",
    "                                    nombre_archivo=os.path.join(os.getcwd(), \"error_tables\", f\"error_{model_i}\"), \n",
    "                                    index=multi_index,\n",
    "                                    caption=\"Tabla con formato LaTeX\",\n",
    "                                    label=\"tab:formateada\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fe92d7d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb69349c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SciML_test_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
