{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc4cce7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys \n",
    "\n",
    "current_directory = os.getcwd()\n",
    "models_directory = os.path.abspath(os.path.join(current_directory, '..'))\n",
    "sys.path.append(models_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8da2b649",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "import json\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import itertools\n",
    "\n",
    "import matplotlib.colors as mcolors\n",
    "import matplotlib.ticker as ticker\n",
    "from matplotlib.ticker import ScalarFormatter\n",
    "from mpl_toolkits.axes_grid1.inset_locator import inset_axes\n",
    "from matplotlib.ticker import LogLocator, LogFormatter\n",
    "from matplotlib.lines import Line2D\n",
    "\n",
    "# Imports de la libreria propia\n",
    "from vecopsciml.kernels.derivative import DerivativeKernels\n",
    "from vecopsciml.utils import TensOps\n",
    "from vecopsciml.operators import zero_order as zo\n",
    "from vecopsciml.algebra import zero_order as azo\n",
    "\n",
    "# Imports de las funciones creadas para este programa\n",
    "from utils.folders import create_folder\n",
    "from utils.load_data import load_data\n",
    "from utils.checkpoints import load_results\n",
    "\n",
    "from vecopsciml.operators.zero_order import Mx, My"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2aafa1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.baseline.model import BaselineNonlinearModel\n",
    "from models.autoencoder.model import Autoencoder, AutoencoderNonlinearModel\n",
    "from models.fourier.model import FFTNonlinearModel\n",
    "from models.POD.model import PODNonlinearModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9c4fd08",
   "metadata": {},
   "outputs": [],
   "source": [
    "def smooth_curve(data, window_size=1000):\n",
    "    window = np.ones(window_size) / window_size\n",
    "    return np.convolve(data, window, mode='valid')\n",
    "\n",
    "def cm_to_in(cm):\n",
    "    return cm * 0.393701\n",
    "\n",
    "def normalize_list(lst):\n",
    "    max_value = np.max(lst)\n",
    "    return [x / max_value for x in lst]\n",
    "\n",
    "linewidth = 1.5  \n",
    "title_fontsize = 14  \n",
    "label_fontsize = 14  \n",
    "legend_fontsize = 12 \n",
    "tick_fontsize = 11  \n",
    "\n",
    "# plt.rc('text', usetex=True)\n",
    "plt.rc('font', family='serif')\n",
    "\n",
    "posX = cm_to_in(10) # posición de la esquina inferior izquierda de la imagen en X\n",
    "posY = cm_to_in(10) # posición de la esquina inferior izquierda de la imagen en Y\n",
    "width = cm_to_in(12)  # ancho de la imagen\n",
    "height = cm_to_in(8) # alto de la imagen\n",
    "\n",
    "color = [0.1, 0, 0.8]  # triplete RGB, valores entre 0 y 1\n",
    "subplot_adjust_left = cm_to_in(0.15)\n",
    "subplot_adjust_bottom = cm_to_in(0.15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dd069ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_name = 'non_linear_5000_1'\n",
    "n_modes = 50\n",
    "\n",
    "# Creamos los paths para las distintas carpetas\n",
    "ROOT_PATH = r'/home/rmunoz/Escritorio/rmunozTMELab/Physically-Guided-Machine-Learning'\n",
    "DATA_PATH = os.path.join(ROOT_PATH, r'data/', data_name, data_name) + '.pkl'\n",
    "RESULTS_FOLDER_PATH = os.path.join(ROOT_PATH, r'results/', data_name)\n",
    "MODEL_RESULTS_PATH = os.path.join(ROOT_PATH, r'results/', data_name, 'baseline_model_') + str(n_modes)\n",
    "\n",
    "# Creamos las carpetas que sean necesarias (si ya están creadas se avisará de ello)\n",
    "create_folder(RESULTS_FOLDER_PATH)\n",
    "create_folder(MODEL_RESULTS_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "295f7b0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_data(DATA_PATH)\n",
    "\n",
    "# Convolutional filters to derivate\n",
    "dx = dataset['x_step_size']\n",
    "dy = dataset['y_step_size']\n",
    "D = DerivativeKernels(dx, dy, 0).grad_kernels_two_dimensions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb9f8269",
   "metadata": {},
   "outputs": [],
   "source": [
    "def relative_error(validation, prediction, dx=dx, dy=dy):\n",
    "\n",
    "    # validation = validation.numpy()\n",
    "    # prediction = prediction.numpy()\n",
    "\n",
    "    prediction_error = np.sqrt((np.trapz(np.trapz((validation - prediction)**2, dx=dy), dx=dx) /\n",
    "                                np.trapz(np.trapz((validation)**2, dx=dy), dx=dx)))\n",
    "\n",
    "    return prediction_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdde2b47",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d333e495",
   "metadata": {},
   "outputs": [],
   "source": [
    "def er_sum(reshaped_predicted_K, reshaped_validation_K, dx=dx, dy=dy):\n",
    "\n",
    "    numerador = np.sum((np.trapz(np.trapz(reshaped_predicted_K - reshaped_validation_K, dx=dx), dx=dx))**2)\n",
    "    denominador = np.sum((np.trapz(np.trapz(reshaped_validation_K, dx=dx), dx=dx))**2)\n",
    "    \n",
    "    er = numerador  / denominador\n",
    "\n",
    "    return er\n",
    "    \n",
    "    print(f\"er(K): {er:.2e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a9caa2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataframe_a_latex(df, nombre_archivo=None, index=False, caption=None, label=None):\n",
    "    \"\"\"\n",
    "    Convierte un DataFrame en una tabla LaTeX.\n",
    "    \n",
    "    Args:\n",
    "        df (pd.DataFrame): DataFrame a convertir.\n",
    "        nombre_archivo (str): Ruta para guardar el archivo .tex (opcional).\n",
    "        index (bool): Si se incluye o no el índice.\n",
    "        caption (str): Título de la tabla (opcional).\n",
    "        label (str): Etiqueta de la tabla para referencia cruzada (opcional).\n",
    "    \n",
    "    Returns:\n",
    "        str: La cadena en formato LaTeX.\n",
    "    \"\"\"\n",
    "    latex_str = df.to_latex(index=index, caption=caption, label=label, escape=False)\n",
    "\n",
    "    if nombre_archivo:\n",
    "        with open(nombre_archivo, 'w', encoding='utf-8') as f:\n",
    "            f.write(latex_str)\n",
    "    \n",
    "    return latex_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba60f0bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# N = [10, 100, 1000] \n",
    "# R = [0, 1, 5]\n",
    "# n_modes = [5, 10, 50]\n",
    "\n",
    "N = [1000] \n",
    "R = [0]\n",
    "n_modes = [20]\n",
    "models = ['baseline_model_', 'POD_model_', 'model_autoencoder_NN_', 'FFT_model_']\n",
    "\n",
    "for model_i in models:\n",
    "    print(model_i)\n",
    "    combinations = list(itertools.product(N, R, n_modes))\n",
    "    n_data_vals, ruido_vals, modos_vals = zip(*combinations)\n",
    "    multi_index = pd.MultiIndex.from_arrays([n_data_vals, ruido_vals, modos_vals], names=[\"N_data\", \"Sigma\", \"Mode\"])\n",
    "\n",
    "    error_table = pd.DataFrame(index=multi_index, columns=[\"Hyperparameters\", \"time\", \"eQ1\", \"eQ2\", \"eQ3\", \"eK\"])\n",
    "\n",
    "    for n_i in N:\n",
    "        for r_i in R:\n",
    "            for mode_i in n_modes:\n",
    "                \n",
    "                data_name = f'non_linear_{n_i}_{r_i}'\n",
    "\n",
    "                # Creamos los paths para las distintas carpetas\n",
    "                ROOT_PATH = r'/home/rmunoz/Escritorio/rmunozTMELab/Physically-Guided-Machine-Learning'\n",
    "                DATA_PATH = os.path.join(ROOT_PATH, r'data/', data_name, data_name) + '.pkl'\n",
    "                RESULTS_FOLDER_PATH = os.path.join(ROOT_PATH, r'results/', data_name)\n",
    "                MODEL_RESULTS_PATH = os.path.join(ROOT_PATH, r'results/', data_name, model_i) + str(mode_i)\n",
    "\n",
    "                dataset = load_data(DATA_PATH)\n",
    "\n",
    "                # Train data splitting in train/test\n",
    "                X = torch.tensor(dataset['X_train'], dtype=torch.float32).unsqueeze(1)\n",
    "                y = torch.tensor(dataset['y_train'], dtype=torch.float32).unsqueeze(1)\n",
    "                K = torch.tensor(dataset['k_train'], dtype=torch.float32).unsqueeze(1)\n",
    "                f = torch.tensor(dataset['f_train'], dtype=torch.float32).unsqueeze(1)\n",
    "\n",
    "                X_train, X_test, y_train, y_test, K_train, K_test, f_train, f_test = train_test_split(X, y, K, f, test_size=0.3, random_state=42)\n",
    "\n",
    "                # Data processing and adequacy with our TensOps library\n",
    "                X_train = X_train.to(DEVICE)\n",
    "                X_test = X_test.to(DEVICE)\n",
    "\n",
    "                y_train = TensOps(y_train.to(DEVICE).requires_grad_(True), space_dimension=2, contravariance=0, covariance=0)\n",
    "                y_test = TensOps(y_test.to(DEVICE).requires_grad_(True), space_dimension=2, contravariance=0, covariance=0)\n",
    "\n",
    "                K_train = TensOps(K_train.to(DEVICE).requires_grad_(True), space_dimension=2, contravariance=0, covariance=0)\n",
    "                K_test = TensOps(K_test.to(DEVICE).requires_grad_(True), space_dimension=2, contravariance=0, covariance=0)\n",
    "\n",
    "                f_train = TensOps(f_train.to(DEVICE).requires_grad_(True), space_dimension=2, contravariance=0, covariance=0)\n",
    "                f_test = TensOps(f_test.to(DEVICE).requires_grad_(True), space_dimension=2, contravariance=0, covariance=0)\n",
    "\n",
    "                # Loading and processing validation data\n",
    "                X_val = torch.tensor(dataset['X_val'], dtype=torch.float32).unsqueeze(1)\n",
    "                y_val = TensOps(torch.tensor(dataset['y_val'], dtype=torch.float32, requires_grad=True).unsqueeze(1), space_dimension=2, contravariance=0, covariance=0)\n",
    "                K_val = TensOps(torch.tensor(dataset['k_val'], dtype=torch.float32, requires_grad=True).unsqueeze(1), space_dimension=2, contravariance=0, covariance=0)\n",
    "                f_val = TensOps(torch.tensor(dataset['f_val'], dtype=torch.float32, requires_grad=True).unsqueeze(1), space_dimension=2, contravariance=0, covariance=0)\n",
    "\n",
    "                # Predictive network architecture\n",
    "                input_shape = X_train[0].shape\n",
    "                predictive_layers = [20, 10, mode_i, 10, 20]\n",
    "                predictive_output = y_train.values[0].shape\n",
    "\n",
    "                # Explanatory network architecture\n",
    "                explanatory_input = Mx(My(y_train)).values[0].shape\n",
    "                explanatory_layers = [10]\n",
    "                explanatory_output = Mx(My(f_train)).values[0].shape\n",
    "\n",
    "                # Other parameters\n",
    "                n_filters_explanatory = 5\n",
    "\n",
    "                \n",
    "                if model_i == 'baseline_model_':\n",
    "                    try:\n",
    "                        model = BaselineNonlinearModel(input_shape, predictive_layers, predictive_output, explanatory_input, explanatory_layers, explanatory_output, n_filters_explanatory).to(DEVICE)\n",
    "                        optimizer = torch.optim.Adam(model.parameters(), lr=1e-5)\n",
    "                        model, optimizer, lists = load_results(model, optimizer, MODEL_RESULTS_PATH, map_location=DEVICE)\n",
    "\n",
    "                        time = np.cumsum(lists['time_list'])[-1]\n",
    "\n",
    "                        hyperparameters = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "                    except:\n",
    "                        pass\n",
    "\n",
    "                elif model_i == 'POD_model_':\n",
    "                    try:\n",
    "                        if X_train.shape[0] < mode_i:\n",
    "                            continue\n",
    "\n",
    "                        U_train, S_train, Vt_train = torch.linalg.svd(y_train.values.detach().squeeze().to('cpu').view(y_train.values.detach().shape[0], -1), full_matrices=False)\n",
    "                        U_reduced_train = U_train[:, :mode_i]\n",
    "                        S_reduced_train = S_train[:mode_i]\n",
    "                        Vt_reduced_train = Vt_train[:mode_i, :]\n",
    "                        POD_base = Vt_reduced_train.to(DEVICE)\n",
    "\n",
    "                        model = PODNonlinearModel(input_shape, predictive_layers, POD_base, predictive_output, explanatory_input, explanatory_layers, explanatory_output, n_filters_explanatory).to(DEVICE)\n",
    "                        optimizer = torch.optim.Adam(model.parameters(), lr=1e-5)\n",
    "                        model, optimizer, lists = load_results(model, optimizer, MODEL_RESULTS_PATH, map_location=DEVICE)\n",
    "\n",
    "                        with open(os.path.join(MODEL_RESULTS_PATH, \"time.txt\"), \"r\") as f:\n",
    "                            time_pod = float(f.read().strip())  # Usa float o int según lo que necesites\n",
    "\n",
    "                        time = np.cumsum(lists['time_list'])[-1] + time_pod\n",
    "\n",
    "                        hyperparameters = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "                    except:\n",
    "                        pass\n",
    "\n",
    "                elif model_i == 'FFT_model_':\n",
    "                    try:\n",
    "                        model = FFTNonlinearModel(input_shape, predictive_layers, mode_i, predictive_output, explanatory_input, explanatory_layers, explanatory_output, n_filters_explanatory, device=DEVICE).to(DEVICE)\n",
    "                        optimizer = torch.optim.Adam(model.parameters(), lr=1e-5)\n",
    "                        model, optimizer, lists = load_results(model, optimizer, MODEL_RESULTS_PATH, map_location=DEVICE)\n",
    "\n",
    "                        time = np.cumsum(lists['time_list'])[-1]\n",
    "\n",
    "                        hyperparameters = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "                    \n",
    "                    except:\n",
    "                        pass\n",
    "\n",
    "                elif model_i == 'model_autoencoder_NN_':\n",
    "                    try:\n",
    "                        MODEL_RESULTS_AE_PATH = os.path.join(ROOT_PATH, r'results/', data_name, 'model_autoencoder_AE_') + str(mode_i)\n",
    "                        MODEL_RESULTS_PGNNIV_PATH = os.path.join(ROOT_PATH, r'results/', data_name, 'model_autoencoder_NN_') + str(mode_i)\n",
    "\n",
    "                        autoencoder_input_shape = y_train.values[0].shape\n",
    "                        latent_space_dim = [20, 10, mode_i, 10, 20]\n",
    "                        autoencoder_output_shape = y_train.values[0].shape\n",
    "\n",
    "                        autoencoder = Autoencoder(autoencoder_input_shape, latent_space_dim, autoencoder_output_shape).to(DEVICE)\n",
    "                        optimizer = torch.optim.Adam(autoencoder.parameters(), lr=1e-4)\n",
    "\n",
    "                        autoencoder, optimizer, lists = load_results(autoencoder, optimizer, MODEL_RESULTS_AE_PATH, map_location=torch.device('cpu'))\n",
    "\n",
    "                        hyperparameters_ae = sum(p.numel() for p in autoencoder.parameters() if p.requires_grad)\n",
    "\n",
    "                        time_ae = np.cumsum(lists['time_list'])[-1]\n",
    "\n",
    "                        pretrained_encoder = autoencoder.encoder\n",
    "                        pretrained_decoder = autoencoder.decoder\n",
    "\n",
    "                        for param in pretrained_decoder.parameters():\n",
    "                            param.requires_grad = False\n",
    "\n",
    "                        pgnniv_model = AutoencoderNonlinearModel(input_shape, predictive_layers, pretrained_decoder, predictive_output, explanatory_input,\n",
    "                                                        explanatory_layers, explanatory_output, n_filters_explanatory).to(DEVICE)\n",
    "                        optimizer = torch.optim.Adam(pgnniv_model.parameters(), lr=1e-4)\n",
    "\n",
    "                        model, optimizer, lists = load_results(pgnniv_model, optimizer, MODEL_RESULTS_PGNNIV_PATH, map_location=torch.device('cpu'))\n",
    "                        optimizer = torch.optim.Adam(model.parameters(), lr=1e-5)\n",
    "                        model, optimizer, lists = load_results(model, optimizer, MODEL_RESULTS_PATH, map_location=DEVICE)\n",
    "\n",
    "                        time = time_ae + np.cumsum(lists['time_list'])[-1]\n",
    "\n",
    "                        hyperparameters = hyperparameters_ae + sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "                        \n",
    "                    except:\n",
    "                        pass\n",
    "                \n",
    "                u_train = y_train.values.detach().numpy() \n",
    "            \n",
    "                u_train = y_train.values.detach().numpy() \n",
    "                u_predicted_train = model(X_train)[0].detach().numpy() \n",
    "                er_u_train = relative_error(u_train, u_predicted_train).flatten()\n",
    "\n",
    "                erQ1_u = np.percentile(er_u_train, 25)\n",
    "                erQ2_u = np.percentile(er_u_train, 50)\n",
    "                erQ3_u = np.percentile(er_u_train, 75)\n",
    "\n",
    "\n",
    "\n",
    "                u_min = u_train.flatten().min()\n",
    "                u_max = u_train.flatten().max()\n",
    "                steps = 1000\n",
    "                u_for_validating = torch.linspace(u_min, u_max, steps=steps).view(steps, 1, 1, 1)\n",
    "                K_for_validating = (u_for_validating*(1-u_for_validating)).detach().cpu().numpy()\n",
    "                K_predicted_for_validating = model.explanatory(u_for_validating.to(DEVICE)).detach().cpu().numpy()\n",
    "                diff_squared = (K_predicted_for_validating - K_for_validating) ** 2\n",
    "                true_squared = K_for_validating ** 2\n",
    "                u_vals = u_for_validating.numpy().flatten()\n",
    "                numerator = np.sqrt(np.trapz(diff_squared.flatten(), u_vals))\n",
    "                denominator = np.sqrt(np.trapz(true_squared.flatten(), u_vals))\n",
    "\n",
    "                er_K_train = numerator / denominator\n",
    "\n",
    "\n",
    "                # K_train_ = Mx(My(K_train)).values.detach().numpy() \n",
    "                # K_predicted_train = model(X_train)[1].detach().numpy()\n",
    "                # er_K_train = er_sum(K_train_, K_predicted_train)\n",
    "\n",
    "                tiempo_minutos = time / 60\n",
    "\n",
    "                idx = (n_i, r_i, mode_i)\n",
    "                error_table.loc[idx, \"Hyperparameters\"] = hyperparameters\n",
    "                error_table.loc[idx, \"time\"] = f\"{tiempo_minutos:.2f}\"\n",
    "                error_table.loc[idx, \"eQ1\"] = f\"{erQ1_u:.2e}\"\n",
    "                error_table.loc[idx, \"eQ2\"] = f\"{erQ2_u:.2e}\"\n",
    "                error_table.loc[idx, \"eQ3\"] = f\"{erQ3_u:.2e}\"\n",
    "                error_table.loc[idx, \"eK\"] = f\"{er_K_train:.2e}\"\n",
    "                \n",
    "\n",
    "    print(model_i)\n",
    "    print(error_table)\n",
    "    tiempo_segundos = time\n",
    "    tiempo_minutos = tiempo_segundos / 60\n",
    "    print(f\"Tiempo actual en segundos: {tiempo_segundos}\")\n",
    "    print(f\"Tiempo actual en minutos: {tiempo_minutos}\")\n",
    "    print(\"\\n\")\n",
    "                \n",
    "\n",
    "    tabla_latex = dataframe_a_latex(error_table, \n",
    "                                    nombre_archivo=os.path.join(os.getcwd(), \"error_tables\", f\"error_{model_i}\"), \n",
    "                                    index=multi_index,\n",
    "                                    caption=\"Tabla con formato LaTeX\",\n",
    "                                    label=\"tab:formateada\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "268e2ac1",
   "metadata": {},
   "outputs": [],
   "source": [
    "fgasdfasd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76902fa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "dsdfsadf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8dd3806",
   "metadata": {},
   "outputs": [],
   "source": [
    "N = [10, 20, 50, 100, 1000, 5000, 10000] \n",
    "R = [0, 1, 5, 10]\n",
    "n_modes = [0, 1, 2, 5, 10, 20, 50, 100]\n",
    "\n",
    "\n",
    "model_i = 'model_autoencoder_NN_'\n",
    "data_name = f'non_linear_{N}_{R}'\n",
    "\n",
    "\n",
    "mode_i = n_modes\n",
    "\n",
    "# Creamos los paths para las distintas carpetas\n",
    "ROOT_PATH = r'/home/rmunoz/Escritorio/rmunozTMELab/Physically-Guided-Machine-Learning'\n",
    "DATA_PATH = os.path.join(ROOT_PATH, r'data/', data_name, data_name) + '.pkl'\n",
    "RESULTS_FOLDER_PATH = os.path.join(ROOT_PATH, r'results/', data_name)\n",
    "MODEL_RESULTS_PATH = os.path.join(ROOT_PATH, r'results/', data_name, model_i) + str(n_modes)\n",
    "\n",
    "dataset = load_data(DATA_PATH)\n",
    "\n",
    "# Train data splitting in train/test\n",
    "X = torch.tensor(dataset['X_train'], dtype=torch.float32).unsqueeze(1)\n",
    "y = torch.tensor(dataset['y_train'], dtype=torch.float32).unsqueeze(1)\n",
    "K = torch.tensor(dataset['k_train'], dtype=torch.float32).unsqueeze(1)\n",
    "f = torch.tensor(dataset['f_train'], dtype=torch.float32).unsqueeze(1)\n",
    "\n",
    "X_train, X_test, y_train, y_test, K_train, K_test, f_train, f_test = train_test_split(X, y, K, f, test_size=0.3, random_state=42)\n",
    "\n",
    "# Data processing and adequacy with our TensOps library\n",
    "X_train = X_train.to(DEVICE)\n",
    "X_test = X_test.to(DEVICE)\n",
    "\n",
    "y_train = TensOps(y_train.to(DEVICE).requires_grad_(True), space_dimension=2, contravariance=0, covariance=0)\n",
    "y_test = TensOps(y_test.to(DEVICE).requires_grad_(True), space_dimension=2, contravariance=0, covariance=0)\n",
    "\n",
    "K_train = TensOps(K_train.to(DEVICE).requires_grad_(True), space_dimension=2, contravariance=0, covariance=0)\n",
    "K_test = TensOps(K_test.to(DEVICE).requires_grad_(True), space_dimension=2, contravariance=0, covariance=0)\n",
    "\n",
    "f_train = TensOps(f_train.to(DEVICE).requires_grad_(True), space_dimension=2, contravariance=0, covariance=0)\n",
    "f_test = TensOps(f_test.to(DEVICE).requires_grad_(True), space_dimension=2, contravariance=0, covariance=0)\n",
    "\n",
    "# Loading and processing validation data\n",
    "X_val = torch.tensor(dataset['X_val'], dtype=torch.float32).unsqueeze(1)\n",
    "y_val = TensOps(torch.tensor(dataset['y_val'], dtype=torch.float32, requires_grad=True).unsqueeze(1), space_dimension=2, contravariance=0, covariance=0)\n",
    "K_val = TensOps(torch.tensor(dataset['k_val'], dtype=torch.float32, requires_grad=True).unsqueeze(1), space_dimension=2, contravariance=0, covariance=0)\n",
    "f_val = TensOps(torch.tensor(dataset['f_val'], dtype=torch.float32, requires_grad=True).unsqueeze(1), space_dimension=2, contravariance=0, covariance=0)\n",
    "\n",
    "# Predictive network architecture\n",
    "input_shape = X_train[0].shape\n",
    "predictive_layers = [20, 10, n_modes, 10, 20]\n",
    "predictive_output = y_train.values[0].shape\n",
    "\n",
    "# Explanatory network architecture\n",
    "explanatory_input = Mx(My(y_train)).values[0].shape\n",
    "explanatory_layers = [10]\n",
    "explanatory_output = Mx(My(f_train)).values[0].shape\n",
    "\n",
    "# Other parameters\n",
    "n_filters_explanatory = 5\n",
    "\n",
    "\n",
    "if model_i == 'baseline_model_':\n",
    "    \n",
    "    color = 'black'\n",
    "\n",
    "    model = BaselineNonlinearModel(input_shape, predictive_layers, predictive_output, explanatory_input, explanatory_layers, explanatory_output, n_filters_explanatory).to(DEVICE)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-5)\n",
    "    model, optimizer, lists = load_results(model, optimizer, MODEL_RESULTS_PATH, map_location=DEVICE)\n",
    "\n",
    "    save_dir = f'/home/rmunoz/Escritorio/rmunozTMELab/Physically-Guided-Machine-Learning/paper/Baseline_model/K_u_BL_N{N}S{R}M{mode_i}'    \n",
    "\n",
    "elif model_i == 'POD_model_':\n",
    "    try:\n",
    "        color = 'green'\n",
    "\n",
    "        U_train, S_train, Vt_train = torch.linalg.svd(y_train.values.detach().squeeze().to('cpu').view(y_train.values.detach().shape[0], -1), full_matrices=False)\n",
    "        U_reduced_train = U_train[:, :mode_i]\n",
    "        S_reduced_train = S_train[:mode_i]\n",
    "        Vt_reduced_train = Vt_train[:mode_i, :]\n",
    "        POD_base = Vt_reduced_train.to(DEVICE)\n",
    "        model = PODNonlinearModel(input_shape, predictive_layers, POD_base, predictive_output, explanatory_input, explanatory_layers, explanatory_output, n_filters_explanatory).to(DEVICE)\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=1e-5)\n",
    "        model, optimizer, lists = load_results(model, optimizer, MODEL_RESULTS_PATH, map_location=DEVICE)\n",
    "\n",
    "        with open(os.path.join(MODEL_RESULTS_PATH, \"time.txt\"), \"r\") as f:\n",
    "            time_pod = float(f.read().strip())  # Usa float o int según lo que necesites\n",
    "\n",
    "        save_dir = f'/home/rmunoz/Escritorio/rmunozTMELab/Physically-Guided-Machine-Learning/paper/POD_model/K_u_POD_N{N}S{R}M{mode_i}'\n",
    "\n",
    "\n",
    "    except:\n",
    "        pass\n",
    "elif model_i == 'FFT_model_':\n",
    " \n",
    "    color = 'blue'\n",
    "\n",
    "    model = FFTNonlinearModel(input_shape, predictive_layers, mode_i, predictive_output, explanatory_input, explanatory_layers, explanatory_output, n_filters_explanatory, device=DEVICE).to(DEVICE)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-5)\n",
    "    model, optimizer, lists = load_results(model, optimizer, MODEL_RESULTS_PATH, map_location=DEVICE)\n",
    "\n",
    "    save_dir = f'/home/rmunoz/Escritorio/rmunozTMELab/Physically-Guided-Machine-Learning/paper/FFT_model/K_u_FFT_N{N}S{R}M{mode_i}'\n",
    "\n",
    "\n",
    "elif model_i == 'model_autoencoder_NN_':\n",
    "\n",
    "    color = 'red'\n",
    "\n",
    "    MODEL_RESULTS_AE_PATH = os.path.join(ROOT_PATH, r'results/', data_name, 'model_autoencoder_AE_') + str(n_modes)\n",
    "    MODEL_RESULTS_PGNNIV_PATH = os.path.join(ROOT_PATH, r'results/', data_name, 'model_autoencoder_NN_') + str(n_modes)\n",
    "\n",
    "    autoencoder_input_shape = y_train.values[0].shape\n",
    "    latent_space_dim = [20, 10, n_modes, 10, 20]\n",
    "    autoencoder_output_shape = y_train.values[0].shape\n",
    "\n",
    "    autoencoder = Autoencoder(autoencoder_input_shape, latent_space_dim, autoencoder_output_shape).to(DEVICE)\n",
    "    optimizer = torch.optim.Adam(autoencoder.parameters(), lr=1e-4)\n",
    "\n",
    "    autoencoder, optimizer, lists = load_results(autoencoder, optimizer, MODEL_RESULTS_AE_PATH, map_location=torch.device('cpu'))\n",
    "\n",
    "    time_ae = np.cumsum(lists['time_list'])[-1]\n",
    "\n",
    "    pretrained_encoder = autoencoder.encoder\n",
    "    pretrained_decoder = autoencoder.decoder\n",
    "\n",
    "    pgnniv_model = AutoencoderNonlinearModel(input_shape, predictive_layers, pretrained_decoder, predictive_output, explanatory_input,\n",
    "                                    explanatory_layers, explanatory_output, n_filters_explanatory).to(DEVICE)\n",
    "    optimizer = torch.optim.Adam(pgnniv_model.parameters(), lr=1e-4)\n",
    "\n",
    "    model, optimizer, lists = load_results(pgnniv_model, optimizer, MODEL_RESULTS_PGNNIV_PATH, map_location=torch.device('cpu'))\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-5)\n",
    "    model, optimizer, lists = load_results(model, optimizer, MODEL_RESULTS_PATH, map_location=DEVICE)\n",
    "\n",
    "    save_dir = f'/home/rmunoz/Escritorio/rmunozTMELab/Physically-Guided-Machine-Learning/paper/Autoencoder_model/K_u_AE_N{N}S{R}M{mode_i}'\n",
    "\n",
    "\n",
    "\n",
    "else:\n",
    "    raise ValueError(\"Not available.\")   \n",
    "\n",
    "\n",
    "u_train = y_train.values.detach().numpy() \n",
    "u_predicted_train = model(X_train)[0].detach().numpy() \n",
    "er_u_train = relative_error(u_train, u_predicted_train).flatten()\n",
    "\n",
    "K_train_ = Mx(My(K_train)).values.detach().numpy() \n",
    "K_predicted_train = model(X_train)[1].detach().numpy() \n",
    "er_K_train = relative_error(K_train_, K_predicted_train).flatten()\n",
    "\n",
    "er_K_train = er_sum(K_train_, K_predicted_train)\n",
    "\n",
    "width = cm_to_in(12)  # ancho de la imagen\n",
    "height = cm_to_in(8) # alto de la imagen\n",
    "\n",
    "plt.figure(figsize=(height, height))\n",
    "plt.scatter(u_train.flatten(), ((K_train)).values.detach().numpy().flatten(), label='Prediction', color='red', s=5, alpha=0.3)\n",
    "plt.scatter(Mx(My(y_train)).values.detach().numpy() , K_predicted_train.flatten(), label='Validation', color='black', s=3, alpha=0.5)\n",
    "\n",
    "plt.xlabel('$u(x, y)$', fontsize=label_fontsize)\n",
    "plt.ylabel('$k(u)$', fontsize=label_fontsize)\n",
    "# plt.title('Lineal homogeneous normalized learning curves', fontsize=s)\n",
    "\n",
    "mantissa, exponent = f\"{er_K_train:.2e}\".split(\"e\")\n",
    "exponent = int(exponent)  # Elimina ceros a la izquierda\n",
    "plt.title(rf\"$\\varepsilon_r (K)$ = {mantissa} $\\times$ 10$^{{{exponent}}}$\", fontsize=label_fontsize-1)\n",
    "\n",
    "\n",
    "plt.grid(True)\n",
    "# Obtener la leyenda actual y cambiar el orden\n",
    "handles, labels = plt.gca().get_legend_handles_labels()\n",
    "order = [1, 0]  # Cambiar el orden de las etiquetas aquí (Validation primero, Prediction segundo)\n",
    "plt.legend([handles[idx] for idx in order], [labels[idx] for idx in order], loc='lower left', fontsize=legend_fontsize)\n",
    "plt.tick_params(axis='both', which='major', labelsize=tick_fontsize)\n",
    "\n",
    "min = torch.min(y_train.values.flatten()) - 0.1*(torch.max(y_train.values.flatten() - torch.min(y_train.values.flatten())))\n",
    "max = torch.max(y_train.values.flatten()) + 0.1*(torch.max(y_train.values.flatten() - torch.min(y_train.values.flatten())))\n",
    "\n",
    "xlim = plt.xlim()\n",
    "ylim = plt.ylim()\n",
    "\n",
    "# plt.xlim(min, max)\n",
    "# plt.ylim(0.7, 1.1)\n",
    "\n",
    "plt.savefig(save_dir+'.png', bbox_inches='tight', dpi=600)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8820dc03",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9796ccde",
   "metadata": {},
   "outputs": [],
   "source": [
    "u_train = y_train.values.detach().numpy() \n",
    "u_predicted_train = model(X_train)[0].detach().numpy() \n",
    "er_u_train = relative_error(u_train, u_predicted_train).flatten()\n",
    "\n",
    "u_val = y_val.values.detach().numpy() \n",
    "u_predicted_val = model(X_val)[0].detach().numpy() \n",
    "er_u_val = relative_error(u_val, u_predicted_val).flatten()\n",
    "\n",
    "K_train_ = Mx(My(K_train)).values.detach().numpy() \n",
    "K_predicted_train = model(X_train)[1].detach().numpy() \n",
    "er_K_train = relative_error(K_train_, K_predicted_train).flatten()\n",
    "\n",
    "K_val_ = Mx(My(K_val)).values.detach().numpy() \n",
    "K_predicted_val = model(X_val)[1].detach().numpy() \n",
    "er_K_val = relative_error(K_val_, K_predicted_val).flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cb50d30",
   "metadata": {},
   "outputs": [],
   "source": [
    "def er_sum(reshaped_predicted_K, reshaped_validation_K, dx=dx, dy=dy):\n",
    "\n",
    "    numerador = np.sum((np.trapz(np.trapz(reshaped_predicted_K - reshaped_validation_K, dx=dx), dx=dx))**2)\n",
    "    denominador = np.sum((np.trapz(np.trapz(reshaped_validation_K, dx=dx), dx=dx))**2)\n",
    "    \n",
    "    er = numerador  / denominador\n",
    "    \n",
    "    print(f\"er(K): {er:.2e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00de7253",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model_i)\n",
    "\n",
    "plt.scatter(u_train.flatten(), ((K_train)).values.detach().numpy().flatten())\n",
    "plt.scatter(Mx(My(y_train)).values.detach().numpy() , K_predicted_train.flatten(), color=color)\n",
    "plt.show()\n",
    "\n",
    "er_sum(Mx(My(K_train)).values.detach().numpy(), K_predicted_train)\n",
    "\n",
    "\n",
    "print(model_i)\n",
    "\n",
    "plt.scatter(u_val.flatten(), ((K_val)).values.detach().numpy().flatten())\n",
    "plt.scatter(Mx(My(y_val)).values.detach().numpy() , K_predicted_val.flatten())\n",
    "plt.show()\n",
    "\n",
    "er_sum(Mx(My(K_val)).values.detach().numpy(), K_predicted_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a962b808",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crea una figura\n",
    "plt.figure(figsize=(8, 6))\n",
    "\n",
    "# Prepara los datos\n",
    "data = [er_u_train, er_u_val, er_K_train, er_K_val]\n",
    "labels = ['u_train', 'u_val', 'K_train', 'K_val']\n",
    "\n",
    "# Dibuja el boxplot\n",
    "sns.boxplot(data=data)\n",
    "plt.xticks(ticks=range(len(labels)), labels=labels)\n",
    "plt.ylabel('Error')\n",
    "plt.title('Boxplot de errores - Train vs Validation')\n",
    "plt.grid(True, linestyle='--', alpha=0.5)\n",
    "plt.yscale('log')\n",
    "\n",
    "# Muestra el gráfico\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# K_predicted_train = model(X_train)[1].detach().numpy() \n",
    "\n",
    "# u_predicted_val = model(X_val)[0].detach().numpy() \n",
    "# K_predicted_val = model(X_val)[1].detach().numpy() \n",
    "\n",
    "# er_u_train = relative_error(u_predicted_train, y_train.values)\n",
    "# er_K_train = relative_error(K_predicted_train, K_train.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d2bf161",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ee050ab",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SciML_test_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
