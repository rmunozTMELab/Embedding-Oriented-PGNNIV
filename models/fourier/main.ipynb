{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import GPUtil\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Imports de la libreria propia\n",
    "from vecopsciml.kernels.derivative import DerivativeKernels\n",
    "from vecopsciml.utils import TensOps\n",
    "\n",
    "# Imports de las funciones creadas para este programa\n",
    "# from models.POD import PODNonlinearModel\n",
    "from utils.folders import create_folder\n",
    "from utils.load_data import load_data\n",
    "from trainers.train import train_loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from vecopsciml.operators.zero_order import Mx, My"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Folder already exists at: /home/rmunoz/Escritorio/rmunozTMELab/Physically-Guided-Machine-Learning/results/non_linear\n",
      "Folder already exists at: /home/rmunoz/Escritorio/rmunozTMELab/Physically-Guided-Machine-Learning/results/non_linear/fourier_model\n"
     ]
    }
   ],
   "source": [
    "# Creamos los paths para las distintas carpetas\n",
    "ROOT_PATH = r'/home/rmunoz/Escritorio/rmunozTMELab/Physically-Guided-Machine-Learning'\n",
    "DATA_PATH = os.path.join(ROOT_PATH, r'data/non_linear/non_linear_1000.pkl')\n",
    "RESULTS_FOLDER_PATH = os.path.join(ROOT_PATH, r'results/non_linear')\n",
    "MODEL_RESULTS_PATH = os.path.join(ROOT_PATH, r'results/non_linear/fourier_model')\n",
    "\n",
    "# Creamos las carpetas que sean necesarias (si ya están creadas se avisará de ello)\n",
    "create_folder(RESULTS_FOLDER_PATH)\n",
    "create_folder(MODEL_RESULTS_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully loaded from: /home/rmunoz/Escritorio/rmunozTMELab/Physically-Guided-Machine-Learning/data/non_linear/non_linear_1000.pkl\n"
     ]
    }
   ],
   "source": [
    "# Load dataset\n",
    "dataset = load_data(DATA_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convolutional filters to derivate\n",
    "dx = dataset['x_step_size']\n",
    "dy = dataset['y_step_size']\n",
    "D = DerivativeKernels(dx, dy, 0).grad_kernels_two_dimensions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "print(f\"Using device: {DEVICE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train data splitting in train/test\n",
    "X = torch.tensor(dataset['X_train'], dtype=torch.float32).unsqueeze(1)\n",
    "y = torch.tensor(dataset['y_train'], dtype=torch.float32).unsqueeze(1)\n",
    "K = torch.tensor(dataset['k_train'], dtype=torch.float32).unsqueeze(1)\n",
    "f = torch.tensor(dataset['f_train'], dtype=torch.float32).unsqueeze(1)\n",
    "\n",
    "X_train, X_test, y_train, y_test, K_train, K_test, f_train, f_test = train_test_split(X, y, K, f, test_size=0.3, random_state=42)\n",
    "\n",
    "# Data processing and adequacy with our TensOps library\n",
    "X_train = X_train.to(DEVICE)\n",
    "X_test = X_test.to(DEVICE)\n",
    "\n",
    "y_train = TensOps(y_train.to(DEVICE).requires_grad_(True), space_dimension=2, contravariance=0, covariance=0)\n",
    "y_test = TensOps(y_test.to(DEVICE).requires_grad_(True), space_dimension=2, contravariance=0, covariance=0)\n",
    "\n",
    "K_train = TensOps(K_train.to(DEVICE).requires_grad_(True), space_dimension=2, contravariance=0, covariance=0)\n",
    "K_test = TensOps(K_test.to(DEVICE).requires_grad_(True), space_dimension=2, contravariance=0, covariance=0)\n",
    "\n",
    "f_train = TensOps(f_train.to(DEVICE).requires_grad_(True), space_dimension=2, contravariance=0, covariance=0)\n",
    "f_test = TensOps(f_test.to(DEVICE).requires_grad_(True), space_dimension=2, contravariance=0, covariance=0)\n",
    "\n",
    "# Loading and processing validation data\n",
    "X_val = torch.tensor(dataset['X_val'], dtype=torch.float32).unsqueeze(1)\n",
    "y_val = TensOps(torch.tensor(dataset['y_val'], dtype=torch.float32, requires_grad=True).unsqueeze(1), space_dimension=2, contravariance=0, covariance=0)\n",
    "K_val = TensOps(torch.tensor(dataset['k_val'], dtype=torch.float32, requires_grad=True).unsqueeze(1), space_dimension=2, contravariance=0, covariance=0)\n",
    "f_val = TensOps(torch.tensor(dataset['f_val'], dtype=torch.float32, requires_grad=True).unsqueeze(1), space_dimension=2, contravariance=0, covariance=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 --> 0.0\n",
      "1 --> 0.9805648326873779\n",
      "2 --> 0.983593225479126\n",
      "3 --> 0.9844560027122498\n",
      "4 --> 0.9849163889884949\n",
      "5 --> 0.9852505922317505\n",
      "6 --> 0.9855523109436035\n",
      "7 --> 0.9858865141868591\n",
      "8 --> 0.9863457679748535\n",
      "9 --> 0.9872096180915833\n",
      "10 --> 0.9902372360229492\n",
      "11 --> 0.9932528734207153\n",
      "12 --> 0.9932627081871033\n",
      "13 --> 0.9932650327682495\n",
      "14 --> 0.993266761302948\n",
      "15 --> 0.9932681322097778\n",
      "16 --> 0.9932698011398315\n",
      "17 --> 0.9932708144187927\n",
      "18 --> 0.9932726621627808\n",
      "19 --> 0.9932757019996643\n",
      "20 --> 0.9932872653007507\n",
      "21 --> 0.9941490888595581\n",
      "22 --> 0.9941521286964417\n",
      "23 --> 0.9941530227661133\n",
      "24 --> 0.9941537380218506\n",
      "25 --> 0.9941538572311401\n",
      "26 --> 0.9941543340682983\n",
      "27 --> 0.994154691696167\n",
      "28 --> 0.9941554069519043\n",
      "29 --> 0.9941568374633789\n",
      "30 --> 0.9941604137420654\n",
      "31 --> 0.9946208596229553\n",
      "32 --> 0.9946224689483643\n",
      "33 --> 0.994623064994812\n",
      "34 --> 0.9946233630180359\n",
      "35 --> 0.9946235418319702\n",
      "36 --> 0.9946237802505493\n",
      "37 --> 0.9946239590644836\n",
      "38 --> 0.9946243166923523\n",
      "39 --> 0.9946247935295105\n",
      "40 --> 0.9946268200874329\n",
      "41 --> 0.9949607253074646\n",
      "42 --> 0.9949619174003601\n",
      "43 --> 0.9949622750282288\n",
      "44 --> 0.9949625134468079\n",
      "45 --> 0.9949626922607422\n",
      "46 --> 0.9949629902839661\n",
      "47 --> 0.9949631690979004\n",
      "48 --> 0.9949633479118347\n",
      "49 --> 0.9949638247489929\n",
      "50 --> 0.9949656128883362\n",
      "51 --> 0.9952685236930847\n",
      "52 --> 0.9952695369720459\n",
      "53 --> 0.9952698945999146\n",
      "54 --> 0.9952701330184937\n",
      "55 --> 0.9952702522277832\n",
      "56 --> 0.9952704310417175\n",
      "57 --> 0.9952704310417175\n",
      "58 --> 0.9952707886695862\n",
      "59 --> 0.9952715039253235\n",
      "60 --> 0.9952725768089294\n",
      "61 --> 0.9956064820289612\n",
      "62 --> 0.9956079721450806\n",
      "63 --> 0.9956084489822388\n",
      "64 --> 0.9956086277961731\n",
      "65 --> 0.9956088066101074\n",
      "66 --> 0.9956089854240417\n",
      "67 --> 0.9956091642379761\n",
      "68 --> 0.9956093430519104\n",
      "69 --> 0.9956098198890686\n",
      "70 --> 0.9956112504005432\n",
      "71 --> 0.9960708618164062\n",
      "72 --> 0.99607253074646\n",
      "73 --> 0.9960733652114868\n",
      "74 --> 0.9960737228393555\n",
      "75 --> 0.9960740804672241\n",
      "76 --> 0.9960742592811584\n",
      "77 --> 0.9960744380950928\n",
      "78 --> 0.9960746765136719\n",
      "79 --> 0.9960752129554749\n",
      "80 --> 0.9960770010948181\n",
      "81 --> 0.9969392418861389\n",
      "82 --> 0.9969428181648254\n",
      "83 --> 0.9969438910484314\n",
      "84 --> 0.9969446063041687\n",
      "85 --> 0.9969449639320374\n",
      "86 --> 0.996945321559906\n",
      "87 --> 0.9969460964202881\n",
      "88 --> 0.9969466328620911\n",
      "89 --> 0.9969474673271179\n",
      "90 --> 0.9969503879547119\n",
      "91 --> 0.9999659061431885\n",
      "92 --> 0.9999769330024719\n",
      "93 --> 0.9999801516532898\n",
      "94 --> 0.9999822974205017\n",
      "95 --> 0.9999834299087524\n",
      "96 --> 0.9999847412109375\n",
      "97 --> 0.9999859929084778\n",
      "98 --> 0.9999876022338867\n",
      "99 --> 0.9999907612800598\n"
     ]
    }
   ],
   "source": [
    "data = y_train.values.detach().to('cpu')\n",
    "n_modes = 100\n",
    "relative_energy = 0.0\n",
    "\n",
    "fft_data = torch.fft.fft2(data)\n",
    "# fft_data_shifted = torch.fft.fftshift(fft_data)\n",
    "fft_data_shifted = (fft_data)\n",
    "energy = torch.abs(fft_data_shifted)**2\n",
    "energy_flattened = energy.flatten(1, 3)\n",
    "\n",
    "for mode_i in range(n_modes):\n",
    "\n",
    "    top_energetic = energy_flattened[:, :mode_i]\n",
    "    relative_energy = (torch.sum(torch.sum(top_energetic))/torch.sum(torch.sum(energy_flattened))).item()\n",
    "\n",
    "    print(mode_i, '-->', relative_energy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### No sirve para lo que queremos. Al buscar asi los modos mas energeticos, la NN sobreentrena. Lo haremos escogiendo los N primeros en vez de los N mas energéticos. \n",
    "\n",
    "def modes_base(data, n_modes):\n",
    "\n",
    "    # FFT decomposition and obtain energy of each mode\n",
    "    fft_data = torch.fft.fft2(data)\n",
    "    fft_data_shifted = torch.fft.fftshift(fft_data)\n",
    "    energy = torch.abs(fft_data_shifted)\n",
    "    energy_flattened = energy.flatten(1, 3)\n",
    "\n",
    "    # Get the n_modes more energetic modes and their indices\n",
    "    top_energetic = torch.topk(energy_flattened, n_modes).indices\n",
    "    top_energetic_indices, _ = zip(*Counter(top_energetic.flatten()).most_common(n_modes))\n",
    "    top_energetic_indices = list(map(int, top_energetic_indices))\n",
    "\n",
    "    # Create an empty template to include the modes\n",
    "    filtered_modes = torch.zeros_like(energy, dtype=torch.complex64)\n",
    "    filtered_modes.flatten(1, 3)[:, top_energetic_indices] = fft_data_shifted.flatten(1, 3)[:, top_energetic_indices]\n",
    "\n",
    "    # Return the base with the 'n_modes' most energetic modes\n",
    "    return top_energetic_indices, filtered_modes\n",
    "\n",
    "def reconstruct_data(coefficients_shifted):\n",
    "    \n",
    "    # Compute inverse FFT and reconstruct data\n",
    "    filtered_modes_base = torch.fft.ifftshift(coefficients_shifted)\n",
    "    reconstructed_data = torch.real(torch.fft.ifft2(filtered_modes_base))\n",
    "\n",
    "    return reconstructed_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def modes_base(data, n_modes):\n",
    "\n",
    "    # FFT decomposition and obtain energy of each mode\n",
    "    fft_data = torch.fft.fft2(data)\n",
    "    energy = torch.abs(fft_data)\n",
    "    energy_flattened = energy.flatten(1, 3)\n",
    "\n",
    "    # Get the n_modes more energetic modes and their indices\n",
    "    top_energetic = energy_flattened[:, :n_modes]\n",
    "\n",
    "    # Create an empty template to include the modes\n",
    "    filtered_modes = torch.zeros_like(energy, dtype=torch.complex64)\n",
    "    filtered_modes.flatten(1, 3)[:, :n_modes] = fft_data.flatten(1, 3)[:, :n_modes]\n",
    "\n",
    "    # Return the base with the 'n_modes' most energetic modes\n",
    "    return filtered_modes\n",
    "\n",
    "def reconstruct_data(coefficients_filtered):\n",
    "    \n",
    "    # Compute inverse FFT and reconstruct data\n",
    "    reconstructed_data = torch.real(torch.fft.ifft2(coefficients_filtered))\n",
    "\n",
    "    return reconstructed_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_modes = 90\n",
    "\n",
    "_ = modes_base(data=y_train.values, n_modes=num_modes)\n",
    "reconstructed_data = reconstruct_data(_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predictive network architecture\n",
    "input_shape = X_train[0].shape\n",
    "predictive_layers = [20, 10, num_modes]\n",
    "predictive_output = y_train.values[0].shape\n",
    "\n",
    "# Explanatory network architecture\n",
    "explanatory_input = Mx(My(y_train)).values[0].shape\n",
    "explanatory_layers = [10, 10]\n",
    "explanatory_output = Mx(My(f_train)).values[0].shape\n",
    "\n",
    "# Other parameters\n",
    "n_filters_explanatory = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from vecopsciml.utils import TensOps\n",
    "from vecopsciml.operators.zero_order import Mx, My\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "\n",
    "    def __init__(self, input_size, hidden_layer_1_size, hidden_layer_2_size, latent_space_size):\n",
    "        super(Encoder, self).__init__()\n",
    "\n",
    "        # Parameters\n",
    "        self.in_size = torch.tensor(input_size)\n",
    "        self.h1_size = hidden_layer_1_size\n",
    "        self.h2_size = hidden_layer_2_size\n",
    "        self.ls_size = latent_space_size\n",
    "\n",
    "        # Architecture\n",
    "        self.flatten_layer = nn.Flatten(start_dim=1, end_dim=-1)\n",
    "        self.hidden1_layer = nn.Linear(torch.prod(self.in_size), self.h1_size)\n",
    "        self.hidden2_layer = nn.Linear(self.h1_size, self.h2_size)\n",
    "        self.latent_space_layer = nn.Linear(self.h2_size, self.ls_size)\n",
    "    \n",
    "    def forward(self, X):\n",
    "        \n",
    "        X = self.flatten_layer(X)\n",
    "        X = torch.sigmoid(self.hidden1_layer(X))\n",
    "        X = torch.sigmoid(self.hidden2_layer(X))\n",
    "        latent_space_output = (self.latent_space_layer(X))\n",
    "\n",
    "        return latent_space_output\n",
    "    \n",
    "class Explanatory(nn.Module):\n",
    "\n",
    "    def __init__(self, input_size, n_filters, hidden_layer_size, output_size):\n",
    "        super(Explanatory, self).__init__()\n",
    "\n",
    "        # Parameters\n",
    "        self.in_size = torch.tensor(input_size)\n",
    "        self.n_filters = n_filters\n",
    "        self.h_layer = hidden_layer_size\n",
    "        self.out_size = torch.tensor(output_size)\n",
    "\n",
    "        # Architecture\n",
    "        self.conv_expand_layer = nn.Conv2d(in_channels=1, out_channels=self.n_filters, kernel_size=1)\n",
    "        self.flatten_layer = nn.Flatten(start_dim=1, end_dim=-1)\n",
    "        self.hidden_layer = nn.Linear(n_filters*torch.prod(self.in_size), n_filters*torch.prod(self.out_size))\n",
    "        self.conv_converge_layer = nn.Conv2d(in_channels=n_filters, out_channels=1, kernel_size=1)\n",
    "        \n",
    "    def forward(self, X):\n",
    "        \n",
    "        X = torch.sigmoid(self.conv_expand_layer(X))\n",
    "        X = self.flatten_layer(X)\n",
    "        X = self.hidden_layer(X)\n",
    "        X = X.view(X.size(0), self.n_filters, self.out_size[1], self.out_size[2])\n",
    "        explanatory_output = self.conv_converge_layer(X)\n",
    "\n",
    "        return explanatory_output\n",
    "    \n",
    "class FFTNonlinearModel(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_size, predictive_layers, FFT_modes_base, output_predictive_size, explanatory_input_size, explanatory_layers, output_explanatory_size, n_filters):\n",
    "        \n",
    "        super(FFTNonlinearModel, self).__init__()\n",
    "\n",
    "        # Parameters\n",
    "        self.in_size = input_size\n",
    "        self.pred_size = predictive_layers\n",
    "        self.out_pred_size = output_predictive_size\n",
    "        \n",
    "        self.in_exp_size = explanatory_input_size\n",
    "        self.exp_size = explanatory_layers\n",
    "        self.out_exp_size = output_explanatory_size\n",
    "\n",
    "        self.n_filters = n_filters\n",
    "\n",
    "        # Architecture\n",
    "        self.encoder = Encoder(self.in_size, self.pred_size[0], self.pred_size[1], 2*self.pred_size[2])\n",
    "        self.base_indices = FFT_modes_base\n",
    "        self.explanatory = Explanatory(self.in_exp_size, self.n_filters, self.exp_size[0], self.out_exp_size)\n",
    "        \n",
    "    def forward(self, X):\n",
    "\n",
    "        # Predictive network\n",
    "        X = self.encoder(X)\n",
    "\n",
    "        # Manipulating output to obtain real and complex part\n",
    "        output_predictive = X.view(X.size(0), self.pred_size[2], 2)\n",
    "        real = output_predictive[..., 0]\n",
    "        imag = output_predictive[..., 1]\n",
    "    \n",
    "        # Reconstruction with FFT and manipulation of prediction output\n",
    "        base = torch.zeros((X.size(0), *self.out_pred_size), dtype=torch.complex64).to(DEVICE)\n",
    "        base.flatten(1, 3)[:, :self.base_indices] = torch.complex(real, imag)\n",
    "\n",
    "        u = reconstruct_data(base).to(DEVICE)        \n",
    "        um = Mx(My(TensOps(u, space_dimension=2, contravariance=0, covariance=0))).values\n",
    "\n",
    "        # Explanatory network\n",
    "        K = self.explanatory(um)\n",
    "        \n",
    "        return u, K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training from scratch.\n",
      "Epoch 0, Train loss: 7.557e+08, Test loss: 9.966e+08, MSE(e): 7.517e+01, MSE(pi1): 3.743e+02, MSE(pi2): 3.031e+01, MSE(pi3): 2.403e+00\n",
      "Epoch 100, Train loss: 6.083e+07, Test loss: 8.761e+07, MSE(e): 6.081e+00, MSE(pi1): 4.760e-01, MSE(pi2): 2.585e+00, MSE(pi3): 1.246e-01\n",
      "Epoch 200, Train loss: 2.445e+07, Test loss: 4.347e+07, MSE(e): 2.443e+00, MSE(pi1): 4.649e-01, MSE(pi2): 1.128e+00, MSE(pi3): 1.249e-01\n",
      "Epoch 300, Train loss: 6.937e+06, Test loss: 9.968e+06, MSE(e): 6.921e-01, MSE(pi1): 5.306e-01, MSE(pi2): 3.998e-01, MSE(pi3): 1.019e-01\n",
      "Epoch 400, Train loss: 2.969e+06, Test loss: 3.525e+06, MSE(e): 2.960e-01, MSE(pi1): 2.161e-01, MSE(pi2): 1.942e-01, MSE(pi3): 6.402e-02\n",
      "Epoch 500, Train loss: 2.666e+05, Test loss: 5.043e+05, MSE(e): 2.636e-02, MSE(pi1): 1.008e-01, MSE(pi2): 1.531e-02, MSE(pi3): 1.943e-02\n",
      "Epoch 600, Train loss: 1.075e+05, Test loss: 2.202e+05, MSE(e): 1.053e-02, MSE(pi1): 7.775e-02, MSE(pi2): 6.603e-03, MSE(pi3): 1.403e-02\n",
      "Epoch 700, Train loss: 5.650e+04, Test loss: 1.225e+05, MSE(e): 5.461e-03, MSE(pi1): 7.396e-02, MSE(pi2): 3.702e-03, MSE(pi3): 1.154e-02\n",
      "Epoch 800, Train loss: 3.728e+04, Test loss: 8.294e+04, MSE(e): 3.571e-03, MSE(pi1): 6.130e-02, MSE(pi2): 2.510e-03, MSE(pi3): 9.539e-03\n",
      "Epoch 900, Train loss: 2.593e+04, Test loss: 5.268e+04, MSE(e): 2.442e-03, MSE(pi1): 7.424e-02, MSE(pi2): 1.688e-03, MSE(pi3): 7.588e-03\n",
      "Epoch 1000, Train loss: 2.811e+04, Test loss: 3.718e+04, MSE(e): 2.623e-03, MSE(pi1): 1.245e-01, MSE(pi2): 1.515e-03, MSE(pi3): 6.325e-03\n",
      "Epoch 1100, Train loss: 2.008e+04, Test loss: 3.392e+04, MSE(e): 1.865e-03, MSE(pi1): 8.994e-02, MSE(pi2): 1.247e-03, MSE(pi3): 5.336e-03\n",
      "Epoch 1200, Train loss: 2.647e+04, Test loss: 3.519e+04, MSE(e): 2.522e-03, MSE(pi1): 8.606e-02, MSE(pi2): 1.543e-03, MSE(pi3): 3.806e-03\n",
      "Epoch 1300, Train loss: 1.171e+04, Test loss: 2.124e+04, MSE(e): 1.072e-03, MSE(pi1): 6.412e-02, MSE(pi2): 8.113e-04, MSE(pi3): 3.484e-03\n",
      "Epoch 1400, Train loss: 1.167e+04, Test loss: 1.832e+04, MSE(e): 1.105e-03, MSE(pi1): 3.305e-02, MSE(pi2): 7.978e-04, MSE(pi3): 2.831e-03\n",
      "Epoch 1500, Train loss: 9.966e+03, Test loss: 1.509e+04, MSE(e): 8.794e-04, MSE(pi1): 9.135e-02, MSE(pi2): 6.522e-04, MSE(pi3): 2.573e-03\n",
      "Epoch 1600, Train loss: 8.071e+03, Test loss: 1.498e+04, MSE(e): 7.489e-04, MSE(pi1): 3.161e-02, MSE(pi2): 5.911e-04, MSE(pi3): 2.654e-03\n",
      "Epoch 1700, Train loss: 2.516e+04, Test loss: 6.728e+04, MSE(e): 2.443e-03, MSE(pi1): 5.680e-02, MSE(pi2): 1.251e-03, MSE(pi3): 1.655e-03\n",
      "Epoch 1800, Train loss: 8.792e+03, Test loss: 1.396e+04, MSE(e): 7.870e-04, MSE(pi1): 7.333e-02, MSE(pi2): 5.672e-04, MSE(pi3): 1.885e-03\n",
      "Epoch 1900, Train loss: 1.025e+04, Test loss: 2.740e+04, MSE(e): 9.272e-04, MSE(pi1): 7.523e-02, MSE(pi2): 6.402e-04, MSE(pi3): 2.240e-03\n",
      "Epoch 2000, Train loss: 8.176e+03, Test loss: 1.498e+04, MSE(e): 7.685e-04, MSE(pi1): 3.305e-02, MSE(pi2): 5.561e-04, MSE(pi3): 1.597e-03\n",
      "Epoch 2100, Train loss: 9.020e+03, Test loss: 1.336e+04, MSE(e): 8.319e-04, MSE(pi1): 5.638e-02, MSE(pi2): 6.075e-04, MSE(pi3): 1.367e-03\n",
      "Epoch 2200, Train loss: 7.940e+03, Test loss: 1.310e+04, MSE(e): 7.327e-04, MSE(pi1): 4.485e-02, MSE(pi2): 5.189e-04, MSE(pi3): 1.641e-03\n",
      "Epoch 2300, Train loss: 7.101e+03, Test loss: 1.224e+04, MSE(e): 6.314e-04, MSE(pi1): 5.817e-02, MSE(pi2): 4.780e-04, MSE(pi3): 2.049e-03\n",
      "Epoch 2400, Train loss: 5.744e+03, Test loss: 1.241e+04, MSE(e): 5.236e-04, MSE(pi1): 3.247e-02, MSE(pi2): 4.115e-04, MSE(pi3): 1.834e-03\n",
      "Epoch 2500, Train loss: 4.908e+03, Test loss: 9.962e+03, MSE(e): 4.560e-04, MSE(pi1): 2.008e-02, MSE(pi2): 3.832e-04, MSE(pi3): 1.471e-03\n",
      "Epoch 2600, Train loss: 5.094e+03, Test loss: 1.056e+04, MSE(e): 4.747e-04, MSE(pi1): 1.889e-02, MSE(pi2): 3.925e-04, MSE(pi3): 1.583e-03\n",
      "Epoch 2700, Train loss: 6.707e+03, Test loss: 1.091e+04, MSE(e): 6.112e-04, MSE(pi1): 4.635e-02, MSE(pi2): 4.184e-04, MSE(pi3): 1.309e-03\n",
      "Epoch 2800, Train loss: 4.816e+03, Test loss: 8.907e+03, MSE(e): 4.520e-04, MSE(pi1): 1.495e-02, MSE(pi2): 3.726e-04, MSE(pi3): 1.456e-03\n",
      "Epoch 2900, Train loss: 5.103e+03, Test loss: 1.110e+04, MSE(e): 4.364e-04, MSE(pi1): 5.155e-02, MSE(pi2): 3.493e-04, MSE(pi3): 2.226e-03\n",
      "Epoch 3000, Train loss: 5.323e+03, Test loss: 8.437e+03, MSE(e): 4.873e-04, MSE(pi1): 3.382e-02, MSE(pi2): 3.559e-04, MSE(pi3): 1.122e-03\n",
      "Epoch 3100, Train loss: 4.835e+03, Test loss: 1.045e+04, MSE(e): 4.191e-04, MSE(pi1): 5.159e-02, MSE(pi2): 3.360e-04, MSE(pi3): 1.278e-03\n",
      "Epoch 3200, Train loss: 4.774e+03, Test loss: 1.033e+04, MSE(e): 4.281e-04, MSE(pi1): 1.817e-02, MSE(pi2): 3.332e-04, MSE(pi3): 3.105e-03\n",
      "Epoch 3300, Train loss: 7.037e+03, Test loss: 1.446e+04, MSE(e): 6.310e-04, MSE(pi1): 5.126e-02, MSE(pi2): 4.801e-04, MSE(pi3): 2.145e-03\n",
      "Epoch 3400, Train loss: 5.125e+03, Test loss: 1.419e+04, MSE(e): 4.764e-04, MSE(pi1): 1.370e-02, MSE(pi2): 3.246e-04, MSE(pi3): 2.237e-03\n",
      "Epoch 3500, Train loss: 5.960e+03, Test loss: 7.900e+03, MSE(e): 5.417e-04, MSE(pi1): 3.671e-02, MSE(pi2): 4.073e-04, MSE(pi3): 1.752e-03\n",
      "Epoch 3600, Train loss: 4.746e+03, Test loss: 8.772e+03, MSE(e): 3.939e-04, MSE(pi1): 4.762e-02, MSE(pi2): 2.962e-04, MSE(pi3): 3.310e-03\n",
      "Epoch 3700, Train loss: 3.934e+03, Test loss: 7.512e+03, MSE(e): 3.285e-04, MSE(pi1): 4.178e-02, MSE(pi2): 2.564e-04, MSE(pi3): 2.312e-03\n",
      "Epoch 3800, Train loss: 3.430e+03, Test loss: 6.736e+03, MSE(e): 3.099e-04, MSE(pi1): 1.695e-02, MSE(pi2): 2.495e-04, MSE(pi3): 1.619e-03\n",
      "Epoch 3900, Train loss: 4.838e+03, Test loss: 8.968e+03, MSE(e): 4.173e-04, MSE(pi1): 4.690e-02, MSE(pi2): 2.799e-04, MSE(pi3): 1.960e-03\n",
      "Epoch 4000, Train loss: 5.231e+03, Test loss: 8.517e+03, MSE(e): 4.599e-04, MSE(pi1): 4.645e-02, MSE(pi2): 3.063e-04, MSE(pi3): 1.668e-03\n",
      "Epoch 4100, Train loss: 4.174e+03, Test loss: 7.853e+03, MSE(e): 3.610e-04, MSE(pi1): 4.049e-02, MSE(pi2): 2.478e-04, MSE(pi3): 1.587e-03\n",
      "Epoch 4200, Train loss: 4.378e+03, Test loss: 9.585e+03, MSE(e): 3.754e-04, MSE(pi1): 4.926e-02, MSE(pi2): 2.499e-04, MSE(pi3): 1.316e-03\n",
      "Epoch 4300, Train loss: 4.458e+03, Test loss: 8.233e+03, MSE(e): 3.816e-04, MSE(pi1): 4.791e-02, MSE(pi2): 2.805e-04, MSE(pi3): 1.624e-03\n",
      "Epoch 4400, Train loss: 4.622e+03, Test loss: 8.879e+03, MSE(e): 4.113e-04, MSE(pi1): 3.197e-02, MSE(pi2): 2.556e-04, MSE(pi3): 1.885e-03\n",
      "Epoch 4500, Train loss: 5.113e+03, Test loss: 9.343e+03, MSE(e): 4.617e-04, MSE(pi1): 3.530e-02, MSE(pi2): 2.778e-04, MSE(pi3): 1.426e-03\n",
      "Epoch 4600, Train loss: 4.599e+03, Test loss: 8.847e+03, MSE(e): 4.010e-04, MSE(pi1): 3.656e-02, MSE(pi2): 2.432e-04, MSE(pi3): 2.230e-03\n",
      "Epoch 4700, Train loss: 6.449e+03, Test loss: 9.689e+03, MSE(e): 6.080e-04, MSE(pi1): 2.179e-02, MSE(pi2): 3.412e-04, MSE(pi3): 1.502e-03\n",
      "Epoch 4800, Train loss: 4.931e+03, Test loss: 7.824e+03, MSE(e): 4.321e-04, MSE(pi1): 4.240e-02, MSE(pi2): 2.541e-04, MSE(pi3): 1.861e-03\n",
      "Epoch 4900, Train loss: 5.088e+03, Test loss: 8.712e+03, MSE(e): 4.564e-04, MSE(pi1): 3.638e-02, MSE(pi2): 2.658e-04, MSE(pi3): 1.594e-03\n",
      "Epoch 5000, Train loss: 4.679e+03, Test loss: 8.546e+03, MSE(e): 4.142e-04, MSE(pi1): 3.506e-02, MSE(pi2): 2.502e-04, MSE(pi3): 1.866e-03\n",
      "Epoch 5100, Train loss: 6.993e+03, Test loss: 9.351e+03, MSE(e): 6.540e-04, MSE(pi1): 3.054e-02, MSE(pi2): 3.440e-04, MSE(pi3): 1.471e-03\n",
      "Epoch 5200, Train loss: 3.445e+03, Test loss: 8.696e+03, MSE(e): 2.996e-04, MSE(pi1): 2.974e-02, MSE(pi2): 1.987e-04, MSE(pi3): 1.508e-03\n",
      "Epoch 5300, Train loss: 2.884e+03, Test loss: 7.994e+03, MSE(e): 2.432e-04, MSE(pi1): 3.016e-02, MSE(pi2): 1.871e-04, MSE(pi3): 1.501e-03\n",
      "Epoch 5400, Train loss: 4.183e+03, Test loss: 8.512e+03, MSE(e): 3.779e-04, MSE(pi1): 2.225e-02, MSE(pi2): 2.273e-04, MSE(pi3): 1.810e-03\n",
      "Epoch 5500, Train loss: 7.257e+03, Test loss: 1.218e+04, MSE(e): 6.720e-04, MSE(pi1): 3.795e-02, MSE(pi2): 3.490e-04, MSE(pi3): 1.569e-03\n",
      "Epoch 5600, Train loss: 3.610e+03, Test loss: 9.724e+03, MSE(e): 3.189e-04, MSE(pi1): 2.549e-02, MSE(pi2): 2.287e-04, MSE(pi3): 1.666e-03\n",
      "Epoch 5700, Train loss: 3.605e+03, Test loss: 9.961e+03, MSE(e): 3.115e-04, MSE(pi1): 3.085e-02, MSE(pi2): 2.190e-04, MSE(pi3): 1.816e-03\n",
      "Epoch 5800, Train loss: 3.093e+03, Test loss: 8.284e+03, MSE(e): 2.597e-04, MSE(pi1): 3.049e-02, MSE(pi2): 1.902e-04, MSE(pi3): 1.905e-03\n",
      "Epoch 5900, Train loss: 3.351e+03, Test loss: 9.527e+03, MSE(e): 2.664e-04, MSE(pi1): 4.158e-02, MSE(pi2): 1.870e-04, MSE(pi3): 2.710e-03\n",
      "Epoch 6000, Train loss: 2.456e+03, Test loss: 7.529e+03, MSE(e): 2.193e-04, MSE(pi1): 1.430e-02, MSE(pi2): 1.714e-04, MSE(pi3): 1.202e-03\n",
      "Epoch 6100, Train loss: 5.682e+03, Test loss: 8.461e+03, MSE(e): 5.243e-04, MSE(pi1): 2.930e-02, MSE(pi2): 3.312e-04, MSE(pi3): 1.454e-03\n",
      "Epoch 6200, Train loss: 2.778e+03, Test loss: 8.499e+03, MSE(e): 2.508e-04, MSE(pi1): 1.359e-02, MSE(pi2): 1.854e-04, MSE(pi3): 1.338e-03\n",
      "Epoch 6300, Train loss: 5.317e+03, Test loss: 1.070e+04, MSE(e): 4.874e-04, MSE(pi1): 2.628e-02, MSE(pi2): 2.892e-04, MSE(pi3): 1.797e-03\n",
      "Epoch 6400, Train loss: 3.173e+03, Test loss: 1.011e+04, MSE(e): 2.787e-04, MSE(pi1): 2.564e-02, MSE(pi2): 1.880e-04, MSE(pi3): 1.296e-03\n",
      "Epoch 6500, Train loss: 2.466e+03, Test loss: 8.380e+03, MSE(e): 2.002e-04, MSE(pi1): 3.143e-02, MSE(pi2): 1.508e-04, MSE(pi3): 1.496e-03\n",
      "Epoch 6600, Train loss: 2.471e+03, Test loss: 7.621e+03, MSE(e): 2.137e-04, MSE(pi1): 1.941e-02, MSE(pi2): 1.703e-04, MSE(pi3): 1.400e-03\n",
      "Epoch 6700, Train loss: 4.063e+03, Test loss: 1.062e+04, MSE(e): 3.680e-04, MSE(pi1): 2.760e-02, MSE(pi2): 2.532e-04, MSE(pi3): 1.064e-03\n",
      "Epoch 6800, Train loss: 3.290e+03, Test loss: 1.064e+04, MSE(e): 2.593e-04, MSE(pi1): 4.346e-02, MSE(pi2): 1.722e-04, MSE(pi3): 2.617e-03\n",
      "Epoch 6900, Train loss: 3.549e+03, Test loss: 6.465e+03, MSE(e): 3.119e-04, MSE(pi1): 3.001e-02, MSE(pi2): 2.161e-04, MSE(pi3): 1.303e-03\n",
      "Epoch 7000, Train loss: 4.028e+03, Test loss: 7.627e+03, MSE(e): 3.345e-04, MSE(pi1): 5.694e-02, MSE(pi2): 2.279e-04, MSE(pi3): 1.134e-03\n",
      "Epoch 7100, Train loss: 2.367e+03, Test loss: 8.950e+03, MSE(e): 2.100e-04, MSE(pi1): 1.672e-02, MSE(pi2): 1.564e-04, MSE(pi3): 9.936e-04\n",
      "Epoch 7200, Train loss: 2.326e+03, Test loss: 8.892e+03, MSE(e): 1.908e-04, MSE(pi1): 2.519e-02, MSE(pi2): 1.518e-04, MSE(pi3): 1.659e-03\n",
      "Epoch 7300, Train loss: 6.974e+03, Test loss: 1.264e+04, MSE(e): 6.336e-04, MSE(pi1): 4.622e-02, MSE(pi2): 3.247e-04, MSE(pi3): 1.759e-03\n",
      "Epoch 7400, Train loss: 8.161e+03, Test loss: 1.322e+04, MSE(e): 7.612e-04, MSE(pi1): 3.515e-02, MSE(pi2): 3.960e-04, MSE(pi3): 1.972e-03\n",
      "Epoch 7500, Train loss: 2.439e+03, Test loss: 8.738e+03, MSE(e): 2.201e-04, MSE(pi1): 1.252e-02, MSE(pi2): 1.582e-04, MSE(pi3): 1.132e-03\n",
      "Epoch 7600, Train loss: 2.878e+03, Test loss: 9.354e+03, MSE(e): 2.653e-04, MSE(pi1): 1.045e-02, MSE(pi2): 1.815e-04, MSE(pi3): 1.203e-03\n",
      "Epoch 7700, Train loss: 3.537e+03, Test loss: 7.609e+03, MSE(e): 2.909e-04, MSE(pi1): 4.852e-02, MSE(pi2): 2.148e-04, MSE(pi3): 1.428e-03\n",
      "Epoch 7800, Train loss: 3.650e+03, Test loss: 9.743e+03, MSE(e): 3.056e-04, MSE(pi1): 3.614e-02, MSE(pi2): 2.219e-04, MSE(pi3): 2.330e-03\n",
      "Epoch 7900, Train loss: 1.995e+03, Test loss: 6.816e+03, MSE(e): 1.727e-04, MSE(pi1): 1.447e-02, MSE(pi2): 1.489e-04, MSE(pi3): 1.235e-03\n",
      "Epoch 8000, Train loss: 2.398e+03, Test loss: 8.094e+03, MSE(e): 2.180e-04, MSE(pi1): 1.113e-02, MSE(pi2): 1.655e-04, MSE(pi3): 1.069e-03\n",
      "Epoch 8100, Train loss: 4.744e+03, Test loss: 1.042e+04, MSE(e): 4.290e-04, MSE(pi1): 3.241e-02, MSE(pi2): 2.829e-04, MSE(pi3): 1.294e-03\n",
      "Epoch 8200, Train loss: 7.847e+03, Test loss: 1.296e+04, MSE(e): 7.503e-04, MSE(pi1): 2.146e-02, MSE(pi2): 3.727e-04, MSE(pi3): 1.294e-03\n",
      "Epoch 8300, Train loss: 4.169e+03, Test loss: 9.997e+03, MSE(e): 3.742e-04, MSE(pi1): 3.203e-02, MSE(pi2): 2.107e-04, MSE(pi3): 1.061e-03\n",
      "Epoch 8400, Train loss: 2.303e+03, Test loss: 7.457e+03, MSE(e): 2.024e-04, MSE(pi1): 1.540e-02, MSE(pi2): 1.551e-04, MSE(pi3): 1.244e-03\n",
      "Epoch 8500, Train loss: 3.470e+03, Test loss: 9.784e+03, MSE(e): 3.061e-04, MSE(pi1): 2.757e-02, MSE(pi2): 1.881e-04, MSE(pi3): 1.331e-03\n",
      "Epoch 8600, Train loss: 2.482e+03, Test loss: 7.705e+03, MSE(e): 1.858e-04, MSE(pi1): 5.014e-02, MSE(pi2): 1.358e-04, MSE(pi3): 1.222e-03\n",
      "Epoch 8700, Train loss: 4.571e+03, Test loss: 8.794e+03, MSE(e): 4.029e-04, MSE(pi1): 3.712e-02, MSE(pi2): 2.302e-04, MSE(pi3): 1.711e-03\n",
      "Epoch 8800, Train loss: 1.876e+03, Test loss: 6.735e+03, MSE(e): 1.492e-04, MSE(pi1): 2.796e-02, MSE(pi2): 1.191e-04, MSE(pi3): 1.049e-03\n",
      "Epoch 8900, Train loss: 2.937e+03, Test loss: 7.825e+03, MSE(e): 2.400e-04, MSE(pi1): 3.927e-02, MSE(pi2): 1.511e-04, MSE(pi3): 1.436e-03\n",
      "Epoch 9000, Train loss: 2.684e+03, Test loss: 8.322e+03, MSE(e): 2.179e-04, MSE(pi1): 4.044e-02, MSE(pi2): 1.385e-04, MSE(pi3): 1.004e-03\n",
      "Epoch 9100, Train loss: 3.651e+03, Test loss: 1.037e+04, MSE(e): 3.124e-04, MSE(pi1): 4.250e-02, MSE(pi2): 1.821e-04, MSE(pi3): 1.017e-03\n",
      "Epoch 9200, Train loss: 3.031e+03, Test loss: 8.778e+03, MSE(e): 2.282e-04, MSE(pi1): 5.888e-02, MSE(pi2): 1.702e-04, MSE(pi3): 1.600e-03\n",
      "Epoch 9300, Train loss: 3.077e+03, Test loss: 7.031e+03, MSE(e): 2.461e-04, MSE(pi1): 4.544e-02, MSE(pi2): 1.834e-04, MSE(pi3): 1.618e-03\n",
      "Epoch 9400, Train loss: 2.390e+03, Test loss: 9.043e+03, MSE(e): 2.104e-04, MSE(pi1): 1.956e-02, MSE(pi2): 1.457e-04, MSE(pi3): 9.033e-04\n",
      "Epoch 9500, Train loss: 4.940e+03, Test loss: 1.295e+04, MSE(e): 4.461e-04, MSE(pi1): 3.199e-02, MSE(pi2): 2.533e-04, MSE(pi3): 1.585e-03\n",
      "Epoch 9600, Train loss: 6.937e+03, Test loss: 7.687e+03, MSE(e): 6.528e-04, MSE(pi1): 2.650e-02, MSE(pi2): 3.172e-04, MSE(pi3): 1.447e-03\n",
      "Epoch 9700, Train loss: 3.703e+03, Test loss: 1.057e+04, MSE(e): 3.029e-04, MSE(pi1): 4.848e-02, MSE(pi2): 1.862e-04, MSE(pi3): 1.893e-03\n",
      "Epoch 9800, Train loss: 2.533e+03, Test loss: 8.792e+03, MSE(e): 2.161e-04, MSE(pi1): 1.777e-02, MSE(pi2): 1.556e-04, MSE(pi3): 1.943e-03\n",
      "Epoch 9900, Train loss: 5.722e+03, Test loss: 1.065e+04, MSE(e): 5.247e-04, MSE(pi1): 3.285e-02, MSE(pi2): 2.751e-04, MSE(pi3): 1.467e-03\n",
      "\n",
      "Training process finished after 10000 epochs\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load model and the optimizer\n",
    "model = FFTNonlinearModel(input_shape, predictive_layers, num_modes, predictive_output, explanatory_input, explanatory_layers, explanatory_output, n_filters_explanatory).to(DEVICE)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-2)\n",
    "\n",
    "# Parametros de entrenamiento\n",
    "start_epoch = 0\n",
    "n_epochs = 10000\n",
    "\n",
    "batch_size = 64\n",
    "n_checkpoints = 10\n",
    "\n",
    "train_loop(model, optimizer, X_train, y_train, f_train, X_test, y_test, f_test,\n",
    "           D,  n_checkpoints, start_epoch=start_epoch, n_epochs=n_epochs, batch_size=batch_size, \n",
    "           model_results_path=MODEL_RESULTS_PATH, device=DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training from a checkpoint. Epoch 9000.\n",
      "Epoch 9000, Train loss: 2.774e+03, Test loss: 7.989e+03, MSE(e): 2.160e-04, MSE(pi1): 4.567e-02, MSE(pi2): 1.439e-04, MSE(pi3): 1.579e-03\n",
      "Epoch 9100, Train loss: 2.091e+03, Test loss: 8.167e+03, MSE(e): 1.732e-04, MSE(pi1): 2.634e-02, MSE(pi2): 1.320e-04, MSE(pi3): 9.560e-04\n",
      "Epoch 9200, Train loss: 5.782e+03, Test loss: 1.343e+04, MSE(e): 4.820e-04, MSE(pi1): 7.729e-02, MSE(pi2): 2.768e-04, MSE(pi3): 1.889e-03\n",
      "Epoch 9300, Train loss: 2.698e+03, Test loss: 7.062e+03, MSE(e): 2.216e-04, MSE(pi1): 3.519e-02, MSE(pi2): 1.578e-04, MSE(pi3): 1.294e-03\n",
      "Epoch 9400, Train loss: 2.078e+03, Test loss: 7.076e+03, MSE(e): 1.724e-04, MSE(pi1): 2.663e-02, MSE(pi2): 1.238e-04, MSE(pi3): 8.793e-04\n",
      "Epoch 9500, Train loss: 3.635e+03, Test loss: 1.007e+04, MSE(e): 3.210e-04, MSE(pi1): 3.160e-02, MSE(pi2): 1.951e-04, MSE(pi3): 1.092e-03\n",
      "Epoch 9600, Train loss: 6.040e+03, Test loss: 1.174e+04, MSE(e): 5.677e-04, MSE(pi1): 2.332e-02, MSE(pi2): 2.936e-04, MSE(pi3): 1.297e-03\n",
      "Epoch 9700, Train loss: 2.093e+03, Test loss: 8.237e+03, MSE(e): 1.769e-04, MSE(pi1): 1.857e-02, MSE(pi2): 1.236e-04, MSE(pi3): 1.381e-03\n",
      "Epoch 9800, Train loss: 2.983e+03, Test loss: 7.406e+03, MSE(e): 2.422e-04, MSE(pi1): 4.587e-02, MSE(pi2): 1.743e-04, MSE(pi3): 1.021e-03\n",
      "Epoch 9900, Train loss: 2.234e+03, Test loss: 7.656e+03, MSE(e): 1.731e-04, MSE(pi1): 3.442e-02, MSE(pi2): 1.256e-04, MSE(pi3): 1.594e-03\n",
      "Epoch 10000, Train loss: 2.396e+03, Test loss: 9.889e+03, MSE(e): 2.056e-04, MSE(pi1): 1.971e-02, MSE(pi2): 1.435e-04, MSE(pi3): 1.426e-03\n",
      "Epoch 10100, Train loss: 2.417e+03, Test loss: 7.850e+03, MSE(e): 1.834e-04, MSE(pi1): 4.361e-02, MSE(pi2): 1.234e-04, MSE(pi3): 1.467e-03\n",
      "Epoch 10200, Train loss: 2.291e+03, Test loss: 8.248e+03, MSE(e): 1.546e-04, MSE(pi1): 6.458e-02, MSE(pi2): 1.118e-04, MSE(pi3): 9.905e-04\n",
      "Epoch 10300, Train loss: 2.179e+03, Test loss: 7.310e+03, MSE(e): 1.590e-04, MSE(pi1): 4.707e-02, MSE(pi2): 1.150e-04, MSE(pi3): 1.180e-03\n",
      "Epoch 10400, Train loss: 2.589e+03, Test loss: 8.494e+03, MSE(e): 1.997e-04, MSE(pi1): 4.790e-02, MSE(pi2): 1.280e-04, MSE(pi3): 1.131e-03\n",
      "Epoch 10500, Train loss: 3.966e+03, Test loss: 1.421e+04, MSE(e): 3.383e-04, MSE(pi1): 4.710e-02, MSE(pi2): 2.281e-04, MSE(pi3): 1.121e-03\n",
      "Epoch 10600, Train loss: 2.116e+03, Test loss: 7.304e+03, MSE(e): 1.706e-04, MSE(pi1): 3.345e-02, MSE(pi2): 1.236e-04, MSE(pi3): 7.610e-04\n",
      "Epoch 10700, Train loss: 3.954e+03, Test loss: 6.990e+03, MSE(e): 3.669e-04, MSE(pi1): 1.534e-02, MSE(pi2): 2.486e-04, MSE(pi3): 1.313e-03\n",
      "Epoch 10800, Train loss: 3.746e+03, Test loss: 8.320e+03, MSE(e): 2.960e-04, MSE(pi1): 6.284e-02, MSE(pi2): 1.565e-04, MSE(pi3): 1.571e-03\n",
      "Epoch 10900, Train loss: 3.139e+03, Test loss: 9.491e+03, MSE(e): 2.610e-04, MSE(pi1): 3.759e-02, MSE(pi2): 1.715e-04, MSE(pi3): 1.520e-03\n",
      "Epoch 11000, Train loss: 2.804e+03, Test loss: 6.220e+03, MSE(e): 2.514e-04, MSE(pi1): 2.159e-02, MSE(pi2): 1.716e-04, MSE(pi3): 7.370e-04\n",
      "Epoch 11100, Train loss: 4.526e+03, Test loss: 1.073e+04, MSE(e): 3.959e-04, MSE(pi1): 3.864e-02, MSE(pi2): 2.143e-04, MSE(pi3): 1.806e-03\n",
      "Epoch 11200, Train loss: 2.335e+03, Test loss: 8.260e+03, MSE(e): 1.944e-04, MSE(pi1): 2.474e-02, MSE(pi2): 1.336e-04, MSE(pi3): 1.431e-03\n",
      "Epoch 11300, Train loss: 2.290e+03, Test loss: 8.439e+03, MSE(e): 1.799e-04, MSE(pi1): 3.265e-02, MSE(pi2): 1.227e-04, MSE(pi3): 1.642e-03\n",
      "Epoch 11400, Train loss: 2.460e+03, Test loss: 6.244e+03, MSE(e): 2.163e-04, MSE(pi1): 2.010e-02, MSE(pi2): 1.401e-04, MSE(pi3): 9.598e-04\n",
      "Epoch 11500, Train loss: 3.989e+03, Test loss: 1.053e+04, MSE(e): 3.628e-04, MSE(pi1): 2.621e-02, MSE(pi2): 2.276e-04, MSE(pi3): 9.911e-04\n",
      "Epoch 11600, Train loss: 2.685e+03, Test loss: 8.542e+03, MSE(e): 2.195e-04, MSE(pi1): 3.732e-02, MSE(pi2): 1.381e-04, MSE(pi3): 1.166e-03\n",
      "Epoch 11700, Train loss: 5.260e+03, Test loss: 8.861e+03, MSE(e): 4.787e-04, MSE(pi1): 3.352e-02, MSE(pi2): 3.353e-04, MSE(pi3): 1.366e-03\n",
      "Epoch 11800, Train loss: 2.246e+03, Test loss: 7.011e+03, MSE(e): 1.987e-04, MSE(pi1): 1.759e-02, MSE(pi2): 1.412e-04, MSE(pi3): 8.283e-04\n",
      "Epoch 11900, Train loss: 4.221e+03, Test loss: 8.605e+03, MSE(e): 3.666e-04, MSE(pi1): 3.619e-02, MSE(pi2): 1.809e-04, MSE(pi3): 1.933e-03\n",
      "Epoch 12000, Train loss: 5.546e+03, Test loss: 1.232e+04, MSE(e): 5.163e-04, MSE(pi1): 2.692e-02, MSE(pi2): 2.654e-04, MSE(pi3): 1.134e-03\n",
      "Epoch 12100, Train loss: 4.016e+03, Test loss: 6.722e+03, MSE(e): 3.570e-04, MSE(pi1): 3.682e-02, MSE(pi2): 2.489e-04, MSE(pi3): 7.719e-04\n",
      "Epoch 12200, Train loss: 2.329e+03, Test loss: 7.838e+03, MSE(e): 1.866e-04, MSE(pi1): 4.039e-02, MSE(pi2): 1.357e-04, MSE(pi3): 5.912e-04\n",
      "Epoch 12300, Train loss: 4.807e+03, Test loss: 1.049e+04, MSE(e): 4.310e-04, MSE(pi1): 3.540e-02, MSE(pi2): 2.425e-04, MSE(pi3): 1.425e-03\n",
      "Epoch 12400, Train loss: 3.196e+03, Test loss: 7.424e+03, MSE(e): 2.986e-04, MSE(pi1): 1.412e-02, MSE(pi2): 1.743e-04, MSE(pi3): 6.815e-04\n",
      "Epoch 12500, Train loss: 2.497e+03, Test loss: 7.588e+03, MSE(e): 2.300e-04, MSE(pi1): 1.127e-02, MSE(pi2): 1.462e-04, MSE(pi3): 8.429e-04\n",
      "Epoch 12600, Train loss: 2.006e+03, Test loss: 7.790e+03, MSE(e): 1.749e-04, MSE(pi1): 1.686e-02, MSE(pi2): 1.152e-04, MSE(pi3): 8.835e-04\n",
      "Epoch 12700, Train loss: 3.240e+03, Test loss: 8.244e+03, MSE(e): 2.645e-04, MSE(pi1): 4.528e-02, MSE(pi2): 1.752e-04, MSE(pi3): 1.425e-03\n",
      "Epoch 12800, Train loss: 4.323e+03, Test loss: 8.688e+03, MSE(e): 3.810e-04, MSE(pi1): 4.155e-02, MSE(pi2): 2.664e-04, MSE(pi3): 9.701e-04\n",
      "Epoch 12900, Train loss: 3.862e+03, Test loss: 8.099e+03, MSE(e): 3.377e-04, MSE(pi1): 3.891e-02, MSE(pi2): 2.370e-04, MSE(pi3): 9.586e-04\n",
      "Epoch 13000, Train loss: 3.788e+03, Test loss: 8.019e+03, MSE(e): 3.169e-04, MSE(pi1): 4.455e-02, MSE(pi2): 2.120e-04, MSE(pi3): 1.733e-03\n",
      "Epoch 13100, Train loss: 2.114e+03, Test loss: 5.852e+03, MSE(e): 1.824e-04, MSE(pi1): 1.977e-02, MSE(pi2): 1.368e-04, MSE(pi3): 9.183e-04\n",
      "Epoch 13200, Train loss: 2.398e+03, Test loss: 7.536e+03, MSE(e): 1.925e-04, MSE(pi1): 3.111e-02, MSE(pi2): 1.303e-04, MSE(pi3): 1.622e-03\n",
      "Epoch 13300, Train loss: 2.446e+03, Test loss: 6.213e+03, MSE(e): 2.173e-04, MSE(pi1): 1.908e-02, MSE(pi2): 1.505e-04, MSE(pi3): 8.257e-04\n",
      "Epoch 13400, Train loss: 2.138e+03, Test loss: 8.322e+03, MSE(e): 1.728e-04, MSE(pi1): 3.096e-02, MSE(pi2): 1.188e-04, MSE(pi3): 1.002e-03\n",
      "Epoch 13500, Train loss: 1.774e+03, Test loss: 8.973e+03, MSE(e): 1.367e-04, MSE(pi1): 2.902e-02, MSE(pi2): 9.674e-05, MSE(pi3): 1.170e-03\n",
      "Epoch 13600, Train loss: 4.610e+03, Test loss: 1.078e+04, MSE(e): 4.303e-04, MSE(pi1): 1.852e-02, MSE(pi2): 2.380e-04, MSE(pi3): 1.218e-03\n",
      "Epoch 13700, Train loss: 2.082e+03, Test loss: 7.858e+03, MSE(e): 1.500e-04, MSE(pi1): 4.063e-02, MSE(pi2): 9.955e-05, MSE(pi3): 1.759e-03\n",
      "Epoch 13800, Train loss: 2.517e+03, Test loss: 6.820e+03, MSE(e): 1.936e-04, MSE(pi1): 4.976e-02, MSE(pi2): 1.204e-04, MSE(pi3): 8.411e-04\n",
      "Epoch 13900, Train loss: 1.977e+03, Test loss: 1.147e+04, MSE(e): 1.498e-04, MSE(pi1): 3.592e-02, MSE(pi2): 1.056e-04, MSE(pi3): 1.197e-03\n",
      "Epoch 14000, Train loss: 2.329e+03, Test loss: 6.742e+03, MSE(e): 1.854e-04, MSE(pi1): 3.596e-02, MSE(pi2): 1.279e-04, MSE(pi3): 1.146e-03\n",
      "Epoch 14100, Train loss: 2.161e+03, Test loss: 7.573e+03, MSE(e): 1.623e-04, MSE(pi1): 4.537e-02, MSE(pi2): 1.106e-04, MSE(pi3): 8.494e-04\n",
      "Epoch 14200, Train loss: 3.578e+03, Test loss: 9.923e+03, MSE(e): 2.812e-04, MSE(pi1): 6.324e-02, MSE(pi2): 1.921e-04, MSE(pi3): 1.331e-03\n",
      "Epoch 14300, Train loss: 3.398e+03, Test loss: 7.488e+03, MSE(e): 2.809e-04, MSE(pi1): 4.096e-02, MSE(pi2): 1.743e-04, MSE(pi3): 1.791e-03\n",
      "Epoch 14400, Train loss: 3.502e+03, Test loss: 6.667e+03, MSE(e): 3.207e-04, MSE(pi1): 2.086e-02, MSE(pi2): 2.179e-04, MSE(pi3): 8.683e-04\n",
      "Epoch 14500, Train loss: 2.464e+03, Test loss: 6.767e+03, MSE(e): 2.220e-04, MSE(pi1): 1.636e-02, MSE(pi2): 1.567e-04, MSE(pi3): 8.040e-04\n",
      "Epoch 14600, Train loss: 1.718e+03, Test loss: 5.682e+03, MSE(e): 1.530e-04, MSE(pi1): 9.282e-03, MSE(pi2): 1.106e-04, MSE(pi3): 9.483e-04\n",
      "Epoch 14700, Train loss: 3.206e+03, Test loss: 8.342e+03, MSE(e): 2.735e-04, MSE(pi1): 3.051e-02, MSE(pi2): 1.852e-04, MSE(pi3): 1.665e-03\n",
      "Epoch 14800, Train loss: 2.257e+03, Test loss: 6.693e+03, MSE(e): 1.671e-04, MSE(pi1): 4.468e-02, MSE(pi2): 1.144e-04, MSE(pi3): 1.387e-03\n",
      "Epoch 14900, Train loss: 2.009e+03, Test loss: 5.700e+03, MSE(e): 1.644e-04, MSE(pi1): 2.753e-02, MSE(pi2): 1.145e-04, MSE(pi3): 8.967e-04\n",
      "Epoch 15000, Train loss: 2.511e+03, Test loss: 7.070e+03, MSE(e): 1.972e-04, MSE(pi1): 4.329e-02, MSE(pi2): 1.343e-04, MSE(pi3): 1.058e-03\n",
      "Epoch 15100, Train loss: 3.124e+03, Test loss: 6.209e+03, MSE(e): 2.738e-04, MSE(pi1): 2.767e-02, MSE(pi2): 1.582e-04, MSE(pi3): 1.098e-03\n",
      "Epoch 15200, Train loss: 1.188e+04, Test loss: 8.988e+03, MSE(e): 1.123e-03, MSE(pi1): 4.038e-02, MSE(pi2): 5.380e-04, MSE(pi3): 2.421e-03\n",
      "Epoch 15300, Train loss: 2.985e+03, Test loss: 1.779e+04, MSE(e): 2.577e-04, MSE(pi1): 2.944e-02, MSE(pi2): 1.537e-04, MSE(pi3): 1.134e-03\n",
      "Epoch 15400, Train loss: 1.063e+04, Test loss: 1.503e+04, MSE(e): 9.964e-04, MSE(pi1): 5.185e-02, MSE(pi2): 4.784e-04, MSE(pi3): 1.452e-03\n",
      "Epoch 15500, Train loss: 2.991e+03, Test loss: 9.206e+03, MSE(e): 2.592e-04, MSE(pi1): 2.621e-02, MSE(pi2): 1.552e-04, MSE(pi3): 1.360e-03\n",
      "Epoch 15600, Train loss: 7.289e+03, Test loss: 1.155e+04, MSE(e): 6.845e-04, MSE(pi1): 2.883e-02, MSE(pi2): 3.924e-04, MSE(pi3): 1.548e-03\n",
      "Epoch 15700, Train loss: 2.923e+03, Test loss: 5.502e+03, MSE(e): 2.623e-04, MSE(pi1): 1.589e-02, MSE(pi2): 1.619e-04, MSE(pi3): 1.403e-03\n",
      "Epoch 15800, Train loss: 1.531e+03, Test loss: 6.157e+03, MSE(e): 1.270e-04, MSE(pi1): 1.864e-02, MSE(pi2): 9.363e-05, MSE(pi3): 7.489e-04\n",
      "Epoch 15900, Train loss: 1.541e+03, Test loss: 5.204e+03, MSE(e): 1.335e-04, MSE(pi1): 1.349e-02, MSE(pi2): 9.650e-05, MSE(pi3): 7.085e-04\n",
      "Epoch 16000, Train loss: 1.846e+03, Test loss: 7.288e+03, MSE(e): 1.410e-04, MSE(pi1): 2.999e-02, MSE(pi2): 9.894e-05, MSE(pi3): 1.363e-03\n",
      "Epoch 16100, Train loss: 2.526e+03, Test loss: 6.065e+03, MSE(e): 2.010e-04, MSE(pi1): 4.198e-02, MSE(pi2): 1.244e-04, MSE(pi3): 9.672e-04\n",
      "Epoch 16200, Train loss: 2.348e+03, Test loss: 7.539e+03, MSE(e): 2.021e-04, MSE(pi1): 1.822e-02, MSE(pi2): 1.310e-04, MSE(pi3): 1.446e-03\n",
      "Epoch 16300, Train loss: 1.649e+04, Test loss: 1.969e+04, MSE(e): 1.591e-03, MSE(pi1): 3.369e-02, MSE(pi2): 7.476e-04, MSE(pi3): 2.361e-03\n",
      "Epoch 16400, Train loss: 1.301e+04, Test loss: 7.278e+03, MSE(e): 1.241e-03, MSE(pi1): 4.383e-02, MSE(pi2): 5.638e-04, MSE(pi3): 1.615e-03\n",
      "Epoch 16500, Train loss: 6.891e+03, Test loss: 6.151e+03, MSE(e): 6.348e-04, MSE(pi1): 4.636e-02, MSE(pi2): 3.172e-04, MSE(pi3): 7.976e-04\n",
      "Epoch 16600, Train loss: 2.106e+03, Test loss: 5.947e+03, MSE(e): 1.763e-04, MSE(pi1): 2.600e-02, MSE(pi2): 1.173e-04, MSE(pi3): 8.323e-04\n",
      "Epoch 16700, Train loss: 1.936e+03, Test loss: 5.266e+03, MSE(e): 1.691e-04, MSE(pi1): 1.679e-02, MSE(pi2): 1.103e-04, MSE(pi3): 7.686e-04\n",
      "Epoch 16800, Train loss: 2.487e+03, Test loss: 6.270e+03, MSE(e): 2.135e-04, MSE(pi1): 2.541e-02, MSE(pi2): 1.502e-04, MSE(pi3): 9.748e-04\n",
      "Epoch 16900, Train loss: 1.580e+03, Test loss: 6.236e+03, MSE(e): 1.253e-04, MSE(pi1): 2.191e-02, MSE(pi2): 8.847e-05, MSE(pi3): 1.072e-03\n",
      "Epoch 17000, Train loss: 2.393e+03, Test loss: 7.022e+03, MSE(e): 1.968e-04, MSE(pi1): 3.095e-02, MSE(pi2): 1.223e-04, MSE(pi3): 1.156e-03\n",
      "Epoch 17100, Train loss: 2.582e+03, Test loss: 6.000e+03, MSE(e): 2.059e-04, MSE(pi1): 3.897e-02, MSE(pi2): 1.279e-04, MSE(pi3): 1.333e-03\n",
      "Epoch 17200, Train loss: 7.441e+03, Test loss: 1.478e+04, MSE(e): 6.349e-04, MSE(pi1): 8.951e-02, MSE(pi2): 3.966e-04, MSE(pi3): 1.970e-03\n",
      "Epoch 17300, Train loss: 3.953e+03, Test loss: 6.390e+03, MSE(e): 3.435e-04, MSE(pi1): 4.393e-02, MSE(pi2): 1.791e-04, MSE(pi3): 7.864e-04\n",
      "Epoch 17400, Train loss: 3.557e+03, Test loss: 7.155e+03, MSE(e): 2.752e-04, MSE(pi1): 7.007e-02, MSE(pi2): 1.566e-04, MSE(pi3): 1.044e-03\n",
      "Epoch 17500, Train loss: 2.164e+03, Test loss: 5.400e+03, MSE(e): 1.909e-04, MSE(pi1): 1.695e-02, MSE(pi2): 1.245e-04, MSE(pi3): 8.525e-04\n",
      "Epoch 17600, Train loss: 4.457e+03, Test loss: 7.180e+03, MSE(e): 4.002e-04, MSE(pi1): 3.552e-02, MSE(pi2): 2.060e-04, MSE(pi3): 9.970e-04\n",
      "Epoch 17700, Train loss: 3.805e+03, Test loss: 7.670e+03, MSE(e): 3.449e-04, MSE(pi1): 2.340e-02, MSE(pi2): 1.912e-04, MSE(pi3): 1.213e-03\n",
      "Epoch 17800, Train loss: 2.907e+04, Test loss: 1.314e+04, MSE(e): 2.817e-03, MSE(pi1): 6.231e-02, MSE(pi2): 1.245e-03, MSE(pi3): 2.805e-03\n",
      "Epoch 17900, Train loss: 2.347e+03, Test loss: 6.203e+03, MSE(e): 1.948e-04, MSE(pi1): 2.790e-02, MSE(pi2): 1.258e-04, MSE(pi3): 1.200e-03\n",
      "Epoch 18000, Train loss: 4.122e+03, Test loss: 5.858e+03, MSE(e): 3.578e-04, MSE(pi1): 3.970e-02, MSE(pi2): 1.807e-04, MSE(pi3): 1.463e-03\n",
      "Epoch 18100, Train loss: 9.326e+03, Test loss: 1.810e+04, MSE(e): 8.792e-04, MSE(pi1): 3.667e-02, MSE(pi2): 4.298e-04, MSE(pi3): 1.660e-03\n",
      "Epoch 18200, Train loss: 1.780e+03, Test loss: 6.855e+03, MSE(e): 1.414e-04, MSE(pi1): 2.409e-02, MSE(pi2): 1.001e-04, MSE(pi3): 1.245e-03\n",
      "Epoch 18300, Train loss: 2.074e+03, Test loss: 6.719e+03, MSE(e): 1.725e-04, MSE(pi1): 2.594e-02, MSE(pi2): 1.075e-04, MSE(pi3): 8.962e-04\n",
      "Epoch 18400, Train loss: 1.995e+03, Test loss: 6.065e+03, MSE(e): 1.644e-04, MSE(pi1): 2.613e-02, MSE(pi2): 1.057e-04, MSE(pi3): 8.916e-04\n",
      "Epoch 18500, Train loss: 4.775e+03, Test loss: 6.339e+03, MSE(e): 4.313e-04, MSE(pi1): 3.707e-02, MSE(pi2): 2.183e-04, MSE(pi3): 9.093e-04\n",
      "Epoch 18600, Train loss: 4.873e+03, Test loss: 6.076e+03, MSE(e): 4.652e-04, MSE(pi1): 1.308e-02, MSE(pi2): 2.235e-04, MSE(pi3): 9.044e-04\n",
      "Epoch 18700, Train loss: 2.937e+03, Test loss: 1.045e+04, MSE(e): 2.374e-04, MSE(pi1): 4.846e-02, MSE(pi2): 1.499e-04, MSE(pi3): 7.830e-04\n",
      "Epoch 18800, Train loss: 4.257e+03, Test loss: 7.024e+03, MSE(e): 3.680e-04, MSE(pi1): 4.161e-02, MSE(pi2): 1.920e-04, MSE(pi3): 1.604e-03\n",
      "Epoch 18900, Train loss: 1.864e+03, Test loss: 6.701e+03, MSE(e): 1.363e-04, MSE(pi1): 3.596e-02, MSE(pi2): 9.345e-05, MSE(pi3): 1.412e-03\n",
      "Epoch 19000, Train loss: 4.079e+03, Test loss: 5.988e+03, MSE(e): 3.572e-04, MSE(pi1): 3.800e-02, MSE(pi2): 1.869e-04, MSE(pi3): 1.262e-03\n",
      "Epoch 19100, Train loss: 2.222e+03, Test loss: 7.424e+03, MSE(e): 1.960e-04, MSE(pi1): 1.816e-02, MSE(pi2): 1.180e-04, MSE(pi3): 8.049e-04\n",
      "Epoch 19200, Train loss: 2.187e+03, Test loss: 6.665e+03, MSE(e): 1.537e-04, MSE(pi1): 4.676e-02, MSE(pi2): 9.229e-05, MSE(pi3): 1.817e-03\n",
      "Epoch 19300, Train loss: 4.035e+03, Test loss: 6.282e+03, MSE(e): 3.575e-04, MSE(pi1): 3.628e-02, MSE(pi2): 1.822e-04, MSE(pi3): 9.707e-04\n",
      "Epoch 19400, Train loss: 2.874e+03, Test loss: 1.007e+04, MSE(e): 2.193e-04, MSE(pi1): 4.437e-02, MSE(pi2): 1.373e-04, MSE(pi3): 2.378e-03\n",
      "Epoch 19500, Train loss: 1.657e+03, Test loss: 6.357e+03, MSE(e): 1.381e-04, MSE(pi1): 1.896e-02, MSE(pi2): 9.479e-05, MSE(pi3): 8.635e-04\n",
      "Epoch 19600, Train loss: 3.229e+03, Test loss: 7.976e+03, MSE(e): 2.676e-04, MSE(pi1): 3.849e-02, MSE(pi2): 1.506e-04, MSE(pi3): 1.680e-03\n",
      "Epoch 19700, Train loss: 5.864e+03, Test loss: 5.696e+03, MSE(e): 5.676e-04, MSE(pi1): 9.316e-03, MSE(pi2): 2.699e-04, MSE(pi3): 9.414e-04\n",
      "Epoch 19800, Train loss: 2.358e+03, Test loss: 7.347e+03, MSE(e): 1.842e-04, MSE(pi1): 4.300e-02, MSE(pi2): 1.249e-04, MSE(pi3): 8.637e-04\n",
      "Epoch 19900, Train loss: 2.605e+03, Test loss: 5.879e+03, MSE(e): 2.218e-04, MSE(pi1): 2.588e-02, MSE(pi2): 1.283e-04, MSE(pi3): 1.279e-03\n",
      "Epoch 20000, Train loss: 2.130e+03, Test loss: 5.886e+03, MSE(e): 1.721e-04, MSE(pi1): 3.226e-02, MSE(pi2): 1.063e-04, MSE(pi3): 8.618e-04\n",
      "Epoch 20100, Train loss: 1.501e+03, Test loss: 5.631e+03, MSE(e): 1.198e-04, MSE(pi1): 1.979e-02, MSE(pi2): 8.577e-05, MSE(pi3): 1.044e-03\n",
      "Epoch 20200, Train loss: 3.172e+03, Test loss: 6.327e+03, MSE(e): 2.686e-04, MSE(pi1): 4.084e-02, MSE(pi2): 1.647e-04, MSE(pi3): 7.801e-04\n",
      "Epoch 20300, Train loss: 5.810e+03, Test loss: 5.554e+03, MSE(e): 5.250e-04, MSE(pi1): 4.684e-02, MSE(pi2): 2.449e-04, MSE(pi3): 9.114e-04\n",
      "Epoch 20400, Train loss: 2.693e+03, Test loss: 7.502e+03, MSE(e): 2.103e-04, MSE(pi1): 4.335e-02, MSE(pi2): 1.353e-04, MSE(pi3): 1.557e-03\n",
      "Epoch 20500, Train loss: 2.609e+03, Test loss: 8.305e+03, MSE(e): 2.214e-04, MSE(pi1): 2.805e-02, MSE(pi2): 1.303e-04, MSE(pi3): 1.147e-03\n",
      "Epoch 20600, Train loss: 4.533e+03, Test loss: 6.771e+03, MSE(e): 4.248e-04, MSE(pi1): 1.719e-02, MSE(pi2): 2.040e-04, MSE(pi3): 1.130e-03\n",
      "Epoch 20700, Train loss: 5.164e+03, Test loss: 6.639e+03, MSE(e): 4.899e-04, MSE(pi1): 1.263e-02, MSE(pi2): 2.571e-04, MSE(pi3): 1.386e-03\n",
      "Epoch 20800, Train loss: 3.624e+03, Test loss: 9.926e+03, MSE(e): 3.236e-04, MSE(pi1): 2.637e-02, MSE(pi2): 1.774e-04, MSE(pi3): 1.233e-03\n",
      "Epoch 20900, Train loss: 8.552e+03, Test loss: 1.280e+04, MSE(e): 8.115e-04, MSE(pi1): 3.707e-02, MSE(pi2): 3.850e-04, MSE(pi3): 6.592e-04\n",
      "Epoch 21000, Train loss: 1.689e+03, Test loss: 5.401e+03, MSE(e): 1.390e-04, MSE(pi1): 1.850e-02, MSE(pi2): 9.126e-05, MSE(pi3): 1.133e-03\n",
      "Epoch 21100, Train loss: 2.588e+03, Test loss: 7.989e+03, MSE(e): 2.173e-04, MSE(pi1): 1.870e-02, MSE(pi2): 1.363e-04, MSE(pi3): 2.281e-03\n",
      "Epoch 21200, Train loss: 1.390e+03, Test loss: 5.596e+03, MSE(e): 1.222e-04, MSE(pi1): 8.891e-03, MSE(pi2): 8.846e-05, MSE(pi3): 7.885e-04\n",
      "Epoch 21300, Train loss: 3.351e+03, Test loss: 5.510e+03, MSE(e): 2.784e-04, MSE(pi1): 4.353e-02, MSE(pi2): 1.836e-04, MSE(pi3): 1.312e-03\n",
      "Epoch 21400, Train loss: 1.261e+04, Test loss: 6.788e+03, MSE(e): 1.209e-03, MSE(pi1): 3.825e-02, MSE(pi2): 5.270e-04, MSE(pi3): 1.335e-03\n",
      "Epoch 21500, Train loss: 2.207e+03, Test loss: 9.587e+03, MSE(e): 1.975e-04, MSE(pi1): 1.222e-02, MSE(pi2): 1.390e-04, MSE(pi3): 1.088e-03\n",
      "Epoch 21600, Train loss: 1.877e+03, Test loss: 5.381e+03, MSE(e): 1.597e-04, MSE(pi1): 1.940e-02, MSE(pi2): 1.052e-04, MSE(pi3): 8.654e-04\n",
      "Epoch 21700, Train loss: 2.834e+03, Test loss: 7.202e+03, MSE(e): 2.277e-04, MSE(pi1): 4.732e-02, MSE(pi2): 1.331e-04, MSE(pi3): 8.388e-04\n",
      "Epoch 21800, Train loss: 2.313e+04, Test loss: 2.093e+04, MSE(e): 2.277e-03, MSE(pi1): 2.776e-02, MSE(pi2): 9.720e-04, MSE(pi3): 8.808e-04\n",
      "Epoch 21900, Train loss: 2.058e+03, Test loss: 6.467e+03, MSE(e): 1.382e-04, MSE(pi1): 5.448e-02, MSE(pi2): 9.338e-05, MSE(pi3): 1.315e-03\n",
      "Epoch 22000, Train loss: 1.827e+03, Test loss: 6.447e+03, MSE(e): 1.588e-04, MSE(pi1): 1.604e-02, MSE(pi2): 1.085e-04, MSE(pi3): 7.911e-04\n",
      "Epoch 22100, Train loss: 1.748e+04, Test loss: 1.194e+04, MSE(e): 1.689e-03, MSE(pi1): 4.757e-02, MSE(pi2): 7.180e-04, MSE(pi3): 1.128e-03\n",
      "Epoch 22200, Train loss: 3.959e+03, Test loss: 7.070e+03, MSE(e): 3.588e-04, MSE(pi1): 2.886e-02, MSE(pi2): 1.984e-04, MSE(pi3): 8.290e-04\n",
      "Epoch 22300, Train loss: 2.240e+03, Test loss: 5.837e+03, MSE(e): 1.819e-04, MSE(pi1): 2.879e-02, MSE(pi2): 1.108e-04, MSE(pi3): 1.338e-03\n",
      "Epoch 22400, Train loss: 3.653e+03, Test loss: 6.463e+03, MSE(e): 3.402e-04, MSE(pi1): 1.626e-02, MSE(pi2): 1.881e-04, MSE(pi3): 8.885e-04\n",
      "Epoch 22500, Train loss: 1.733e+03, Test loss: 5.522e+03, MSE(e): 1.401e-04, MSE(pi1): 2.246e-02, MSE(pi2): 9.193e-05, MSE(pi3): 1.079e-03\n",
      "Epoch 22600, Train loss: 3.334e+03, Test loss: 8.234e+03, MSE(e): 2.822e-04, MSE(pi1): 3.353e-02, MSE(pi2): 1.685e-04, MSE(pi3): 1.769e-03\n",
      "Epoch 22700, Train loss: 2.507e+03, Test loss: 6.542e+03, MSE(e): 2.069e-04, MSE(pi1): 3.659e-02, MSE(pi2): 1.349e-04, MSE(pi3): 7.241e-04\n",
      "Epoch 22800, Train loss: 1.450e+03, Test loss: 5.420e+03, MSE(e): 1.225e-04, MSE(pi1): 1.125e-02, MSE(pi2): 8.766e-05, MSE(pi3): 1.117e-03\n",
      "Epoch 22900, Train loss: 1.946e+03, Test loss: 5.513e+03, MSE(e): 1.523e-04, MSE(pi1): 3.006e-02, MSE(pi2): 1.056e-04, MSE(pi3): 1.226e-03\n",
      "Epoch 23000, Train loss: 1.613e+03, Test loss: 5.577e+03, MSE(e): 1.298e-04, MSE(pi1): 2.339e-02, MSE(pi2): 9.278e-05, MSE(pi3): 8.061e-04\n",
      "Epoch 23100, Train loss: 1.548e+03, Test loss: 6.463e+03, MSE(e): 1.321e-04, MSE(pi1): 1.351e-02, MSE(pi2): 9.009e-05, MSE(pi3): 9.124e-04\n",
      "Epoch 23200, Train loss: 1.904e+03, Test loss: 5.800e+03, MSE(e): 1.559e-04, MSE(pi1): 2.036e-02, MSE(pi2): 9.661e-05, MSE(pi3): 1.413e-03\n",
      "Epoch 23300, Train loss: 2.293e+03, Test loss: 6.860e+03, MSE(e): 2.023e-04, MSE(pi1): 1.413e-02, MSE(pi2): 1.407e-04, MSE(pi3): 1.289e-03\n",
      "Epoch 23400, Train loss: 7.447e+03, Test loss: 2.145e+04, MSE(e): 6.808e-04, MSE(pi1): 5.189e-02, MSE(pi2): 3.020e-04, MSE(pi3): 1.197e-03\n",
      "Epoch 23500, Train loss: 1.761e+03, Test loss: 6.232e+03, MSE(e): 1.224e-04, MSE(pi1): 4.144e-02, MSE(pi2): 8.595e-05, MSE(pi3): 1.229e-03\n",
      "Epoch 23600, Train loss: 2.291e+03, Test loss: 6.238e+03, MSE(e): 1.674e-04, MSE(pi1): 5.109e-02, MSE(pi2): 9.881e-05, MSE(pi3): 1.056e-03\n",
      "Epoch 23700, Train loss: 1.971e+03, Test loss: 6.896e+03, MSE(e): 1.444e-04, MSE(pi1): 3.657e-02, MSE(pi2): 9.500e-05, MSE(pi3): 1.609e-03\n",
      "Epoch 23800, Train loss: 5.540e+03, Test loss: 1.068e+04, MSE(e): 4.938e-04, MSE(pi1): 5.231e-02, MSE(pi2): 2.658e-04, MSE(pi3): 7.897e-04\n",
      "Epoch 23900, Train loss: 2.124e+03, Test loss: 7.437e+03, MSE(e): 1.629e-04, MSE(pi1): 3.880e-02, MSE(pi2): 1.024e-04, MSE(pi3): 1.062e-03\n",
      "Epoch 24000, Train loss: 3.612e+03, Test loss: 7.313e+03, MSE(e): 3.161e-04, MSE(pi1): 3.804e-02, MSE(pi2): 1.858e-04, MSE(pi3): 7.060e-04\n",
      "Epoch 24100, Train loss: 2.744e+03, Test loss: 6.498e+03, MSE(e): 2.167e-04, MSE(pi1): 4.033e-02, MSE(pi2): 1.223e-04, MSE(pi3): 1.735e-03\n",
      "Epoch 24200, Train loss: 6.529e+03, Test loss: 2.374e+04, MSE(e): 5.680e-04, MSE(pi1): 5.352e-02, MSE(pi2): 3.110e-04, MSE(pi3): 3.136e-03\n",
      "Epoch 24300, Train loss: 3.496e+03, Test loss: 6.617e+03, MSE(e): 2.986e-04, MSE(pi1): 3.611e-02, MSE(pi2): 1.746e-04, MSE(pi3): 1.488e-03\n",
      "Epoch 24400, Train loss: 2.306e+03, Test loss: 9.720e+03, MSE(e): 2.091e-04, MSE(pi1): 1.175e-02, MSE(pi2): 1.383e-04, MSE(pi3): 9.698e-04\n",
      "Epoch 24500, Train loss: 3.831e+03, Test loss: 9.563e+03, MSE(e): 3.122e-04, MSE(pi1): 5.018e-02, MSE(pi2): 1.819e-04, MSE(pi3): 2.074e-03\n",
      "Epoch 24600, Train loss: 2.302e+03, Test loss: 8.983e+03, MSE(e): 1.493e-04, MSE(pi1): 5.568e-02, MSE(pi2): 9.769e-05, MSE(pi3): 2.522e-03\n",
      "Epoch 24700, Train loss: 1.819e+03, Test loss: 6.054e+03, MSE(e): 1.322e-04, MSE(pi1): 4.046e-02, MSE(pi2): 8.911e-05, MSE(pi3): 9.197e-04\n",
      "Epoch 24800, Train loss: 4.283e+03, Test loss: 7.552e+03, MSE(e): 4.076e-04, MSE(pi1): 8.949e-03, MSE(pi2): 2.202e-04, MSE(pi3): 1.173e-03\n",
      "Epoch 24900, Train loss: 2.187e+03, Test loss: 5.181e+03, MSE(e): 1.886e-04, MSE(pi1): 2.003e-02, MSE(pi2): 1.116e-04, MSE(pi3): 1.006e-03\n",
      "Epoch 25000, Train loss: 3.655e+03, Test loss: 6.438e+03, MSE(e): 3.314e-04, MSE(pi1): 2.663e-02, MSE(pi2): 1.674e-04, MSE(pi3): 7.449e-04\n",
      "Epoch 25100, Train loss: 1.527e+03, Test loss: 6.484e+03, MSE(e): 1.218e-04, MSE(pi1): 2.202e-02, MSE(pi2): 8.727e-05, MSE(pi3): 8.811e-04\n",
      "Epoch 25200, Train loss: 1.904e+03, Test loss: 8.131e+03, MSE(e): 1.522e-04, MSE(pi1): 2.427e-02, MSE(pi2): 9.992e-05, MSE(pi3): 1.392e-03\n",
      "Epoch 25300, Train loss: 3.360e+03, Test loss: 6.748e+03, MSE(e): 2.668e-04, MSE(pi1): 5.000e-02, MSE(pi2): 1.463e-04, MSE(pi3): 1.928e-03\n",
      "Epoch 25400, Train loss: 1.824e+03, Test loss: 6.707e+03, MSE(e): 1.420e-04, MSE(pi1): 2.966e-02, MSE(pi2): 9.215e-05, MSE(pi3): 1.076e-03\n",
      "Epoch 25500, Train loss: 1.743e+03, Test loss: 6.121e+03, MSE(e): 1.486e-04, MSE(pi1): 1.670e-02, MSE(pi2): 9.242e-05, MSE(pi3): 8.966e-04\n",
      "Epoch 25600, Train loss: 4.225e+03, Test loss: 8.243e+03, MSE(e): 3.373e-04, MSE(pi1): 7.436e-02, MSE(pi2): 1.692e-04, MSE(pi3): 1.077e-03\n",
      "Epoch 25700, Train loss: 2.072e+03, Test loss: 5.403e+03, MSE(e): 1.628e-04, MSE(pi1): 3.355e-02, MSE(pi2): 1.033e-04, MSE(pi3): 1.079e-03\n",
      "Epoch 25800, Train loss: 7.611e+03, Test loss: 1.298e+04, MSE(e): 7.265e-04, MSE(pi1): 2.590e-02, MSE(pi2): 3.717e-04, MSE(pi3): 8.742e-04\n",
      "Epoch 25900, Train loss: 2.640e+03, Test loss: 7.889e+03, MSE(e): 1.732e-04, MSE(pi1): 6.939e-02, MSE(pi2): 1.003e-04, MSE(pi3): 2.138e-03\n",
      "Epoch 26000, Train loss: 7.424e+03, Test loss: 6.815e+03, MSE(e): 7.019e-04, MSE(pi1): 3.266e-02, MSE(pi2): 3.217e-04, MSE(pi3): 7.809e-04\n",
      "Epoch 26100, Train loss: 2.056e+03, Test loss: 5.022e+03, MSE(e): 1.719e-04, MSE(pi1): 2.509e-02, MSE(pi2): 1.133e-04, MSE(pi3): 8.556e-04\n",
      "Epoch 26200, Train loss: 1.825e+03, Test loss: 6.005e+03, MSE(e): 1.476e-04, MSE(pi1): 2.523e-02, MSE(pi2): 9.215e-05, MSE(pi3): 9.621e-04\n",
      "Epoch 26300, Train loss: 1.242e+04, Test loss: 1.695e+04, MSE(e): 1.193e-03, MSE(pi1): 3.745e-02, MSE(pi2): 5.335e-04, MSE(pi3): 1.187e-03\n",
      "Epoch 26400, Train loss: 1.816e+03, Test loss: 8.869e+03, MSE(e): 1.585e-04, MSE(pi1): 1.199e-02, MSE(pi2): 1.075e-04, MSE(pi3): 1.109e-03\n",
      "Epoch 26500, Train loss: 7.522e+03, Test loss: 6.675e+03, MSE(e): 7.150e-04, MSE(pi1): 2.572e-02, MSE(pi2): 3.340e-04, MSE(pi3): 1.136e-03\n",
      "Epoch 26600, Train loss: 2.881e+03, Test loss: 6.348e+03, MSE(e): 2.617e-04, MSE(pi1): 1.556e-02, MSE(pi2): 1.440e-04, MSE(pi3): 1.080e-03\n",
      "Epoch 26700, Train loss: 4.102e+03, Test loss: 7.215e+03, MSE(e): 3.797e-04, MSE(pi1): 1.838e-02, MSE(pi2): 2.263e-04, MSE(pi3): 1.211e-03\n",
      "Epoch 26800, Train loss: 1.075e+04, Test loss: 9.948e+03, MSE(e): 1.021e-03, MSE(pi1): 3.461e-02, MSE(pi2): 4.909e-04, MSE(pi3): 1.879e-03\n",
      "Epoch 26900, Train loss: 1.179e+04, Test loss: 1.594e+04, MSE(e): 1.138e-03, MSE(pi1): 2.499e-02, MSE(pi2): 5.000e-04, MSE(pi3): 1.621e-03\n",
      "Epoch 27000, Train loss: 1.814e+03, Test loss: 5.858e+03, MSE(e): 1.299e-04, MSE(pi1): 4.138e-02, MSE(pi2): 8.719e-05, MSE(pi3): 1.018e-03\n",
      "Epoch 27100, Train loss: 2.573e+03, Test loss: 5.059e+03, MSE(e): 2.427e-04, MSE(pi1): 6.552e-03, MSE(pi2): 1.394e-04, MSE(pi3): 8.010e-04\n",
      "Epoch 27200, Train loss: 2.152e+03, Test loss: 5.352e+03, MSE(e): 1.624e-04, MSE(pi1): 4.397e-02, MSE(pi2): 1.014e-04, MSE(pi3): 8.793e-04\n",
      "Epoch 27300, Train loss: 2.039e+03, Test loss: 5.747e+03, MSE(e): 1.504e-04, MSE(pi1): 4.376e-02, MSE(pi2): 1.035e-04, MSE(pi3): 9.719e-04\n",
      "Epoch 27400, Train loss: 3.482e+03, Test loss: 6.953e+03, MSE(e): 3.035e-04, MSE(pi1): 2.429e-02, MSE(pi2): 1.621e-04, MSE(pi3): 2.045e-03\n",
      "Epoch 27500, Train loss: 1.740e+03, Test loss: 5.197e+03, MSE(e): 1.435e-04, MSE(pi1): 1.901e-02, MSE(pi2): 8.919e-05, MSE(pi3): 1.141e-03\n",
      "Epoch 27600, Train loss: 1.495e+03, Test loss: 5.678e+03, MSE(e): 1.206e-04, MSE(pi1): 2.104e-02, MSE(pi2): 8.504e-05, MSE(pi3): 7.790e-04\n",
      "Epoch 27700, Train loss: 1.531e+03, Test loss: 6.120e+03, MSE(e): 1.108e-04, MSE(pi1): 3.351e-02, MSE(pi2): 7.906e-05, MSE(pi3): 8.768e-04\n",
      "Epoch 27800, Train loss: 1.989e+03, Test loss: 5.677e+03, MSE(e): 1.694e-04, MSE(pi1): 2.058e-02, MSE(pi2): 1.061e-04, MSE(pi3): 8.938e-04\n",
      "Epoch 27900, Train loss: 2.733e+03, Test loss: 5.847e+03, MSE(e): 2.425e-04, MSE(pi1): 2.327e-02, MSE(pi2): 1.319e-04, MSE(pi3): 7.526e-04\n",
      "Epoch 28000, Train loss: 2.508e+03, Test loss: 5.353e+03, MSE(e): 2.019e-04, MSE(pi1): 3.448e-02, MSE(pi2): 1.161e-04, MSE(pi3): 1.436e-03\n",
      "Epoch 28100, Train loss: 3.793e+03, Test loss: 8.300e+03, MSE(e): 3.407e-04, MSE(pi1): 2.643e-02, MSE(pi2): 1.672e-04, MSE(pi3): 1.216e-03\n",
      "Epoch 28200, Train loss: 3.524e+03, Test loss: 8.092e+03, MSE(e): 3.011e-04, MSE(pi1): 3.998e-02, MSE(pi2): 1.683e-04, MSE(pi3): 1.126e-03\n",
      "Epoch 28300, Train loss: 2.203e+03, Test loss: 7.450e+03, MSE(e): 1.684e-04, MSE(pi1): 3.679e-02, MSE(pi2): 1.001e-04, MSE(pi3): 1.508e-03\n",
      "Epoch 28400, Train loss: 2.825e+03, Test loss: 7.800e+03, MSE(e): 2.378e-04, MSE(pi1): 3.324e-02, MSE(pi2): 1.295e-04, MSE(pi3): 1.153e-03\n",
      "Epoch 28500, Train loss: 2.362e+03, Test loss: 6.969e+03, MSE(e): 2.163e-04, MSE(pi1): 1.178e-02, MSE(pi2): 1.219e-04, MSE(pi3): 8.095e-04\n",
      "Epoch 28600, Train loss: 3.518e+03, Test loss: 7.286e+03, MSE(e): 2.746e-04, MSE(pi1): 6.587e-02, MSE(pi2): 1.446e-04, MSE(pi3): 1.129e-03\n",
      "Epoch 28700, Train loss: 5.197e+03, Test loss: 1.054e+04, MSE(e): 4.788e-04, MSE(pi1): 2.556e-02, MSE(pi2): 2.154e-04, MSE(pi3): 1.531e-03\n",
      "Epoch 28800, Train loss: 3.196e+03, Test loss: 7.005e+03, MSE(e): 2.848e-04, MSE(pi1): 2.350e-02, MSE(pi2): 1.524e-04, MSE(pi3): 1.124e-03\n",
      "Epoch 28900, Train loss: 1.827e+04, Test loss: 1.664e+04, MSE(e): 1.803e-03, MSE(pi1): 1.044e-02, MSE(pi2): 7.828e-04, MSE(pi3): 1.372e-03\n",
      "Epoch 29000, Train loss: 4.150e+03, Test loss: 1.104e+04, MSE(e): 3.825e-04, MSE(pi1): 2.168e-02, MSE(pi2): 1.933e-04, MSE(pi3): 1.081e-03\n",
      "Epoch 29100, Train loss: 1.401e+03, Test loss: 5.633e+03, MSE(e): 1.119e-04, MSE(pi1): 1.958e-02, MSE(pi2): 7.858e-05, MSE(pi3): 8.578e-04\n",
      "Epoch 29200, Train loss: 1.365e+04, Test loss: 1.357e+04, MSE(e): 1.340e-03, MSE(pi1): 1.691e-02, MSE(pi2): 5.598e-04, MSE(pi3): 8.402e-04\n",
      "Epoch 29300, Train loss: 1.612e+03, Test loss: 5.288e+03, MSE(e): 1.235e-04, MSE(pi1): 2.946e-02, MSE(pi2): 8.230e-05, MSE(pi3): 8.138e-04\n",
      "Epoch 29400, Train loss: 2.041e+03, Test loss: 5.804e+03, MSE(e): 1.537e-04, MSE(pi1): 3.986e-02, MSE(pi2): 9.580e-05, MSE(pi3): 1.047e-03\n",
      "Epoch 29500, Train loss: 6.177e+03, Test loss: 7.177e+03, MSE(e): 5.492e-04, MSE(pi1): 4.500e-02, MSE(pi2): 2.660e-04, MSE(pi3): 2.346e-03\n",
      "Epoch 29600, Train loss: 3.072e+03, Test loss: 1.885e+04, MSE(e): 2.663e-04, MSE(pi1): 2.699e-02, MSE(pi2): 1.503e-04, MSE(pi3): 1.390e-03\n",
      "Epoch 29700, Train loss: 2.518e+03, Test loss: 1.060e+04, MSE(e): 2.086e-04, MSE(pi1): 3.616e-02, MSE(pi2): 1.347e-04, MSE(pi3): 7.078e-04\n",
      "Epoch 29800, Train loss: 2.444e+04, Test loss: 9.055e+03, MSE(e): 2.412e-03, MSE(pi1): 2.182e-02, MSE(pi2): 9.839e-04, MSE(pi3): 9.368e-04\n",
      "Epoch 29900, Train loss: 7.191e+03, Test loss: 1.967e+04, MSE(e): 6.574e-04, MSE(pi1): 4.522e-02, MSE(pi2): 3.210e-04, MSE(pi3): 1.645e-03\n",
      "Epoch 30000, Train loss: 1.893e+03, Test loss: 7.369e+03, MSE(e): 1.297e-04, MSE(pi1): 4.189e-02, MSE(pi2): 8.286e-05, MSE(pi3): 1.770e-03\n",
      "Epoch 30100, Train loss: 3.997e+03, Test loss: 5.640e+03, MSE(e): 3.632e-04, MSE(pi1): 2.235e-02, MSE(pi2): 1.861e-04, MSE(pi3): 1.409e-03\n",
      "Epoch 30200, Train loss: 5.445e+03, Test loss: 6.628e+03, MSE(e): 4.999e-04, MSE(pi1): 3.262e-02, MSE(pi2): 2.480e-04, MSE(pi3): 1.195e-03\n",
      "Epoch 30300, Train loss: 2.268e+03, Test loss: 5.904e+03, MSE(e): 2.014e-04, MSE(pi1): 1.267e-02, MSE(pi2): 1.182e-04, MSE(pi3): 1.269e-03\n",
      "Epoch 30400, Train loss: 2.187e+03, Test loss: 5.917e+03, MSE(e): 1.675e-04, MSE(pi1): 3.661e-02, MSE(pi2): 9.605e-05, MSE(pi3): 1.465e-03\n",
      "Epoch 30500, Train loss: 2.261e+04, Test loss: 9.195e+03, MSE(e): 2.218e-03, MSE(pi1): 3.232e-02, MSE(pi2): 9.002e-04, MSE(pi3): 9.853e-04\n",
      "Epoch 30600, Train loss: 2.735e+03, Test loss: 5.215e+03, MSE(e): 2.519e-04, MSE(pi1): 1.374e-02, MSE(pi2): 1.338e-04, MSE(pi3): 7.905e-04\n",
      "Epoch 30700, Train loss: 5.363e+03, Test loss: 7.369e+03, MSE(e): 4.946e-04, MSE(pi1): 3.268e-02, MSE(pi2): 2.267e-04, MSE(pi3): 9.064e-04\n",
      "Epoch 30800, Train loss: 2.102e+03, Test loss: 7.369e+03, MSE(e): 1.477e-04, MSE(pi1): 4.809e-02, MSE(pi2): 9.263e-05, MSE(pi3): 1.443e-03\n",
      "Epoch 30900, Train loss: 1.772e+03, Test loss: 5.171e+03, MSE(e): 1.606e-04, MSE(pi1): 8.530e-03, MSE(pi2): 9.757e-05, MSE(pi3): 8.102e-04\n",
      "Epoch 31000, Train loss: 2.001e+03, Test loss: 6.499e+03, MSE(e): 1.528e-04, MSE(pi1): 3.165e-02, MSE(pi2): 1.050e-04, MSE(pi3): 1.564e-03\n",
      "Epoch 31100, Train loss: 5.355e+03, Test loss: 7.598e+03, MSE(e): 4.875e-04, MSE(pi1): 3.601e-02, MSE(pi2): 2.126e-04, MSE(pi3): 1.204e-03\n",
      "Epoch 31200, Train loss: 2.672e+03, Test loss: 5.100e+03, MSE(e): 2.030e-04, MSE(pi1): 5.590e-02, MSE(pi2): 1.136e-04, MSE(pi3): 8.288e-04\n",
      "Epoch 31300, Train loss: 2.863e+03, Test loss: 5.632e+03, MSE(e): 2.626e-04, MSE(pi1): 1.377e-02, MSE(pi2): 1.367e-04, MSE(pi3): 9.923e-04\n",
      "Epoch 31400, Train loss: 4.197e+03, Test loss: 9.892e+03, MSE(e): 3.817e-04, MSE(pi1): 9.925e-03, MSE(pi2): 1.987e-04, MSE(pi3): 2.808e-03\n",
      "Epoch 31500, Train loss: 3.938e+03, Test loss: 5.636e+03, MSE(e): 3.742e-04, MSE(pi1): 1.047e-02, MSE(pi2): 1.908e-04, MSE(pi3): 9.076e-04\n",
      "Epoch 31600, Train loss: 2.352e+03, Test loss: 7.161e+03, MSE(e): 1.894e-04, MSE(pi1): 3.483e-02, MSE(pi2): 1.142e-04, MSE(pi3): 1.095e-03\n",
      "Epoch 31700, Train loss: 2.719e+03, Test loss: 6.330e+03, MSE(e): 2.216e-04, MSE(pi1): 4.060e-02, MSE(pi2): 1.194e-04, MSE(pi3): 9.679e-04\n",
      "Epoch 31800, Train loss: 1.684e+03, Test loss: 5.578e+03, MSE(e): 1.256e-04, MSE(pi1): 3.574e-02, MSE(pi2): 8.199e-05, MSE(pi3): 7.083e-04\n",
      "Epoch 31900, Train loss: 1.740e+03, Test loss: 5.432e+03, MSE(e): 1.487e-04, MSE(pi1): 1.666e-02, MSE(pi2): 9.042e-05, MSE(pi3): 8.651e-04\n",
      "Epoch 32000, Train loss: 4.852e+03, Test loss: 1.260e+04, MSE(e): 4.402e-04, MSE(pi1): 2.721e-02, MSE(pi2): 2.822e-04, MSE(pi3): 1.780e-03\n",
      "Epoch 32100, Train loss: 1.512e+03, Test loss: 5.529e+03, MSE(e): 1.248e-04, MSE(pi1): 1.805e-02, MSE(pi2): 8.310e-05, MSE(pi3): 8.366e-04\n",
      "Epoch 32200, Train loss: 1.797e+03, Test loss: 5.944e+03, MSE(e): 1.358e-04, MSE(pi1): 3.594e-02, MSE(pi2): 8.647e-05, MSE(pi3): 7.995e-04\n",
      "Epoch 32300, Train loss: 2.397e+03, Test loss: 5.680e+03, MSE(e): 2.227e-04, MSE(pi1): 7.435e-03, MSE(pi2): 1.157e-04, MSE(pi3): 9.567e-04\n",
      "Epoch 32400, Train loss: 8.622e+03, Test loss: 1.408e+04, MSE(e): 8.306e-04, MSE(pi1): 1.405e-02, MSE(pi2): 3.845e-04, MSE(pi3): 1.752e-03\n",
      "Epoch 32500, Train loss: 2.087e+03, Test loss: 6.678e+03, MSE(e): 1.639e-04, MSE(pi1): 3.244e-02, MSE(pi2): 1.028e-04, MSE(pi3): 1.240e-03\n",
      "Epoch 32600, Train loss: 5.328e+03, Test loss: 6.577e+03, MSE(e): 4.886e-04, MSE(pi1): 3.156e-02, MSE(pi2): 2.288e-04, MSE(pi3): 1.259e-03\n",
      "Epoch 32700, Train loss: 1.610e+03, Test loss: 6.695e+03, MSE(e): 1.295e-04, MSE(pi1): 2.238e-02, MSE(pi2): 8.636e-05, MSE(pi3): 9.093e-04\n",
      "Epoch 32800, Train loss: 2.352e+03, Test loss: 5.746e+03, MSE(e): 1.888e-04, MSE(pi1): 3.765e-02, MSE(pi2): 1.077e-04, MSE(pi3): 8.806e-04\n",
      "Epoch 32900, Train loss: 3.812e+03, Test loss: 6.071e+03, MSE(e): 3.388e-04, MSE(pi1): 2.998e-02, MSE(pi2): 1.913e-04, MSE(pi3): 1.241e-03\n",
      "Epoch 33000, Train loss: 2.275e+03, Test loss: 8.813e+03, MSE(e): 1.760e-04, MSE(pi1): 2.360e-02, MSE(pi2): 9.919e-05, MSE(pi3): 2.782e-03\n",
      "Epoch 33100, Train loss: 4.831e+03, Test loss: 9.794e+03, MSE(e): 4.354e-04, MSE(pi1): 3.726e-02, MSE(pi2): 2.144e-04, MSE(pi3): 1.036e-03\n",
      "Epoch 33200, Train loss: 2.372e+03, Test loss: 8.155e+03, MSE(e): 1.722e-04, MSE(pi1): 4.599e-02, MSE(pi2): 9.844e-05, MSE(pi3): 1.898e-03\n",
      "Epoch 33300, Train loss: 2.815e+03, Test loss: 8.067e+03, MSE(e): 2.201e-04, MSE(pi1): 5.061e-02, MSE(pi2): 1.143e-04, MSE(pi3): 1.080e-03\n",
      "Epoch 33400, Train loss: 5.537e+03, Test loss: 9.972e+03, MSE(e): 5.025e-04, MSE(pi1): 3.882e-02, MSE(pi2): 2.280e-04, MSE(pi3): 1.238e-03\n",
      "Epoch 33500, Train loss: 5.819e+03, Test loss: 1.010e+04, MSE(e): 5.403e-04, MSE(pi1): 2.725e-02, MSE(pi2): 2.515e-04, MSE(pi3): 1.442e-03\n",
      "Epoch 33600, Train loss: 5.568e+03, Test loss: 1.061e+04, MSE(e): 4.984e-04, MSE(pi1): 4.584e-02, MSE(pi2): 2.198e-04, MSE(pi3): 1.251e-03\n",
      "Epoch 33700, Train loss: 3.480e+03, Test loss: 8.464e+03, MSE(e): 3.093e-04, MSE(pi1): 2.454e-02, MSE(pi2): 1.491e-04, MSE(pi3): 1.412e-03\n",
      "Epoch 33800, Train loss: 2.872e+03, Test loss: 8.952e+03, MSE(e): 2.354e-04, MSE(pi1): 2.657e-02, MSE(pi2): 1.211e-04, MSE(pi3): 2.528e-03\n",
      "Epoch 33900, Train loss: 1.813e+03, Test loss: 7.725e+03, MSE(e): 1.402e-04, MSE(pi1): 3.101e-02, MSE(pi2): 8.456e-05, MSE(pi3): 1.012e-03\n",
      "Epoch 34000, Train loss: 3.812e+03, Test loss: 9.407e+03, MSE(e): 3.502e-04, MSE(pi1): 2.220e-02, MSE(pi2): 1.875e-04, MSE(pi3): 8.840e-04\n",
      "Epoch 34100, Train loss: 2.635e+03, Test loss: 5.943e+03, MSE(e): 2.170e-04, MSE(pi1): 3.600e-02, MSE(pi2): 1.210e-04, MSE(pi3): 1.052e-03\n",
      "Epoch 34200, Train loss: 3.670e+03, Test loss: 7.707e+03, MSE(e): 3.189e-04, MSE(pi1): 3.002e-02, MSE(pi2): 1.606e-04, MSE(pi3): 1.807e-03\n",
      "Epoch 34300, Train loss: 1.649e+03, Test loss: 5.906e+03, MSE(e): 1.294e-04, MSE(pi1): 2.697e-02, MSE(pi2): 8.200e-05, MSE(pi3): 8.526e-04\n",
      "Epoch 34400, Train loss: 1.756e+03, Test loss: 7.520e+03, MSE(e): 1.185e-04, MSE(pi1): 4.649e-02, MSE(pi2): 7.640e-05, MSE(pi3): 1.068e-03\n",
      "Epoch 34500, Train loss: 2.233e+03, Test loss: 8.510e+03, MSE(e): 1.820e-04, MSE(pi1): 2.892e-02, MSE(pi2): 1.102e-04, MSE(pi3): 1.237e-03\n",
      "Epoch 34600, Train loss: 1.605e+03, Test loss: 6.901e+03, MSE(e): 1.366e-04, MSE(pi1): 8.559e-03, MSE(pi2): 8.598e-05, MSE(pi3): 1.536e-03\n",
      "Epoch 34700, Train loss: 2.090e+03, Test loss: 5.736e+03, MSE(e): 1.619e-04, MSE(pi1): 3.316e-02, MSE(pi2): 8.914e-05, MSE(pi3): 1.398e-03\n",
      "Epoch 34800, Train loss: 5.746e+03, Test loss: 7.312e+03, MSE(e): 5.356e-04, MSE(pi1): 2.808e-02, MSE(pi2): 2.418e-04, MSE(pi3): 1.098e-03\n",
      "Epoch 34900, Train loss: 4.008e+03, Test loss: 1.398e+04, MSE(e): 3.066e-04, MSE(pi1): 7.222e-02, MSE(pi2): 1.954e-04, MSE(pi3): 2.191e-03\n",
      "Epoch 35000, Train loss: 1.980e+03, Test loss: 6.159e+03, MSE(e): 1.583e-04, MSE(pi1): 2.952e-02, MSE(pi2): 9.829e-05, MSE(pi3): 1.023e-03\n",
      "Epoch 35100, Train loss: 2.546e+03, Test loss: 5.674e+03, MSE(e): 2.118e-04, MSE(pi1): 3.144e-02, MSE(pi2): 1.122e-04, MSE(pi3): 1.136e-03\n",
      "Epoch 35200, Train loss: 7.148e+03, Test loss: 1.070e+04, MSE(e): 6.559e-04, MSE(pi1): 4.747e-02, MSE(pi2): 2.875e-04, MSE(pi3): 1.139e-03\n",
      "Epoch 35300, Train loss: 1.635e+03, Test loss: 6.479e+03, MSE(e): 1.340e-04, MSE(pi1): 1.794e-02, MSE(pi2): 8.622e-05, MSE(pi3): 1.157e-03\n",
      "Epoch 35400, Train loss: 1.893e+03, Test loss: 5.835e+03, MSE(e): 1.402e-04, MSE(pi1): 3.668e-02, MSE(pi2): 8.466e-05, MSE(pi3): 1.246e-03\n",
      "Epoch 35500, Train loss: 2.221e+03, Test loss: 6.255e+03, MSE(e): 1.806e-04, MSE(pi1): 2.903e-02, MSE(pi2): 9.885e-05, MSE(pi3): 1.239e-03\n",
      "Epoch 35600, Train loss: 2.456e+03, Test loss: 5.957e+03, MSE(e): 1.982e-04, MSE(pi1): 3.833e-02, MSE(pi2): 1.067e-04, MSE(pi3): 9.026e-04\n",
      "Epoch 35700, Train loss: 2.658e+03, Test loss: 1.062e+04, MSE(e): 2.044e-04, MSE(pi1): 4.324e-02, MSE(pi2): 1.227e-04, MSE(pi3): 1.811e-03\n",
      "Epoch 35800, Train loss: 2.729e+03, Test loss: 7.461e+03, MSE(e): 2.426e-04, MSE(pi1): 1.936e-02, MSE(pi2): 1.257e-04, MSE(pi3): 1.091e-03\n",
      "Epoch 35900, Train loss: 5.512e+03, Test loss: 1.069e+04, MSE(e): 4.903e-04, MSE(pi1): 5.028e-02, MSE(pi2): 2.223e-04, MSE(pi3): 1.055e-03\n",
      "Epoch 36000, Train loss: 4.531e+03, Test loss: 9.635e+03, MSE(e): 4.111e-04, MSE(pi1): 2.359e-02, MSE(pi2): 2.085e-04, MSE(pi3): 1.839e-03\n",
      "Epoch 36100, Train loss: 4.166e+03, Test loss: 8.884e+03, MSE(e): 3.827e-04, MSE(pi1): 2.177e-02, MSE(pi2): 1.941e-04, MSE(pi3): 1.205e-03\n",
      "Epoch 36200, Train loss: 5.882e+03, Test loss: 1.017e+04, MSE(e): 5.251e-04, MSE(pi1): 4.276e-02, MSE(pi2): 2.522e-04, MSE(pi3): 2.026e-03\n",
      "Epoch 36300, Train loss: 5.961e+03, Test loss: 9.577e+03, MSE(e): 5.324e-04, MSE(pi1): 5.006e-02, MSE(pi2): 2.273e-04, MSE(pi3): 1.367e-03\n",
      "Epoch 36400, Train loss: 1.799e+03, Test loss: 6.163e+03, MSE(e): 1.423e-04, MSE(pi1): 2.551e-02, MSE(pi2): 8.113e-05, MSE(pi3): 1.208e-03\n",
      "Epoch 36500, Train loss: 2.175e+03, Test loss: 7.935e+03, MSE(e): 1.672e-04, MSE(pi1): 3.312e-02, MSE(pi2): 9.510e-05, MSE(pi3): 1.723e-03\n",
      "Epoch 36600, Train loss: 2.786e+03, Test loss: 9.057e+03, MSE(e): 2.005e-04, MSE(pi1): 4.735e-02, MSE(pi2): 1.149e-04, MSE(pi3): 3.069e-03\n",
      "Epoch 36700, Train loss: 1.602e+03, Test loss: 5.991e+03, MSE(e): 1.250e-04, MSE(pi1): 2.407e-02, MSE(pi2): 8.346e-05, MSE(pi3): 1.111e-03\n",
      "Epoch 36800, Train loss: 1.504e+03, Test loss: 5.442e+03, MSE(e): 1.273e-04, MSE(pi1): 1.606e-02, MSE(pi2): 8.193e-05, MSE(pi3): 6.992e-04\n",
      "Epoch 36900, Train loss: 1.935e+03, Test loss: 5.808e+03, MSE(e): 1.575e-04, MSE(pi1): 2.525e-02, MSE(pi2): 8.981e-05, MSE(pi3): 1.071e-03\n",
      "Epoch 37000, Train loss: 1.693e+03, Test loss: 7.000e+03, MSE(e): 1.254e-04, MSE(pi1): 3.005e-02, MSE(pi2): 7.740e-05, MSE(pi3): 1.376e-03\n",
      "Epoch 37100, Train loss: 2.814e+03, Test loss: 7.487e+03, MSE(e): 2.095e-04, MSE(pi1): 5.202e-02, MSE(pi2): 1.182e-04, MSE(pi3): 1.990e-03\n",
      "Epoch 37200, Train loss: 8.281e+03, Test loss: 1.617e+04, MSE(e): 7.963e-04, MSE(pi1): 2.148e-02, MSE(pi2): 3.755e-04, MSE(pi3): 1.029e-03\n",
      "Epoch 37300, Train loss: 1.561e+03, Test loss: 5.740e+03, MSE(e): 1.300e-04, MSE(pi1): 1.750e-02, MSE(pi2): 8.243e-05, MSE(pi3): 8.597e-04\n",
      "Epoch 37400, Train loss: 1.682e+03, Test loss: 6.451e+03, MSE(e): 1.379e-04, MSE(pi1): 1.702e-02, MSE(pi2): 8.776e-05, MSE(pi3): 1.323e-03\n",
      "Epoch 37500, Train loss: 1.312e+03, Test loss: 6.068e+03, MSE(e): 1.039e-04, MSE(pi1): 1.671e-02, MSE(pi2): 7.150e-05, MSE(pi3): 1.052e-03\n",
      "Epoch 37600, Train loss: 3.161e+03, Test loss: 1.115e+04, MSE(e): 2.744e-04, MSE(pi1): 2.830e-02, MSE(pi2): 1.543e-04, MSE(pi3): 1.339e-03\n",
      "Epoch 37700, Train loss: 1.464e+03, Test loss: 6.090e+03, MSE(e): 1.267e-04, MSE(pi1): 1.053e-02, MSE(pi2): 8.570e-05, MSE(pi3): 9.167e-04\n",
      "Epoch 37800, Train loss: 3.453e+03, Test loss: 1.302e+04, MSE(e): 2.902e-04, MSE(pi1): 3.196e-02, MSE(pi2): 1.544e-04, MSE(pi3): 2.310e-03\n",
      "Epoch 37900, Train loss: 4.508e+03, Test loss: 8.846e+03, MSE(e): 4.217e-04, MSE(pi1): 2.016e-02, MSE(pi2): 2.087e-04, MSE(pi3): 8.941e-04\n",
      "Epoch 38000, Train loss: 2.104e+03, Test loss: 9.436e+03, MSE(e): 1.569e-04, MSE(pi1): 3.313e-02, MSE(pi2): 9.283e-05, MSE(pi3): 2.036e-03\n",
      "Epoch 38100, Train loss: 3.070e+03, Test loss: 7.621e+03, MSE(e): 2.719e-04, MSE(pi1): 2.662e-02, MSE(pi2): 1.352e-04, MSE(pi3): 8.473e-04\n",
      "Epoch 38200, Train loss: 5.458e+03, Test loss: 1.042e+04, MSE(e): 4.992e-04, MSE(pi1): 3.387e-02, MSE(pi2): 2.350e-04, MSE(pi3): 1.271e-03\n",
      "Epoch 38300, Train loss: 3.528e+03, Test loss: 6.936e+03, MSE(e): 3.201e-04, MSE(pi1): 2.040e-02, MSE(pi2): 1.770e-04, MSE(pi3): 1.221e-03\n",
      "Epoch 38400, Train loss: 2.419e+03, Test loss: 7.310e+03, MSE(e): 2.102e-04, MSE(pi1): 2.151e-02, MSE(pi2): 1.179e-04, MSE(pi3): 1.020e-03\n",
      "Epoch 38500, Train loss: 2.304e+03, Test loss: 7.223e+03, MSE(e): 2.015e-04, MSE(pi1): 1.769e-02, MSE(pi2): 1.142e-04, MSE(pi3): 1.121e-03\n",
      "Epoch 38600, Train loss: 3.447e+03, Test loss: 7.460e+03, MSE(e): 3.192e-04, MSE(pi1): 1.311e-02, MSE(pi2): 1.679e-04, MSE(pi3): 1.235e-03\n",
      "Epoch 38700, Train loss: 5.876e+03, Test loss: 7.951e+03, MSE(e): 5.534e-04, MSE(pi1): 2.409e-02, MSE(pi2): 2.518e-04, MSE(pi3): 1.007e-03\n",
      "Epoch 38800, Train loss: 4.450e+03, Test loss: 1.032e+04, MSE(e): 3.945e-04, MSE(pi1): 3.430e-02, MSE(pi2): 2.155e-04, MSE(pi3): 1.614e-03\n",
      "Epoch 38900, Train loss: 6.055e+03, Test loss: 1.007e+04, MSE(e): 5.727e-04, MSE(pi1): 2.122e-02, MSE(pi2): 2.841e-04, MSE(pi3): 1.154e-03\n",
      "Epoch 39000, Train loss: 3.074e+03, Test loss: 5.843e+03, MSE(e): 2.752e-04, MSE(pi1): 2.076e-02, MSE(pi2): 1.425e-04, MSE(pi3): 1.140e-03\n",
      "Epoch 39100, Train loss: 9.608e+03, Test loss: 9.939e+03, MSE(e): 9.127e-04, MSE(pi1): 2.795e-02, MSE(pi2): 4.450e-04, MSE(pi3): 2.011e-03\n",
      "Epoch 39200, Train loss: 2.117e+03, Test loss: 6.639e+03, MSE(e): 1.459e-04, MSE(pi1): 5.677e-02, MSE(pi2): 9.099e-05, MSE(pi3): 8.966e-04\n",
      "Epoch 39300, Train loss: 2.057e+03, Test loss: 7.441e+03, MSE(e): 1.702e-04, MSE(pi1): 2.349e-02, MSE(pi2): 9.928e-05, MSE(pi3): 1.204e-03\n",
      "Epoch 39400, Train loss: 1.693e+03, Test loss: 5.810e+03, MSE(e): 1.422e-04, MSE(pi1): 1.777e-02, MSE(pi2): 8.375e-05, MSE(pi3): 9.295e-04\n",
      "Epoch 39500, Train loss: 5.992e+03, Test loss: 1.717e+04, MSE(e): 5.409e-04, MSE(pi1): 4.625e-02, MSE(pi2): 2.664e-04, MSE(pi3): 1.207e-03\n",
      "Epoch 39600, Train loss: 2.646e+03, Test loss: 8.946e+03, MSE(e): 2.331e-04, MSE(pi1): 2.124e-02, MSE(pi2): 1.261e-04, MSE(pi3): 1.029e-03\n",
      "Epoch 39700, Train loss: 4.945e+03, Test loss: 8.736e+03, MSE(e): 4.563e-04, MSE(pi1): 2.367e-02, MSE(pi2): 2.248e-04, MSE(pi3): 1.449e-03\n",
      "Epoch 39800, Train loss: 2.235e+03, Test loss: 6.750e+03, MSE(e): 1.907e-04, MSE(pi1): 2.326e-02, MSE(pi2): 1.116e-04, MSE(pi3): 9.601e-04\n",
      "Epoch 39900, Train loss: 2.299e+03, Test loss: 8.893e+03, MSE(e): 1.909e-04, MSE(pi1): 2.282e-02, MSE(pi2): 1.065e-04, MSE(pi3): 1.617e-03\n",
      "Epoch 40000, Train loss: 2.889e+03, Test loss: 7.436e+03, MSE(e): 2.432e-04, MSE(pi1): 2.989e-02, MSE(pi2): 1.250e-04, MSE(pi3): 1.573e-03\n",
      "Epoch 40100, Train loss: 4.215e+03, Test loss: 8.563e+03, MSE(e): 3.772e-04, MSE(pi1): 3.159e-02, MSE(pi2): 1.737e-04, MSE(pi3): 1.267e-03\n",
      "Epoch 40200, Train loss: 4.410e+03, Test loss: 7.772e+03, MSE(e): 4.127e-04, MSE(pi1): 1.469e-02, MSE(pi2): 1.989e-04, MSE(pi3): 1.361e-03\n",
      "Epoch 40300, Train loss: 4.245e+03, Test loss: 6.860e+03, MSE(e): 3.868e-04, MSE(pi1): 2.307e-02, MSE(pi2): 1.855e-04, MSE(pi3): 1.457e-03\n",
      "Epoch 40400, Train loss: 2.181e+03, Test loss: 6.251e+03, MSE(e): 1.731e-04, MSE(pi1): 3.421e-02, MSE(pi2): 9.284e-05, MSE(pi3): 1.079e-03\n",
      "Epoch 40500, Train loss: 3.277e+03, Test loss: 8.197e+03, MSE(e): 2.821e-04, MSE(pi1): 2.465e-02, MSE(pi2): 1.508e-04, MSE(pi3): 2.094e-03\n",
      "Epoch 40600, Train loss: 1.938e+03, Test loss: 6.040e+03, MSE(e): 1.547e-04, MSE(pi1): 2.925e-02, MSE(pi2): 9.218e-05, MSE(pi3): 9.862e-04\n",
      "Epoch 40700, Train loss: 1.783e+03, Test loss: 5.725e+03, MSE(e): 1.548e-04, MSE(pi1): 1.446e-02, MSE(pi2): 9.633e-05, MSE(pi3): 9.047e-04\n",
      "Epoch 40800, Train loss: 1.077e+04, Test loss: 6.799e+03, MSE(e): 1.052e-03, MSE(pi1): 1.320e-02, MSE(pi2): 4.847e-04, MSE(pi3): 1.249e-03\n",
      "Epoch 40900, Train loss: 1.964e+03, Test loss: 6.291e+03, MSE(e): 1.608e-04, MSE(pi1): 2.085e-02, MSE(pi2): 1.004e-04, MSE(pi3): 1.475e-03\n",
      "Epoch 41000, Train loss: 1.883e+03, Test loss: 7.275e+03, MSE(e): 1.472e-04, MSE(pi1): 2.797e-02, MSE(pi2): 9.031e-05, MSE(pi3): 1.310e-03\n",
      "Epoch 41100, Train loss: 2.043e+03, Test loss: 5.332e+03, MSE(e): 1.691e-04, MSE(pi1): 2.296e-02, MSE(pi2): 9.184e-05, MSE(pi3): 1.230e-03\n",
      "Epoch 41200, Train loss: 9.089e+03, Test loss: 1.138e+04, MSE(e): 8.734e-04, MSE(pi1): 2.127e-02, MSE(pi2): 4.326e-04, MSE(pi3): 1.416e-03\n",
      "Epoch 41300, Train loss: 2.417e+03, Test loss: 6.432e+03, MSE(e): 2.066e-04, MSE(pi1): 2.505e-02, MSE(pi2): 1.148e-04, MSE(pi3): 1.003e-03\n",
      "Epoch 41400, Train loss: 4.013e+03, Test loss: 8.742e+03, MSE(e): 3.553e-04, MSE(pi1): 3.368e-02, MSE(pi2): 1.858e-04, MSE(pi3): 1.231e-03\n",
      "Epoch 41500, Train loss: 2.322e+03, Test loss: 6.200e+03, MSE(e): 2.090e-04, MSE(pi1): 1.330e-02, MSE(pi2): 1.123e-04, MSE(pi3): 9.897e-04\n",
      "Epoch 41600, Train loss: 3.152e+03, Test loss: 7.504e+03, MSE(e): 2.806e-04, MSE(pi1): 2.513e-02, MSE(pi2): 1.509e-04, MSE(pi3): 9.490e-04\n",
      "Epoch 41700, Train loss: 2.237e+03, Test loss: 7.665e+03, MSE(e): 1.922e-04, MSE(pi1): 2.009e-02, MSE(pi2): 1.078e-04, MSE(pi3): 1.140e-03\n",
      "Epoch 41800, Train loss: 5.743e+03, Test loss: 9.388e+03, MSE(e): 5.285e-04, MSE(pi1): 3.473e-02, MSE(pi2): 2.437e-04, MSE(pi3): 1.102e-03\n",
      "Epoch 41900, Train loss: 2.071e+03, Test loss: 8.268e+03, MSE(e): 1.530e-04, MSE(pi1): 4.404e-02, MSE(pi2): 8.571e-05, MSE(pi3): 1.013e-03\n",
      "Epoch 42000, Train loss: 5.121e+03, Test loss: 7.689e+03, MSE(e): 4.559e-04, MSE(pi1): 3.967e-02, MSE(pi2): 2.152e-04, MSE(pi3): 1.645e-03\n",
      "Epoch 42100, Train loss: 3.743e+03, Test loss: 8.627e+03, MSE(e): 3.144e-04, MSE(pi1): 4.680e-02, MSE(pi2): 1.593e-04, MSE(pi3): 1.300e-03\n",
      "Epoch 42200, Train loss: 2.052e+03, Test loss: 6.830e+03, MSE(e): 1.748e-04, MSE(pi1): 1.939e-02, MSE(pi2): 9.980e-05, MSE(pi3): 1.096e-03\n",
      "Epoch 42300, Train loss: 3.162e+03, Test loss: 6.523e+03, MSE(e): 2.938e-04, MSE(pi1): 1.180e-02, MSE(pi2): 1.501e-04, MSE(pi3): 1.058e-03\n",
      "Epoch 42400, Train loss: 4.804e+03, Test loss: 6.827e+03, MSE(e): 4.636e-04, MSE(pi1): 9.067e-03, MSE(pi2): 2.198e-04, MSE(pi3): 7.693e-04\n",
      "Epoch 42500, Train loss: 2.318e+03, Test loss: 5.898e+03, MSE(e): 1.707e-04, MSE(pi1): 4.652e-02, MSE(pi2): 8.737e-05, MSE(pi3): 1.466e-03\n",
      "Epoch 42600, Train loss: 2.047e+03, Test loss: 7.039e+03, MSE(e): 1.335e-04, MSE(pi1): 6.298e-02, MSE(pi2): 8.282e-05, MSE(pi3): 8.242e-04\n",
      "Epoch 42700, Train loss: 1.903e+03, Test loss: 8.057e+03, MSE(e): 1.452e-04, MSE(pi1): 2.780e-02, MSE(pi2): 8.680e-05, MSE(pi3): 1.725e-03\n",
      "Epoch 42800, Train loss: 2.009e+03, Test loss: 6.236e+03, MSE(e): 1.775e-04, MSE(pi1): 1.402e-02, MSE(pi2): 9.425e-05, MSE(pi3): 9.438e-04\n",
      "Epoch 42900, Train loss: 2.527e+03, Test loss: 2.215e+04, MSE(e): 2.223e-04, MSE(pi1): 2.265e-02, MSE(pi2): 1.241e-04, MSE(pi3): 7.734e-04\n",
      "Epoch 43000, Train loss: 2.930e+04, Test loss: 1.012e+04, MSE(e): 2.904e-03, MSE(pi1): 1.269e-02, MSE(pi2): 1.255e-03, MSE(pi3): 1.384e-03\n",
      "Epoch 43100, Train loss: 2.027e+03, Test loss: 5.755e+03, MSE(e): 1.534e-04, MSE(pi1): 4.302e-02, MSE(pi2): 8.651e-05, MSE(pi3): 6.230e-04\n",
      "Epoch 43200, Train loss: 2.970e+03, Test loss: 6.324e+03, MSE(e): 2.602e-04, MSE(pi1): 2.878e-02, MSE(pi2): 1.274e-04, MSE(pi3): 8.030e-04\n",
      "Epoch 43300, Train loss: 1.847e+03, Test loss: 5.495e+03, MSE(e): 1.630e-04, MSE(pi1): 1.145e-02, MSE(pi2): 1.027e-04, MSE(pi3): 1.024e-03\n",
      "Epoch 43400, Train loss: 1.896e+03, Test loss: 5.888e+03, MSE(e): 1.630e-04, MSE(pi1): 1.700e-02, MSE(pi2): 9.074e-05, MSE(pi3): 9.631e-04\n",
      "Epoch 43500, Train loss: 2.755e+03, Test loss: 7.490e+03, MSE(e): 2.491e-04, MSE(pi1): 1.584e-02, MSE(pi2): 1.323e-04, MSE(pi3): 1.059e-03\n",
      "Epoch 43600, Train loss: 2.605e+03, Test loss: 8.608e+03, MSE(e): 1.725e-04, MSE(pi1): 7.745e-02, MSE(pi2): 1.099e-04, MSE(pi3): 1.047e-03\n",
      "Epoch 43700, Train loss: 3.194e+03, Test loss: 7.699e+03, MSE(e): 2.808e-04, MSE(pi1): 2.427e-02, MSE(pi2): 1.476e-04, MSE(pi3): 1.427e-03\n",
      "Epoch 43800, Train loss: 4.585e+03, Test loss: 7.688e+03, MSE(e): 4.281e-04, MSE(pi1): 2.042e-02, MSE(pi2): 2.055e-04, MSE(pi3): 9.940e-04\n",
      "Epoch 43900, Train loss: 5.929e+03, Test loss: 9.993e+03, MSE(e): 5.551e-04, MSE(pi1): 2.141e-02, MSE(pi2): 2.663e-04, MSE(pi3): 1.641e-03\n",
      "Epoch 44000, Train loss: 5.263e+03, Test loss: 9.705e+03, MSE(e): 4.869e-04, MSE(pi1): 3.054e-02, MSE(pi2): 2.341e-04, MSE(pi3): 8.876e-04\n",
      "Epoch 44100, Train loss: 4.320e+03, Test loss: 1.053e+04, MSE(e): 3.661e-04, MSE(pi1): 4.157e-02, MSE(pi2): 1.927e-04, MSE(pi3): 2.431e-03\n",
      "Epoch 44200, Train loss: 4.199e+03, Test loss: 9.479e+03, MSE(e): 3.698e-04, MSE(pi1): 3.106e-02, MSE(pi2): 1.958e-04, MSE(pi3): 1.911e-03\n",
      "Epoch 44300, Train loss: 5.261e+03, Test loss: 9.323e+03, MSE(e): 4.945e-04, MSE(pi1): 2.156e-02, MSE(pi2): 2.383e-04, MSE(pi3): 1.001e-03\n",
      "Epoch 44400, Train loss: 5.683e+03, Test loss: 1.008e+04, MSE(e): 5.173e-04, MSE(pi1): 4.106e-02, MSE(pi2): 2.373e-04, MSE(pi3): 9.937e-04\n",
      "Epoch 44500, Train loss: 4.712e+03, Test loss: 8.671e+03, MSE(e): 4.365e-04, MSE(pi1): 2.144e-02, MSE(pi2): 2.113e-04, MSE(pi3): 1.325e-03\n",
      "Epoch 44600, Train loss: 2.723e+03, Test loss: 6.841e+03, MSE(e): 2.245e-04, MSE(pi1): 3.266e-02, MSE(pi2): 1.185e-04, MSE(pi3): 1.504e-03\n",
      "Epoch 44700, Train loss: 1.829e+03, Test loss: 8.189e+03, MSE(e): 1.509e-04, MSE(pi1): 2.136e-02, MSE(pi2): 8.534e-05, MSE(pi3): 1.064e-03\n",
      "Epoch 44800, Train loss: 2.796e+03, Test loss: 7.099e+03, MSE(e): 2.404e-04, MSE(pi1): 3.027e-02, MSE(pi2): 1.319e-04, MSE(pi3): 8.983e-04\n",
      "Epoch 44900, Train loss: 5.896e+03, Test loss: 9.854e+03, MSE(e): 5.421e-04, MSE(pi1): 3.773e-02, MSE(pi2): 2.446e-04, MSE(pi3): 9.829e-04\n",
      "Epoch 45000, Train loss: 2.122e+03, Test loss: 7.259e+03, MSE(e): 1.830e-04, MSE(pi1): 1.939e-02, MSE(pi2): 1.004e-04, MSE(pi3): 9.836e-04\n",
      "Epoch 45100, Train loss: 5.409e+03, Test loss: 1.015e+04, MSE(e): 5.078e-04, MSE(pi1): 2.391e-02, MSE(pi2): 2.423e-04, MSE(pi3): 9.193e-04\n",
      "Epoch 45200, Train loss: 2.737e+03, Test loss: 7.659e+03, MSE(e): 2.398e-04, MSE(pi1): 2.025e-02, MSE(pi2): 1.227e-04, MSE(pi3): 1.362e-03\n",
      "Epoch 45300, Train loss: 4.893e+03, Test loss: 9.519e+03, MSE(e): 4.537e-04, MSE(pi1): 2.328e-02, MSE(pi2): 2.210e-04, MSE(pi3): 1.238e-03\n",
      "Epoch 45400, Train loss: 3.478e+03, Test loss: 8.060e+03, MSE(e): 3.295e-04, MSE(pi1): 7.991e-03, MSE(pi2): 1.676e-04, MSE(pi3): 1.031e-03\n",
      "Epoch 45500, Train loss: 7.159e+03, Test loss: 8.640e+03, MSE(e): 6.800e-04, MSE(pi1): 2.140e-02, MSE(pi2): 3.154e-04, MSE(pi3): 1.441e-03\n",
      "Epoch 45600, Train loss: 1.943e+03, Test loss: 7.542e+03, MSE(e): 1.521e-04, MSE(pi1): 3.253e-02, MSE(pi2): 8.637e-05, MSE(pi3): 9.674e-04\n",
      "Epoch 45700, Train loss: 3.533e+03, Test loss: 7.177e+03, MSE(e): 3.189e-04, MSE(pi1): 2.380e-02, MSE(pi2): 1.730e-04, MSE(pi3): 1.060e-03\n",
      "Epoch 45800, Train loss: 1.676e+03, Test loss: 6.555e+03, MSE(e): 1.419e-04, MSE(pi1): 1.660e-02, MSE(pi2): 8.705e-05, MSE(pi3): 9.027e-04\n",
      "Epoch 45900, Train loss: 2.714e+03, Test loss: 8.741e+03, MSE(e): 2.039e-04, MSE(pi1): 4.482e-02, MSE(pi2): 9.728e-05, MSE(pi3): 2.267e-03\n",
      "Epoch 46000, Train loss: 1.965e+03, Test loss: 6.407e+03, MSE(e): 1.501e-04, MSE(pi1): 3.234e-02, MSE(pi2): 8.173e-05, MSE(pi3): 1.407e-03\n",
      "Epoch 46100, Train loss: 3.113e+03, Test loss: 1.349e+04, MSE(e): 2.701e-04, MSE(pi1): 2.895e-02, MSE(pi2): 1.774e-04, MSE(pi3): 1.217e-03\n",
      "Epoch 46200, Train loss: 5.490e+03, Test loss: 8.644e+03, MSE(e): 4.782e-04, MSE(pi1): 4.184e-02, MSE(pi2): 2.352e-04, MSE(pi3): 2.891e-03\n",
      "Epoch 46300, Train loss: 2.610e+03, Test loss: 7.372e+03, MSE(e): 2.303e-04, MSE(pi1): 2.106e-02, MSE(pi2): 1.144e-04, MSE(pi3): 9.695e-04\n",
      "Epoch 46400, Train loss: 2.689e+03, Test loss: 8.338e+03, MSE(e): 2.312e-04, MSE(pi1): 2.419e-02, MSE(pi2): 1.303e-04, MSE(pi3): 1.343e-03\n",
      "Epoch 46500, Train loss: 2.208e+03, Test loss: 5.995e+03, MSE(e): 1.823e-04, MSE(pi1): 2.794e-02, MSE(pi2): 1.183e-04, MSE(pi3): 1.047e-03\n",
      "Epoch 46600, Train loss: 1.570e+03, Test loss: 7.176e+03, MSE(e): 1.052e-04, MSE(pi1): 3.827e-02, MSE(pi2): 6.710e-05, MSE(pi3): 1.353e-03\n",
      "Epoch 46700, Train loss: 1.590e+03, Test loss: 5.432e+03, MSE(e): 1.399e-04, MSE(pi1): 1.116e-02, MSE(pi2): 8.213e-05, MSE(pi3): 7.928e-04\n",
      "Epoch 46800, Train loss: 7.874e+03, Test loss: 9.140e+03, MSE(e): 7.452e-04, MSE(pi1): 3.212e-02, MSE(pi2): 3.403e-04, MSE(pi3): 1.007e-03\n",
      "Epoch 46900, Train loss: 1.793e+03, Test loss: 5.711e+03, MSE(e): 1.483e-04, MSE(pi1): 2.103e-02, MSE(pi2): 8.564e-05, MSE(pi3): 9.942e-04\n",
      "Epoch 47000, Train loss: 2.859e+03, Test loss: 1.167e+04, MSE(e): 2.497e-04, MSE(pi1): 2.463e-02, MSE(pi2): 1.302e-04, MSE(pi3): 1.151e-03\n",
      "Epoch 47100, Train loss: 2.520e+03, Test loss: 9.140e+03, MSE(e): 2.240e-04, MSE(pi1): 1.957e-02, MSE(pi2): 1.200e-04, MSE(pi3): 8.440e-04\n",
      "Epoch 47200, Train loss: 2.221e+03, Test loss: 8.338e+03, MSE(e): 1.944e-04, MSE(pi1): 1.531e-02, MSE(pi2): 1.054e-04, MSE(pi3): 1.244e-03\n",
      "Epoch 47300, Train loss: 3.230e+03, Test loss: 7.651e+03, MSE(e): 2.904e-04, MSE(pi1): 2.381e-02, MSE(pi2): 1.454e-04, MSE(pi3): 8.761e-04\n",
      "Epoch 47400, Train loss: 4.334e+03, Test loss: 7.995e+03, MSE(e): 4.085e-04, MSE(pi1): 1.559e-02, MSE(pi2): 2.078e-04, MSE(pi3): 9.326e-04\n",
      "Epoch 47500, Train loss: 2.191e+03, Test loss: 1.025e+04, MSE(e): 1.593e-04, MSE(pi1): 4.052e-02, MSE(pi2): 9.334e-05, MSE(pi3): 1.925e-03\n",
      "Epoch 47600, Train loss: 2.740e+03, Test loss: 7.215e+03, MSE(e): 2.374e-04, MSE(pi1): 2.776e-02, MSE(pi2): 1.243e-04, MSE(pi3): 8.868e-04\n",
      "Epoch 47700, Train loss: 3.919e+03, Test loss: 8.850e+03, MSE(e): 3.634e-04, MSE(pi1): 1.949e-02, MSE(pi2): 1.883e-04, MSE(pi3): 9.025e-04\n",
      "Epoch 47800, Train loss: 2.380e+03, Test loss: 1.062e+04, MSE(e): 1.881e-04, MSE(pi1): 3.480e-02, MSE(pi2): 9.983e-05, MSE(pi3): 1.512e-03\n",
      "Epoch 47900, Train loss: 5.770e+03, Test loss: 1.014e+04, MSE(e): 5.377e-04, MSE(pi1): 2.370e-02, MSE(pi2): 2.561e-04, MSE(pi3): 1.562e-03\n",
      "Epoch 48000, Train loss: 1.853e+03, Test loss: 1.029e+04, MSE(e): 1.507e-04, MSE(pi1): 1.862e-02, MSE(pi2): 9.248e-05, MSE(pi3): 1.597e-03\n",
      "Epoch 48100, Train loss: 3.273e+03, Test loss: 9.487e+03, MSE(e): 2.802e-04, MSE(pi1): 3.423e-02, MSE(pi2): 1.394e-04, MSE(pi3): 1.282e-03\n",
      "Epoch 48200, Train loss: 2.787e+03, Test loss: 8.717e+03, MSE(e): 2.464e-04, MSE(pi1): 1.915e-02, MSE(pi2): 1.420e-04, MSE(pi3): 1.314e-03\n",
      "Epoch 48300, Train loss: 2.128e+03, Test loss: 9.106e+03, MSE(e): 1.912e-04, MSE(pi1): 1.239e-02, MSE(pi2): 1.066e-04, MSE(pi3): 9.204e-04\n",
      "Epoch 48400, Train loss: 2.756e+03, Test loss: 8.558e+03, MSE(e): 2.377e-04, MSE(pi1): 2.772e-02, MSE(pi2): 1.244e-04, MSE(pi3): 1.018e-03\n",
      "Epoch 48500, Train loss: 2.108e+03, Test loss: 9.504e+03, MSE(e): 1.761e-04, MSE(pi1): 2.237e-02, MSE(pi2): 1.083e-04, MSE(pi3): 1.239e-03\n",
      "Epoch 48600, Train loss: 2.333e+03, Test loss: 8.901e+03, MSE(e): 1.921e-04, MSE(pi1): 3.013e-02, MSE(pi2): 1.029e-04, MSE(pi3): 1.104e-03\n",
      "Epoch 48700, Train loss: 2.598e+03, Test loss: 9.172e+03, MSE(e): 2.344e-04, MSE(pi1): 1.129e-02, MSE(pi2): 1.236e-04, MSE(pi3): 1.410e-03\n",
      "Epoch 48800, Train loss: 1.907e+03, Test loss: 7.483e+03, MSE(e): 1.609e-04, MSE(pi1): 2.320e-02, MSE(pi2): 9.618e-05, MSE(pi3): 6.508e-04\n",
      "Epoch 48900, Train loss: 2.409e+03, Test loss: 7.571e+03, MSE(e): 2.089e-04, MSE(pi1): 2.487e-02, MSE(pi2): 1.092e-04, MSE(pi3): 7.147e-04\n",
      "Epoch 49000, Train loss: 4.694e+03, Test loss: 7.517e+03, MSE(e): 4.219e-04, MSE(pi1): 3.241e-02, MSE(pi2): 2.024e-04, MSE(pi3): 1.504e-03\n",
      "Epoch 49100, Train loss: 4.607e+03, Test loss: 7.438e+03, MSE(e): 4.435e-04, MSE(pi1): 6.535e-03, MSE(pi2): 2.070e-04, MSE(pi3): 1.069e-03\n",
      "Epoch 49200, Train loss: 5.304e+03, Test loss: 7.581e+03, MSE(e): 5.141e-04, MSE(pi1): 5.869e-03, MSE(pi2): 2.353e-04, MSE(pi3): 1.042e-03\n",
      "Epoch 49300, Train loss: 2.045e+03, Test loss: 8.427e+03, MSE(e): 1.710e-04, MSE(pi1): 2.139e-02, MSE(pi2): 1.010e-04, MSE(pi3): 1.207e-03\n",
      "Epoch 49400, Train loss: 1.683e+03, Test loss: 6.702e+03, MSE(e): 1.443e-04, MSE(pi1): 1.386e-02, MSE(pi2): 8.135e-05, MSE(pi3): 1.014e-03\n",
      "Epoch 49500, Train loss: 1.859e+03, Test loss: 7.781e+03, MSE(e): 1.688e-04, MSE(pi1): 8.422e-03, MSE(pi2): 8.926e-05, MSE(pi3): 8.680e-04\n",
      "Epoch 49600, Train loss: 2.583e+03, Test loss: 1.208e+04, MSE(e): 2.134e-04, MSE(pi1): 3.427e-02, MSE(pi2): 1.228e-04, MSE(pi3): 1.066e-03\n",
      "Epoch 49700, Train loss: 4.225e+03, Test loss: 1.052e+04, MSE(e): 3.843e-04, MSE(pi1): 2.622e-02, MSE(pi2): 2.100e-04, MSE(pi3): 1.193e-03\n",
      "Epoch 49800, Train loss: 1.907e+03, Test loss: 6.257e+03, MSE(e): 1.588e-04, MSE(pi1): 2.406e-02, MSE(pi2): 9.481e-05, MSE(pi3): 7.798e-04\n",
      "Epoch 49900, Train loss: 1.382e+03, Test loss: 7.147e+03, MSE(e): 1.147e-04, MSE(pi1): 1.423e-02, MSE(pi2): 7.548e-05, MSE(pi3): 9.263e-04\n",
      "Epoch 50000, Train loss: 1.768e+03, Test loss: 7.334e+03, MSE(e): 1.360e-04, MSE(pi1): 3.245e-02, MSE(pi2): 7.947e-05, MSE(pi3): 8.290e-04\n",
      "Epoch 50100, Train loss: 2.488e+03, Test loss: 1.194e+04, MSE(e): 1.998e-04, MSE(pi1): 2.535e-02, MSE(pi2): 1.228e-04, MSE(pi3): 2.367e-03\n",
      "Epoch 50200, Train loss: 2.645e+03, Test loss: 8.009e+03, MSE(e): 2.376e-04, MSE(pi1): 1.911e-02, MSE(pi2): 1.190e-04, MSE(pi3): 7.741e-04\n",
      "Epoch 50300, Train loss: 3.736e+03, Test loss: 1.075e+04, MSE(e): 3.178e-04, MSE(pi1): 4.634e-02, MSE(pi2): 1.644e-04, MSE(pi3): 9.409e-04\n",
      "Epoch 50400, Train loss: 3.767e+03, Test loss: 1.260e+04, MSE(e): 3.334e-04, MSE(pi1): 3.194e-02, MSE(pi2): 1.969e-04, MSE(pi3): 1.137e-03\n",
      "Epoch 50500, Train loss: 1.883e+03, Test loss: 8.886e+03, MSE(e): 1.109e-04, MSE(pi1): 5.853e-02, MSE(pi2): 6.515e-05, MSE(pi3): 1.893e-03\n",
      "Epoch 50600, Train loss: 1.364e+03, Test loss: 8.885e+03, MSE(e): 9.916e-05, MSE(pi1): 2.349e-02, MSE(pi2): 6.668e-05, MSE(pi3): 1.370e-03\n",
      "Epoch 50700, Train loss: 1.416e+03, Test loss: 8.779e+03, MSE(e): 1.089e-04, MSE(pi1): 2.392e-02, MSE(pi2): 6.760e-05, MSE(pi3): 8.755e-04\n",
      "Epoch 50800, Train loss: 4.279e+03, Test loss: 1.095e+04, MSE(e): 4.008e-04, MSE(pi1): 1.439e-02, MSE(pi2): 2.070e-04, MSE(pi3): 1.263e-03\n",
      "Epoch 50900, Train loss: 1.503e+03, Test loss: 9.373e+03, MSE(e): 1.315e-04, MSE(pi1): 9.682e-03, MSE(pi2): 7.951e-05, MSE(pi3): 9.061e-04\n",
      "Epoch 51000, Train loss: 2.100e+03, Test loss: 1.612e+04, MSE(e): 1.674e-04, MSE(pi1): 2.795e-02, MSE(pi2): 1.070e-04, MSE(pi3): 1.460e-03\n",
      "Epoch 51100, Train loss: 4.385e+03, Test loss: 1.360e+04, MSE(e): 3.892e-04, MSE(pi1): 3.982e-02, MSE(pi2): 1.959e-04, MSE(pi3): 9.450e-04\n",
      "Epoch 51200, Train loss: 2.689e+03, Test loss: 1.936e+04, MSE(e): 2.274e-04, MSE(pi1): 2.539e-02, MSE(pi2): 1.209e-04, MSE(pi3): 1.609e-03\n",
      "Epoch 51300, Train loss: 2.238e+03, Test loss: 2.327e+04, MSE(e): 1.790e-04, MSE(pi1): 2.491e-02, MSE(pi2): 1.136e-04, MSE(pi3): 1.996e-03\n",
      "Epoch 51400, Train loss: 4.124e+03, Test loss: 2.463e+04, MSE(e): 3.708e-04, MSE(pi1): 2.916e-02, MSE(pi2): 1.796e-04, MSE(pi3): 1.241e-03\n",
      "Epoch 51500, Train loss: 3.046e+03, Test loss: 3.155e+04, MSE(e): 2.424e-04, MSE(pi1): 5.342e-02, MSE(pi2): 1.334e-04, MSE(pi3): 8.744e-04\n",
      "Epoch 51600, Train loss: 2.809e+03, Test loss: 3.866e+04, MSE(e): 2.504e-04, MSE(pi1): 2.435e-02, MSE(pi2): 1.309e-04, MSE(pi3): 6.183e-04\n",
      "Epoch 51700, Train loss: 5.386e+03, Test loss: 2.898e+04, MSE(e): 4.933e-04, MSE(pi1): 3.343e-02, MSE(pi2): 2.392e-04, MSE(pi3): 1.185e-03\n",
      "Epoch 51800, Train loss: 2.698e+03, Test loss: 5.109e+04, MSE(e): 2.335e-04, MSE(pi1): 2.311e-02, MSE(pi2): 1.453e-04, MSE(pi3): 1.318e-03\n",
      "Epoch 51900, Train loss: 2.208e+03, Test loss: 6.809e+04, MSE(e): 1.756e-04, MSE(pi1): 3.381e-02, MSE(pi2): 9.117e-05, MSE(pi3): 1.139e-03\n",
      "Epoch 52000, Train loss: 2.736e+03, Test loss: 7.207e+04, MSE(e): 2.128e-04, MSE(pi1): 4.445e-02, MSE(pi2): 1.180e-04, MSE(pi3): 1.638e-03\n",
      "Epoch 52100, Train loss: 5.710e+03, Test loss: 3.368e+04, MSE(e): 5.318e-04, MSE(pi1): 2.741e-02, MSE(pi2): 2.448e-04, MSE(pi3): 1.183e-03\n",
      "Epoch 52200, Train loss: 2.931e+03, Test loss: 4.394e+04, MSE(e): 2.437e-04, MSE(pi1): 3.645e-02, MSE(pi2): 1.308e-04, MSE(pi3): 1.301e-03\n",
      "Epoch 52300, Train loss: 2.069e+03, Test loss: 6.280e+04, MSE(e): 1.728e-04, MSE(pi1): 2.391e-02, MSE(pi2): 9.065e-05, MSE(pi3): 1.015e-03\n",
      "Epoch 52400, Train loss: 1.772e+03, Test loss: 5.647e+04, MSE(e): 1.385e-04, MSE(pi1): 2.657e-02, MSE(pi2): 8.359e-05, MSE(pi3): 1.211e-03\n",
      "Epoch 52500, Train loss: 6.255e+03, Test loss: 5.638e+04, MSE(e): 5.791e-04, MSE(pi1): 3.592e-02, MSE(pi2): 2.578e-04, MSE(pi3): 1.051e-03\n",
      "Epoch 52600, Train loss: 1.805e+03, Test loss: 5.189e+04, MSE(e): 1.417e-04, MSE(pi1): 2.531e-02, MSE(pi2): 7.921e-05, MSE(pi3): 1.352e-03\n",
      "Epoch 52700, Train loss: 5.092e+03, Test loss: 6.891e+04, MSE(e): 4.850e-04, MSE(pi1): 1.312e-02, MSE(pi2): 2.325e-04, MSE(pi3): 1.109e-03\n",
      "Epoch 52800, Train loss: 3.157e+03, Test loss: 8.518e+04, MSE(e): 2.828e-04, MSE(pi1): 2.256e-02, MSE(pi2): 1.571e-04, MSE(pi3): 1.025e-03\n",
      "Epoch 52900, Train loss: 5.246e+03, Test loss: 7.006e+04, MSE(e): 4.701e-04, MSE(pi1): 3.876e-02, MSE(pi2): 2.129e-04, MSE(pi3): 1.570e-03\n",
      "Epoch 53000, Train loss: 3.408e+03, Test loss: 9.386e+04, MSE(e): 3.003e-04, MSE(pi1): 2.733e-02, MSE(pi2): 1.516e-04, MSE(pi3): 1.310e-03\n",
      "Epoch 53100, Train loss: 6.190e+03, Test loss: 5.077e+04, MSE(e): 5.952e-04, MSE(pi1): 1.384e-02, MSE(pi2): 2.670e-04, MSE(pi3): 9.986e-04\n",
      "Epoch 53200, Train loss: 1.540e+03, Test loss: 5.759e+04, MSE(e): 1.386e-04, MSE(pi1): 7.047e-03, MSE(pi2): 8.064e-05, MSE(pi3): 8.275e-04\n",
      "Epoch 53300, Train loss: 2.117e+03, Test loss: 6.267e+04, MSE(e): 1.826e-04, MSE(pi1): 1.650e-02, MSE(pi2): 1.186e-04, MSE(pi3): 1.251e-03\n",
      "Epoch 53400, Train loss: 2.553e+03, Test loss: 5.719e+04, MSE(e): 2.313e-04, MSE(pi1): 1.441e-02, MSE(pi2): 1.180e-04, MSE(pi3): 9.612e-04\n",
      "Epoch 53500, Train loss: 3.363e+03, Test loss: 4.106e+04, MSE(e): 3.082e-04, MSE(pi1): 2.002e-02, MSE(pi2): 1.432e-04, MSE(pi3): 8.072e-04\n",
      "Epoch 53600, Train loss: 2.053e+03, Test loss: 5.153e+04, MSE(e): 1.409e-04, MSE(pi1): 4.913e-02, MSE(pi2): 7.629e-05, MSE(pi3): 1.530e-03\n",
      "Epoch 53700, Train loss: 5.104e+03, Test loss: 4.364e+04, MSE(e): 4.607e-04, MSE(pi1): 3.910e-02, MSE(pi2): 2.056e-04, MSE(pi3): 1.055e-03\n",
      "Epoch 53800, Train loss: 3.876e+03, Test loss: 5.869e+04, MSE(e): 3.539e-04, MSE(pi1): 2.267e-02, MSE(pi2): 1.792e-04, MSE(pi3): 1.108e-03\n",
      "Epoch 53900, Train loss: 2.923e+03, Test loss: 4.977e+04, MSE(e): 2.622e-04, MSE(pi1): 2.159e-02, MSE(pi2): 1.286e-04, MSE(pi3): 8.537e-04\n",
      "Epoch 54000, Train loss: 2.618e+03, Test loss: 7.088e+04, MSE(e): 2.279e-04, MSE(pi1): 2.456e-02, MSE(pi2): 1.161e-04, MSE(pi3): 9.330e-04\n",
      "Epoch 54100, Train loss: 1.764e+03, Test loss: 6.497e+04, MSE(e): 1.128e-04, MSE(pi1): 4.259e-02, MSE(pi2): 6.712e-05, MSE(pi3): 2.096e-03\n",
      "Epoch 54200, Train loss: 3.548e+03, Test loss: 5.743e+04, MSE(e): 2.966e-04, MSE(pi1): 4.131e-02, MSE(pi2): 1.345e-04, MSE(pi3): 1.686e-03\n",
      "Epoch 54300, Train loss: 1.444e+03, Test loss: 6.706e+04, MSE(e): 1.073e-04, MSE(pi1): 2.573e-02, MSE(pi2): 7.033e-05, MSE(pi3): 1.134e-03\n",
      "Epoch 54400, Train loss: 3.335e+03, Test loss: 4.684e+04, MSE(e): 2.991e-04, MSE(pi1): 2.497e-02, MSE(pi2): 1.452e-04, MSE(pi3): 9.413e-04\n",
      "Epoch 54500, Train loss: 6.433e+03, Test loss: 8.965e+04, MSE(e): 6.128e-04, MSE(pi1): 2.118e-02, MSE(pi2): 2.715e-04, MSE(pi3): 9.282e-04\n",
      "Epoch 54600, Train loss: 1.548e+03, Test loss: 6.257e+04, MSE(e): 1.154e-04, MSE(pi1): 3.071e-02, MSE(pi2): 6.832e-05, MSE(pi3): 8.686e-04\n",
      "Epoch 54700, Train loss: 2.035e+03, Test loss: 5.868e+04, MSE(e): 1.642e-04, MSE(pi1): 3.232e-02, MSE(pi2): 8.931e-05, MSE(pi3): 7.003e-04\n",
      "Epoch 54800, Train loss: 6.699e+03, Test loss: 5.787e+04, MSE(e): 6.229e-04, MSE(pi1): 3.473e-02, MSE(pi2): 2.991e-04, MSE(pi3): 1.220e-03\n",
      "Epoch 54900, Train loss: 1.610e+03, Test loss: 5.032e+04, MSE(e): 1.042e-04, MSE(pi1): 4.590e-02, MSE(pi2): 6.708e-05, MSE(pi3): 1.087e-03\n",
      "Epoch 55000, Train loss: 1.732e+03, Test loss: 6.558e+04, MSE(e): 1.304e-04, MSE(pi1): 2.998e-02, MSE(pi2): 7.178e-05, MSE(pi3): 1.287e-03\n",
      "Epoch 55100, Train loss: 1.342e+04, Test loss: 7.227e+04, MSE(e): 1.319e-03, MSE(pi1): 1.722e-02, MSE(pi2): 5.526e-04, MSE(pi3): 6.605e-04\n",
      "Epoch 55200, Train loss: 1.690e+03, Test loss: 7.227e+04, MSE(e): 1.398e-04, MSE(pi1): 1.912e-02, MSE(pi2): 8.356e-05, MSE(pi3): 1.001e-03\n",
      "Epoch 55300, Train loss: 1.848e+03, Test loss: 6.069e+04, MSE(e): 1.516e-04, MSE(pi1): 2.518e-02, MSE(pi2): 8.430e-05, MSE(pi3): 8.029e-04\n",
      "Epoch 55400, Train loss: 2.052e+03, Test loss: 6.115e+04, MSE(e): 1.829e-04, MSE(pi1): 1.058e-02, MSE(pi2): 1.056e-04, MSE(pi3): 1.168e-03\n",
      "Epoch 55500, Train loss: 1.919e+03, Test loss: 8.047e+04, MSE(e): 1.331e-04, MSE(pi1): 3.697e-02, MSE(pi2): 8.156e-05, MSE(pi3): 2.175e-03\n",
      "Epoch 55600, Train loss: 2.288e+03, Test loss: 6.957e+04, MSE(e): 1.793e-04, MSE(pi1): 3.256e-02, MSE(pi2): 9.440e-05, MSE(pi3): 1.686e-03\n",
      "Epoch 55700, Train loss: 4.691e+03, Test loss: 6.647e+04, MSE(e): 4.469e-04, MSE(pi1): 1.265e-02, MSE(pi2): 2.108e-04, MSE(pi3): 9.548e-04\n",
      "Epoch 55800, Train loss: 4.428e+03, Test loss: 7.503e+04, MSE(e): 3.828e-04, MSE(pi1): 5.134e-02, MSE(pi2): 1.780e-04, MSE(pi3): 8.653e-04\n",
      "Epoch 55900, Train loss: 1.749e+03, Test loss: 8.722e+04, MSE(e): 1.258e-04, MSE(pi1): 3.997e-02, MSE(pi2): 7.766e-05, MSE(pi3): 9.170e-04\n",
      "Epoch 56000, Train loss: 2.158e+03, Test loss: 7.679e+04, MSE(e): 1.853e-04, MSE(pi1): 1.772e-02, MSE(pi2): 9.616e-05, MSE(pi3): 1.283e-03\n",
      "Epoch 56100, Train loss: 1.665e+03, Test loss: 7.655e+04, MSE(e): 1.465e-04, MSE(pi1): 9.120e-03, MSE(pi2): 8.977e-05, MSE(pi3): 1.083e-03\n",
      "Epoch 56200, Train loss: 3.609e+03, Test loss: 7.557e+04, MSE(e): 3.238e-04, MSE(pi1): 2.210e-02, MSE(pi2): 1.650e-04, MSE(pi3): 1.502e-03\n",
      "Epoch 56300, Train loss: 3.904e+03, Test loss: 8.047e+04, MSE(e): 3.425e-04, MSE(pi1): 3.722e-02, MSE(pi2): 1.554e-04, MSE(pi3): 1.067e-03\n",
      "Epoch 56400, Train loss: 6.552e+03, Test loss: 8.527e+04, MSE(e): 6.199e-04, MSE(pi1): 2.299e-02, MSE(pi2): 2.941e-04, MSE(pi3): 1.225e-03\n",
      "Epoch 56500, Train loss: 5.114e+03, Test loss: 9.435e+04, MSE(e): 4.603e-04, MSE(pi1): 3.971e-02, MSE(pi2): 2.163e-04, MSE(pi3): 1.135e-03\n",
      "Epoch 56600, Train loss: 4.534e+03, Test loss: 9.785e+04, MSE(e): 4.215e-04, MSE(pi1): 2.259e-02, MSE(pi2): 2.177e-04, MSE(pi3): 9.268e-04\n",
      "Epoch 56700, Train loss: 3.037e+03, Test loss: 1.094e+05, MSE(e): 2.784e-04, MSE(pi1): 1.842e-02, MSE(pi2): 1.519e-04, MSE(pi3): 6.809e-04\n",
      "Epoch 56800, Train loss: 4.239e+03, Test loss: 1.022e+05, MSE(e): 3.826e-04, MSE(pi1): 3.275e-02, MSE(pi2): 1.897e-04, MSE(pi3): 8.479e-04\n",
      "Epoch 56900, Train loss: 4.663e+03, Test loss: 1.012e+05, MSE(e): 4.352e-04, MSE(pi1): 2.373e-02, MSE(pi2): 2.040e-04, MSE(pi3): 7.367e-04\n",
      "Epoch 57000, Train loss: 2.221e+03, Test loss: 9.861e+04, MSE(e): 1.891e-04, MSE(pi1): 2.212e-02, MSE(pi2): 1.021e-04, MSE(pi3): 1.088e-03\n",
      "Epoch 57100, Train loss: 3.958e+03, Test loss: 8.737e+04, MSE(e): 3.299e-04, MSE(pi1): 5.481e-02, MSE(pi2): 1.459e-04, MSE(pi3): 1.103e-03\n",
      "Epoch 57200, Train loss: 5.006e+03, Test loss: 9.890e+04, MSE(e): 4.562e-04, MSE(pi1): 3.422e-02, MSE(pi2): 2.061e-04, MSE(pi3): 1.014e-03\n",
      "Epoch 57300, Train loss: 1.463e+03, Test loss: 1.042e+05, MSE(e): 1.119e-04, MSE(pi1): 2.599e-02, MSE(pi2): 6.800e-05, MSE(pi3): 8.312e-04\n",
      "Epoch 57400, Train loss: 1.973e+03, Test loss: 9.427e+04, MSE(e): 1.556e-04, MSE(pi1): 2.665e-02, MSE(pi2): 8.895e-05, MSE(pi3): 1.499e-03\n",
      "Epoch 57500, Train loss: 1.593e+03, Test loss: 9.980e+04, MSE(e): 1.286e-04, MSE(pi1): 2.113e-02, MSE(pi2): 7.708e-05, MSE(pi3): 9.627e-04\n",
      "Epoch 57600, Train loss: 2.338e+03, Test loss: 1.039e+05, MSE(e): 2.004e-04, MSE(pi1): 2.334e-02, MSE(pi2): 1.103e-04, MSE(pi3): 9.994e-04\n",
      "Epoch 57700, Train loss: 4.172e+03, Test loss: 1.020e+05, MSE(e): 3.817e-04, MSE(pi1): 2.572e-02, MSE(pi2): 1.745e-04, MSE(pi3): 9.673e-04\n",
      "Epoch 57800, Train loss: 3.508e+03, Test loss: 1.264e+05, MSE(e): 2.876e-04, MSE(pi1): 4.434e-02, MSE(pi2): 1.502e-04, MSE(pi3): 1.883e-03\n",
      "Epoch 57900, Train loss: 1.804e+03, Test loss: 9.246e+04, MSE(e): 1.445e-04, MSE(pi1): 1.882e-02, MSE(pi2): 9.025e-05, MSE(pi3): 1.709e-03\n",
      "Epoch 58000, Train loss: 3.232e+03, Test loss: 9.362e+04, MSE(e): 2.949e-04, MSE(pi1): 1.839e-02, MSE(pi2): 1.417e-04, MSE(pi3): 9.898e-04\n",
      "Epoch 58100, Train loss: 2.256e+03, Test loss: 1.114e+05, MSE(e): 1.729e-04, MSE(pi1): 3.212e-02, MSE(pi2): 9.668e-05, MSE(pi3): 2.062e-03\n",
      "Epoch 58200, Train loss: 1.619e+03, Test loss: 1.133e+05, MSE(e): 1.209e-04, MSE(pi1): 3.431e-02, MSE(pi2): 7.181e-05, MSE(pi3): 6.739e-04\n",
      "Epoch 58300, Train loss: 1.478e+03, Test loss: 1.088e+05, MSE(e): 1.060e-04, MSE(pi1): 3.048e-02, MSE(pi2): 6.671e-05, MSE(pi3): 1.132e-03\n",
      "Epoch 58400, Train loss: 1.534e+03, Test loss: 9.172e+04, MSE(e): 1.276e-04, MSE(pi1): 1.412e-02, MSE(pi2): 7.973e-05, MSE(pi3): 1.167e-03\n",
      "Epoch 58500, Train loss: 5.128e+03, Test loss: 1.047e+05, MSE(e): 4.944e-04, MSE(pi1): 8.003e-03, MSE(pi2): 2.330e-04, MSE(pi3): 1.038e-03\n",
      "Epoch 58600, Train loss: 4.066e+03, Test loss: 9.750e+04, MSE(e): 3.691e-04, MSE(pi1): 2.770e-02, MSE(pi2): 1.669e-04, MSE(pi3): 9.716e-04\n",
      "Epoch 58700, Train loss: 5.191e+03, Test loss: 1.067e+05, MSE(e): 4.828e-04, MSE(pi1): 2.581e-02, MSE(pi2): 2.173e-04, MSE(pi3): 1.049e-03\n",
      "Epoch 58800, Train loss: 2.091e+03, Test loss: 1.305e+05, MSE(e): 1.813e-04, MSE(pi1): 2.040e-02, MSE(pi2): 1.057e-04, MSE(pi3): 7.311e-04\n",
      "Epoch 58900, Train loss: 2.707e+03, Test loss: 1.341e+05, MSE(e): 2.190e-04, MSE(pi1): 4.405e-02, MSE(pi2): 1.145e-04, MSE(pi3): 7.553e-04\n",
      "Epoch 59000, Train loss: 2.010e+03, Test loss: 1.281e+05, MSE(e): 1.634e-04, MSE(pi1): 3.045e-02, MSE(pi2): 9.177e-05, MSE(pi3): 7.167e-04\n",
      "Epoch 59100, Train loss: 2.490e+03, Test loss: 1.293e+05, MSE(e): 2.299e-04, MSE(pi1): 1.220e-02, MSE(pi2): 1.231e-04, MSE(pi3): 6.798e-04\n",
      "Epoch 59200, Train loss: 2.274e+03, Test loss: 1.284e+05, MSE(e): 2.005e-04, MSE(pi1): 1.948e-02, MSE(pi2): 1.113e-04, MSE(pi3): 7.369e-04\n",
      "Epoch 59300, Train loss: 4.206e+03, Test loss: 1.229e+05, MSE(e): 3.715e-04, MSE(pi1): 3.559e-02, MSE(pi2): 1.904e-04, MSE(pi3): 1.353e-03\n",
      "Epoch 59400, Train loss: 5.203e+03, Test loss: 1.261e+05, MSE(e): 4.769e-04, MSE(pi1): 2.903e-02, MSE(pi2): 2.352e-04, MSE(pi3): 1.441e-03\n",
      "Epoch 59500, Train loss: 2.090e+03, Test loss: 1.126e+05, MSE(e): 1.845e-04, MSE(pi1): 1.508e-02, MSE(pi2): 9.838e-05, MSE(pi3): 9.409e-04\n",
      "Epoch 59600, Train loss: 6.007e+03, Test loss: 1.217e+05, MSE(e): 5.743e-04, MSE(pi1): 1.734e-02, MSE(pi2): 2.616e-04, MSE(pi3): 8.973e-04\n",
      "Epoch 59700, Train loss: 4.412e+03, Test loss: 1.112e+05, MSE(e): 3.999e-04, MSE(pi1): 3.098e-02, MSE(pi2): 1.887e-04, MSE(pi3): 1.027e-03\n",
      "Epoch 59800, Train loss: 3.235e+03, Test loss: 1.248e+05, MSE(e): 2.734e-04, MSE(pi1): 3.669e-02, MSE(pi2): 1.408e-04, MSE(pi3): 1.334e-03\n",
      "Epoch 59900, Train loss: 1.721e+03, Test loss: 1.160e+05, MSE(e): 1.423e-04, MSE(pi1): 1.521e-02, MSE(pi2): 8.509e-05, MSE(pi3): 1.457e-03\n",
      "Epoch 60000, Train loss: 2.651e+03, Test loss: 1.270e+05, MSE(e): 2.349e-04, MSE(pi1): 1.839e-02, MSE(pi2): 1.176e-04, MSE(pi3): 1.181e-03\n",
      "Epoch 60100, Train loss: 1.579e+03, Test loss: 1.187e+05, MSE(e): 1.161e-04, MSE(pi1): 2.438e-02, MSE(pi2): 7.227e-05, MSE(pi3): 1.746e-03\n",
      "Epoch 60200, Train loss: 6.607e+03, Test loss: 1.239e+05, MSE(e): 6.400e-04, MSE(pi1): 1.107e-02, MSE(pi2): 3.045e-04, MSE(pi3): 9.552e-04\n",
      "Epoch 60300, Train loss: 3.661e+03, Test loss: 1.127e+05, MSE(e): 3.256e-04, MSE(pi1): 2.579e-02, MSE(pi2): 1.633e-04, MSE(pi3): 1.465e-03\n",
      "Epoch 60400, Train loss: 2.855e+03, Test loss: 1.279e+05, MSE(e): 2.628e-04, MSE(pi1): 1.369e-02, MSE(pi2): 1.332e-04, MSE(pi3): 8.976e-04\n",
      "Epoch 60500, Train loss: 2.195e+03, Test loss: 1.387e+05, MSE(e): 1.841e-04, MSE(pi1): 2.591e-02, MSE(pi2): 1.034e-04, MSE(pi3): 9.476e-04\n",
      "Epoch 60600, Train loss: 1.976e+03, Test loss: 1.221e+05, MSE(e): 1.712e-04, MSE(pi1): 1.826e-02, MSE(pi2): 9.378e-05, MSE(pi3): 8.175e-04\n",
      "Epoch 60700, Train loss: 1.570e+03, Test loss: 1.124e+05, MSE(e): 1.147e-04, MSE(pi1): 2.194e-02, MSE(pi2): 7.319e-05, MSE(pi3): 2.037e-03\n",
      "Epoch 60800, Train loss: 2.845e+03, Test loss: 1.434e+05, MSE(e): 2.590e-04, MSE(pi1): 1.431e-02, MSE(pi2): 1.449e-04, MSE(pi3): 1.122e-03\n",
      "Epoch 60900, Train loss: 3.747e+03, Test loss: 1.114e+05, MSE(e): 3.405e-04, MSE(pi1): 2.416e-02, MSE(pi2): 1.639e-04, MSE(pi3): 1.004e-03\n",
      "Epoch 61000, Train loss: 1.889e+03, Test loss: 1.300e+05, MSE(e): 1.678e-04, MSE(pi1): 1.237e-02, MSE(pi2): 9.136e-05, MSE(pi3): 8.669e-04\n",
      "Epoch 61100, Train loss: 2.844e+03, Test loss: 1.152e+05, MSE(e): 2.648e-04, MSE(pi1): 9.049e-03, MSE(pi2): 1.356e-04, MSE(pi3): 1.061e-03\n",
      "Epoch 61200, Train loss: 2.229e+03, Test loss: 1.189e+05, MSE(e): 1.670e-04, MSE(pi1): 3.843e-02, MSE(pi2): 9.205e-05, MSE(pi3): 1.747e-03\n",
      "Epoch 61300, Train loss: 4.536e+03, Test loss: 1.346e+05, MSE(e): 4.296e-04, MSE(pi1): 1.423e-02, MSE(pi2): 2.175e-04, MSE(pi3): 9.737e-04\n",
      "Epoch 61400, Train loss: 2.005e+03, Test loss: 1.141e+05, MSE(e): 1.541e-04, MSE(pi1): 3.602e-02, MSE(pi2): 8.787e-05, MSE(pi3): 1.043e-03\n",
      "Epoch 61500, Train loss: 2.625e+03, Test loss: 1.381e+05, MSE(e): 2.353e-04, MSE(pi1): 1.965e-02, MSE(pi2): 1.259e-04, MSE(pi3): 7.514e-04\n",
      "Epoch 61600, Train loss: 2.176e+03, Test loss: 1.206e+05, MSE(e): 1.907e-04, MSE(pi1): 1.525e-02, MSE(pi2): 1.101e-04, MSE(pi3): 1.162e-03\n",
      "Epoch 61700, Train loss: 1.588e+03, Test loss: 1.331e+05, MSE(e): 1.350e-04, MSE(pi1): 1.476e-02, MSE(pi2): 8.545e-05, MSE(pi3): 9.036e-04\n",
      "Epoch 61800, Train loss: 5.541e+03, Test loss: 1.194e+05, MSE(e): 5.250e-04, MSE(pi1): 1.678e-02, MSE(pi2): 2.402e-04, MSE(pi3): 1.236e-03\n",
      "Epoch 61900, Train loss: 1.571e+03, Test loss: 1.280e+05, MSE(e): 1.200e-04, MSE(pi1): 2.803e-02, MSE(pi2): 7.453e-05, MSE(pi3): 9.089e-04\n",
      "Epoch 62000, Train loss: 2.910e+03, Test loss: 1.177e+05, MSE(e): 2.394e-04, MSE(pi1): 3.725e-02, MSE(pi2): 1.253e-04, MSE(pi3): 1.439e-03\n",
      "Epoch 62100, Train loss: 4.620e+03, Test loss: 1.330e+05, MSE(e): 3.815e-04, MSE(pi1): 6.894e-02, MSE(pi2): 1.799e-04, MSE(pi3): 1.153e-03\n",
      "Epoch 62200, Train loss: 2.532e+03, Test loss: 1.296e+05, MSE(e): 2.226e-04, MSE(pi1): 2.223e-02, MSE(pi2): 1.121e-04, MSE(pi3): 8.402e-04\n",
      "Epoch 62300, Train loss: 4.234e+03, Test loss: 1.290e+05, MSE(e): 3.740e-04, MSE(pi1): 4.203e-02, MSE(pi2): 1.716e-04, MSE(pi3): 7.370e-04\n",
      "Epoch 62400, Train loss: 2.034e+03, Test loss: 1.317e+05, MSE(e): 1.479e-04, MSE(pi1): 4.252e-02, MSE(pi2): 8.698e-05, MSE(pi3): 1.302e-03\n",
      "Epoch 62500, Train loss: 1.399e+03, Test loss: 1.171e+05, MSE(e): 1.199e-04, MSE(pi1): 8.561e-03, MSE(pi2): 7.851e-05, MSE(pi3): 1.147e-03\n",
      "Epoch 62600, Train loss: 7.350e+03, Test loss: 1.238e+05, MSE(e): 7.143e-04, MSE(pi1): 9.899e-03, MSE(pi2): 3.357e-04, MSE(pi3): 1.078e-03\n",
      "Epoch 62700, Train loss: 1.398e+03, Test loss: 1.323e+05, MSE(e): 1.161e-04, MSE(pi1): 1.521e-02, MSE(pi2): 7.159e-05, MSE(pi3): 8.477e-04\n",
      "Epoch 62800, Train loss: 2.212e+03, Test loss: 1.227e+05, MSE(e): 1.786e-04, MSE(pi1): 2.849e-02, MSE(pi2): 9.623e-05, MSE(pi3): 1.414e-03\n",
      "Epoch 62900, Train loss: 2.252e+03, Test loss: 1.197e+05, MSE(e): 2.046e-04, MSE(pi1): 1.212e-02, MSE(pi2): 1.183e-04, MSE(pi3): 8.432e-04\n",
      "Epoch 63000, Train loss: 4.422e+03, Test loss: 1.121e+05, MSE(e): 4.245e-04, MSE(pi1): 8.430e-03, MSE(pi2): 2.022e-04, MSE(pi3): 9.196e-04\n",
      "Epoch 63100, Train loss: 4.719e+03, Test loss: 1.260e+05, MSE(e): 4.408e-04, MSE(pi1): 2.139e-02, MSE(pi2): 2.196e-04, MSE(pi3): 9.688e-04\n",
      "Epoch 63200, Train loss: 2.026e+03, Test loss: 1.297e+05, MSE(e): 1.719e-04, MSE(pi1): 2.235e-02, MSE(pi2): 9.147e-05, MSE(pi3): 8.382e-04\n",
      "Epoch 63300, Train loss: 2.895e+03, Test loss: 1.250e+05, MSE(e): 2.659e-04, MSE(pi1): 1.342e-02, MSE(pi2): 1.328e-04, MSE(pi3): 1.024e-03\n",
      "Epoch 63400, Train loss: 1.452e+03, Test loss: 1.123e+05, MSE(e): 1.169e-04, MSE(pi1): 1.700e-02, MSE(pi2): 7.709e-05, MSE(pi3): 1.124e-03\n",
      "Epoch 63500, Train loss: 1.740e+03, Test loss: 1.190e+05, MSE(e): 1.434e-04, MSE(pi1): 1.683e-02, MSE(pi2): 8.444e-05, MSE(pi3): 1.374e-03\n",
      "Epoch 63600, Train loss: 1.971e+03, Test loss: 1.140e+05, MSE(e): 1.608e-04, MSE(pi1): 2.189e-02, MSE(pi2): 1.084e-04, MSE(pi3): 1.443e-03\n",
      "Epoch 63700, Train loss: 2.697e+03, Test loss: 1.204e+05, MSE(e): 2.194e-04, MSE(pi1): 4.128e-02, MSE(pi2): 1.206e-04, MSE(pi3): 8.968e-04\n",
      "Epoch 63800, Train loss: 2.246e+03, Test loss: 1.328e+05, MSE(e): 1.972e-04, MSE(pi1): 2.093e-02, MSE(pi2): 1.033e-04, MSE(pi3): 6.464e-04\n",
      "Epoch 63900, Train loss: 3.109e+03, Test loss: 1.277e+05, MSE(e): 2.776e-04, MSE(pi1): 2.489e-02, MSE(pi2): 1.369e-04, MSE(pi3): 8.383e-04\n",
      "Epoch 64000, Train loss: 3.009e+03, Test loss: 1.185e+05, MSE(e): 2.724e-04, MSE(pi1): 1.945e-02, MSE(pi2): 1.305e-04, MSE(pi3): 9.057e-04\n",
      "Epoch 64100, Train loss: 1.358e+03, Test loss: 1.195e+05, MSE(e): 1.049e-04, MSE(pi1): 2.186e-02, MSE(pi2): 6.641e-05, MSE(pi3): 8.968e-04\n",
      "Epoch 64200, Train loss: 7.236e+03, Test loss: 1.184e+05, MSE(e): 6.567e-04, MSE(pi1): 4.590e-02, MSE(pi2): 3.333e-04, MSE(pi3): 2.102e-03\n",
      "Epoch 64300, Train loss: 2.622e+03, Test loss: 9.956e+04, MSE(e): 1.898e-04, MSE(pi1): 4.852e-02, MSE(pi2): 1.031e-04, MSE(pi3): 2.386e-03\n",
      "Epoch 64400, Train loss: 1.509e+03, Test loss: 1.045e+05, MSE(e): 1.308e-04, MSE(pi1): 1.078e-02, MSE(pi2): 8.147e-05, MSE(pi3): 9.290e-04\n",
      "Epoch 64500, Train loss: 1.554e+03, Test loss: 1.152e+05, MSE(e): 1.286e-04, MSE(pi1): 1.919e-02, MSE(pi2): 8.240e-05, MSE(pi3): 7.593e-04\n",
      "Epoch 64600, Train loss: 1.980e+03, Test loss: 1.150e+05, MSE(e): 1.578e-04, MSE(pi1): 3.081e-02, MSE(pi2): 9.198e-05, MSE(pi3): 9.304e-04\n",
      "Epoch 64700, Train loss: 1.462e+03, Test loss: 8.944e+04, MSE(e): 1.234e-04, MSE(pi1): 1.522e-02, MSE(pi2): 7.375e-05, MSE(pi3): 7.547e-04\n",
      "Epoch 64800, Train loss: 2.026e+03, Test loss: 8.505e+04, MSE(e): 1.689e-04, MSE(pi1): 2.538e-02, MSE(pi2): 9.709e-05, MSE(pi3): 8.317e-04\n",
      "Epoch 64900, Train loss: 1.278e+03, Test loss: 5.943e+04, MSE(e): 1.078e-04, MSE(pi1): 1.154e-02, MSE(pi2): 6.895e-05, MSE(pi3): 8.470e-04\n",
      "Epoch 65000, Train loss: 1.253e+03, Test loss: 8.239e+04, MSE(e): 8.790e-05, MSE(pi1): 2.838e-02, MSE(pi2): 5.925e-05, MSE(pi3): 9.031e-04\n",
      "Epoch 65100, Train loss: 4.469e+03, Test loss: 6.803e+04, MSE(e): 4.295e-04, MSE(pi1): 1.042e-02, MSE(pi2): 2.051e-04, MSE(pi3): 7.001e-04\n",
      "Epoch 65200, Train loss: 2.112e+03, Test loss: 3.780e+04, MSE(e): 1.866e-04, MSE(pi1): 1.485e-02, MSE(pi2): 1.274e-04, MSE(pi3): 9.801e-04\n",
      "Epoch 65300, Train loss: 1.533e+03, Test loss: 7.185e+04, MSE(e): 1.057e-04, MSE(pi1): 2.988e-02, MSE(pi2): 6.561e-05, MSE(pi3): 1.765e-03\n",
      "Epoch 65400, Train loss: 3.066e+03, Test loss: 6.438e+04, MSE(e): 2.506e-04, MSE(pi1): 3.995e-02, MSE(pi2): 1.187e-04, MSE(pi3): 1.603e-03\n",
      "Epoch 65500, Train loss: 1.256e+03, Test loss: 4.973e+04, MSE(e): 9.718e-05, MSE(pi1): 2.097e-02, MSE(pi2): 6.487e-05, MSE(pi3): 7.488e-04\n",
      "Epoch 65600, Train loss: 5.132e+03, Test loss: 2.890e+04, MSE(e): 4.678e-04, MSE(pi1): 3.240e-02, MSE(pi2): 2.397e-04, MSE(pi3): 1.294e-03\n",
      "Epoch 65700, Train loss: 1.259e+03, Test loss: 5.268e+04, MSE(e): 8.851e-05, MSE(pi1): 2.650e-02, MSE(pi2): 6.034e-05, MSE(pi3): 1.084e-03\n",
      "Epoch 65800, Train loss: 2.831e+03, Test loss: 5.121e+04, MSE(e): 2.318e-04, MSE(pi1): 4.033e-02, MSE(pi2): 1.196e-04, MSE(pi3): 1.095e-03\n",
      "Epoch 65900, Train loss: 3.810e+03, Test loss: 4.962e+04, MSE(e): 3.436e-04, MSE(pi1): 2.683e-02, MSE(pi2): 1.692e-04, MSE(pi3): 1.062e-03\n",
      "Epoch 66000, Train loss: 1.323e+03, Test loss: 4.566e+04, MSE(e): 1.013e-04, MSE(pi1): 2.151e-02, MSE(pi2): 6.304e-05, MSE(pi3): 9.473e-04\n",
      "Epoch 66100, Train loss: 3.110e+03, Test loss: 4.327e+04, MSE(e): 2.826e-04, MSE(pi1): 2.181e-02, MSE(pi2): 1.405e-04, MSE(pi3): 6.551e-04\n",
      "Epoch 66200, Train loss: 1.962e+03, Test loss: 4.451e+04, MSE(e): 1.092e-04, MSE(pi1): 7.442e-02, MSE(pi2): 6.345e-05, MSE(pi3): 1.261e-03\n",
      "Epoch 66300, Train loss: 3.788e+03, Test loss: 4.334e+04, MSE(e): 3.514e-04, MSE(pi1): 1.423e-02, MSE(pi2): 1.802e-04, MSE(pi3): 1.322e-03\n",
      "Epoch 66400, Train loss: 3.379e+03, Test loss: 4.622e+04, MSE(e): 3.020e-04, MSE(pi1): 1.964e-02, MSE(pi2): 1.488e-04, MSE(pi3): 1.633e-03\n",
      "Epoch 66500, Train loss: 1.574e+03, Test loss: 4.763e+04, MSE(e): 1.273e-04, MSE(pi1): 1.962e-02, MSE(pi2): 7.918e-05, MSE(pi3): 1.054e-03\n",
      "Epoch 66600, Train loss: 1.945e+03, Test loss: 5.173e+04, MSE(e): 1.540e-04, MSE(pi1): 2.320e-02, MSE(pi2): 8.943e-05, MSE(pi3): 1.733e-03\n",
      "Epoch 66700, Train loss: 2.953e+03, Test loss: 5.097e+04, MSE(e): 2.668e-04, MSE(pi1): 1.674e-02, MSE(pi2): 1.380e-04, MSE(pi3): 1.182e-03\n",
      "Epoch 66800, Train loss: 6.807e+03, Test loss: 5.453e+04, MSE(e): 6.429e-04, MSE(pi1): 2.605e-02, MSE(pi2): 2.946e-04, MSE(pi3): 1.172e-03\n",
      "Epoch 66900, Train loss: 1.922e+03, Test loss: 6.751e+04, MSE(e): 1.627e-04, MSE(pi1): 2.214e-02, MSE(pi2): 9.889e-05, MSE(pi3): 7.350e-04\n",
      "Epoch 67000, Train loss: 3.801e+03, Test loss: 5.636e+04, MSE(e): 3.453e-04, MSE(pi1): 2.435e-02, MSE(pi2): 1.727e-04, MSE(pi3): 1.045e-03\n",
      "Epoch 67100, Train loss: 2.047e+03, Test loss: 5.203e+04, MSE(e): 1.729e-04, MSE(pi1): 2.006e-02, MSE(pi2): 9.914e-05, MSE(pi3): 1.169e-03\n",
      "Epoch 67200, Train loss: 3.607e+03, Test loss: 6.388e+04, MSE(e): 3.279e-04, MSE(pi1): 2.135e-02, MSE(pi2): 1.667e-04, MSE(pi3): 1.141e-03\n",
      "Epoch 67300, Train loss: 1.739e+03, Test loss: 6.784e+04, MSE(e): 1.362e-04, MSE(pi1): 2.577e-02, MSE(pi2): 8.521e-05, MSE(pi3): 1.191e-03\n",
      "Epoch 67400, Train loss: 2.768e+03, Test loss: 5.542e+04, MSE(e): 2.440e-04, MSE(pi1): 2.246e-02, MSE(pi2): 1.377e-04, MSE(pi3): 1.025e-03\n",
      "Epoch 67500, Train loss: 4.245e+03, Test loss: 5.826e+04, MSE(e): 4.001e-04, MSE(pi1): 1.264e-02, MSE(pi2): 2.007e-04, MSE(pi3): 1.171e-03\n",
      "Epoch 67600, Train loss: 2.700e+03, Test loss: 4.966e+04, MSE(e): 2.523e-04, MSE(pi1): 9.478e-03, MSE(pi2): 1.306e-04, MSE(pi3): 8.193e-04\n",
      "Epoch 67700, Train loss: 2.106e+03, Test loss: 5.189e+04, MSE(e): 1.731e-04, MSE(pi1): 2.698e-02, MSE(pi2): 1.072e-04, MSE(pi3): 1.055e-03\n",
      "Epoch 67800, Train loss: 2.872e+03, Test loss: 6.008e+04, MSE(e): 2.670e-04, MSE(pi1): 1.292e-02, MSE(pi2): 1.437e-04, MSE(pi3): 7.356e-04\n",
      "Epoch 67900, Train loss: 3.152e+03, Test loss: 5.364e+04, MSE(e): 2.887e-04, MSE(pi1): 1.959e-02, MSE(pi2): 1.429e-04, MSE(pi3): 6.849e-04\n",
      "Epoch 68000, Train loss: 1.688e+03, Test loss: 4.217e+04, MSE(e): 1.470e-04, MSE(pi1): 1.414e-02, MSE(pi2): 9.334e-05, MSE(pi3): 7.658e-04\n",
      "Epoch 68100, Train loss: 4.092e+03, Test loss: 4.732e+04, MSE(e): 3.556e-04, MSE(pi1): 3.777e-02, MSE(pi2): 1.758e-04, MSE(pi3): 1.581e-03\n",
      "Epoch 68200, Train loss: 2.960e+03, Test loss: 5.100e+04, MSE(e): 2.792e-04, MSE(pi1): 8.507e-03, MSE(pi2): 1.462e-04, MSE(pi3): 8.272e-04\n",
      "Epoch 68300, Train loss: 1.752e+03, Test loss: 5.584e+04, MSE(e): 1.508e-04, MSE(pi1): 1.503e-02, MSE(pi2): 9.216e-05, MSE(pi3): 9.299e-04\n",
      "Epoch 68400, Train loss: 2.181e+03, Test loss: 3.899e+04, MSE(e): 1.709e-04, MSE(pi1): 3.529e-02, MSE(pi2): 9.421e-05, MSE(pi3): 1.192e-03\n",
      "Epoch 68500, Train loss: 1.622e+03, Test loss: 4.276e+04, MSE(e): 1.423e-04, MSE(pi1): 1.106e-02, MSE(pi2): 9.356e-05, MSE(pi3): 8.832e-04\n",
      "Epoch 68600, Train loss: 4.067e+03, Test loss: 4.302e+04, MSE(e): 3.867e-04, MSE(pi1): 1.051e-02, MSE(pi2): 1.844e-04, MSE(pi3): 9.522e-04\n",
      "Epoch 68700, Train loss: 2.576e+03, Test loss: 5.308e+04, MSE(e): 2.067e-04, MSE(pi1): 4.009e-02, MSE(pi2): 1.273e-04, MSE(pi3): 1.082e-03\n",
      "Epoch 68800, Train loss: 2.420e+03, Test loss: 4.287e+04, MSE(e): 2.143e-04, MSE(pi1): 1.844e-02, MSE(pi2): 1.058e-04, MSE(pi3): 9.180e-04\n",
      "Epoch 68900, Train loss: 1.377e+03, Test loss: 3.491e+04, MSE(e): 9.809e-05, MSE(pi1): 3.199e-02, MSE(pi2): 6.402e-05, MSE(pi3): 7.598e-04\n",
      "Epoch 69000, Train loss: 2.517e+03, Test loss: 3.606e+04, MSE(e): 1.888e-04, MSE(pi1): 5.093e-02, MSE(pi2): 1.066e-04, MSE(pi3): 1.188e-03\n",
      "Epoch 69100, Train loss: 3.032e+03, Test loss: 3.368e+04, MSE(e): 2.727e-04, MSE(pi1): 1.885e-02, MSE(pi2): 1.525e-04, MSE(pi3): 1.169e-03\n",
      "Epoch 69200, Train loss: 2.037e+03, Test loss: 3.123e+04, MSE(e): 1.499e-04, MSE(pi1): 3.703e-02, MSE(pi2): 8.732e-05, MSE(pi3): 1.675e-03\n",
      "Epoch 69300, Train loss: 3.042e+03, Test loss: 3.613e+04, MSE(e): 2.656e-04, MSE(pi1): 2.383e-02, MSE(pi2): 1.409e-04, MSE(pi3): 1.475e-03\n",
      "Epoch 69400, Train loss: 1.275e+03, Test loss: 3.373e+04, MSE(e): 1.025e-04, MSE(pi1): 1.714e-02, MSE(pi2): 6.962e-05, MSE(pi3): 7.908e-04\n",
      "Epoch 69500, Train loss: 2.609e+03, Test loss: 3.034e+04, MSE(e): 2.223e-04, MSE(pi1): 2.428e-02, MSE(pi2): 1.264e-04, MSE(pi3): 1.432e-03\n",
      "Epoch 69600, Train loss: 2.974e+03, Test loss: 3.257e+04, MSE(e): 2.597e-04, MSE(pi1): 2.308e-02, MSE(pi2): 1.354e-04, MSE(pi3): 1.455e-03\n",
      "Epoch 69700, Train loss: 2.093e+03, Test loss: 2.864e+04, MSE(e): 1.659e-04, MSE(pi1): 3.550e-02, MSE(pi2): 9.704e-05, MSE(pi3): 7.947e-04\n",
      "Epoch 69800, Train loss: 4.144e+03, Test loss: 2.697e+04, MSE(e): 3.576e-04, MSE(pi1): 4.470e-02, MSE(pi2): 1.649e-04, MSE(pi3): 1.210e-03\n",
      "Epoch 69900, Train loss: 2.703e+03, Test loss: 2.701e+04, MSE(e): 2.418e-04, MSE(pi1): 2.009e-02, MSE(pi2): 1.239e-04, MSE(pi3): 8.320e-04\n",
      "Epoch 70000, Train loss: 2.961e+03, Test loss: 3.228e+04, MSE(e): 2.162e-04, MSE(pi1): 6.671e-02, MSE(pi2): 1.174e-04, MSE(pi3): 1.318e-03\n",
      "Epoch 70100, Train loss: 1.973e+03, Test loss: 3.009e+04, MSE(e): 1.684e-04, MSE(pi1): 2.117e-02, MSE(pi2): 9.590e-05, MSE(pi3): 7.639e-04\n",
      "Epoch 70200, Train loss: 2.481e+03, Test loss: 2.252e+04, MSE(e): 1.965e-04, MSE(pi1): 3.078e-02, MSE(pi2): 1.107e-04, MSE(pi3): 2.084e-03\n",
      "Epoch 70300, Train loss: 4.630e+03, Test loss: 2.186e+04, MSE(e): 4.133e-04, MSE(pi1): 4.201e-02, MSE(pi2): 2.051e-04, MSE(pi3): 7.705e-04\n",
      "Epoch 70400, Train loss: 2.826e+03, Test loss: 2.496e+04, MSE(e): 2.460e-04, MSE(pi1): 2.857e-02, MSE(pi2): 1.237e-04, MSE(pi3): 8.001e-04\n",
      "Epoch 70500, Train loss: 1.804e+03, Test loss: 2.464e+04, MSE(e): 1.636e-04, MSE(pi1): 9.655e-03, MSE(pi2): 1.011e-04, MSE(pi3): 7.120e-04\n",
      "Epoch 70600, Train loss: 2.292e+03, Test loss: 2.240e+04, MSE(e): 2.093e-04, MSE(pi1): 1.208e-02, MSE(pi2): 1.091e-04, MSE(pi3): 7.767e-04\n",
      "Epoch 70700, Train loss: 3.795e+03, Test loss: 1.942e+04, MSE(e): 3.371e-04, MSE(pi1): 3.276e-02, MSE(pi2): 1.550e-04, MSE(pi3): 9.626e-04\n",
      "Epoch 70800, Train loss: 2.133e+03, Test loss: 1.858e+04, MSE(e): 1.558e-04, MSE(pi1): 4.593e-02, MSE(pi2): 8.714e-05, MSE(pi3): 1.151e-03\n",
      "Epoch 70900, Train loss: 1.897e+03, Test loss: 1.888e+04, MSE(e): 1.522e-04, MSE(pi1): 2.811e-02, MSE(pi2): 8.504e-05, MSE(pi3): 9.384e-04\n",
      "Epoch 71000, Train loss: 3.887e+03, Test loss: 1.668e+04, MSE(e): 3.746e-04, MSE(pi1): 5.088e-03, MSE(pi2): 1.989e-04, MSE(pi3): 8.920e-04\n",
      "Epoch 71100, Train loss: 3.177e+03, Test loss: 1.743e+04, MSE(e): 2.571e-04, MSE(pi1): 4.370e-02, MSE(pi2): 1.280e-04, MSE(pi3): 1.695e-03\n",
      "Epoch 71200, Train loss: 3.961e+03, Test loss: 1.525e+04, MSE(e): 3.710e-04, MSE(pi1): 1.500e-02, MSE(pi2): 1.771e-04, MSE(pi3): 1.000e-03\n",
      "Epoch 71300, Train loss: 4.841e+03, Test loss: 1.488e+04, MSE(e): 4.529e-04, MSE(pi1): 2.347e-02, MSE(pi2): 2.119e-04, MSE(pi3): 7.633e-04\n",
      "Epoch 71400, Train loss: 1.736e+03, Test loss: 1.514e+04, MSE(e): 1.509e-04, MSE(pi1): 1.583e-02, MSE(pi2): 9.347e-05, MSE(pi3): 6.861e-04\n",
      "Epoch 71500, Train loss: 6.419e+03, Test loss: 1.327e+04, MSE(e): 5.817e-04, MSE(pi1): 5.283e-02, MSE(pi2): 2.707e-04, MSE(pi3): 7.324e-04\n",
      "Epoch 71600, Train loss: 2.022e+03, Test loss: 1.365e+04, MSE(e): 1.537e-04, MSE(pi1): 3.807e-02, MSE(pi2): 9.111e-05, MSE(pi3): 1.039e-03\n",
      "Epoch 71700, Train loss: 2.891e+03, Test loss: 1.151e+04, MSE(e): 2.671e-04, MSE(pi1): 1.471e-02, MSE(pi2): 1.322e-04, MSE(pi3): 7.333e-04\n",
      "Epoch 71800, Train loss: 2.451e+03, Test loss: 1.389e+04, MSE(e): 1.871e-04, MSE(pi1): 4.635e-02, MSE(pi2): 1.120e-04, MSE(pi3): 1.158e-03\n",
      "Epoch 71900, Train loss: 1.940e+03, Test loss: 1.229e+04, MSE(e): 1.656e-04, MSE(pi1): 1.771e-02, MSE(pi2): 1.045e-04, MSE(pi3): 1.065e-03\n",
      "Epoch 72000, Train loss: 4.501e+03, Test loss: 1.126e+04, MSE(e): 4.295e-04, MSE(pi1): 1.200e-02, MSE(pi2): 1.983e-04, MSE(pi3): 8.561e-04\n",
      "Epoch 72100, Train loss: 3.319e+03, Test loss: 1.127e+04, MSE(e): 3.026e-04, MSE(pi1): 1.921e-02, MSE(pi2): 1.697e-04, MSE(pi3): 1.006e-03\n",
      "Epoch 72200, Train loss: 1.711e+03, Test loss: 1.163e+04, MSE(e): 1.280e-04, MSE(pi1): 3.554e-02, MSE(pi2): 8.051e-05, MSE(pi3): 7.516e-04\n",
      "Epoch 72300, Train loss: 3.056e+03, Test loss: 1.052e+04, MSE(e): 2.778e-04, MSE(pi1): 1.902e-02, MSE(pi2): 1.709e-04, MSE(pi3): 8.785e-04\n",
      "Epoch 72400, Train loss: 4.769e+04, Test loss: 5.915e+04, MSE(e): 4.733e-03, MSE(pi1): 2.734e-02, MSE(pi2): 2.067e-03, MSE(pi3): 8.045e-04\n",
      "Epoch 72500, Train loss: 2.892e+03, Test loss: 1.082e+04, MSE(e): 2.530e-04, MSE(pi1): 3.009e-02, MSE(pi2): 1.371e-04, MSE(pi3): 6.059e-04\n",
      "Epoch 72600, Train loss: 1.554e+03, Test loss: 1.207e+04, MSE(e): 1.164e-04, MSE(pi1): 2.678e-02, MSE(pi2): 7.781e-05, MSE(pi3): 1.219e-03\n",
      "Epoch 72700, Train loss: 3.024e+03, Test loss: 9.984e+03, MSE(e): 2.649e-04, MSE(pi1): 2.504e-02, MSE(pi2): 1.493e-04, MSE(pi3): 1.242e-03\n",
      "Epoch 72800, Train loss: 1.313e+03, Test loss: 8.658e+03, MSE(e): 1.004e-04, MSE(pi1): 2.263e-02, MSE(pi2): 6.633e-05, MSE(pi3): 8.236e-04\n",
      "Epoch 72900, Train loss: 1.491e+03, Test loss: 8.413e+03, MSE(e): 1.081e-04, MSE(pi1): 3.038e-02, MSE(pi2): 6.743e-05, MSE(pi3): 1.056e-03\n",
      "Epoch 73000, Train loss: 2.980e+03, Test loss: 8.813e+03, MSE(e): 2.754e-04, MSE(pi1): 1.527e-02, MSE(pi2): 1.441e-04, MSE(pi3): 7.381e-04\n",
      "Epoch 73100, Train loss: 1.471e+03, Test loss: 8.050e+03, MSE(e): 1.105e-04, MSE(pi1): 2.686e-02, MSE(pi2): 7.360e-05, MSE(pi3): 9.666e-04\n",
      "Epoch 73200, Train loss: 1.354e+03, Test loss: 8.256e+03, MSE(e): 1.185e-04, MSE(pi1): 9.086e-03, MSE(pi2): 7.439e-05, MSE(pi3): 7.767e-04\n",
      "Epoch 73300, Train loss: 8.859e+03, Test loss: 1.260e+04, MSE(e): 8.399e-04, MSE(pi1): 3.320e-02, MSE(pi2): 3.927e-04, MSE(pi3): 1.283e-03\n",
      "Epoch 73400, Train loss: 1.549e+03, Test loss: 9.164e+03, MSE(e): 1.079e-04, MSE(pi1): 3.775e-02, MSE(pi2): 7.247e-05, MSE(pi3): 9.313e-04\n",
      "Epoch 73500, Train loss: 1.327e+03, Test loss: 8.522e+03, MSE(e): 9.701e-05, MSE(pi1): 2.733e-02, MSE(pi2): 6.602e-05, MSE(pi3): 8.334e-04\n",
      "Epoch 73600, Train loss: 3.003e+03, Test loss: 9.295e+03, MSE(e): 2.715e-04, MSE(pi1): 2.104e-02, MSE(pi2): 1.402e-04, MSE(pi3): 7.796e-04\n",
      "Epoch 73700, Train loss: 1.835e+03, Test loss: 8.289e+03, MSE(e): 1.604e-04, MSE(pi1): 1.500e-02, MSE(pi2): 9.017e-05, MSE(pi3): 8.115e-04\n",
      "Epoch 73800, Train loss: 2.090e+03, Test loss: 9.247e+03, MSE(e): 1.649e-04, MSE(pi1): 3.023e-02, MSE(pi2): 8.841e-05, MSE(pi3): 1.386e-03\n",
      "Epoch 73900, Train loss: 5.295e+03, Test loss: 1.246e+04, MSE(e): 5.093e-04, MSE(pi1): 9.528e-03, MSE(pi2): 2.602e-04, MSE(pi3): 1.065e-03\n",
      "Epoch 74000, Train loss: 3.516e+03, Test loss: 9.168e+03, MSE(e): 3.258e-04, MSE(pi1): 1.601e-02, MSE(pi2): 1.516e-04, MSE(pi3): 9.822e-04\n",
      "Epoch 74100, Train loss: 2.279e+03, Test loss: 9.120e+03, MSE(e): 1.913e-04, MSE(pi1): 2.637e-02, MSE(pi2): 9.658e-05, MSE(pi3): 1.026e-03\n",
      "Epoch 74200, Train loss: 2.711e+03, Test loss: 1.409e+04, MSE(e): 2.013e-04, MSE(pi1): 4.786e-02, MSE(pi2): 1.065e-04, MSE(pi3): 2.199e-03\n",
      "Epoch 74300, Train loss: 2.550e+03, Test loss: 9.429e+03, MSE(e): 2.150e-04, MSE(pi1): 3.254e-02, MSE(pi2): 1.124e-04, MSE(pi3): 7.449e-04\n",
      "Epoch 74400, Train loss: 2.684e+03, Test loss: 1.166e+04, MSE(e): 2.348e-04, MSE(pi1): 2.582e-02, MSE(pi2): 1.242e-04, MSE(pi3): 7.834e-04\n",
      "Epoch 74500, Train loss: 1.721e+03, Test loss: 9.505e+03, MSE(e): 1.414e-04, MSE(pi1): 2.403e-02, MSE(pi2): 8.888e-05, MSE(pi3): 6.687e-04\n",
      "Epoch 74600, Train loss: 1.211e+04, Test loss: 1.446e+04, MSE(e): 1.167e-03, MSE(pi1): 2.566e-02, MSE(pi2): 5.163e-04, MSE(pi3): 1.825e-03\n",
      "Epoch 74700, Train loss: 1.993e+03, Test loss: 1.151e+04, MSE(e): 1.761e-04, MSE(pi1): 1.453e-02, MSE(pi2): 1.011e-04, MSE(pi3): 8.649e-04\n",
      "Epoch 74800, Train loss: 1.890e+03, Test loss: 9.347e+03, MSE(e): 1.450e-04, MSE(pi1): 3.169e-02, MSE(pi2): 7.789e-05, MSE(pi3): 1.225e-03\n",
      "Epoch 74900, Train loss: 1.605e+03, Test loss: 9.987e+03, MSE(e): 1.057e-04, MSE(pi1): 3.995e-02, MSE(pi2): 6.519e-05, MSE(pi3): 1.491e-03\n",
      "Epoch 75000, Train loss: 1.238e+04, Test loss: 2.981e+04, MSE(e): 1.194e-03, MSE(pi1): 3.565e-02, MSE(pi2): 4.914e-04, MSE(pi3): 7.984e-04\n",
      "Epoch 75100, Train loss: 2.112e+03, Test loss: 1.007e+04, MSE(e): 1.734e-04, MSE(pi1): 2.488e-02, MSE(pi2): 8.955e-05, MSE(pi3): 1.288e-03\n",
      "Epoch 75200, Train loss: 1.225e+03, Test loss: 8.969e+03, MSE(e): 1.005e-04, MSE(pi1): 1.494e-02, MSE(pi2): 6.674e-05, MSE(pi3): 7.009e-04\n",
      "Epoch 75300, Train loss: 2.842e+03, Test loss: 1.159e+04, MSE(e): 2.423e-04, MSE(pi1): 3.499e-02, MSE(pi2): 1.423e-04, MSE(pi3): 6.921e-04\n",
      "Epoch 75400, Train loss: 5.868e+03, Test loss: 1.016e+04, MSE(e): 5.597e-04, MSE(pi1): 1.854e-02, MSE(pi2): 2.616e-04, MSE(pi3): 8.508e-04\n",
      "Epoch 75500, Train loss: 3.350e+03, Test loss: 2.491e+04, MSE(e): 2.988e-04, MSE(pi1): 2.336e-02, MSE(pi2): 1.566e-04, MSE(pi3): 1.282e-03\n",
      "Epoch 75600, Train loss: 1.447e+03, Test loss: 1.093e+04, MSE(e): 1.138e-04, MSE(pi1): 2.094e-02, MSE(pi2): 7.539e-05, MSE(pi3): 9.898e-04\n",
      "Epoch 75700, Train loss: 1.408e+03, Test loss: 1.046e+04, MSE(e): 1.074e-04, MSE(pi1): 2.571e-02, MSE(pi2): 7.266e-05, MSE(pi3): 7.644e-04\n",
      "Epoch 75800, Train loss: 2.307e+03, Test loss: 1.103e+04, MSE(e): 1.607e-04, MSE(pi1): 5.695e-02, MSE(pi2): 8.502e-05, MSE(pi3): 1.300e-03\n",
      "Epoch 75900, Train loss: 1.627e+03, Test loss: 1.018e+04, MSE(e): 1.424e-04, MSE(pi1): 1.303e-02, MSE(pi2): 8.342e-05, MSE(pi3): 7.266e-04\n",
      "Epoch 76000, Train loss: 1.322e+03, Test loss: 9.410e+03, MSE(e): 1.008e-04, MSE(pi1): 2.451e-02, MSE(pi2): 6.622e-05, MSE(pi3): 6.936e-04\n",
      "Epoch 76100, Train loss: 5.131e+03, Test loss: 1.238e+04, MSE(e): 4.831e-04, MSE(pi1): 1.928e-02, MSE(pi2): 2.441e-04, MSE(pi3): 1.077e-03\n",
      "Epoch 76200, Train loss: 4.967e+03, Test loss: 1.645e+04, MSE(e): 4.556e-04, MSE(pi1): 3.256e-02, MSE(pi2): 2.180e-04, MSE(pi3): 8.526e-04\n",
      "Epoch 76300, Train loss: 1.133e+03, Test loss: 9.907e+03, MSE(e): 9.055e-05, MSE(pi1): 1.562e-02, MSE(pi2): 6.269e-05, MSE(pi3): 7.125e-04\n",
      "Epoch 76400, Train loss: 2.008e+03, Test loss: 1.056e+04, MSE(e): 1.606e-04, MSE(pi1): 3.280e-02, MSE(pi2): 9.173e-05, MSE(pi3): 7.416e-04\n",
      "Epoch 76500, Train loss: 1.291e+03, Test loss: 1.071e+04, MSE(e): 1.008e-04, MSE(pi1): 1.653e-02, MSE(pi2): 6.633e-05, MSE(pi3): 1.183e-03\n",
      "Epoch 76600, Train loss: 1.273e+03, Test loss: 9.155e+03, MSE(e): 1.035e-04, MSE(pi1): 1.496e-02, MSE(pi2): 7.263e-05, MSE(pi3): 8.871e-04\n",
      "Epoch 76700, Train loss: 2.891e+03, Test loss: 1.167e+04, MSE(e): 2.389e-04, MSE(pi1): 3.743e-02, MSE(pi2): 1.307e-04, MSE(pi3): 1.274e-03\n",
      "Epoch 76800, Train loss: 1.354e+03, Test loss: 9.852e+03, MSE(e): 1.014e-04, MSE(pi1): 2.725e-02, MSE(pi2): 6.741e-05, MSE(pi3): 6.781e-04\n",
      "Epoch 76900, Train loss: 2.539e+03, Test loss: 1.427e+04, MSE(e): 1.825e-04, MSE(pi1): 5.277e-02, MSE(pi2): 1.055e-04, MSE(pi3): 1.862e-03\n",
      "Epoch 77000, Train loss: 2.330e+03, Test loss: 1.282e+04, MSE(e): 1.796e-04, MSE(pi1): 4.574e-02, MSE(pi2): 1.065e-04, MSE(pi3): 7.620e-04\n",
      "Epoch 77100, Train loss: 3.197e+03, Test loss: 1.298e+04, MSE(e): 2.591e-04, MSE(pi1): 5.333e-02, MSE(pi2): 1.327e-04, MSE(pi3): 7.223e-04\n",
      "Epoch 77200, Train loss: 2.916e+03, Test loss: 1.392e+04, MSE(e): 2.537e-04, MSE(pi1): 2.569e-02, MSE(pi2): 1.286e-04, MSE(pi3): 1.226e-03\n",
      "Epoch 77300, Train loss: 1.779e+03, Test loss: 1.071e+04, MSE(e): 1.245e-04, MSE(pi1): 4.557e-02, MSE(pi2): 7.588e-05, MSE(pi3): 7.884e-04\n",
      "Epoch 77400, Train loss: 1.254e+03, Test loss: 9.628e+03, MSE(e): 9.468e-05, MSE(pi1): 2.063e-02, MSE(pi2): 6.401e-05, MSE(pi3): 1.012e-03\n",
      "Epoch 77500, Train loss: 1.435e+03, Test loss: 1.081e+04, MSE(e): 1.173e-04, MSE(pi1): 1.903e-02, MSE(pi2): 7.663e-05, MSE(pi3): 7.187e-04\n",
      "Epoch 77600, Train loss: 2.478e+03, Test loss: 1.169e+04, MSE(e): 2.236e-04, MSE(pi1): 1.597e-02, MSE(pi2): 1.250e-04, MSE(pi3): 8.209e-04\n",
      "Epoch 77700, Train loss: 5.192e+03, Test loss: 1.272e+04, MSE(e): 4.756e-04, MSE(pi1): 3.191e-02, MSE(pi2): 2.208e-04, MSE(pi3): 1.160e-03\n",
      "Epoch 77800, Train loss: 3.389e+03, Test loss: 1.384e+04, MSE(e): 2.899e-04, MSE(pi1): 4.021e-02, MSE(pi2): 1.446e-04, MSE(pi3): 8.716e-04\n",
      "Epoch 77900, Train loss: 1.575e+03, Test loss: 1.179e+04, MSE(e): 1.380e-04, MSE(pi1): 1.011e-02, MSE(pi2): 8.832e-05, MSE(pi3): 9.383e-04\n",
      "Epoch 78000, Train loss: 3.117e+03, Test loss: 1.458e+04, MSE(e): 2.728e-04, MSE(pi1): 3.052e-02, MSE(pi2): 1.398e-04, MSE(pi3): 8.404e-04\n",
      "Epoch 78100, Train loss: 2.452e+03, Test loss: 1.158e+04, MSE(e): 1.959e-04, MSE(pi1): 3.833e-02, MSE(pi2): 1.153e-04, MSE(pi3): 1.094e-03\n",
      "Epoch 78200, Train loss: 3.081e+03, Test loss: 1.054e+04, MSE(e): 2.832e-04, MSE(pi1): 1.538e-02, MSE(pi2): 1.474e-04, MSE(pi3): 9.583e-04\n",
      "Epoch 78300, Train loss: 1.550e+03, Test loss: 1.155e+04, MSE(e): 1.379e-04, MSE(pi1): 9.291e-03, MSE(pi2): 8.797e-05, MSE(pi3): 7.836e-04\n",
      "Epoch 78400, Train loss: 2.192e+03, Test loss: 1.290e+04, MSE(e): 1.941e-04, MSE(pi1): 1.553e-02, MSE(pi2): 1.075e-04, MSE(pi3): 9.605e-04\n",
      "Epoch 78500, Train loss: 3.418e+03, Test loss: 1.119e+04, MSE(e): 3.001e-04, MSE(pi1): 2.705e-02, MSE(pi2): 1.556e-04, MSE(pi3): 1.464e-03\n",
      "Epoch 78600, Train loss: 2.203e+03, Test loss: 1.336e+04, MSE(e): 1.872e-04, MSE(pi1): 2.582e-02, MSE(pi2): 1.103e-04, MSE(pi3): 7.324e-04\n",
      "Epoch 78700, Train loss: 2.547e+03, Test loss: 1.169e+04, MSE(e): 1.992e-04, MSE(pi1): 4.771e-02, MSE(pi2): 1.105e-04, MSE(pi3): 7.704e-04\n",
      "Epoch 78800, Train loss: 1.730e+03, Test loss: 1.333e+04, MSE(e): 1.230e-04, MSE(pi1): 4.053e-02, MSE(pi2): 7.457e-05, MSE(pi3): 9.410e-04\n",
      "Epoch 78900, Train loss: 3.491e+03, Test loss: 1.580e+04, MSE(e): 3.153e-04, MSE(pi1): 2.532e-02, MSE(pi2): 1.773e-04, MSE(pi3): 8.455e-04\n",
      "Epoch 79000, Train loss: 1.975e+03, Test loss: 1.251e+04, MSE(e): 1.412e-04, MSE(pi1): 4.111e-02, MSE(pi2): 8.648e-05, MSE(pi3): 1.516e-03\n",
      "Epoch 79100, Train loss: 2.273e+03, Test loss: 1.406e+04, MSE(e): 1.813e-04, MSE(pi1): 3.254e-02, MSE(pi2): 9.731e-05, MSE(pi3): 1.351e-03\n",
      "Epoch 79200, Train loss: 2.852e+03, Test loss: 1.186e+04, MSE(e): 2.417e-04, MSE(pi1): 3.469e-02, MSE(pi2): 1.309e-04, MSE(pi3): 8.842e-04\n",
      "Epoch 79300, Train loss: 2.887e+03, Test loss: 1.390e+04, MSE(e): 2.456e-04, MSE(pi1): 3.193e-02, MSE(pi2): 1.322e-04, MSE(pi3): 1.116e-03\n",
      "Epoch 79400, Train loss: 3.039e+03, Test loss: 1.422e+04, MSE(e): 2.774e-04, MSE(pi1): 1.736e-02, MSE(pi2): 1.393e-04, MSE(pi3): 9.150e-04\n",
      "Epoch 79500, Train loss: 1.361e+03, Test loss: 1.190e+04, MSE(e): 1.183e-04, MSE(pi1): 1.037e-02, MSE(pi2): 8.000e-05, MSE(pi3): 7.434e-04\n",
      "Epoch 79600, Train loss: 1.583e+03, Test loss: 1.340e+04, MSE(e): 1.190e-04, MSE(pi1): 2.770e-02, MSE(pi2): 7.795e-05, MSE(pi3): 1.152e-03\n",
      "Epoch 79700, Train loss: 3.374e+03, Test loss: 1.331e+04, MSE(e): 3.146e-04, MSE(pi1): 1.487e-02, MSE(pi2): 1.775e-04, MSE(pi3): 7.930e-04\n",
      "Epoch 79800, Train loss: 2.751e+03, Test loss: 1.366e+04, MSE(e): 2.411e-04, MSE(pi1): 2.578e-02, MSE(pi2): 1.288e-04, MSE(pi3): 8.157e-04\n",
      "Epoch 79900, Train loss: 3.967e+03, Test loss: 1.559e+04, MSE(e): 3.678e-04, MSE(pi1): 2.030e-02, MSE(pi2): 1.891e-04, MSE(pi3): 8.550e-04\n",
      "Epoch 80000, Train loss: 1.998e+03, Test loss: 1.352e+04, MSE(e): 1.707e-04, MSE(pi1): 1.986e-02, MSE(pi2): 9.671e-05, MSE(pi3): 9.253e-04\n",
      "Epoch 80100, Train loss: 3.613e+03, Test loss: 1.385e+04, MSE(e): 3.161e-04, MSE(pi1): 3.469e-02, MSE(pi2): 1.612e-04, MSE(pi3): 1.049e-03\n",
      "Epoch 80200, Train loss: 1.787e+03, Test loss: 1.323e+04, MSE(e): 1.521e-04, MSE(pi1): 1.535e-02, MSE(pi2): 9.311e-05, MSE(pi3): 1.122e-03\n",
      "Epoch 80300, Train loss: 3.028e+03, Test loss: 1.278e+04, MSE(e): 2.683e-04, MSE(pi1): 2.660e-02, MSE(pi2): 1.550e-04, MSE(pi3): 7.936e-04\n",
      "Epoch 80400, Train loss: 1.799e+03, Test loss: 1.419e+04, MSE(e): 1.331e-04, MSE(pi1): 3.769e-02, MSE(pi2): 7.940e-05, MSE(pi3): 9.105e-04\n",
      "Epoch 80500, Train loss: 3.765e+03, Test loss: 1.221e+04, MSE(e): 3.539e-04, MSE(pi1): 1.393e-02, MSE(pi2): 1.794e-04, MSE(pi3): 8.666e-04\n",
      "Epoch 80600, Train loss: 2.138e+03, Test loss: 1.462e+04, MSE(e): 1.786e-04, MSE(pi1): 2.774e-02, MSE(pi2): 1.105e-04, MSE(pi3): 7.498e-04\n",
      "Epoch 80700, Train loss: 3.193e+03, Test loss: 1.524e+04, MSE(e): 2.551e-04, MSE(pi1): 5.115e-02, MSE(pi2): 1.332e-04, MSE(pi3): 1.305e-03\n",
      "Epoch 80800, Train loss: 3.148e+03, Test loss: 1.455e+04, MSE(e): 2.752e-04, MSE(pi1): 3.101e-02, MSE(pi2): 1.591e-04, MSE(pi3): 8.647e-04\n",
      "Epoch 80900, Train loss: 1.500e+03, Test loss: 1.377e+04, MSE(e): 1.169e-04, MSE(pi1): 2.244e-02, MSE(pi2): 7.690e-05, MSE(pi3): 1.065e-03\n",
      "Epoch 81000, Train loss: 1.413e+03, Test loss: 1.309e+04, MSE(e): 1.148e-04, MSE(pi1): 1.878e-02, MSE(pi2): 7.772e-05, MSE(pi3): 7.778e-04\n",
      "Epoch 81100, Train loss: 3.073e+03, Test loss: 1.761e+04, MSE(e): 2.678e-04, MSE(pi1): 2.787e-02, MSE(pi2): 1.363e-04, MSE(pi3): 1.162e-03\n",
      "Epoch 81200, Train loss: 3.111e+03, Test loss: 1.554e+04, MSE(e): 2.728e-04, MSE(pi1): 2.902e-02, MSE(pi2): 1.387e-04, MSE(pi3): 9.304e-04\n",
      "Epoch 81300, Train loss: 2.926e+03, Test loss: 1.470e+04, MSE(e): 2.498e-04, MSE(pi1): 3.410e-02, MSE(pi2): 1.355e-04, MSE(pi3): 8.713e-04\n",
      "Epoch 81400, Train loss: 2.665e+03, Test loss: 1.561e+04, MSE(e): 2.017e-04, MSE(pi1): 4.383e-02, MSE(pi2): 1.134e-04, MSE(pi3): 2.098e-03\n",
      "Epoch 81500, Train loss: 1.381e+03, Test loss: 1.303e+04, MSE(e): 1.163e-04, MSE(pi1): 1.173e-02, MSE(pi2): 7.704e-05, MSE(pi3): 1.006e-03\n",
      "Epoch 81600, Train loss: 2.174e+03, Test loss: 1.674e+04, MSE(e): 1.906e-04, MSE(pi1): 1.870e-02, MSE(pi2): 1.064e-04, MSE(pi3): 8.052e-04\n",
      "Epoch 81700, Train loss: 2.435e+03, Test loss: 1.589e+04, MSE(e): 1.893e-04, MSE(pi1): 4.334e-02, MSE(pi2): 1.031e-04, MSE(pi3): 1.088e-03\n",
      "Epoch 81800, Train loss: 3.436e+03, Test loss: 1.427e+04, MSE(e): 3.118e-04, MSE(pi1): 2.008e-02, MSE(pi2): 1.635e-04, MSE(pi3): 1.176e-03\n",
      "Epoch 81900, Train loss: 2.036e+03, Test loss: 1.619e+04, MSE(e): 1.520e-04, MSE(pi1): 3.638e-02, MSE(pi2): 8.929e-05, MSE(pi3): 1.527e-03\n",
      "Epoch 82000, Train loss: 1.877e+03, Test loss: 1.547e+04, MSE(e): 1.245e-04, MSE(pi1): 4.494e-02, MSE(pi2): 7.679e-05, MSE(pi3): 1.826e-03\n",
      "Epoch 82100, Train loss: 3.308e+03, Test loss: 1.607e+04, MSE(e): 2.936e-04, MSE(pi1): 2.653e-02, MSE(pi2): 1.597e-04, MSE(pi3): 1.067e-03\n",
      "Epoch 82200, Train loss: 2.889e+03, Test loss: 1.578e+04, MSE(e): 2.399e-04, MSE(pi1): 4.148e-02, MSE(pi2): 1.206e-04, MSE(pi3): 7.519e-04\n",
      "Epoch 82300, Train loss: 4.171e+03, Test loss: 1.591e+04, MSE(e): 3.816e-04, MSE(pi1): 2.405e-02, MSE(pi2): 1.784e-04, MSE(pi3): 1.146e-03\n",
      "Epoch 82400, Train loss: 1.950e+03, Test loss: 1.649e+04, MSE(e): 1.375e-04, MSE(pi1): 4.759e-02, MSE(pi2): 8.526e-05, MSE(pi3): 9.953e-04\n",
      "Epoch 82500, Train loss: 1.704e+03, Test loss: 1.375e+04, MSE(e): 1.354e-04, MSE(pi1): 2.647e-02, MSE(pi2): 8.300e-05, MSE(pi3): 8.509e-04\n",
      "Epoch 82600, Train loss: 3.198e+03, Test loss: 1.913e+04, MSE(e): 2.867e-04, MSE(pi1): 2.380e-02, MSE(pi2): 1.447e-04, MSE(pi3): 9.360e-04\n",
      "Epoch 82700, Train loss: 1.931e+03, Test loss: 1.584e+04, MSE(e): 1.441e-04, MSE(pi1): 4.063e-02, MSE(pi2): 8.331e-05, MSE(pi3): 8.407e-04\n",
      "Epoch 82800, Train loss: 4.301e+03, Test loss: 2.024e+04, MSE(e): 3.725e-04, MSE(pi1): 4.067e-02, MSE(pi2): 1.800e-04, MSE(pi3): 1.688e-03\n",
      "Epoch 82900, Train loss: 2.414e+03, Test loss: 1.616e+04, MSE(e): 2.199e-04, MSE(pi1): 1.274e-02, MSE(pi2): 1.176e-04, MSE(pi3): 8.781e-04\n",
      "Epoch 83000, Train loss: 4.547e+03, Test loss: 1.747e+04, MSE(e): 4.158e-04, MSE(pi1): 2.620e-02, MSE(pi2): 1.967e-04, MSE(pi3): 1.263e-03\n",
      "Epoch 83100, Train loss: 3.184e+03, Test loss: 1.711e+04, MSE(e): 2.483e-04, MSE(pi1): 5.327e-02, MSE(pi2): 1.337e-04, MSE(pi3): 1.681e-03\n",
      "Epoch 83200, Train loss: 3.356e+03, Test loss: 1.540e+04, MSE(e): 2.926e-04, MSE(pi1): 3.491e-02, MSE(pi2): 1.536e-04, MSE(pi3): 8.063e-04\n",
      "Epoch 83300, Train loss: 2.333e+03, Test loss: 1.714e+04, MSE(e): 1.737e-04, MSE(pi1): 4.626e-02, MSE(pi2): 1.008e-04, MSE(pi3): 1.332e-03\n",
      "Epoch 83400, Train loss: 3.639e+03, Test loss: 1.798e+04, MSE(e): 3.367e-04, MSE(pi1): 2.014e-02, MSE(pi2): 1.690e-04, MSE(pi3): 6.991e-04\n",
      "Epoch 83500, Train loss: 2.329e+03, Test loss: 1.653e+04, MSE(e): 1.881e-04, MSE(pi1): 3.327e-02, MSE(pi2): 1.058e-04, MSE(pi3): 1.152e-03\n",
      "Epoch 83600, Train loss: 2.951e+03, Test loss: 1.869e+04, MSE(e): 2.608e-04, MSE(pi1): 2.668e-02, MSE(pi2): 1.365e-04, MSE(pi3): 7.620e-04\n",
      "Epoch 83700, Train loss: 1.904e+03, Test loss: 1.474e+04, MSE(e): 1.605e-04, MSE(pi1): 1.781e-02, MSE(pi2): 9.740e-05, MSE(pi3): 1.208e-03\n",
      "Epoch 83800, Train loss: 1.647e+03, Test loss: 1.669e+04, MSE(e): 1.137e-04, MSE(pi1): 4.329e-02, MSE(pi2): 7.158e-05, MSE(pi3): 7.645e-04\n",
      "Epoch 83900, Train loss: 4.754e+03, Test loss: 1.718e+04, MSE(e): 4.417e-04, MSE(pi1): 2.007e-02, MSE(pi2): 2.049e-04, MSE(pi3): 1.360e-03\n",
      "Epoch 84000, Train loss: 1.638e+03, Test loss: 1.612e+04, MSE(e): 1.444e-04, MSE(pi1): 1.116e-02, MSE(pi2): 8.676e-05, MSE(pi3): 8.181e-04\n",
      "Epoch 84100, Train loss: 3.533e+03, Test loss: 1.869e+04, MSE(e): 2.870e-04, MSE(pi1): 4.986e-02, MSE(pi2): 1.393e-04, MSE(pi3): 1.645e-03\n",
      "Epoch 84200, Train loss: 2.839e+03, Test loss: 1.493e+04, MSE(e): 2.606e-04, MSE(pi1): 1.412e-02, MSE(pi2): 1.361e-04, MSE(pi3): 9.097e-04\n",
      "Epoch 84300, Train loss: 1.682e+03, Test loss: 1.709e+04, MSE(e): 1.302e-04, MSE(pi1): 2.226e-02, MSE(pi2): 8.211e-05, MSE(pi3): 1.569e-03\n",
      "Epoch 84400, Train loss: 4.502e+03, Test loss: 1.872e+04, MSE(e): 3.914e-04, MSE(pi1): 4.917e-02, MSE(pi2): 1.987e-04, MSE(pi3): 9.673e-04\n",
      "Epoch 84500, Train loss: 1.660e+03, Test loss: 1.675e+04, MSE(e): 1.202e-04, MSE(pi1): 3.750e-02, MSE(pi2): 7.577e-05, MSE(pi3): 8.299e-04\n",
      "Epoch 84600, Train loss: 2.063e+03, Test loss: 1.835e+04, MSE(e): 1.608e-04, MSE(pi1): 3.504e-02, MSE(pi2): 9.154e-05, MSE(pi3): 1.043e-03\n",
      "Epoch 84700, Train loss: 4.591e+03, Test loss: 1.773e+04, MSE(e): 4.258e-04, MSE(pi1): 2.465e-02, MSE(pi2): 2.136e-04, MSE(pi3): 8.605e-04\n",
      "Epoch 84800, Train loss: 1.643e+03, Test loss: 1.684e+04, MSE(e): 1.244e-04, MSE(pi1): 2.752e-02, MSE(pi2): 7.909e-05, MSE(pi3): 1.243e-03\n",
      "Epoch 84900, Train loss: 1.357e+03, Test loss: 1.684e+04, MSE(e): 1.148e-04, MSE(pi1): 1.052e-02, MSE(pi2): 7.366e-05, MSE(pi3): 1.038e-03\n",
      "Epoch 85000, Train loss: 3.877e+03, Test loss: 1.737e+04, MSE(e): 3.583e-04, MSE(pi1): 1.983e-02, MSE(pi2): 1.745e-04, MSE(pi3): 9.628e-04\n",
      "Epoch 85100, Train loss: 1.376e+03, Test loss: 1.603e+04, MSE(e): 1.135e-04, MSE(pi1): 1.529e-02, MSE(pi2): 7.531e-05, MSE(pi3): 8.872e-04\n",
      "Epoch 85200, Train loss: 1.813e+03, Test loss: 1.716e+04, MSE(e): 1.517e-04, MSE(pi1): 2.165e-02, MSE(pi2): 8.970e-05, MSE(pi3): 8.010e-04\n",
      "Epoch 85300, Train loss: 4.278e+03, Test loss: 2.150e+04, MSE(e): 3.880e-04, MSE(pi1): 3.170e-02, MSE(pi2): 1.744e-04, MSE(pi3): 8.095e-04\n",
      "Epoch 85400, Train loss: 4.726e+03, Test loss: 1.766e+04, MSE(e): 4.164e-04, MSE(pi1): 4.571e-02, MSE(pi2): 2.239e-04, MSE(pi3): 1.045e-03\n",
      "Epoch 85500, Train loss: 1.966e+03, Test loss: 1.545e+04, MSE(e): 1.750e-04, MSE(pi1): 1.324e-02, MSE(pi2): 9.957e-05, MSE(pi3): 8.317e-04\n",
      "Epoch 85600, Train loss: 2.309e+03, Test loss: 1.792e+04, MSE(e): 2.014e-04, MSE(pi1): 2.204e-02, MSE(pi2): 1.232e-04, MSE(pi3): 7.477e-04\n",
      "Epoch 85700, Train loss: 1.417e+03, Test loss: 1.724e+04, MSE(e): 1.030e-04, MSE(pi1): 3.127e-02, MSE(pi2): 7.089e-05, MSE(pi3): 7.491e-04\n",
      "Epoch 85800, Train loss: 3.523e+03, Test loss: 2.088e+04, MSE(e): 3.189e-04, MSE(pi1): 2.343e-02, MSE(pi2): 1.546e-04, MSE(pi3): 9.955e-04\n",
      "Epoch 85900, Train loss: 3.399e+03, Test loss: 1.718e+04, MSE(e): 3.036e-04, MSE(pi1): 2.887e-02, MSE(pi2): 1.565e-04, MSE(pi3): 7.439e-04\n",
      "Epoch 86000, Train loss: 2.128e+03, Test loss: 1.705e+04, MSE(e): 1.387e-04, MSE(pi1): 5.714e-02, MSE(pi2): 8.125e-05, MSE(pi3): 1.686e-03\n",
      "Epoch 86100, Train loss: 2.081e+03, Test loss: 1.699e+04, MSE(e): 1.490e-04, MSE(pi1): 5.018e-02, MSE(pi2): 8.653e-05, MSE(pi3): 8.949e-04\n",
      "Epoch 86200, Train loss: 1.994e+03, Test loss: 1.750e+04, MSE(e): 1.488e-04, MSE(pi1): 4.194e-02, MSE(pi2): 8.785e-05, MSE(pi3): 8.650e-04\n",
      "Epoch 86300, Train loss: 3.329e+03, Test loss: 1.955e+04, MSE(e): 3.048e-04, MSE(pi1): 2.062e-02, MSE(pi2): 1.528e-04, MSE(pi3): 7.397e-04\n",
      "Epoch 86400, Train loss: 3.297e+03, Test loss: 1.651e+04, MSE(e): 3.075e-04, MSE(pi1): 1.296e-02, MSE(pi2): 1.563e-04, MSE(pi3): 9.229e-04\n",
      "Epoch 86500, Train loss: 1.765e+03, Test loss: 1.611e+04, MSE(e): 1.537e-04, MSE(pi1): 1.252e-02, MSE(pi2): 9.108e-05, MSE(pi3): 1.024e-03\n",
      "Epoch 86600, Train loss: 2.237e+03, Test loss: 1.697e+04, MSE(e): 1.585e-04, MSE(pi1): 5.243e-02, MSE(pi2): 9.619e-05, MSE(pi3): 1.275e-03\n",
      "Epoch 86700, Train loss: 1.685e+03, Test loss: 1.648e+04, MSE(e): 1.399e-04, MSE(pi1): 1.948e-02, MSE(pi2): 8.816e-05, MSE(pi3): 9.072e-04\n",
      "Epoch 86800, Train loss: 1.963e+03, Test loss: 1.785e+04, MSE(e): 1.436e-04, MSE(pi1): 4.241e-02, MSE(pi2): 8.693e-05, MSE(pi3): 1.027e-03\n",
      "Epoch 86900, Train loss: 4.116e+03, Test loss: 2.011e+04, MSE(e): 3.348e-04, MSE(pi1): 5.910e-02, MSE(pi2): 1.555e-04, MSE(pi3): 1.766e-03\n",
      "Epoch 87000, Train loss: 2.985e+03, Test loss: 1.725e+04, MSE(e): 2.761e-04, MSE(pi1): 1.511e-02, MSE(pi2): 1.477e-04, MSE(pi3): 7.351e-04\n",
      "Epoch 87100, Train loss: 2.302e+03, Test loss: 1.564e+04, MSE(e): 2.049e-04, MSE(pi1): 1.633e-02, MSE(pi2): 1.105e-04, MSE(pi3): 8.943e-04\n",
      "Epoch 87200, Train loss: 1.696e+03, Test loss: 1.695e+04, MSE(e): 1.416e-04, MSE(pi1): 1.733e-02, MSE(pi2): 9.075e-05, MSE(pi3): 1.062e-03\n",
      "Epoch 87300, Train loss: 2.250e+03, Test loss: 1.790e+04, MSE(e): 1.527e-04, MSE(pi1): 5.493e-02, MSE(pi2): 8.999e-05, MSE(pi3): 1.742e-03\n",
      "Epoch 87400, Train loss: 1.921e+03, Test loss: 1.726e+04, MSE(e): 1.283e-04, MSE(pi1): 4.878e-02, MSE(pi2): 7.842e-05, MSE(pi3): 1.497e-03\n",
      "Epoch 87500, Train loss: 2.081e+03, Test loss: 1.817e+04, MSE(e): 1.295e-04, MSE(pi1): 5.768e-02, MSE(pi2): 7.878e-05, MSE(pi3): 2.094e-03\n",
      "Epoch 87600, Train loss: 1.418e+03, Test loss: 1.726e+04, MSE(e): 1.065e-04, MSE(pi1): 2.469e-02, MSE(pi2): 7.280e-05, MSE(pi3): 1.063e-03\n",
      "Epoch 87700, Train loss: 2.328e+03, Test loss: 1.853e+04, MSE(e): 2.016e-04, MSE(pi1): 2.100e-02, MSE(pi2): 1.080e-04, MSE(pi3): 1.019e-03\n",
      "Epoch 87800, Train loss: 3.626e+03, Test loss: 2.089e+04, MSE(e): 2.953e-04, MSE(pi1): 5.534e-02, MSE(pi2): 1.372e-04, MSE(pi3): 1.195e-03\n",
      "Epoch 87900, Train loss: 2.924e+03, Test loss: 1.801e+04, MSE(e): 2.681e-04, MSE(pi1): 1.711e-02, MSE(pi2): 1.422e-04, MSE(pi3): 7.091e-04\n",
      "Epoch 88000, Train loss: 3.256e+03, Test loss: 1.834e+04, MSE(e): 2.767e-04, MSE(pi1): 4.048e-02, MSE(pi2): 1.422e-04, MSE(pi3): 8.346e-04\n",
      "Epoch 88100, Train loss: 3.371e+03, Test loss: 1.672e+04, MSE(e): 2.898e-04, MSE(pi1): 3.999e-02, MSE(pi2): 1.495e-04, MSE(pi3): 7.226e-04\n",
      "Epoch 88200, Train loss: 2.613e+03, Test loss: 1.652e+04, MSE(e): 2.161e-04, MSE(pi1): 3.642e-02, MSE(pi2): 1.185e-04, MSE(pi3): 8.824e-04\n",
      "Epoch 88300, Train loss: 3.224e+03, Test loss: 1.635e+04, MSE(e): 2.714e-04, MSE(pi1): 4.430e-02, MSE(pi2): 1.479e-04, MSE(pi3): 6.744e-04\n",
      "Epoch 88400, Train loss: 1.978e+03, Test loss: 1.629e+04, MSE(e): 1.564e-04, MSE(pi1): 3.192e-02, MSE(pi2): 9.071e-05, MSE(pi3): 9.519e-04\n",
      "Epoch 88500, Train loss: 1.903e+03, Test loss: 1.660e+04, MSE(e): 1.478e-04, MSE(pi1): 3.565e-02, MSE(pi2): 9.207e-05, MSE(pi3): 6.868e-04\n",
      "Epoch 88600, Train loss: 2.173e+03, Test loss: 1.680e+04, MSE(e): 1.692e-04, MSE(pi1): 3.564e-02, MSE(pi2): 1.008e-04, MSE(pi3): 1.238e-03\n",
      "Epoch 88700, Train loss: 1.892e+03, Test loss: 1.731e+04, MSE(e): 1.478e-04, MSE(pi1): 3.434e-02, MSE(pi2): 9.620e-05, MSE(pi3): 7.103e-04\n",
      "Epoch 88800, Train loss: 1.767e+03, Test loss: 1.770e+04, MSE(e): 1.433e-04, MSE(pi1): 2.609e-02, MSE(pi2): 9.595e-05, MSE(pi3): 7.304e-04\n",
      "Epoch 88900, Train loss: 2.005e+03, Test loss: 1.704e+04, MSE(e): 1.456e-04, MSE(pi1): 4.758e-02, MSE(pi2): 9.232e-05, MSE(pi3): 7.351e-04\n",
      "Epoch 89000, Train loss: 1.771e+03, Test loss: 1.770e+04, MSE(e): 1.612e-04, MSE(pi1): 9.479e-03, MSE(pi2): 9.934e-05, MSE(pi3): 6.417e-04\n",
      "Epoch 89100, Train loss: 2.132e+03, Test loss: 1.768e+04, MSE(e): 1.695e-04, MSE(pi1): 3.122e-02, MSE(pi2): 1.016e-04, MSE(pi3): 1.241e-03\n",
      "Epoch 89200, Train loss: 2.205e+03, Test loss: 1.799e+04, MSE(e): 1.784e-04, MSE(pi1): 2.910e-02, MSE(pi2): 1.134e-04, MSE(pi3): 1.291e-03\n",
      "Epoch 89300, Train loss: 1.649e+03, Test loss: 1.700e+04, MSE(e): 1.341e-04, MSE(pi1): 2.263e-02, MSE(pi2): 8.608e-05, MSE(pi3): 8.125e-04\n",
      "Epoch 89400, Train loss: 2.200e+03, Test loss: 1.981e+04, MSE(e): 1.552e-04, MSE(pi1): 4.048e-02, MSE(pi2): 9.283e-05, MSE(pi3): 2.436e-03\n",
      "Epoch 89500, Train loss: 1.943e+03, Test loss: 1.714e+04, MSE(e): 1.753e-04, MSE(pi1): 1.216e-02, MSE(pi2): 1.186e-04, MSE(pi3): 6.872e-04\n",
      "Epoch 89600, Train loss: 1.998e+03, Test loss: 1.721e+04, MSE(e): 1.703e-04, MSE(pi1): 2.263e-02, MSE(pi2): 1.090e-04, MSE(pi3): 6.845e-04\n",
      "Epoch 89700, Train loss: 1.676e+03, Test loss: 1.776e+04, MSE(e): 1.395e-04, MSE(pi1): 1.857e-02, MSE(pi2): 9.178e-05, MSE(pi3): 9.532e-04\n",
      "Epoch 89800, Train loss: 1.584e+03, Test loss: 1.709e+04, MSE(e): 1.370e-04, MSE(pi1): 1.367e-02, MSE(pi2): 8.853e-05, MSE(pi3): 7.662e-04\n",
      "Epoch 89900, Train loss: 2.054e+03, Test loss: 1.826e+04, MSE(e): 1.536e-04, MSE(pi1): 3.620e-02, MSE(pi2): 9.734e-05, MSE(pi3): 1.564e-03\n",
      "Epoch 90000, Train loss: 1.896e+03, Test loss: 1.760e+04, MSE(e): 1.465e-04, MSE(pi1): 3.386e-02, MSE(pi2): 9.059e-05, MSE(pi3): 9.274e-04\n",
      "Epoch 90100, Train loss: 1.990e+03, Test loss: 1.733e+04, MSE(e): 1.310e-04, MSE(pi1): 5.006e-02, MSE(pi2): 7.873e-05, MSE(pi3): 1.793e-03\n",
      "Epoch 90200, Train loss: 1.814e+03, Test loss: 1.682e+04, MSE(e): 1.287e-04, MSE(pi1): 3.883e-02, MSE(pi2): 7.763e-05, MSE(pi3): 1.390e-03\n",
      "Epoch 90300, Train loss: 1.665e+03, Test loss: 1.570e+04, MSE(e): 1.457e-04, MSE(pi1): 1.403e-02, MSE(pi2): 9.267e-05, MSE(pi3): 6.743e-04\n",
      "Epoch 90400, Train loss: 2.305e+03, Test loss: 1.571e+04, MSE(e): 1.964e-04, MSE(pi1): 2.764e-02, MSE(pi2): 1.134e-04, MSE(pi3): 6.492e-04\n",
      "Epoch 90500, Train loss: 4.333e+03, Test loss: 1.642e+04, MSE(e): 4.024e-04, MSE(pi1): 2.372e-02, MSE(pi2): 2.049e-04, MSE(pi3): 7.121e-04\n",
      "Epoch 90600, Train loss: 3.343e+03, Test loss: 1.611e+04, MSE(e): 3.065e-04, MSE(pi1): 1.803e-02, MSE(pi2): 1.514e-04, MSE(pi3): 9.689e-04\n",
      "Epoch 90700, Train loss: 3.496e+03, Test loss: 1.645e+04, MSE(e): 3.294e-04, MSE(pi1): 1.240e-02, MSE(pi2): 1.609e-04, MSE(pi3): 7.781e-04\n",
      "Epoch 90800, Train loss: 4.269e+03, Test loss: 1.824e+04, MSE(e): 3.533e-04, MSE(pi1): 5.667e-02, MSE(pi2): 1.643e-04, MSE(pi3): 1.692e-03\n",
      "Epoch 90900, Train loss: 3.598e+03, Test loss: 1.707e+04, MSE(e): 3.022e-04, MSE(pi1): 5.013e-02, MSE(pi2): 1.587e-04, MSE(pi3): 7.424e-04\n",
      "Epoch 91000, Train loss: 3.479e+03, Test loss: 1.823e+04, MSE(e): 3.190e-04, MSE(pi1): 2.071e-02, MSE(pi2): 1.636e-04, MSE(pi3): 8.241e-04\n",
      "Epoch 91100, Train loss: 3.840e+03, Test loss: 1.868e+04, MSE(e): 3.210e-04, MSE(pi1): 4.697e-02, MSE(pi2): 1.521e-04, MSE(pi3): 1.597e-03\n",
      "Epoch 91200, Train loss: 3.776e+03, Test loss: 2.000e+04, MSE(e): 3.562e-04, MSE(pi1): 1.367e-02, MSE(pi2): 1.676e-04, MSE(pi3): 7.674e-04\n",
      "Epoch 91300, Train loss: 2.836e+03, Test loss: 1.812e+04, MSE(e): 2.618e-04, MSE(pi1): 1.368e-02, MSE(pi2): 1.318e-04, MSE(pi3): 8.106e-04\n",
      "Epoch 91400, Train loss: 2.219e+03, Test loss: 1.660e+04, MSE(e): 2.025e-04, MSE(pi1): 1.242e-02, MSE(pi2): 1.067e-04, MSE(pi3): 7.020e-04\n",
      "Epoch 91500, Train loss: 1.865e+03, Test loss: 1.755e+04, MSE(e): 1.455e-04, MSE(pi1): 2.482e-02, MSE(pi2): 8.187e-05, MSE(pi3): 1.626e-03\n",
      "Epoch 91600, Train loss: 1.820e+03, Test loss: 1.664e+04, MSE(e): 1.312e-04, MSE(pi1): 3.638e-02, MSE(pi2): 8.160e-05, MSE(pi3): 1.440e-03\n",
      "Epoch 91700, Train loss: 1.830e+03, Test loss: 1.646e+04, MSE(e): 1.681e-04, MSE(pi1): 6.795e-03, MSE(pi2): 1.041e-04, MSE(pi3): 8.185e-04\n",
      "Epoch 91800, Train loss: 1.679e+03, Test loss: 1.641e+04, MSE(e): 1.323e-04, MSE(pi1): 2.906e-02, MSE(pi2): 8.283e-05, MSE(pi3): 6.473e-04\n",
      "Epoch 91900, Train loss: 2.002e+03, Test loss: 1.685e+04, MSE(e): 1.619e-04, MSE(pi1): 2.909e-02, MSE(pi2): 1.047e-04, MSE(pi3): 9.167e-04\n",
      "Epoch 92000, Train loss: 1.783e+03, Test loss: 1.647e+04, MSE(e): 1.312e-04, MSE(pi1): 4.062e-02, MSE(pi2): 8.545e-05, MSE(pi3): 6.519e-04\n",
      "Epoch 92100, Train loss: 1.974e+03, Test loss: 1.574e+04, MSE(e): 1.702e-04, MSE(pi1): 1.687e-02, MSE(pi2): 1.014e-04, MSE(pi3): 1.037e-03\n",
      "Epoch 92200, Train loss: 2.054e+03, Test loss: 1.525e+04, MSE(e): 1.717e-04, MSE(pi1): 2.592e-02, MSE(pi2): 9.949e-05, MSE(pi3): 7.766e-04\n",
      "Epoch 92300, Train loss: 3.164e+03, Test loss: 1.762e+04, MSE(e): 2.825e-04, MSE(pi1): 2.255e-02, MSE(pi2): 1.509e-04, MSE(pi3): 1.135e-03\n",
      "Epoch 92400, Train loss: 3.612e+03, Test loss: 1.727e+04, MSE(e): 3.140e-04, MSE(pi1): 3.600e-02, MSE(pi2): 1.522e-04, MSE(pi3): 1.123e-03\n",
      "Epoch 92500, Train loss: 3.436e+03, Test loss: 1.785e+04, MSE(e): 3.241e-04, MSE(pi1): 1.183e-02, MSE(pi2): 1.648e-04, MSE(pi3): 7.597e-04\n",
      "Epoch 92600, Train loss: 2.820e+03, Test loss: 1.922e+04, MSE(e): 2.415e-04, MSE(pi1): 2.869e-02, MSE(pi2): 1.157e-04, MSE(pi3): 1.186e-03\n",
      "Epoch 92700, Train loss: 1.370e+03, Test loss: 1.564e+04, MSE(e): 1.143e-04, MSE(pi1): 1.300e-02, MSE(pi2): 7.907e-05, MSE(pi3): 9.722e-04\n",
      "Epoch 92800, Train loss: 2.113e+03, Test loss: 1.707e+04, MSE(e): 1.851e-04, MSE(pi1): 1.485e-02, MSE(pi2): 1.116e-04, MSE(pi3): 1.139e-03\n",
      "Epoch 92900, Train loss: 1.948e+03, Test loss: 1.668e+04, MSE(e): 1.596e-04, MSE(pi1): 2.739e-02, MSE(pi2): 9.939e-05, MSE(pi3): 7.730e-04\n",
      "Epoch 93000, Train loss: 1.731e+03, Test loss: 1.535e+04, MSE(e): 1.351e-04, MSE(pi1): 2.801e-02, MSE(pi2): 8.458e-05, MSE(pi3): 9.896e-04\n",
      "Epoch 93100, Train loss: 1.999e+03, Test loss: 1.508e+04, MSE(e): 1.784e-04, MSE(pi1): 1.519e-02, MSE(pi2): 1.054e-04, MSE(pi3): 6.233e-04\n",
      "Epoch 93200, Train loss: 2.955e+03, Test loss: 1.574e+04, MSE(e): 2.813e-04, MSE(pi1): 6.970e-03, MSE(pi2): 1.488e-04, MSE(pi3): 7.291e-04\n",
      "Epoch 93300, Train loss: 4.684e+03, Test loss: 1.773e+04, MSE(e): 4.378e-04, MSE(pi1): 1.975e-02, MSE(pi2): 1.994e-04, MSE(pi3): 1.084e-03\n",
      "Epoch 93400, Train loss: 1.836e+03, Test loss: 1.539e+04, MSE(e): 1.597e-04, MSE(pi1): 1.552e-02, MSE(pi2): 9.134e-05, MSE(pi3): 8.366e-04\n",
      "Epoch 93500, Train loss: 1.871e+03, Test loss: 1.605e+04, MSE(e): 1.365e-04, MSE(pi1): 3.574e-02, MSE(pi2): 8.758e-05, MSE(pi3): 1.479e-03\n",
      "Epoch 93600, Train loss: 2.352e+03, Test loss: 1.746e+04, MSE(e): 2.085e-04, MSE(pi1): 1.752e-02, MSE(pi2): 1.165e-04, MSE(pi3): 9.232e-04\n",
      "Epoch 93700, Train loss: 1.519e+03, Test loss: 1.562e+04, MSE(e): 1.258e-04, MSE(pi1): 1.755e-02, MSE(pi2): 8.264e-05, MSE(pi3): 8.477e-04\n",
      "Epoch 93800, Train loss: 3.719e+03, Test loss: 1.536e+04, MSE(e): 3.510e-04, MSE(pi1): 1.241e-02, MSE(pi2): 1.772e-04, MSE(pi3): 8.452e-04\n",
      "Epoch 93900, Train loss: 3.708e+03, Test loss: 1.653e+04, MSE(e): 3.499e-04, MSE(pi1): 1.208e-02, MSE(pi2): 1.846e-04, MSE(pi3): 8.844e-04\n",
      "Epoch 94000, Train loss: 3.717e+03, Test loss: 1.841e+04, MSE(e): 3.325e-04, MSE(pi1): 3.250e-02, MSE(pi2): 1.613e-04, MSE(pi3): 6.712e-04\n",
      "Epoch 94100, Train loss: 2.016e+03, Test loss: 1.679e+04, MSE(e): 1.447e-04, MSE(pi1): 4.669e-02, MSE(pi2): 8.617e-05, MSE(pi3): 1.019e-03\n",
      "Epoch 94200, Train loss: 1.948e+03, Test loss: 1.609e+04, MSE(e): 1.646e-04, MSE(pi1): 1.945e-02, MSE(pi2): 1.001e-04, MSE(pi3): 1.074e-03\n",
      "Epoch 94300, Train loss: 1.693e+03, Test loss: 1.595e+04, MSE(e): 1.265e-04, MSE(pi1): 3.451e-02, MSE(pi2): 7.973e-05, MSE(pi3): 8.352e-04\n",
      "Epoch 94400, Train loss: 2.121e+03, Test loss: 1.499e+04, MSE(e): 1.731e-04, MSE(pi1): 3.125e-02, MSE(pi2): 1.034e-04, MSE(pi3): 7.671e-04\n",
      "Epoch 94500, Train loss: 3.135e+03, Test loss: 1.618e+04, MSE(e): 2.908e-04, MSE(pi1): 1.405e-02, MSE(pi2): 1.477e-04, MSE(pi3): 8.712e-04\n",
      "Epoch 94600, Train loss: 3.653e+03, Test loss: 1.825e+04, MSE(e): 3.312e-04, MSE(pi1): 2.686e-02, MSE(pi2): 1.685e-04, MSE(pi3): 7.173e-04\n",
      "Epoch 94700, Train loss: 2.073e+03, Test loss: 1.618e+04, MSE(e): 1.497e-04, MSE(pi1): 4.563e-02, MSE(pi2): 8.260e-05, MSE(pi3): 1.191e-03\n",
      "Epoch 94800, Train loss: 1.870e+03, Test loss: 1.606e+04, MSE(e): 1.609e-04, MSE(pi1): 1.885e-02, MSE(pi2): 9.443e-05, MSE(pi3): 7.217e-04\n",
      "Epoch 94900, Train loss: 1.554e+03, Test loss: 1.634e+04, MSE(e): 1.226e-04, MSE(pi1): 2.211e-02, MSE(pi2): 8.079e-05, MSE(pi3): 1.069e-03\n",
      "Epoch 95000, Train loss: 2.077e+03, Test loss: 1.533e+04, MSE(e): 1.819e-04, MSE(pi1): 1.538e-02, MSE(pi2): 1.030e-04, MSE(pi3): 1.042e-03\n",
      "Epoch 95100, Train loss: 3.910e+03, Test loss: 1.733e+04, MSE(e): 3.346e-04, MSE(pi1): 4.012e-02, MSE(pi2): 1.665e-04, MSE(pi3): 1.628e-03\n",
      "Epoch 95200, Train loss: 2.313e+03, Test loss: 1.471e+04, MSE(e): 1.959e-04, MSE(pi1): 2.406e-02, MSE(pi2): 1.023e-04, MSE(pi3): 1.125e-03\n",
      "Epoch 95300, Train loss: 2.177e+03, Test loss: 1.663e+04, MSE(e): 1.699e-04, MSE(pi1): 3.122e-02, MSE(pi2): 1.067e-04, MSE(pi3): 1.659e-03\n",
      "Epoch 95400, Train loss: 1.602e+03, Test loss: 1.470e+04, MSE(e): 1.307e-04, MSE(pi1): 1.744e-02, MSE(pi2): 8.233e-05, MSE(pi3): 1.210e-03\n",
      "Epoch 95500, Train loss: 3.285e+03, Test loss: 1.546e+04, MSE(e): 3.100e-04, MSE(pi1): 1.163e-02, MSE(pi2): 1.658e-04, MSE(pi3): 6.771e-04\n",
      "Epoch 95600, Train loss: 3.390e+03, Test loss: 1.588e+04, MSE(e): 3.152e-04, MSE(pi1): 1.701e-02, MSE(pi2): 1.519e-04, MSE(pi3): 6.826e-04\n",
      "Epoch 95700, Train loss: 1.891e+03, Test loss: 1.497e+04, MSE(e): 1.496e-04, MSE(pi1): 2.874e-02, MSE(pi2): 9.798e-05, MSE(pi3): 1.069e-03\n",
      "Epoch 95800, Train loss: 2.129e+03, Test loss: 1.657e+04, MSE(e): 1.565e-04, MSE(pi1): 4.090e-02, MSE(pi2): 9.322e-05, MSE(pi3): 1.541e-03\n",
      "Epoch 95900, Train loss: 2.518e+03, Test loss: 1.489e+04, MSE(e): 1.944e-04, MSE(pi1): 4.163e-02, MSE(pi2): 1.024e-04, MSE(pi3): 1.575e-03\n",
      "Epoch 96000, Train loss: 4.895e+03, Test loss: 1.778e+04, MSE(e): 4.298e-04, MSE(pi1): 5.197e-02, MSE(pi2): 2.105e-04, MSE(pi3): 7.671e-04\n",
      "Epoch 96100, Train loss: 2.298e+03, Test loss: 1.647e+04, MSE(e): 1.848e-04, MSE(pi1): 3.406e-02, MSE(pi2): 9.765e-05, MSE(pi3): 1.093e-03\n",
      "Epoch 96200, Train loss: 1.550e+03, Test loss: 1.461e+04, MSE(e): 1.383e-04, MSE(pi1): 9.216e-03, MSE(pi2): 9.078e-05, MSE(pi3): 7.411e-04\n",
      "Epoch 96300, Train loss: 1.700e+03, Test loss: 1.392e+04, MSE(e): 1.384e-04, MSE(pi1): 2.128e-02, MSE(pi2): 8.410e-05, MSE(pi3): 1.033e-03\n",
      "Epoch 96400, Train loss: 3.211e+03, Test loss: 1.482e+04, MSE(e): 2.945e-04, MSE(pi1): 1.638e-02, MSE(pi2): 1.592e-04, MSE(pi3): 1.021e-03\n",
      "Epoch 96500, Train loss: 3.723e+03, Test loss: 1.708e+04, MSE(e): 3.413e-04, MSE(pi1): 2.356e-02, MSE(pi2): 1.715e-04, MSE(pi3): 7.392e-04\n",
      "Epoch 96600, Train loss: 1.966e+03, Test loss: 1.485e+04, MSE(e): 1.464e-04, MSE(pi1): 4.242e-02, MSE(pi2): 8.957e-05, MSE(pi3): 7.826e-04\n",
      "Epoch 96700, Train loss: 1.610e+03, Test loss: 1.394e+04, MSE(e): 1.439e-04, MSE(pi1): 1.022e-02, MSE(pi2): 9.728e-05, MSE(pi3): 6.822e-04\n",
      "Epoch 96800, Train loss: 1.686e+03, Test loss: 1.381e+04, MSE(e): 1.515e-04, MSE(pi1): 9.292e-03, MSE(pi2): 9.279e-05, MSE(pi3): 7.798e-04\n",
      "Epoch 96900, Train loss: 3.359e+03, Test loss: 1.753e+04, MSE(e): 2.979e-04, MSE(pi1): 2.310e-02, MSE(pi2): 1.613e-04, MSE(pi3): 1.480e-03\n",
      "Epoch 97000, Train loss: 3.817e+03, Test loss: 1.699e+04, MSE(e): 3.517e-04, MSE(pi1): 2.315e-02, MSE(pi2): 1.657e-04, MSE(pi3): 6.923e-04\n",
      "Epoch 97100, Train loss: 1.380e+03, Test loss: 1.392e+04, MSE(e): 1.209e-04, MSE(pi1): 8.793e-03, MSE(pi2): 8.237e-05, MSE(pi3): 8.331e-04\n",
      "Epoch 97200, Train loss: 1.887e+03, Test loss: 1.554e+04, MSE(e): 1.459e-04, MSE(pi1): 2.848e-02, MSE(pi2): 8.905e-05, MSE(pi3): 1.427e-03\n",
      "Epoch 97300, Train loss: 1.854e+03, Test loss: 1.449e+04, MSE(e): 1.570e-04, MSE(pi1): 2.003e-02, MSE(pi2): 9.619e-05, MSE(pi3): 8.388e-04\n",
      "Epoch 97400, Train loss: 4.581e+03, Test loss: 1.591e+04, MSE(e): 4.302e-04, MSE(pi1): 1.994e-02, MSE(pi2): 2.042e-04, MSE(pi3): 7.941e-04\n",
      "Epoch 97500, Train loss: 4.264e+03, Test loss: 1.800e+04, MSE(e): 3.949e-04, MSE(pi1): 2.491e-02, MSE(pi2): 1.983e-04, MSE(pi3): 6.559e-04\n",
      "Epoch 97600, Train loss: 2.315e+03, Test loss: 1.514e+04, MSE(e): 1.899e-04, MSE(pi1): 3.412e-02, MSE(pi2): 1.073e-04, MSE(pi3): 7.440e-04\n",
      "Epoch 97700, Train loss: 1.716e+03, Test loss: 1.384e+04, MSE(e): 1.290e-04, MSE(pi1): 3.152e-02, MSE(pi2): 7.806e-05, MSE(pi3): 1.107e-03\n",
      "Epoch 97800, Train loss: 2.186e+03, Test loss: 1.531e+04, MSE(e): 1.709e-04, MSE(pi1): 4.124e-02, MSE(pi2): 1.040e-04, MSE(pi3): 6.438e-04\n",
      "Epoch 97900, Train loss: 1.478e+03, Test loss: 1.465e+04, MSE(e): 1.260e-04, MSE(pi1): 1.385e-02, MSE(pi2): 8.230e-05, MSE(pi3): 7.955e-04\n",
      "Epoch 98000, Train loss: 2.719e+03, Test loss: 1.447e+04, MSE(e): 2.077e-04, MSE(pi1): 4.787e-02, MSE(pi2): 1.065e-04, MSE(pi3): 1.633e-03\n",
      "Epoch 98100, Train loss: 3.794e+03, Test loss: 1.570e+04, MSE(e): 3.338e-04, MSE(pi1): 3.037e-02, MSE(pi2): 1.650e-04, MSE(pi3): 1.521e-03\n",
      "Epoch 98200, Train loss: 4.153e+03, Test loss: 1.642e+04, MSE(e): 3.958e-04, MSE(pi1): 1.320e-02, MSE(pi2): 1.934e-04, MSE(pi3): 6.311e-04\n",
      "Epoch 98300, Train loss: 2.807e+03, Test loss: 1.478e+04, MSE(e): 2.567e-04, MSE(pi1): 1.779e-02, MSE(pi2): 1.361e-04, MSE(pi3): 6.163e-04\n",
      "Epoch 98400, Train loss: 1.573e+03, Test loss: 1.352e+04, MSE(e): 1.359e-04, MSE(pi1): 1.362e-02, MSE(pi2): 8.137e-05, MSE(pi3): 7.740e-04\n",
      "Epoch 98500, Train loss: 1.701e+03, Test loss: 1.447e+04, MSE(e): 1.312e-04, MSE(pi1): 2.546e-02, MSE(pi2): 8.576e-05, MSE(pi3): 1.346e-03\n",
      "Epoch 98600, Train loss: 1.325e+03, Test loss: 1.385e+04, MSE(e): 1.127e-04, MSE(pi1): 1.174e-02, MSE(pi2): 7.354e-05, MSE(pi3): 8.104e-04\n",
      "Epoch 98700, Train loss: 1.854e+03, Test loss: 1.485e+04, MSE(e): 1.373e-04, MSE(pi1): 3.857e-02, MSE(pi2): 8.198e-05, MSE(pi3): 9.525e-04\n",
      "Epoch 98800, Train loss: 1.692e+03, Test loss: 1.449e+04, MSE(e): 1.419e-04, MSE(pi1): 1.714e-02, MSE(pi2): 8.985e-05, MSE(pi3): 1.006e-03\n",
      "Epoch 98900, Train loss: 1.756e+03, Test loss: 1.500e+04, MSE(e): 1.305e-04, MSE(pi1): 3.741e-02, MSE(pi2): 8.399e-05, MSE(pi3): 7.750e-04\n",
      "Epoch 99000, Train loss: 2.307e+03, Test loss: 1.408e+04, MSE(e): 2.113e-04, MSE(pi1): 1.184e-02, MSE(pi2): 1.195e-04, MSE(pi3): 7.620e-04\n",
      "Epoch 99100, Train loss: 4.321e+03, Test loss: 1.617e+04, MSE(e): 3.663e-04, MSE(pi1): 5.467e-02, MSE(pi2): 1.919e-04, MSE(pi3): 1.108e-03\n",
      "Epoch 99200, Train loss: 3.117e+03, Test loss: 1.448e+04, MSE(e): 2.547e-04, MSE(pi1): 4.195e-02, MSE(pi2): 1.319e-04, MSE(pi3): 1.502e-03\n",
      "Epoch 99300, Train loss: 3.557e+03, Test loss: 1.519e+04, MSE(e): 3.203e-04, MSE(pi1): 2.732e-02, MSE(pi2): 1.801e-04, MSE(pi3): 8.038e-04\n",
      "Epoch 99400, Train loss: 3.828e+03, Test loss: 1.643e+04, MSE(e): 3.583e-04, MSE(pi1): 1.701e-02, MSE(pi2): 1.779e-04, MSE(pi3): 7.460e-04\n",
      "Epoch 99500, Train loss: 4.682e+03, Test loss: 1.808e+04, MSE(e): 4.380e-04, MSE(pi1): 2.174e-02, MSE(pi2): 2.067e-04, MSE(pi3): 8.411e-04\n",
      "Epoch 99600, Train loss: 4.103e+03, Test loss: 1.638e+04, MSE(e): 3.779e-04, MSE(pi1): 2.255e-02, MSE(pi2): 1.779e-04, MSE(pi3): 9.850e-04\n",
      "Epoch 99700, Train loss: 5.217e+03, Test loss: 1.637e+04, MSE(e): 4.984e-04, MSE(pi1): 1.231e-02, MSE(pi2): 2.889e-04, MSE(pi3): 1.099e-03\n",
      "Epoch 99800, Train loss: 4.606e+03, Test loss: 1.777e+04, MSE(e): 4.265e-04, MSE(pi1): 2.716e-02, MSE(pi2): 2.004e-04, MSE(pi3): 6.981e-04\n",
      "Epoch 99900, Train loss: 3.378e+03, Test loss: 1.548e+04, MSE(e): 3.098e-04, MSE(pi1): 1.765e-02, MSE(pi2): 1.481e-04, MSE(pi3): 1.038e-03\n",
      "\n",
      "Training process finished after 100000 epochs\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Parametros de entrenamiento\n",
    "start_epoch = 9000\n",
    "n_epochs = 100000\n",
    "\n",
    "batch_size = 64\n",
    "n_checkpoints = 100\n",
    "\n",
    "second_lr = 1e-2\n",
    "\n",
    "train_loop(model, optimizer, X_train, y_train, f_train, X_test, y_test, f_test,\n",
    "           D,  n_checkpoints, start_epoch=start_epoch, n_epochs=n_epochs, batch_size=batch_size, \n",
    "           model_results_path=MODEL_RESULTS_PATH, device=DEVICE, new_lr=second_lr)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SciML_test_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
