{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import GPUtil\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Imports de la libreria propia\n",
    "from vecopsciml.kernels.derivative import DerivativeKernels\n",
    "from vecopsciml.utils import TensOps\n",
    "\n",
    "# Imports de las funciones creadas para este programa\n",
    "from utils.folders import create_folder\n",
    "from utils.load_data import load_data\n",
    "from trainers.train import train_loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Folder already exists at: /home/rmunoz/Escritorio/rmunozTMELab/Physically-Guided-Machine-Learning/results/non_linear\n",
      "Folder successfully created at: /home/rmunoz/Escritorio/rmunozTMELab/Physically-Guided-Machine-Learning/results/non_linear/model_autoencoder_AE\n",
      "Folder successfully created at: /home/rmunoz/Escritorio/rmunozTMELab/Physically-Guided-Machine-Learning/results/non_linear/model_autoencoder_NN\n"
     ]
    }
   ],
   "source": [
    "# Creamos los paths para las distintas carpetas\n",
    "ROOT_PATH = r'/home/rmunoz/Escritorio/rmunozTMELab/Physically-Guided-Machine-Learning'\n",
    "DATA_PATH = os.path.join(ROOT_PATH, r'data/non_linear/non_linear_10000.pkl')\n",
    "RESULTS_FOLDER_PATH = os.path.join(ROOT_PATH, r'results/non_linear')\n",
    "MODEL_RESULTS_AE_PATH = os.path.join(ROOT_PATH, r'results/non_linear/model_autoencoder_AE')\n",
    "MODEL_RESULTS_PGNNIV_PATH = os.path.join(ROOT_PATH, r'results/non_linear/model_autoencoder_NN')\n",
    "\n",
    "\n",
    "# Creamos las carpetas que sean necesarias (si ya están creadas se avisará de ello)\n",
    "create_folder(RESULTS_FOLDER_PATH)\n",
    "create_folder(MODEL_RESULTS_AE_PATH)\n",
    "create_folder(MODEL_RESULTS_PGNNIV_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully loaded from: /home/rmunoz/Escritorio/rmunozTMELab/Physically-Guided-Machine-Learning/data/non_linear/non_linear_10000.pkl\n"
     ]
    }
   ],
   "source": [
    "# Load dataset\n",
    "dataset = load_data(DATA_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convolutional filters to derivate\n",
    "dx = dataset['x_step_size']\n",
    "dy = dataset['y_step_size']\n",
    "D = DerivativeKernels(dx, dy, 0).grad_kernels_two_dimensions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "print(f\"Using device: {DEVICE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### División de los datos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset length: 8000\n",
      "Validation dataset length: 2000\n"
     ]
    }
   ],
   "source": [
    "X_train = torch.Tensor(dataset['X_train']).unsqueeze(1)\n",
    "y_train = torch.Tensor(dataset['y_train']).unsqueeze(1)\n",
    "K_train = torch.tensor(dataset['k_train']).unsqueeze(1)\n",
    "f_train = torch.tensor(dataset['f_train']).unsqueeze(1).to(torch.float32)\n",
    "\n",
    "X_val = torch.Tensor(dataset['X_val']).unsqueeze(1)\n",
    "y_val = TensOps(torch.Tensor(dataset['y_val']).unsqueeze(1).requires_grad_(True), space_dimension=2, contravariance=0, covariance=0)\n",
    "K_val = TensOps(torch.tensor(dataset['k_val']).unsqueeze(1).requires_grad_(True), space_dimension=2, contravariance=0, covariance=0)\n",
    "f_val = TensOps(torch.tensor(dataset['f_val']).to(torch.float32).unsqueeze(1).requires_grad_(True), space_dimension=2, contravariance=0, covariance=0)\n",
    "\n",
    "print(\"Train dataset length:\", len(X_train))\n",
    "print(\"Validation dataset length:\", len(X_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset length for the autoencoder: 4000\n",
      "Dataset length for the PGNNIV: 4000\n"
     ]
    }
   ],
   "source": [
    "N_data_AE = len(X_train)//2\n",
    "N_data_NN = len(X_train) - len(X_train)//2\n",
    "prop_data_NN = 1 - N_data_AE/(N_data_NN + N_data_AE)\n",
    "\n",
    "print(\"Dataset length for the autoencoder:\", N_data_AE)\n",
    "print(\"Dataset length for the PGNNIV:\", N_data_NN)\n",
    "\n",
    "X_AE, X_NN, y_AE, y_NN, K_AE, K_NN, f_AE, f_NN = train_test_split(X_train, y_train, K_train, f_train, test_size=prop_data_NN, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Datos para el autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_AE, y_test_AE = train_test_split(y_AE, test_size=0.2, random_state=42)\n",
    "\n",
    "y_train_AE = TensOps(y_train_AE.requires_grad_(True).to(DEVICE), space_dimension=2, contravariance=0, covariance=0)\n",
    "y_test_AE = TensOps(y_test_AE.requires_grad_(True).to(DEVICE), space_dimension=2, contravariance=0, covariance=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Datos para la PGNNIV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_NN, X_test_NN, y_train_NN, y_test_NN, K_train_NN, K_test_NN, f_train_NN, f_test_NN = train_test_split(X_NN, y_NN, K_NN, f_NN, test_size=0.2, random_state=42)\n",
    "\n",
    "X_train_NN = X_train_NN.to(DEVICE)\n",
    "X_test_NN = X_test_NN.to(DEVICE)\n",
    "\n",
    "y_train_NN = TensOps(y_train_NN.requires_grad_(True).to(DEVICE), space_dimension=2, contravariance=0, covariance=0)\n",
    "y_test_NN = TensOps(y_test_NN.requires_grad_(True).to(DEVICE), space_dimension=2, contravariance=0, covariance=0)\n",
    "\n",
    "K_train_NN = TensOps(K_train_NN.to(DEVICE), space_dimension=2, contravariance=0, covariance=0)\n",
    "K_test_NN = TensOps(K_test_NN.to(DEVICE), space_dimension=2, contravariance=0, covariance=0)\n",
    "\n",
    "f_train_NN = TensOps(f_train_NN.to(DEVICE), space_dimension=2, contravariance=0, covariance=0)\n",
    "f_test_NN = TensOps(f_test_NN.to(DEVICE), space_dimension=2, contravariance=0, covariance=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model import Autoencoder\n",
    "from trainers.train import train_autoencoder_loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "autoencoder_input_shape = y_train_AE.values[0].shape\n",
    "latent_space_dim = [15, 10, 3, 10, 15]\n",
    "autoencoder_output_shape = y_train_AE.values[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = y_train_AE.values\n",
    "y_train = y_train_AE\n",
    "\n",
    "X_test = y_test_AE.values\n",
    "y_test = y_test_AE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training from scratch.\n",
      "Epoch 0, Train loss: 4.323e+00, Test loss: 4.388e+00\n",
      "Epoch 100, Train loss: 5.006e-01, Test loss: 4.657e-01\n",
      "Epoch 200, Train loss: 2.312e-01, Test loss: 1.892e-01\n",
      "Epoch 300, Train loss: 1.186e-02, Test loss: 2.122e-02\n",
      "Epoch 400, Train loss: 3.329e-03, Test loss: 5.806e-03\n",
      "Epoch 500, Train loss: 5.577e-03, Test loss: 1.023e-02\n",
      "Epoch 600, Train loss: 2.464e-03, Test loss: 2.774e-03\n",
      "Epoch 700, Train loss: 2.872e-03, Test loss: 4.983e-03\n",
      "Epoch 800, Train loss: 1.241e-02, Test loss: 9.406e-03\n",
      "Epoch 900, Train loss: 7.351e-03, Test loss: 1.086e-02\n",
      "Epoch 1000, Train loss: 4.231e-03, Test loss: 9.199e-03\n",
      "Epoch 1100, Train loss: 5.478e-03, Test loss: 9.434e-03\n",
      "Epoch 1200, Train loss: 1.107e-03, Test loss: 2.805e-03\n",
      "Epoch 1300, Train loss: 1.897e-03, Test loss: 4.299e-03\n",
      "Epoch 1400, Train loss: 2.755e-03, Test loss: 4.882e-03\n",
      "Epoch 1500, Train loss: 5.208e-03, Test loss: 9.262e-03\n",
      "Epoch 1600, Train loss: 6.394e-03, Test loss: 9.990e-03\n",
      "Epoch 1700, Train loss: 5.777e-03, Test loss: 3.154e-03\n",
      "Epoch 1800, Train loss: 4.488e-03, Test loss: 8.405e-03\n",
      "Epoch 1900, Train loss: 1.274e-03, Test loss: 2.759e-03\n",
      "Epoch 2000, Train loss: 2.652e-03, Test loss: 5.753e-03\n",
      "Epoch 2100, Train loss: 2.040e-03, Test loss: 2.831e-03\n",
      "Epoch 2200, Train loss: 4.023e-03, Test loss: 6.853e-03\n",
      "Epoch 2300, Train loss: 1.717e-03, Test loss: 4.801e-03\n",
      "Epoch 2400, Train loss: 1.573e-03, Test loss: 3.107e-03\n",
      "Epoch 2500, Train loss: 1.811e-03, Test loss: 2.805e-03\n",
      "Epoch 2600, Train loss: 7.559e-03, Test loss: 1.305e-02\n",
      "Epoch 2700, Train loss: 1.933e-03, Test loss: 1.944e-03\n",
      "Epoch 2800, Train loss: 7.176e-04, Test loss: 2.056e-03\n",
      "Epoch 2900, Train loss: 7.202e-04, Test loss: 1.800e-03\n",
      "Epoch 3000, Train loss: 2.721e-03, Test loss: 4.652e-03\n",
      "Epoch 3100, Train loss: 7.602e-04, Test loss: 2.463e-03\n",
      "Epoch 3200, Train loss: 2.341e-03, Test loss: 2.268e-03\n",
      "Epoch 3300, Train loss: 4.987e-03, Test loss: 1.107e-02\n",
      "Epoch 3400, Train loss: 7.031e-04, Test loss: 1.876e-03\n",
      "Epoch 3500, Train loss: 7.061e-04, Test loss: 2.184e-03\n",
      "Epoch 3600, Train loss: 9.489e-04, Test loss: 2.244e-03\n",
      "Epoch 3700, Train loss: 1.643e-03, Test loss: 1.963e-03\n",
      "Epoch 3800, Train loss: 9.786e-04, Test loss: 1.937e-03\n",
      "Epoch 3900, Train loss: 1.387e-03, Test loss: 1.720e-03\n",
      "Epoch 4000, Train loss: 6.334e-04, Test loss: 2.264e-03\n",
      "Epoch 4100, Train loss: 2.163e-03, Test loss: 5.101e-03\n",
      "Epoch 4200, Train loss: 6.207e-04, Test loss: 2.579e-03\n",
      "Epoch 4300, Train loss: 1.298e-03, Test loss: 2.479e-03\n",
      "Epoch 4400, Train loss: 6.590e-04, Test loss: 2.306e-03\n",
      "Epoch 4500, Train loss: 6.134e-04, Test loss: 2.532e-03\n",
      "Epoch 4600, Train loss: 2.114e-03, Test loss: 3.713e-03\n",
      "Epoch 4700, Train loss: 2.612e-03, Test loss: 4.753e-03\n",
      "Epoch 4800, Train loss: 1.787e-03, Test loss: 2.588e-03\n",
      "Epoch 4900, Train loss: 1.479e-03, Test loss: 1.740e-03\n",
      "Epoch 5000, Train loss: 1.229e-03, Test loss: 3.378e-03\n",
      "Epoch 5100, Train loss: 1.015e-03, Test loss: 1.966e-03\n",
      "Epoch 5200, Train loss: 1.515e-03, Test loss: 1.999e-03\n",
      "Epoch 5300, Train loss: 6.132e-04, Test loss: 1.625e-03\n",
      "Epoch 5400, Train loss: 1.992e-03, Test loss: 4.052e-03\n",
      "Epoch 5500, Train loss: 8.728e-04, Test loss: 1.706e-03\n",
      "Epoch 5600, Train loss: 5.262e-04, Test loss: 1.514e-03\n",
      "Epoch 5700, Train loss: 2.361e-03, Test loss: 4.968e-03\n",
      "Epoch 5800, Train loss: 4.753e-03, Test loss: 7.789e-03\n",
      "Epoch 5900, Train loss: 3.594e-03, Test loss: 8.468e-03\n",
      "Epoch 6000, Train loss: 7.402e-04, Test loss: 1.888e-03\n",
      "Epoch 6100, Train loss: 1.157e-03, Test loss: 2.028e-03\n",
      "Epoch 6200, Train loss: 1.281e-03, Test loss: 2.264e-03\n",
      "Epoch 6300, Train loss: 7.076e-04, Test loss: 2.007e-03\n",
      "Epoch 6400, Train loss: 1.866e-03, Test loss: 3.941e-03\n",
      "Epoch 6500, Train loss: 4.396e-03, Test loss: 4.442e-03\n",
      "Epoch 6600, Train loss: 1.444e-03, Test loss: 3.451e-03\n",
      "Epoch 6700, Train loss: 6.227e-04, Test loss: 1.926e-03\n",
      "Epoch 6800, Train loss: 1.257e-03, Test loss: 1.821e-03\n",
      "Epoch 6900, Train loss: 1.309e-03, Test loss: 2.309e-03\n",
      "Epoch 7000, Train loss: 2.048e-03, Test loss: 2.119e-03\n",
      "Epoch 7100, Train loss: 6.572e-04, Test loss: 1.853e-03\n",
      "Epoch 7200, Train loss: 7.467e-04, Test loss: 1.830e-03\n",
      "Epoch 7300, Train loss: 1.128e-02, Test loss: 6.236e-03\n",
      "Epoch 7400, Train loss: 6.480e-04, Test loss: 1.988e-03\n",
      "Epoch 7500, Train loss: 1.374e-03, Test loss: 1.974e-03\n",
      "Epoch 7600, Train loss: 8.652e-04, Test loss: 2.216e-03\n",
      "Epoch 7700, Train loss: 1.911e-03, Test loss: 3.844e-03\n",
      "Epoch 7800, Train loss: 7.008e-04, Test loss: 2.617e-03\n",
      "Epoch 7900, Train loss: 1.757e-03, Test loss: 3.820e-03\n",
      "Epoch 8000, Train loss: 8.306e-04, Test loss: 2.154e-03\n",
      "Epoch 8100, Train loss: 4.753e-04, Test loss: 1.914e-03\n",
      "Epoch 8200, Train loss: 3.537e-03, Test loss: 4.134e-03\n",
      "Epoch 8300, Train loss: 2.301e-03, Test loss: 2.742e-03\n",
      "Epoch 8400, Train loss: 8.101e-04, Test loss: 2.322e-03\n",
      "Epoch 8500, Train loss: 1.244e-03, Test loss: 2.725e-03\n",
      "Epoch 8600, Train loss: 8.210e-04, Test loss: 2.252e-03\n",
      "Epoch 8700, Train loss: 1.593e-03, Test loss: 3.214e-03\n",
      "Epoch 8800, Train loss: 1.859e-03, Test loss: 2.533e-03\n",
      "Epoch 8900, Train loss: 7.805e-04, Test loss: 1.760e-03\n",
      "Epoch 9000, Train loss: 1.254e-02, Test loss: 5.330e-03\n",
      "Epoch 9100, Train loss: 4.627e-03, Test loss: 2.648e-03\n",
      "Epoch 9200, Train loss: 1.084e-03, Test loss: 2.788e-03\n",
      "Epoch 9300, Train loss: 5.145e-03, Test loss: 3.969e-03\n",
      "Epoch 9400, Train loss: 2.185e-03, Test loss: 4.526e-03\n",
      "Epoch 9500, Train loss: 1.607e-03, Test loss: 2.261e-03\n",
      "Epoch 9600, Train loss: 8.185e-04, Test loss: 2.330e-03\n",
      "Epoch 9700, Train loss: 1.105e-03, Test loss: 1.860e-03\n",
      "Epoch 9800, Train loss: 1.029e-03, Test loss: 2.191e-03\n",
      "Epoch 9900, Train loss: 5.582e-04, Test loss: 1.994e-03\n"
     ]
    }
   ],
   "source": [
    "autoencoder = Autoencoder(autoencoder_input_shape, latent_space_dim, autoencoder_output_shape).to(DEVICE)\n",
    "optimizer = torch.optim.Adam(autoencoder.parameters(), lr=1e-2)\n",
    "\n",
    "start_epoch = 0\n",
    "n_epochs = 10000\n",
    "batch_size = 64\n",
    "n_checkpoint = 10\n",
    "new_lr = None\n",
    "\n",
    "train_autoencoder_loop(autoencoder, optimizer, X_train, y_train, X_test, y_test,  \n",
    "                       n_checkpoint, start_epoch, n_epochs, batch_size, MODEL_RESULTS_AE_PATH, DEVICE, new_lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training from a checkpoint. Epoch 9000.\n",
      "Epoch 9000, Train loss: 7.591e-04, Test loss: 1.628e-03\n",
      "Epoch 9100, Train loss: 3.674e-04, Test loss: 1.382e-03\n",
      "Epoch 9200, Train loss: 3.343e-04, Test loss: 1.329e-03\n",
      "Epoch 9300, Train loss: 3.122e-04, Test loss: 1.296e-03\n",
      "Epoch 9400, Train loss: 2.958e-04, Test loss: 1.275e-03\n",
      "Epoch 9500, Train loss: 2.827e-04, Test loss: 1.260e-03\n",
      "Epoch 9600, Train loss: 2.716e-04, Test loss: 1.248e-03\n",
      "Epoch 9700, Train loss: 2.620e-04, Test loss: 1.237e-03\n",
      "Epoch 9800, Train loss: 2.534e-04, Test loss: 1.229e-03\n",
      "Epoch 9900, Train loss: 2.457e-04, Test loss: 1.220e-03\n",
      "Epoch 10000, Train loss: 2.386e-04, Test loss: 1.213e-03\n",
      "Epoch 10100, Train loss: 2.321e-04, Test loss: 1.206e-03\n",
      "Epoch 10200, Train loss: 2.261e-04, Test loss: 1.200e-03\n",
      "Epoch 10300, Train loss: 2.204e-04, Test loss: 1.193e-03\n",
      "Epoch 10400, Train loss: 2.151e-04, Test loss: 1.188e-03\n",
      "Epoch 10500, Train loss: 2.101e-04, Test loss: 1.182e-03\n",
      "Epoch 10600, Train loss: 2.054e-04, Test loss: 1.177e-03\n",
      "Epoch 10700, Train loss: 2.010e-04, Test loss: 1.172e-03\n",
      "Epoch 10800, Train loss: 1.968e-04, Test loss: 1.167e-03\n",
      "Epoch 10900, Train loss: 1.929e-04, Test loss: 1.163e-03\n",
      "Epoch 11000, Train loss: 1.892e-04, Test loss: 1.159e-03\n",
      "Epoch 11100, Train loss: 1.857e-04, Test loss: 1.155e-03\n",
      "Epoch 11200, Train loss: 1.824e-04, Test loss: 1.150e-03\n",
      "Epoch 11300, Train loss: 1.793e-04, Test loss: 1.147e-03\n",
      "Epoch 11400, Train loss: 1.763e-04, Test loss: 1.143e-03\n",
      "Epoch 11500, Train loss: 1.736e-04, Test loss: 1.139e-03\n",
      "Epoch 11600, Train loss: 1.710e-04, Test loss: 1.136e-03\n",
      "Epoch 11700, Train loss: 1.685e-04, Test loss: 1.132e-03\n",
      "Epoch 11800, Train loss: 1.661e-04, Test loss: 1.129e-03\n",
      "Epoch 11900, Train loss: 1.638e-04, Test loss: 1.125e-03\n",
      "Epoch 12000, Train loss: 1.615e-04, Test loss: 1.122e-03\n",
      "Epoch 12100, Train loss: 1.593e-04, Test loss: 1.119e-03\n",
      "Epoch 12200, Train loss: 1.572e-04, Test loss: 1.116e-03\n",
      "Epoch 12300, Train loss: 1.551e-04, Test loss: 1.112e-03\n",
      "Epoch 12400, Train loss: 1.531e-04, Test loss: 1.109e-03\n",
      "Epoch 12500, Train loss: 1.512e-04, Test loss: 1.106e-03\n",
      "Epoch 12600, Train loss: 1.494e-04, Test loss: 1.103e-03\n",
      "Epoch 12700, Train loss: 1.476e-04, Test loss: 1.100e-03\n",
      "Epoch 12800, Train loss: 1.459e-04, Test loss: 1.098e-03\n",
      "Epoch 12900, Train loss: 1.443e-04, Test loss: 1.095e-03\n",
      "Epoch 13000, Train loss: 1.427e-04, Test loss: 1.092e-03\n",
      "Epoch 13100, Train loss: 1.411e-04, Test loss: 1.089e-03\n",
      "Epoch 13200, Train loss: 1.396e-04, Test loss: 1.086e-03\n",
      "Epoch 13300, Train loss: 1.382e-04, Test loss: 1.084e-03\n",
      "Epoch 13400, Train loss: 1.368e-04, Test loss: 1.081e-03\n",
      "Epoch 13500, Train loss: 1.354e-04, Test loss: 1.079e-03\n",
      "Epoch 13600, Train loss: 1.341e-04, Test loss: 1.076e-03\n",
      "Epoch 13700, Train loss: 1.328e-04, Test loss: 1.074e-03\n",
      "Epoch 13800, Train loss: 1.316e-04, Test loss: 1.071e-03\n",
      "Epoch 13900, Train loss: 1.304e-04, Test loss: 1.069e-03\n",
      "Epoch 14000, Train loss: 1.292e-04, Test loss: 1.066e-03\n",
      "Epoch 14100, Train loss: 1.281e-04, Test loss: 1.064e-03\n",
      "Epoch 14200, Train loss: 1.269e-04, Test loss: 1.062e-03\n",
      "Epoch 14300, Train loss: 1.258e-04, Test loss: 1.059e-03\n",
      "Epoch 14400, Train loss: 1.247e-04, Test loss: 1.057e-03\n",
      "Epoch 14500, Train loss: 1.237e-04, Test loss: 1.054e-03\n",
      "Epoch 14600, Train loss: 1.226e-04, Test loss: 1.052e-03\n",
      "Epoch 14700, Train loss: 1.216e-04, Test loss: 1.050e-03\n",
      "Epoch 14800, Train loss: 1.205e-04, Test loss: 1.048e-03\n",
      "Epoch 14900, Train loss: 1.195e-04, Test loss: 1.046e-03\n",
      "Epoch 15000, Train loss: 1.183e-04, Test loss: 1.044e-03\n",
      "Epoch 15100, Train loss: 1.172e-04, Test loss: 1.042e-03\n",
      "Epoch 15200, Train loss: 1.161e-04, Test loss: 1.040e-03\n",
      "Epoch 15300, Train loss: 1.152e-04, Test loss: 1.038e-03\n",
      "Epoch 15400, Train loss: 1.143e-04, Test loss: 1.035e-03\n",
      "Epoch 15500, Train loss: 1.134e-04, Test loss: 1.033e-03\n",
      "Epoch 15600, Train loss: 1.125e-04, Test loss: 1.030e-03\n",
      "Epoch 15700, Train loss: 1.117e-04, Test loss: 1.028e-03\n",
      "Epoch 15800, Train loss: 1.108e-04, Test loss: 1.026e-03\n",
      "Epoch 15900, Train loss: 1.100e-04, Test loss: 1.023e-03\n",
      "Epoch 16000, Train loss: 1.092e-04, Test loss: 1.021e-03\n",
      "Epoch 16100, Train loss: 1.084e-04, Test loss: 1.019e-03\n",
      "Epoch 16200, Train loss: 1.076e-04, Test loss: 1.016e-03\n",
      "Epoch 16300, Train loss: 1.069e-04, Test loss: 1.014e-03\n",
      "Epoch 16400, Train loss: 1.061e-04, Test loss: 1.011e-03\n",
      "Epoch 16500, Train loss: 1.052e-04, Test loss: 1.009e-03\n",
      "Epoch 16600, Train loss: 1.042e-04, Test loss: 1.007e-03\n",
      "Epoch 16700, Train loss: 1.012e-04, Test loss: 1.004e-03\n",
      "Epoch 16800, Train loss: 1.003e-04, Test loss: 1.002e-03\n",
      "Epoch 16900, Train loss: 9.940e-05, Test loss: 9.994e-04\n",
      "Epoch 17000, Train loss: 9.855e-05, Test loss: 9.971e-04\n",
      "Epoch 17100, Train loss: 9.769e-05, Test loss: 9.948e-04\n",
      "Epoch 17200, Train loss: 9.681e-05, Test loss: 9.925e-04\n",
      "Epoch 17300, Train loss: 9.592e-05, Test loss: 9.903e-04\n",
      "Epoch 17400, Train loss: 9.504e-05, Test loss: 9.881e-04\n",
      "Epoch 17500, Train loss: 9.416e-05, Test loss: 9.859e-04\n",
      "Epoch 17600, Train loss: 9.328e-05, Test loss: 9.838e-04\n",
      "Epoch 17700, Train loss: 9.245e-05, Test loss: 9.817e-04\n",
      "Epoch 17800, Train loss: 9.167e-05, Test loss: 9.795e-04\n",
      "Epoch 17900, Train loss: 9.092e-05, Test loss: 9.774e-04\n",
      "Epoch 18000, Train loss: 9.019e-05, Test loss: 9.752e-04\n",
      "Epoch 18100, Train loss: 8.950e-05, Test loss: 9.730e-04\n",
      "Epoch 18200, Train loss: 8.883e-05, Test loss: 9.708e-04\n",
      "Epoch 18300, Train loss: 8.817e-05, Test loss: 9.687e-04\n",
      "Epoch 18400, Train loss: 8.753e-05, Test loss: 9.665e-04\n",
      "Epoch 18500, Train loss: 8.690e-05, Test loss: 9.644e-04\n",
      "Epoch 18600, Train loss: 8.628e-05, Test loss: 9.622e-04\n",
      "Epoch 18700, Train loss: 8.568e-05, Test loss: 9.601e-04\n",
      "Epoch 18800, Train loss: 8.510e-05, Test loss: 9.580e-04\n",
      "Epoch 18900, Train loss: 8.456e-05, Test loss: 9.559e-04\n",
      "Epoch 19000, Train loss: 8.426e-05, Test loss: 9.533e-04\n",
      "Epoch 19100, Train loss: 8.367e-05, Test loss: 9.501e-04\n",
      "Epoch 19200, Train loss: 8.305e-05, Test loss: 9.477e-04\n",
      "Epoch 19300, Train loss: 8.245e-05, Test loss: 9.453e-04\n",
      "Epoch 19400, Train loss: 8.184e-05, Test loss: 9.430e-04\n",
      "Epoch 19500, Train loss: 8.125e-05, Test loss: 9.408e-04\n",
      "Epoch 19600, Train loss: 8.064e-05, Test loss: 9.385e-04\n",
      "Epoch 19700, Train loss: 8.006e-05, Test loss: 9.362e-04\n",
      "Epoch 19800, Train loss: 7.948e-05, Test loss: 9.340e-04\n",
      "Epoch 19900, Train loss: 7.890e-05, Test loss: 9.318e-04\n"
     ]
    }
   ],
   "source": [
    "start_epoch = 9000\n",
    "n_epochs = 20000\n",
    "batch_size = 64\n",
    "n_checkpoint = 10\n",
    "new_lr = 1e-4\n",
    "4\n",
    "train_autoencoder_loop(autoencoder, optimizer, X_train, y_train, X_test, y_test,  \n",
    "                       n_checkpoint, start_epoch, n_epochs, batch_size, MODEL_RESULTS_AE_PATH, DEVICE, new_lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PGNNIV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vecopsciml.operators.zero_order import Mx, My\n",
    "from model.ae_nonlinear_model import AutoencoderNonlinearModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predictive network architecture\n",
    "input_shape = X_train_NN[0].shape\n",
    "predictive_layers = [15, 10, 3]\n",
    "predictive_output = y_train_NN.values[0].shape\n",
    "\n",
    "# Explanatory network architecture\n",
    "explanatory_input = Mx(My(y_train_NN)).values[0].shape\n",
    "explanatory_layers = [10, 10]\n",
    "explanatory_output = Mx(My(f_train_NN)).values[0].shape\n",
    "\n",
    "# Other parameters\n",
    "n_filters_explanatory = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hidden1_layer.weight: requires_grad=False\n",
      "hidden1_layer.bias: requires_grad=False\n",
      "hidden2_layer.weight: requires_grad=False\n",
      "hidden2_layer.bias: requires_grad=False\n",
      "output_layer.weight: requires_grad=False\n",
      "output_layer.bias: requires_grad=False\n"
     ]
    }
   ],
   "source": [
    "pretrained_decoder = autoencoder.decoder\n",
    "\n",
    "for param in pretrained_decoder.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "for name, param in pretrained_decoder.named_parameters():\n",
    "    print(f\"{name}: requires_grad={param.requires_grad}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training from scratch.\n",
      "Epoch 0, Train loss: 8.100e+08, Test loss: 8.041e+08, MSE(e): 7.854e+01, MSE(pi1): 1.928e+03, MSE(pi2): 5.232e+01, MSE(pi3): 5.354e+01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100, Train loss: 6.621e+06, Test loss: 6.980e+06, MSE(e): 6.600e-01, MSE(pi1): 7.701e-01, MSE(pi2): 3.973e-01, MSE(pi3): 1.278e-01\n",
      "Epoch 200, Train loss: 2.879e+06, Test loss: 3.139e+06, MSE(e): 2.864e-01, MSE(pi1): 6.573e-01, MSE(pi2): 1.997e-01, MSE(pi3): 7.917e-02\n",
      "Epoch 300, Train loss: 8.319e+05, Test loss: 8.086e+05, MSE(e): 8.242e-02, MSE(pi1): 4.528e-01, MSE(pi2): 6.153e-02, MSE(pi3): 3.107e-02\n",
      "Epoch 400, Train loss: 7.325e+05, Test loss: 7.501e+05, MSE(e): 7.282e-02, MSE(pi1): 1.832e-01, MSE(pi2): 5.756e-02, MSE(pi3): 2.439e-02\n",
      "Epoch 500, Train loss: 7.125e+05, Test loss: 6.655e+05, MSE(e): 7.091e-02, MSE(pi1): 1.410e-01, MSE(pi2): 5.563e-02, MSE(pi3): 1.960e-02\n",
      "Epoch 600, Train loss: 6.607e+05, Test loss: 6.201e+05, MSE(e): 6.576e-02, MSE(pi1): 1.285e-01, MSE(pi2): 5.508e-02, MSE(pi3): 1.764e-02\n",
      "Epoch 700, Train loss: 6.276e+05, Test loss: 6.077e+05, MSE(e): 6.249e-02, MSE(pi1): 1.049e-01, MSE(pi2): 5.190e-02, MSE(pi3): 1.595e-02\n",
      "Epoch 800, Train loss: 5.988e+05, Test loss: 5.940e+05, MSE(e): 5.966e-02, MSE(pi1): 8.016e-02, MSE(pi2): 4.802e-02, MSE(pi3): 1.337e-02\n",
      "Epoch 900, Train loss: 5.822e+05, Test loss: 5.878e+05, MSE(e): 5.803e-02, MSE(pi1): 6.985e-02, MSE(pi2): 4.748e-02, MSE(pi3): 1.152e-02\n",
      "Epoch 1000, Train loss: 5.729e+05, Test loss: 5.684e+05, MSE(e): 5.709e-02, MSE(pi1): 6.635e-02, MSE(pi2): 4.930e-02, MSE(pi3): 1.242e-02\n",
      "Epoch 1100, Train loss: 5.699e+05, Test loss: 5.770e+05, MSE(e): 5.680e-02, MSE(pi1): 6.308e-02, MSE(pi2): 4.893e-02, MSE(pi3): 1.222e-02\n",
      "Epoch 1200, Train loss: 5.965e+05, Test loss: 5.454e+05, MSE(e): 5.947e-02, MSE(pi1): 5.965e-02, MSE(pi2): 4.626e-02, MSE(pi3): 1.191e-02\n",
      "Epoch 1300, Train loss: 5.631e+05, Test loss: 5.338e+05, MSE(e): 5.612e-02, MSE(pi1): 5.780e-02, MSE(pi2): 4.602e-02, MSE(pi3): 1.233e-02\n",
      "Epoch 1400, Train loss: 5.770e+05, Test loss: 5.683e+05, MSE(e): 5.751e-02, MSE(pi1): 6.362e-02, MSE(pi2): 4.965e-02, MSE(pi3): 1.254e-02\n",
      "Epoch 1500, Train loss: 5.714e+05, Test loss: 5.577e+05, MSE(e): 5.696e-02, MSE(pi1): 6.025e-02, MSE(pi2): 4.771e-02, MSE(pi3): 1.154e-02\n",
      "Epoch 1600, Train loss: 5.565e+05, Test loss: 5.627e+05, MSE(e): 5.548e-02, MSE(pi1): 5.446e-02, MSE(pi2): 4.501e-02, MSE(pi3): 1.104e-02\n",
      "Epoch 1700, Train loss: 5.790e+05, Test loss: 5.563e+05, MSE(e): 5.773e-02, MSE(pi1): 6.065e-02, MSE(pi2): 4.845e-02, MSE(pi3): 1.102e-02\n",
      "Epoch 1800, Train loss: 5.847e+05, Test loss: 5.351e+05, MSE(e): 5.831e-02, MSE(pi1): 5.407e-02, MSE(pi2): 4.780e-02, MSE(pi3): 1.055e-02\n",
      "Epoch 1900, Train loss: 5.786e+05, Test loss: 5.319e+05, MSE(e): 5.770e-02, MSE(pi1): 5.117e-02, MSE(pi2): 4.684e-02, MSE(pi3): 1.059e-02\n",
      "Epoch 2000, Train loss: 5.615e+05, Test loss: 5.510e+05, MSE(e): 5.598e-02, MSE(pi1): 5.054e-02, MSE(pi2): 4.700e-02, MSE(pi3): 1.166e-02\n",
      "Epoch 2100, Train loss: 5.616e+05, Test loss: 5.406e+05, MSE(e): 5.599e-02, MSE(pi1): 5.353e-02, MSE(pi2): 4.529e-02, MSE(pi3): 1.087e-02\n",
      "Epoch 2200, Train loss: 5.624e+05, Test loss: 5.409e+05, MSE(e): 5.608e-02, MSE(pi1): 5.217e-02, MSE(pi2): 4.641e-02, MSE(pi3): 1.095e-02\n",
      "Epoch 2300, Train loss: 5.571e+05, Test loss: 5.360e+05, MSE(e): 5.553e-02, MSE(pi1): 5.410e-02, MSE(pi2): 4.609e-02, MSE(pi3): 1.184e-02\n",
      "Epoch 2400, Train loss: 5.887e+05, Test loss: 5.522e+05, MSE(e): 5.870e-02, MSE(pi1): 5.464e-02, MSE(pi2): 4.795e-02, MSE(pi3): 1.112e-02\n",
      "Epoch 2500, Train loss: 5.424e+05, Test loss: 5.294e+05, MSE(e): 5.406e-02, MSE(pi1): 4.842e-02, MSE(pi2): 4.514e-02, MSE(pi3): 1.188e-02\n",
      "Epoch 2600, Train loss: 5.416e+05, Test loss: 5.508e+05, MSE(e): 5.400e-02, MSE(pi1): 4.895e-02, MSE(pi2): 4.484e-02, MSE(pi3): 1.070e-02\n",
      "Epoch 2700, Train loss: 5.409e+05, Test loss: 5.320e+05, MSE(e): 5.393e-02, MSE(pi1): 4.730e-02, MSE(pi2): 4.442e-02, MSE(pi3): 1.159e-02\n",
      "Epoch 2800, Train loss: 5.562e+05, Test loss: 5.286e+05, MSE(e): 5.544e-02, MSE(pi1): 5.210e-02, MSE(pi2): 4.528e-02, MSE(pi3): 1.203e-02\n",
      "Epoch 2900, Train loss: 5.462e+05, Test loss: 5.479e+05, MSE(e): 5.446e-02, MSE(pi1): 4.686e-02, MSE(pi2): 4.545e-02, MSE(pi3): 1.065e-02\n",
      "Epoch 3000, Train loss: 5.558e+05, Test loss: 5.353e+05, MSE(e): 5.542e-02, MSE(pi1): 5.032e-02, MSE(pi2): 4.526e-02, MSE(pi3): 1.137e-02\n",
      "Epoch 3100, Train loss: 5.511e+05, Test loss: 5.293e+05, MSE(e): 5.494e-02, MSE(pi1): 4.990e-02, MSE(pi2): 4.528e-02, MSE(pi3): 1.185e-02\n",
      "Epoch 3200, Train loss: 5.507e+05, Test loss: 5.298e+05, MSE(e): 5.489e-02, MSE(pi1): 5.246e-02, MSE(pi2): 4.516e-02, MSE(pi3): 1.203e-02\n",
      "Epoch 3300, Train loss: 5.632e+05, Test loss: 5.442e+05, MSE(e): 5.615e-02, MSE(pi1): 5.047e-02, MSE(pi2): 4.576e-02, MSE(pi3): 1.114e-02\n",
      "Epoch 3400, Train loss: 5.555e+05, Test loss: 5.308e+05, MSE(e): 5.538e-02, MSE(pi1): 4.698e-02, MSE(pi2): 4.443e-02, MSE(pi3): 1.203e-02\n",
      "Epoch 3500, Train loss: 5.528e+05, Test loss: 5.287e+05, MSE(e): 5.511e-02, MSE(pi1): 4.707e-02, MSE(pi2): 4.439e-02, MSE(pi3): 1.207e-02\n",
      "Epoch 3600, Train loss: 5.654e+05, Test loss: 5.418e+05, MSE(e): 5.635e-02, MSE(pi1): 7.302e-02, MSE(pi2): 4.642e-02, MSE(pi3): 1.139e-02\n",
      "Epoch 3700, Train loss: 5.443e+05, Test loss: 5.306e+05, MSE(e): 5.425e-02, MSE(pi1): 5.092e-02, MSE(pi2): 4.371e-02, MSE(pi3): 1.247e-02\n",
      "Epoch 3800, Train loss: 5.563e+05, Test loss: 5.339e+05, MSE(e): 5.546e-02, MSE(pi1): 5.180e-02, MSE(pi2): 4.583e-02, MSE(pi3): 1.182e-02\n",
      "Epoch 3900, Train loss: 5.578e+05, Test loss: 5.310e+05, MSE(e): 5.563e-02, MSE(pi1): 4.622e-02, MSE(pi2): 4.534e-02, MSE(pi3): 1.079e-02\n",
      "Epoch 4000, Train loss: 5.491e+05, Test loss: 5.287e+05, MSE(e): 5.475e-02, MSE(pi1): 4.443e-02, MSE(pi2): 4.459e-02, MSE(pi3): 1.090e-02\n",
      "Epoch 4100, Train loss: 5.411e+05, Test loss: 5.257e+05, MSE(e): 5.395e-02, MSE(pi1): 4.645e-02, MSE(pi2): 4.495e-02, MSE(pi3): 1.138e-02\n",
      "Epoch 4200, Train loss: 5.455e+05, Test loss: 5.235e+05, MSE(e): 5.438e-02, MSE(pi1): 4.548e-02, MSE(pi2): 4.510e-02, MSE(pi3): 1.188e-02\n",
      "Epoch 4300, Train loss: 5.425e+05, Test loss: 5.285e+05, MSE(e): 5.409e-02, MSE(pi1): 4.559e-02, MSE(pi2): 4.447e-02, MSE(pi3): 1.091e-02\n",
      "Epoch 4400, Train loss: 5.848e+05, Test loss: 5.287e+05, MSE(e): 5.828e-02, MSE(pi1): 5.450e-02, MSE(pi2): 4.646e-02, MSE(pi3): 1.360e-02\n",
      "Epoch 4500, Train loss: 5.437e+05, Test loss: 5.344e+05, MSE(e): 5.420e-02, MSE(pi1): 4.624e-02, MSE(pi2): 4.438e-02, MSE(pi3): 1.209e-02\n",
      "Epoch 4600, Train loss: 5.379e+05, Test loss: 5.386e+05, MSE(e): 5.362e-02, MSE(pi1): 4.566e-02, MSE(pi2): 4.517e-02, MSE(pi3): 1.170e-02\n",
      "Epoch 4700, Train loss: 5.389e+05, Test loss: 5.282e+05, MSE(e): 5.373e-02, MSE(pi1): 4.622e-02, MSE(pi2): 4.432e-02, MSE(pi3): 1.164e-02\n",
      "Epoch 4800, Train loss: 5.415e+05, Test loss: 5.321e+05, MSE(e): 5.398e-02, MSE(pi1): 5.440e-02, MSE(pi2): 4.396e-02, MSE(pi3): 1.180e-02\n",
      "Epoch 4900, Train loss: 5.405e+05, Test loss: 5.235e+05, MSE(e): 5.388e-02, MSE(pi1): 4.532e-02, MSE(pi2): 4.493e-02, MSE(pi3): 1.164e-02\n",
      "Epoch 5000, Train loss: 5.459e+05, Test loss: 5.219e+05, MSE(e): 5.442e-02, MSE(pi1): 4.871e-02, MSE(pi2): 4.495e-02, MSE(pi3): 1.163e-02\n",
      "Epoch 5100, Train loss: 5.345e+05, Test loss: 5.168e+05, MSE(e): 5.328e-02, MSE(pi1): 4.837e-02, MSE(pi2): 4.426e-02, MSE(pi3): 1.176e-02\n",
      "Epoch 5200, Train loss: 5.550e+05, Test loss: 5.283e+05, MSE(e): 5.533e-02, MSE(pi1): 5.139e-02, MSE(pi2): 4.518e-02, MSE(pi3): 1.180e-02\n",
      "Epoch 5300, Train loss: 5.514e+05, Test loss: 5.261e+05, MSE(e): 5.497e-02, MSE(pi1): 4.865e-02, MSE(pi2): 4.422e-02, MSE(pi3): 1.142e-02\n",
      "Epoch 5400, Train loss: 5.403e+05, Test loss: 5.194e+05, MSE(e): 5.386e-02, MSE(pi1): 4.768e-02, MSE(pi2): 4.402e-02, MSE(pi3): 1.197e-02\n",
      "Epoch 5500, Train loss: 5.415e+05, Test loss: 5.199e+05, MSE(e): 5.397e-02, MSE(pi1): 5.259e-02, MSE(pi2): 4.296e-02, MSE(pi3): 1.241e-02\n",
      "Epoch 5600, Train loss: 5.472e+05, Test loss: 5.188e+05, MSE(e): 5.454e-02, MSE(pi1): 4.854e-02, MSE(pi2): 4.518e-02, MSE(pi3): 1.271e-02\n",
      "Epoch 5700, Train loss: 5.424e+05, Test loss: 5.194e+05, MSE(e): 5.405e-02, MSE(pi1): 5.101e-02, MSE(pi2): 4.393e-02, MSE(pi3): 1.337e-02\n",
      "Epoch 5800, Train loss: 5.451e+05, Test loss: 5.173e+05, MSE(e): 5.434e-02, MSE(pi1): 4.519e-02, MSE(pi2): 4.415e-02, MSE(pi3): 1.234e-02\n",
      "Epoch 5900, Train loss: 5.526e+05, Test loss: 5.394e+05, MSE(e): 5.508e-02, MSE(pi1): 4.660e-02, MSE(pi2): 4.452e-02, MSE(pi3): 1.331e-02\n",
      "Epoch 6000, Train loss: 5.388e+05, Test loss: 5.293e+05, MSE(e): 5.371e-02, MSE(pi1): 4.455e-02, MSE(pi2): 4.435e-02, MSE(pi3): 1.240e-02\n",
      "Epoch 6100, Train loss: 5.413e+05, Test loss: 5.136e+05, MSE(e): 5.394e-02, MSE(pi1): 4.684e-02, MSE(pi2): 4.438e-02, MSE(pi3): 1.303e-02\n",
      "Epoch 6200, Train loss: 5.360e+05, Test loss: 5.179e+05, MSE(e): 5.344e-02, MSE(pi1): 4.489e-02, MSE(pi2): 4.468e-02, MSE(pi3): 1.157e-02\n",
      "Epoch 6300, Train loss: 5.348e+05, Test loss: 5.255e+05, MSE(e): 5.332e-02, MSE(pi1): 4.615e-02, MSE(pi2): 4.417e-02, MSE(pi3): 1.123e-02\n",
      "Epoch 6400, Train loss: 5.398e+05, Test loss: 5.424e+05, MSE(e): 5.381e-02, MSE(pi1): 4.939e-02, MSE(pi2): 4.512e-02, MSE(pi3): 1.194e-02\n",
      "Epoch 6500, Train loss: 5.361e+05, Test loss: 5.299e+05, MSE(e): 5.344e-02, MSE(pi1): 4.763e-02, MSE(pi2): 4.457e-02, MSE(pi3): 1.212e-02\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[37], line 12\u001b[0m\n\u001b[1;32m      9\u001b[0m batch_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m64\u001b[39m\n\u001b[1;32m     10\u001b[0m n_checkpoints \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m10\u001b[39m\n\u001b[0;32m---> 12\u001b[0m train_loop(model, optimizer, X_train_NN, y_train_NN, f_train_NN, X_test_NN, y_test_NN, f_test_NN,\n\u001b[1;32m     13\u001b[0m            D, n_checkpoints, start_epoch\u001b[38;5;241m=\u001b[39mstart_epoch, n_epochs\u001b[38;5;241m=\u001b[39mn_epochs, batch_size\u001b[38;5;241m=\u001b[39mbatch_size, \n\u001b[1;32m     14\u001b[0m            model_results_path\u001b[38;5;241m=\u001b[39mMODEL_RESULTS_PGNNIV_PATH, device\u001b[38;5;241m=\u001b[39mDEVICE)\n",
      "File \u001b[0;32m~/Escritorio/rmunozTMELab/Physically-Guided-Machine-Learning/models/autoencoder/trainers/train.py:160\u001b[0m, in \u001b[0;36mtrain_loop\u001b[0;34m(model, optimizer, X_train, y_train, f_train, X_test, y_test, f_test, D, n_checkpoints, start_epoch, n_epochs, batch_size, model_results_path, device, new_lr)\u001b[0m\n\u001b[1;32m    157\u001b[0m     f_batch \u001b[38;5;241m=\u001b[39m TensOps(f_train\u001b[38;5;241m.\u001b[39mvalues[batch_start:(batch_start\u001b[38;5;241m+\u001b[39mbatch_size)]\u001b[38;5;241m.\u001b[39mto(device), space_dimension\u001b[38;5;241m=\u001b[39mf_train\u001b[38;5;241m.\u001b[39mspace_dim, contravariance\u001b[38;5;241m=\u001b[39mf_train\u001b[38;5;241m.\u001b[39morder[\u001b[38;5;241m0\u001b[39m], covariance\u001b[38;5;241m=\u001b[39mf_train\u001b[38;5;241m.\u001b[39morder[\u001b[38;5;241m1\u001b[39m])\n\u001b[1;32m    159\u001b[0m     \u001b[38;5;66;03m# Train a batch\u001b[39;00m\n\u001b[0;32m--> 160\u001b[0m     loss, e_loss, pi1_loss, pi2_loss, pi3_loss \u001b[38;5;241m=\u001b[39m train_epoch(model, optimizer, X_batch, y_batch, f_batch, D)\n\u001b[1;32m    162\u001b[0m \u001b[38;5;66;03m# Time instant when ends  the training of the model for one epoch and obtaining the total time\u001b[39;00m\n\u001b[1;32m    163\u001b[0m time_end \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n",
      "File \u001b[0;32m~/Escritorio/rmunozTMELab/Physically-Guided-Machine-Learning/models/autoencoder/trainers/train.py:36\u001b[0m, in \u001b[0;36mtrain_epoch\u001b[0;34m(model, optimizer, X_train, y_train, f_train, D)\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;66;03m# Reset the gradients, perform backpropagation, and update model's parameters using optimizer and compute gradients.\u001b[39;00m\n\u001b[1;32m     35\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 36\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward(retain_graph\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     37\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     39\u001b[0m \u001b[38;5;66;03m# Return the total loss and its components.\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/SciML_test_env/lib/python3.12/site-packages/torch/_tensor.py:521\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    511\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    512\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    513\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    514\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    519\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    520\u001b[0m     )\n\u001b[0;32m--> 521\u001b[0m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mbackward(\n\u001b[1;32m    522\u001b[0m     \u001b[38;5;28mself\u001b[39m, gradient, retain_graph, create_graph, inputs\u001b[38;5;241m=\u001b[39minputs\n\u001b[1;32m    523\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/envs/SciML_test_env/lib/python3.12/site-packages/torch/autograd/__init__.py:289\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    284\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    286\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    287\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    288\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 289\u001b[0m _engine_run_backward(\n\u001b[1;32m    290\u001b[0m     tensors,\n\u001b[1;32m    291\u001b[0m     grad_tensors_,\n\u001b[1;32m    292\u001b[0m     retain_graph,\n\u001b[1;32m    293\u001b[0m     create_graph,\n\u001b[1;32m    294\u001b[0m     inputs,\n\u001b[1;32m    295\u001b[0m     allow_unreachable\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    296\u001b[0m     accumulate_grad\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    297\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/envs/SciML_test_env/lib/python3.12/site-packages/torch/autograd/graph.py:769\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    767\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    768\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 769\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Variable\u001b[38;5;241m.\u001b[39m_execution_engine\u001b[38;5;241m.\u001b[39mrun_backward(  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    770\u001b[0m         t_outputs, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[1;32m    771\u001b[0m     )  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    772\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    773\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model = AutoencoderNonlinearModel(input_shape, predictive_layers, pretrained_decoder, predictive_output, explanatory_input,\n",
    "                                   explanatory_layers, explanatory_output, n_filters_explanatory).to(DEVICE)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-2)\n",
    "\n",
    "# Parametros de entrenamiento\n",
    "start_epoch = 0\n",
    "n_epochs = 10000\n",
    "\n",
    "batch_size = 64\n",
    "n_checkpoints = 10\n",
    "\n",
    "train_loop(model, optimizer, X_train_NN, y_train_NN, f_train_NN, X_test_NN, y_test_NN, f_test_NN,\n",
    "           D, n_checkpoints, start_epoch=start_epoch, n_epochs=n_epochs, batch_size=batch_size, \n",
    "           model_results_path=MODEL_RESULTS_PGNNIV_PATH, device=DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training from a checkpoint. Epoch 9000.\n",
      "Epoch 9000, Train loss: 8.327e+02, Test loss: 1.153e+03, MSE(e): 7.252e-05, MSE(pi1): 2.688e-03, MSE(pi2): 4.618e-05, MSE(pi3): 8.062e-04\n",
      "Epoch 9100, Train loss: 6.275e+02, Test loss: 9.711e+02, MSE(e): 5.204e-05, MSE(pi1): 2.831e-03, MSE(pi2): 3.602e-05, MSE(pi3): 7.882e-04\n",
      "Epoch 9200, Train loss: 6.154e+02, Test loss: 9.405e+02, MSE(e): 5.085e-05, MSE(pi1): 2.899e-03, MSE(pi2): 3.525e-05, MSE(pi3): 7.790e-04\n",
      "Epoch 9300, Train loss: 5.932e+02, Test loss: 9.180e+02, MSE(e): 4.864e-05, MSE(pi1): 2.891e-03, MSE(pi2): 3.389e-05, MSE(pi3): 7.784e-04\n",
      "Epoch 9400, Train loss: 5.759e+02, Test loss: 9.012e+02, MSE(e): 4.692e-05, MSE(pi1): 2.887e-03, MSE(pi2): 3.285e-05, MSE(pi3): 7.778e-04\n",
      "Epoch 9500, Train loss: 5.618e+02, Test loss: 8.877e+02, MSE(e): 4.552e-05, MSE(pi1): 2.885e-03, MSE(pi2): 3.202e-05, MSE(pi3): 7.772e-04\n",
      "Epoch 9600, Train loss: 5.500e+02, Test loss: 8.762e+02, MSE(e): 4.435e-05, MSE(pi1): 2.884e-03, MSE(pi2): 3.133e-05, MSE(pi3): 7.766e-04\n",
      "Epoch 9700, Train loss: 5.398e+02, Test loss: 8.662e+02, MSE(e): 4.334e-05, MSE(pi1): 2.884e-03, MSE(pi2): 3.075e-05, MSE(pi3): 7.761e-04\n",
      "Epoch 9800, Train loss: 5.309e+02, Test loss: 8.572e+02, MSE(e): 4.245e-05, MSE(pi1): 2.885e-03, MSE(pi2): 3.025e-05, MSE(pi3): 7.755e-04\n",
      "Epoch 9900, Train loss: 5.230e+02, Test loss: 8.491e+02, MSE(e): 4.167e-05, MSE(pi1): 2.885e-03, MSE(pi2): 2.981e-05, MSE(pi3): 7.751e-04\n",
      "Epoch 10000, Train loss: 5.159e+02, Test loss: 8.416e+02, MSE(e): 4.095e-05, MSE(pi1): 2.886e-03, MSE(pi2): 2.942e-05, MSE(pi3): 7.746e-04\n",
      "Epoch 10100, Train loss: 5.094e+02, Test loss: 8.347e+02, MSE(e): 4.031e-05, MSE(pi1): 2.887e-03, MSE(pi2): 2.906e-05, MSE(pi3): 7.741e-04\n",
      "Epoch 10200, Train loss: 5.034e+02, Test loss: 8.282e+02, MSE(e): 3.972e-05, MSE(pi1): 2.887e-03, MSE(pi2): 2.873e-05, MSE(pi3): 7.737e-04\n",
      "Epoch 10300, Train loss: 4.979e+02, Test loss: 8.221e+02, MSE(e): 3.916e-05, MSE(pi1): 2.888e-03, MSE(pi2): 2.843e-05, MSE(pi3): 7.733e-04\n",
      "Epoch 10400, Train loss: 4.927e+02, Test loss: 8.163e+02, MSE(e): 3.865e-05, MSE(pi1): 2.888e-03, MSE(pi2): 2.814e-05, MSE(pi3): 7.729e-04\n",
      "Epoch 10500, Train loss: 4.878e+02, Test loss: 8.109e+02, MSE(e): 3.816e-05, MSE(pi1): 2.888e-03, MSE(pi2): 2.787e-05, MSE(pi3): 7.725e-04\n",
      "Epoch 10600, Train loss: 4.831e+02, Test loss: 8.057e+02, MSE(e): 3.770e-05, MSE(pi1): 2.888e-03, MSE(pi2): 2.760e-05, MSE(pi3): 7.721e-04\n",
      "Epoch 10700, Train loss: 4.787e+02, Test loss: 8.007e+02, MSE(e): 3.726e-05, MSE(pi1): 2.889e-03, MSE(pi2): 2.736e-05, MSE(pi3): 7.718e-04\n",
      "Epoch 10800, Train loss: 4.745e+02, Test loss: 7.960e+02, MSE(e): 3.685e-05, MSE(pi1): 2.888e-03, MSE(pi2): 2.712e-05, MSE(pi3): 7.715e-04\n",
      "Epoch 10900, Train loss: 4.705e+02, Test loss: 7.915e+02, MSE(e): 3.644e-05, MSE(pi1): 2.888e-03, MSE(pi2): 2.688e-05, MSE(pi3): 7.712e-04\n",
      "Epoch 11000, Train loss: 4.666e+02, Test loss: 7.871e+02, MSE(e): 3.606e-05, MSE(pi1): 2.887e-03, MSE(pi2): 2.666e-05, MSE(pi3): 7.709e-04\n",
      "Epoch 11100, Train loss: 4.629e+02, Test loss: 7.829e+02, MSE(e): 3.569e-05, MSE(pi1): 2.886e-03, MSE(pi2): 2.644e-05, MSE(pi3): 7.707e-04\n",
      "Epoch 11200, Train loss: 4.593e+02, Test loss: 7.789e+02, MSE(e): 3.533e-05, MSE(pi1): 2.885e-03, MSE(pi2): 2.622e-05, MSE(pi3): 7.705e-04\n",
      "Epoch 11300, Train loss: 4.558e+02, Test loss: 7.750e+02, MSE(e): 3.499e-05, MSE(pi1): 2.884e-03, MSE(pi2): 2.602e-05, MSE(pi3): 7.703e-04\n",
      "Epoch 11400, Train loss: 4.524e+02, Test loss: 7.713e+02, MSE(e): 3.466e-05, MSE(pi1): 2.882e-03, MSE(pi2): 2.581e-05, MSE(pi3): 7.701e-04\n",
      "Epoch 11500, Train loss: 4.492e+02, Test loss: 7.677e+02, MSE(e): 3.433e-05, MSE(pi1): 2.881e-03, MSE(pi2): 2.562e-05, MSE(pi3): 7.699e-04\n",
      "Epoch 11600, Train loss: 4.460e+02, Test loss: 7.642e+02, MSE(e): 3.402e-05, MSE(pi1): 2.879e-03, MSE(pi2): 2.542e-05, MSE(pi3): 7.698e-04\n",
      "Epoch 11700, Train loss: 4.430e+02, Test loss: 7.609e+02, MSE(e): 3.372e-05, MSE(pi1): 2.877e-03, MSE(pi2): 2.523e-05, MSE(pi3): 7.696e-04\n",
      "Epoch 11800, Train loss: 4.400e+02, Test loss: 7.577e+02, MSE(e): 3.343e-05, MSE(pi1): 2.876e-03, MSE(pi2): 2.505e-05, MSE(pi3): 7.695e-04\n",
      "Epoch 11900, Train loss: 4.372e+02, Test loss: 7.545e+02, MSE(e): 3.315e-05, MSE(pi1): 2.874e-03, MSE(pi2): 2.487e-05, MSE(pi3): 7.694e-04\n",
      "Epoch 12000, Train loss: 4.344e+02, Test loss: 7.515e+02, MSE(e): 3.288e-05, MSE(pi1): 2.872e-03, MSE(pi2): 2.470e-05, MSE(pi3): 7.693e-04\n",
      "Epoch 12100, Train loss: 4.318e+02, Test loss: 7.486e+02, MSE(e): 3.261e-05, MSE(pi1): 2.870e-03, MSE(pi2): 2.453e-05, MSE(pi3): 7.691e-04\n",
      "Epoch 12200, Train loss: 4.292e+02, Test loss: 7.458e+02, MSE(e): 3.236e-05, MSE(pi1): 2.868e-03, MSE(pi2): 2.437e-05, MSE(pi3): 7.690e-04\n",
      "Epoch 12300, Train loss: 4.267e+02, Test loss: 7.431e+02, MSE(e): 3.212e-05, MSE(pi1): 2.867e-03, MSE(pi2): 2.421e-05, MSE(pi3): 7.689e-04\n",
      "Epoch 12400, Train loss: 4.244e+02, Test loss: 7.406e+02, MSE(e): 3.188e-05, MSE(pi1): 2.865e-03, MSE(pi2): 2.406e-05, MSE(pi3): 7.687e-04\n",
      "Epoch 12500, Train loss: 4.221e+02, Test loss: 7.380e+02, MSE(e): 3.166e-05, MSE(pi1): 2.864e-03, MSE(pi2): 2.391e-05, MSE(pi3): 7.686e-04\n",
      "Epoch 12600, Train loss: 4.199e+02, Test loss: 7.356e+02, MSE(e): 3.144e-05, MSE(pi1): 2.862e-03, MSE(pi2): 2.377e-05, MSE(pi3): 7.684e-04\n",
      "Epoch 12700, Train loss: 4.178e+02, Test loss: 7.333e+02, MSE(e): 3.123e-05, MSE(pi1): 2.861e-03, MSE(pi2): 2.363e-05, MSE(pi3): 7.683e-04\n",
      "Epoch 12800, Train loss: 4.157e+02, Test loss: 7.310e+02, MSE(e): 3.103e-05, MSE(pi1): 2.860e-03, MSE(pi2): 2.350e-05, MSE(pi3): 7.681e-04\n",
      "Epoch 12900, Train loss: 4.138e+02, Test loss: 7.288e+02, MSE(e): 3.084e-05, MSE(pi1): 2.859e-03, MSE(pi2): 2.337e-05, MSE(pi3): 7.679e-04\n",
      "Epoch 13000, Train loss: 4.119e+02, Test loss: 7.267e+02, MSE(e): 3.065e-05, MSE(pi1): 2.859e-03, MSE(pi2): 2.325e-05, MSE(pi3): 7.677e-04\n",
      "Epoch 13100, Train loss: 4.101e+02, Test loss: 7.247e+02, MSE(e): 3.048e-05, MSE(pi1): 2.858e-03, MSE(pi2): 2.313e-05, MSE(pi3): 7.675e-04\n",
      "Epoch 13200, Train loss: 4.084e+02, Test loss: 7.227e+02, MSE(e): 3.031e-05, MSE(pi1): 2.858e-03, MSE(pi2): 2.302e-05, MSE(pi3): 7.672e-04\n",
      "Epoch 13300, Train loss: 4.067e+02, Test loss: 7.208e+02, MSE(e): 3.014e-05, MSE(pi1): 2.858e-03, MSE(pi2): 2.291e-05, MSE(pi3): 7.670e-04\n",
      "Epoch 13400, Train loss: 4.051e+02, Test loss: 7.189e+02, MSE(e): 2.998e-05, MSE(pi1): 2.857e-03, MSE(pi2): 2.280e-05, MSE(pi3): 7.668e-04\n",
      "Epoch 13500, Train loss: 4.035e+02, Test loss: 7.171e+02, MSE(e): 2.982e-05, MSE(pi1): 2.857e-03, MSE(pi2): 2.270e-05, MSE(pi3): 7.665e-04\n",
      "Epoch 13600, Train loss: 4.020e+02, Test loss: 7.154e+02, MSE(e): 2.968e-05, MSE(pi1): 2.858e-03, MSE(pi2): 2.260e-05, MSE(pi3): 7.662e-04\n",
      "Epoch 13700, Train loss: 4.005e+02, Test loss: 7.137e+02, MSE(e): 2.953e-05, MSE(pi1): 2.858e-03, MSE(pi2): 2.250e-05, MSE(pi3): 7.660e-04\n",
      "Epoch 13800, Train loss: 3.991e+02, Test loss: 7.120e+02, MSE(e): 2.939e-05, MSE(pi1): 2.858e-03, MSE(pi2): 2.241e-05, MSE(pi3): 7.657e-04\n",
      "Epoch 13900, Train loss: 3.977e+02, Test loss: 7.104e+02, MSE(e): 2.925e-05, MSE(pi1): 2.858e-03, MSE(pi2): 2.232e-05, MSE(pi3): 7.654e-04\n",
      "Epoch 14000, Train loss: 3.963e+02, Test loss: 7.088e+02, MSE(e): 2.912e-05, MSE(pi1): 2.858e-03, MSE(pi2): 2.223e-05, MSE(pi3): 7.652e-04\n",
      "Epoch 14100, Train loss: 3.949e+02, Test loss: 7.073e+02, MSE(e): 2.898e-05, MSE(pi1): 2.858e-03, MSE(pi2): 2.214e-05, MSE(pi3): 7.649e-04\n",
      "Epoch 14200, Train loss: 3.936e+02, Test loss: 7.058e+02, MSE(e): 2.885e-05, MSE(pi1): 2.859e-03, MSE(pi2): 2.205e-05, MSE(pi3): 7.647e-04\n",
      "Epoch 14300, Train loss: 3.922e+02, Test loss: 7.043e+02, MSE(e): 2.872e-05, MSE(pi1): 2.858e-03, MSE(pi2): 2.197e-05, MSE(pi3): 7.645e-04\n",
      "Epoch 14400, Train loss: 3.908e+02, Test loss: 7.028e+02, MSE(e): 2.858e-05, MSE(pi1): 2.858e-03, MSE(pi2): 2.188e-05, MSE(pi3): 7.643e-04\n",
      "Epoch 14500, Train loss: 3.894e+02, Test loss: 7.014e+02, MSE(e): 2.844e-05, MSE(pi1): 2.858e-03, MSE(pi2): 2.179e-05, MSE(pi3): 7.641e-04\n",
      "Epoch 14600, Train loss: 3.879e+02, Test loss: 7.000e+02, MSE(e): 2.829e-05, MSE(pi1): 2.857e-03, MSE(pi2): 2.171e-05, MSE(pi3): 7.639e-04\n",
      "Epoch 14700, Train loss: 3.863e+02, Test loss: 6.987e+02, MSE(e): 2.813e-05, MSE(pi1): 2.856e-03, MSE(pi2): 2.162e-05, MSE(pi3): 7.638e-04\n",
      "Epoch 14800, Train loss: 3.846e+02, Test loss: 6.976e+02, MSE(e): 2.797e-05, MSE(pi1): 2.856e-03, MSE(pi2): 2.153e-05, MSE(pi3): 7.636e-04\n",
      "Epoch 14900, Train loss: 3.830e+02, Test loss: 6.965e+02, MSE(e): 2.781e-05, MSE(pi1): 2.855e-03, MSE(pi2): 2.145e-05, MSE(pi3): 7.635e-04\n",
      "Epoch 15000, Train loss: 3.814e+02, Test loss: 6.956e+02, MSE(e): 2.765e-05, MSE(pi1): 2.855e-03, MSE(pi2): 2.137e-05, MSE(pi3): 7.633e-04\n",
      "Epoch 15100, Train loss: 3.799e+02, Test loss: 6.947e+02, MSE(e): 2.750e-05, MSE(pi1): 2.854e-03, MSE(pi2): 2.129e-05, MSE(pi3): 7.631e-04\n",
      "Epoch 15200, Train loss: 3.785e+02, Test loss: 6.940e+02, MSE(e): 2.736e-05, MSE(pi1): 2.854e-03, MSE(pi2): 2.122e-05, MSE(pi3): 7.629e-04\n",
      "Epoch 15300, Train loss: 3.771e+02, Test loss: 6.933e+02, MSE(e): 2.722e-05, MSE(pi1): 2.854e-03, MSE(pi2): 2.115e-05, MSE(pi3): 7.628e-04\n",
      "Epoch 15400, Train loss: 3.757e+02, Test loss: 6.927e+02, MSE(e): 2.709e-05, MSE(pi1): 2.854e-03, MSE(pi2): 2.108e-05, MSE(pi3): 7.626e-04\n",
      "Epoch 15500, Train loss: 3.744e+02, Test loss: 6.922e+02, MSE(e): 2.696e-05, MSE(pi1): 2.854e-03, MSE(pi2): 2.101e-05, MSE(pi3): 7.624e-04\n",
      "Epoch 15600, Train loss: 3.732e+02, Test loss: 6.917e+02, MSE(e): 2.684e-05, MSE(pi1): 2.854e-03, MSE(pi2): 2.095e-05, MSE(pi3): 7.622e-04\n",
      "Epoch 15700, Train loss: 3.720e+02, Test loss: 6.913e+02, MSE(e): 2.672e-05, MSE(pi1): 2.853e-03, MSE(pi2): 2.089e-05, MSE(pi3): 7.620e-04\n",
      "Epoch 15800, Train loss: 3.708e+02, Test loss: 6.908e+02, MSE(e): 2.661e-05, MSE(pi1): 2.853e-03, MSE(pi2): 2.083e-05, MSE(pi3): 7.618e-04\n",
      "Epoch 15900, Train loss: 3.697e+02, Test loss: 6.905e+02, MSE(e): 2.650e-05, MSE(pi1): 2.853e-03, MSE(pi2): 2.077e-05, MSE(pi3): 7.616e-04\n",
      "Epoch 16000, Train loss: 3.686e+02, Test loss: 6.901e+02, MSE(e): 2.639e-05, MSE(pi1): 2.854e-03, MSE(pi2): 2.071e-05, MSE(pi3): 7.615e-04\n",
      "Epoch 16100, Train loss: 3.675e+02, Test loss: 6.898e+02, MSE(e): 2.629e-05, MSE(pi1): 2.854e-03, MSE(pi2): 2.066e-05, MSE(pi3): 7.613e-04\n",
      "Epoch 16200, Train loss: 3.665e+02, Test loss: 6.896e+02, MSE(e): 2.618e-05, MSE(pi1): 2.853e-03, MSE(pi2): 2.060e-05, MSE(pi3): 7.611e-04\n",
      "Epoch 16300, Train loss: 3.655e+02, Test loss: 6.894e+02, MSE(e): 2.609e-05, MSE(pi1): 2.853e-03, MSE(pi2): 2.055e-05, MSE(pi3): 7.610e-04\n",
      "Epoch 16400, Train loss: 3.646e+02, Test loss: 6.892e+02, MSE(e): 2.599e-05, MSE(pi1): 2.853e-03, MSE(pi2): 2.050e-05, MSE(pi3): 7.608e-04\n",
      "Epoch 16500, Train loss: 3.636e+02, Test loss: 6.891e+02, MSE(e): 2.590e-05, MSE(pi1): 2.853e-03, MSE(pi2): 2.045e-05, MSE(pi3): 7.607e-04\n",
      "Epoch 16600, Train loss: 3.627e+02, Test loss: 6.889e+02, MSE(e): 2.581e-05, MSE(pi1): 2.853e-03, MSE(pi2): 2.040e-05, MSE(pi3): 7.605e-04\n",
      "Epoch 16700, Train loss: 3.618e+02, Test loss: 6.888e+02, MSE(e): 2.572e-05, MSE(pi1): 2.853e-03, MSE(pi2): 2.035e-05, MSE(pi3): 7.604e-04\n",
      "Epoch 16800, Train loss: 3.609e+02, Test loss: 6.888e+02, MSE(e): 2.564e-05, MSE(pi1): 2.853e-03, MSE(pi2): 2.031e-05, MSE(pi3): 7.603e-04\n",
      "Epoch 16900, Train loss: 3.601e+02, Test loss: 6.888e+02, MSE(e): 2.555e-05, MSE(pi1): 2.853e-03, MSE(pi2): 2.026e-05, MSE(pi3): 7.601e-04\n",
      "Epoch 17000, Train loss: 3.593e+02, Test loss: 6.889e+02, MSE(e): 2.547e-05, MSE(pi1): 2.852e-03, MSE(pi2): 2.022e-05, MSE(pi3): 7.600e-04\n",
      "Epoch 17100, Train loss: 3.584e+02, Test loss: 6.890e+02, MSE(e): 2.539e-05, MSE(pi1): 2.852e-03, MSE(pi2): 2.017e-05, MSE(pi3): 7.599e-04\n",
      "Epoch 17200, Train loss: 3.576e+02, Test loss: 6.892e+02, MSE(e): 2.531e-05, MSE(pi1): 2.852e-03, MSE(pi2): 2.013e-05, MSE(pi3): 7.598e-04\n",
      "Epoch 17300, Train loss: 3.569e+02, Test loss: 6.894e+02, MSE(e): 2.524e-05, MSE(pi1): 2.851e-03, MSE(pi2): 2.009e-05, MSE(pi3): 7.597e-04\n",
      "Epoch 17400, Train loss: 3.561e+02, Test loss: 6.896e+02, MSE(e): 2.516e-05, MSE(pi1): 2.851e-03, MSE(pi2): 2.005e-05, MSE(pi3): 7.595e-04\n",
      "Epoch 17500, Train loss: 3.554e+02, Test loss: 6.898e+02, MSE(e): 2.509e-05, MSE(pi1): 2.851e-03, MSE(pi2): 2.000e-05, MSE(pi3): 7.594e-04\n",
      "Epoch 17600, Train loss: 3.546e+02, Test loss: 6.901e+02, MSE(e): 2.502e-05, MSE(pi1): 2.851e-03, MSE(pi2): 1.996e-05, MSE(pi3): 7.593e-04\n",
      "Epoch 17700, Train loss: 3.539e+02, Test loss: 6.903e+02, MSE(e): 2.495e-05, MSE(pi1): 2.850e-03, MSE(pi2): 1.993e-05, MSE(pi3): 7.592e-04\n",
      "Epoch 17800, Train loss: 3.532e+02, Test loss: 6.907e+02, MSE(e): 2.488e-05, MSE(pi1): 2.850e-03, MSE(pi2): 1.989e-05, MSE(pi3): 7.592e-04\n",
      "Epoch 17900, Train loss: 3.525e+02, Test loss: 6.910e+02, MSE(e): 2.481e-05, MSE(pi1): 2.850e-03, MSE(pi2): 1.985e-05, MSE(pi3): 7.591e-04\n",
      "Epoch 18000, Train loss: 3.519e+02, Test loss: 6.914e+02, MSE(e): 2.475e-05, MSE(pi1): 2.849e-03, MSE(pi2): 1.981e-05, MSE(pi3): 7.590e-04\n",
      "Epoch 18100, Train loss: 3.512e+02, Test loss: 6.918e+02, MSE(e): 2.468e-05, MSE(pi1): 2.848e-03, MSE(pi2): 1.978e-05, MSE(pi3): 7.590e-04\n",
      "Epoch 18200, Train loss: 3.506e+02, Test loss: 6.922e+02, MSE(e): 2.462e-05, MSE(pi1): 2.848e-03, MSE(pi2): 1.974e-05, MSE(pi3): 7.589e-04\n",
      "Epoch 18300, Train loss: 3.499e+02, Test loss: 6.926e+02, MSE(e): 2.455e-05, MSE(pi1): 2.847e-03, MSE(pi2): 1.970e-05, MSE(pi3): 7.589e-04\n",
      "Epoch 18400, Train loss: 3.493e+02, Test loss: 6.931e+02, MSE(e): 2.449e-05, MSE(pi1): 2.846e-03, MSE(pi2): 1.967e-05, MSE(pi3): 7.589e-04\n",
      "Epoch 18500, Train loss: 3.487e+02, Test loss: 6.935e+02, MSE(e): 2.443e-05, MSE(pi1): 2.844e-03, MSE(pi2): 1.963e-05, MSE(pi3): 7.589e-04\n",
      "Epoch 18600, Train loss: 3.481e+02, Test loss: 6.941e+02, MSE(e): 2.438e-05, MSE(pi1): 2.844e-03, MSE(pi2): 1.960e-05, MSE(pi3): 7.588e-04\n",
      "Epoch 18700, Train loss: 3.475e+02, Test loss: 6.947e+02, MSE(e): 2.432e-05, MSE(pi1): 2.843e-03, MSE(pi2): 1.957e-05, MSE(pi3): 7.587e-04\n",
      "Epoch 18800, Train loss: 3.469e+02, Test loss: 6.953e+02, MSE(e): 2.426e-05, MSE(pi1): 2.842e-03, MSE(pi2): 1.953e-05, MSE(pi3): 7.587e-04\n",
      "Epoch 18900, Train loss: 3.463e+02, Test loss: 6.959e+02, MSE(e): 2.420e-05, MSE(pi1): 2.841e-03, MSE(pi2): 1.950e-05, MSE(pi3): 7.587e-04\n",
      "Epoch 19000, Train loss: 3.458e+02, Test loss: 6.966e+02, MSE(e): 2.415e-05, MSE(pi1): 2.840e-03, MSE(pi2): 1.947e-05, MSE(pi3): 7.587e-04\n",
      "Epoch 19100, Train loss: 3.452e+02, Test loss: 6.973e+02, MSE(e): 2.409e-05, MSE(pi1): 2.839e-03, MSE(pi2): 1.944e-05, MSE(pi3): 7.586e-04\n",
      "Epoch 19200, Train loss: 3.447e+02, Test loss: 6.980e+02, MSE(e): 2.404e-05, MSE(pi1): 2.839e-03, MSE(pi2): 1.940e-05, MSE(pi3): 7.586e-04\n",
      "Epoch 19300, Train loss: 3.441e+02, Test loss: 6.987e+02, MSE(e): 2.399e-05, MSE(pi1): 2.838e-03, MSE(pi2): 1.937e-05, MSE(pi3): 7.586e-04\n",
      "Epoch 19400, Train loss: 3.436e+02, Test loss: 6.994e+02, MSE(e): 2.394e-05, MSE(pi1): 2.837e-03, MSE(pi2): 1.934e-05, MSE(pi3): 7.585e-04\n",
      "Epoch 19500, Train loss: 3.431e+02, Test loss: 7.001e+02, MSE(e): 2.389e-05, MSE(pi1): 2.836e-03, MSE(pi2): 1.931e-05, MSE(pi3): 7.585e-04\n",
      "Epoch 19600, Train loss: 3.426e+02, Test loss: 7.008e+02, MSE(e): 2.383e-05, MSE(pi1): 2.835e-03, MSE(pi2): 1.928e-05, MSE(pi3): 7.585e-04\n",
      "Epoch 19700, Train loss: 3.421e+02, Test loss: 7.016e+02, MSE(e): 2.379e-05, MSE(pi1): 2.834e-03, MSE(pi2): 1.925e-05, MSE(pi3): 7.585e-04\n",
      "Epoch 19800, Train loss: 3.416e+02, Test loss: 7.024e+02, MSE(e): 2.374e-05, MSE(pi1): 2.833e-03, MSE(pi2): 1.922e-05, MSE(pi3): 7.585e-04\n",
      "Epoch 19900, Train loss: 3.411e+02, Test loss: 7.033e+02, MSE(e): 2.369e-05, MSE(pi1): 2.832e-03, MSE(pi2): 1.919e-05, MSE(pi3): 7.585e-04\n",
      "Epoch 20000, Train loss: 3.406e+02, Test loss: 7.042e+02, MSE(e): 2.364e-05, MSE(pi1): 2.831e-03, MSE(pi2): 1.916e-05, MSE(pi3): 7.585e-04\n",
      "Epoch 20100, Train loss: 3.401e+02, Test loss: 7.050e+02, MSE(e): 2.359e-05, MSE(pi1): 2.830e-03, MSE(pi2): 1.913e-05, MSE(pi3): 7.585e-04\n",
      "Epoch 20200, Train loss: 3.396e+02, Test loss: 7.059e+02, MSE(e): 2.355e-05, MSE(pi1): 2.829e-03, MSE(pi2): 1.910e-05, MSE(pi3): 7.585e-04\n",
      "Epoch 20300, Train loss: 3.392e+02, Test loss: 7.069e+02, MSE(e): 2.350e-05, MSE(pi1): 2.828e-03, MSE(pi2): 1.907e-05, MSE(pi3): 7.584e-04\n",
      "Epoch 20400, Train loss: 3.387e+02, Test loss: 7.078e+02, MSE(e): 2.345e-05, MSE(pi1): 2.827e-03, MSE(pi2): 1.905e-05, MSE(pi3): 7.584e-04\n",
      "Epoch 20500, Train loss: 3.382e+02, Test loss: 7.087e+02, MSE(e): 2.341e-05, MSE(pi1): 2.826e-03, MSE(pi2): 1.902e-05, MSE(pi3): 7.584e-04\n",
      "Epoch 20600, Train loss: 3.378e+02, Test loss: 7.097e+02, MSE(e): 2.337e-05, MSE(pi1): 2.826e-03, MSE(pi2): 1.899e-05, MSE(pi3): 7.584e-04\n",
      "Epoch 20700, Train loss: 3.373e+02, Test loss: 7.108e+02, MSE(e): 2.332e-05, MSE(pi1): 2.825e-03, MSE(pi2): 1.896e-05, MSE(pi3): 7.584e-04\n",
      "Epoch 20800, Train loss: 3.369e+02, Test loss: 7.119e+02, MSE(e): 2.328e-05, MSE(pi1): 2.825e-03, MSE(pi2): 1.893e-05, MSE(pi3): 7.583e-04\n",
      "Epoch 20900, Train loss: 3.364e+02, Test loss: 7.130e+02, MSE(e): 2.323e-05, MSE(pi1): 2.824e-03, MSE(pi2): 1.890e-05, MSE(pi3): 7.583e-04\n",
      "Epoch 21000, Train loss: 3.360e+02, Test loss: 7.141e+02, MSE(e): 2.319e-05, MSE(pi1): 2.823e-03, MSE(pi2): 1.888e-05, MSE(pi3): 7.583e-04\n",
      "Epoch 21100, Train loss: 3.355e+02, Test loss: 7.152e+02, MSE(e): 2.315e-05, MSE(pi1): 2.822e-03, MSE(pi2): 1.885e-05, MSE(pi3): 7.583e-04\n",
      "Epoch 21200, Train loss: 3.351e+02, Test loss: 7.163e+02, MSE(e): 2.310e-05, MSE(pi1): 2.821e-03, MSE(pi2): 1.882e-05, MSE(pi3): 7.583e-04\n",
      "Epoch 21300, Train loss: 3.347e+02, Test loss: 7.174e+02, MSE(e): 2.306e-05, MSE(pi1): 2.820e-03, MSE(pi2): 1.879e-05, MSE(pi3): 7.583e-04\n",
      "Epoch 21400, Train loss: 3.342e+02, Test loss: 7.185e+02, MSE(e): 2.302e-05, MSE(pi1): 2.819e-03, MSE(pi2): 1.876e-05, MSE(pi3): 7.583e-04\n",
      "Epoch 21500, Train loss: 3.338e+02, Test loss: 7.197e+02, MSE(e): 2.298e-05, MSE(pi1): 2.818e-03, MSE(pi2): 1.873e-05, MSE(pi3): 7.583e-04\n",
      "Epoch 21600, Train loss: 3.334e+02, Test loss: 7.208e+02, MSE(e): 2.293e-05, MSE(pi1): 2.817e-03, MSE(pi2): 1.870e-05, MSE(pi3): 7.583e-04\n",
      "Epoch 21700, Train loss: 3.329e+02, Test loss: 7.220e+02, MSE(e): 2.289e-05, MSE(pi1): 2.816e-03, MSE(pi2): 1.868e-05, MSE(pi3): 7.584e-04\n",
      "Epoch 21800, Train loss: 3.325e+02, Test loss: 7.231e+02, MSE(e): 2.285e-05, MSE(pi1): 2.815e-03, MSE(pi2): 1.865e-05, MSE(pi3): 7.584e-04\n",
      "Epoch 21900, Train loss: 3.321e+02, Test loss: 7.243e+02, MSE(e): 2.281e-05, MSE(pi1): 2.814e-03, MSE(pi2): 1.862e-05, MSE(pi3): 7.584e-04\n",
      "Epoch 22000, Train loss: 3.317e+02, Test loss: 7.255e+02, MSE(e): 2.277e-05, MSE(pi1): 2.812e-03, MSE(pi2): 1.859e-05, MSE(pi3): 7.585e-04\n",
      "Epoch 22100, Train loss: 3.312e+02, Test loss: 7.267e+02, MSE(e): 2.272e-05, MSE(pi1): 2.811e-03, MSE(pi2): 1.856e-05, MSE(pi3): 7.585e-04\n",
      "Epoch 22200, Train loss: 3.308e+02, Test loss: 7.278e+02, MSE(e): 2.268e-05, MSE(pi1): 2.810e-03, MSE(pi2): 1.853e-05, MSE(pi3): 7.586e-04\n",
      "Epoch 22300, Train loss: 3.304e+02, Test loss: 7.291e+02, MSE(e): 2.264e-05, MSE(pi1): 2.809e-03, MSE(pi2): 1.850e-05, MSE(pi3): 7.586e-04\n",
      "Epoch 22400, Train loss: 3.299e+02, Test loss: 7.303e+02, MSE(e): 2.260e-05, MSE(pi1): 2.807e-03, MSE(pi2): 1.847e-05, MSE(pi3): 7.587e-04\n",
      "Epoch 22500, Train loss: 3.295e+02, Test loss: 7.316e+02, MSE(e): 2.256e-05, MSE(pi1): 2.806e-03, MSE(pi2): 1.844e-05, MSE(pi3): 7.588e-04\n",
      "Epoch 22600, Train loss: 3.291e+02, Test loss: 7.328e+02, MSE(e): 2.251e-05, MSE(pi1): 2.804e-03, MSE(pi2): 1.841e-05, MSE(pi3): 7.589e-04\n",
      "Epoch 22700, Train loss: 3.286e+02, Test loss: 7.340e+02, MSE(e): 2.247e-05, MSE(pi1): 2.803e-03, MSE(pi2): 1.838e-05, MSE(pi3): 7.589e-04\n",
      "Epoch 22800, Train loss: 3.282e+02, Test loss: 7.352e+02, MSE(e): 2.243e-05, MSE(pi1): 2.802e-03, MSE(pi2): 1.834e-05, MSE(pi3): 7.589e-04\n",
      "Epoch 22900, Train loss: 3.278e+02, Test loss: 7.366e+02, MSE(e): 2.238e-05, MSE(pi1): 2.801e-03, MSE(pi2): 1.831e-05, MSE(pi3): 7.589e-04\n",
      "Epoch 23000, Train loss: 3.273e+02, Test loss: 7.379e+02, MSE(e): 2.234e-05, MSE(pi1): 2.799e-03, MSE(pi2): 1.828e-05, MSE(pi3): 7.590e-04\n",
      "Epoch 23100, Train loss: 3.269e+02, Test loss: 7.393e+02, MSE(e): 2.230e-05, MSE(pi1): 2.797e-03, MSE(pi2): 1.824e-05, MSE(pi3): 7.591e-04\n",
      "Epoch 23200, Train loss: 3.264e+02, Test loss: 7.406e+02, MSE(e): 2.225e-05, MSE(pi1): 2.795e-03, MSE(pi2): 1.821e-05, MSE(pi3): 7.592e-04\n",
      "Epoch 23300, Train loss: 3.259e+02, Test loss: 7.420e+02, MSE(e): 2.220e-05, MSE(pi1): 2.793e-03, MSE(pi2): 1.817e-05, MSE(pi3): 7.593e-04\n",
      "Epoch 23400, Train loss: 3.255e+02, Test loss: 7.433e+02, MSE(e): 2.216e-05, MSE(pi1): 2.791e-03, MSE(pi2): 1.814e-05, MSE(pi3): 7.595e-04\n",
      "Epoch 23500, Train loss: 3.250e+02, Test loss: 7.448e+02, MSE(e): 2.211e-05, MSE(pi1): 2.789e-03, MSE(pi2): 1.810e-05, MSE(pi3): 7.596e-04\n",
      "Epoch 23600, Train loss: 3.245e+02, Test loss: 7.461e+02, MSE(e): 2.206e-05, MSE(pi1): 2.787e-03, MSE(pi2): 1.806e-05, MSE(pi3): 7.598e-04\n",
      "Epoch 23700, Train loss: 3.240e+02, Test loss: 7.475e+02, MSE(e): 2.201e-05, MSE(pi1): 2.784e-03, MSE(pi2): 1.802e-05, MSE(pi3): 7.599e-04\n",
      "Epoch 23800, Train loss: 3.235e+02, Test loss: 7.489e+02, MSE(e): 2.196e-05, MSE(pi1): 2.782e-03, MSE(pi2): 1.797e-05, MSE(pi3): 7.601e-04\n",
      "Epoch 23900, Train loss: 3.230e+02, Test loss: 7.503e+02, MSE(e): 2.191e-05, MSE(pi1): 2.779e-03, MSE(pi2): 1.793e-05, MSE(pi3): 7.603e-04\n",
      "Epoch 24000, Train loss: 3.224e+02, Test loss: 7.518e+02, MSE(e): 2.186e-05, MSE(pi1): 2.777e-03, MSE(pi2): 1.788e-05, MSE(pi3): 7.604e-04\n",
      "Epoch 24100, Train loss: 3.219e+02, Test loss: 7.533e+02, MSE(e): 2.181e-05, MSE(pi1): 2.774e-03, MSE(pi2): 1.784e-05, MSE(pi3): 7.606e-04\n",
      "Epoch 24200, Train loss: 3.214e+02, Test loss: 7.548e+02, MSE(e): 2.176e-05, MSE(pi1): 2.772e-03, MSE(pi2): 1.779e-05, MSE(pi3): 7.608e-04\n",
      "Epoch 24300, Train loss: 3.209e+02, Test loss: 7.563e+02, MSE(e): 2.171e-05, MSE(pi1): 2.770e-03, MSE(pi2): 1.775e-05, MSE(pi3): 7.610e-04\n",
      "Epoch 24400, Train loss: 3.204e+02, Test loss: 7.578e+02, MSE(e): 2.166e-05, MSE(pi1): 2.767e-03, MSE(pi2): 1.770e-05, MSE(pi3): 7.611e-04\n",
      "Epoch 24500, Train loss: 3.199e+02, Test loss: 7.594e+02, MSE(e): 2.161e-05, MSE(pi1): 2.765e-03, MSE(pi2): 1.766e-05, MSE(pi3): 7.613e-04\n",
      "Epoch 24600, Train loss: 3.194e+02, Test loss: 7.609e+02, MSE(e): 2.156e-05, MSE(pi1): 2.763e-03, MSE(pi2): 1.762e-05, MSE(pi3): 7.614e-04\n",
      "Epoch 24700, Train loss: 3.189e+02, Test loss: 7.625e+02, MSE(e): 2.151e-05, MSE(pi1): 2.761e-03, MSE(pi2): 1.758e-05, MSE(pi3): 7.616e-04\n",
      "Epoch 24800, Train loss: 3.185e+02, Test loss: 7.641e+02, MSE(e): 2.147e-05, MSE(pi1): 2.759e-03, MSE(pi2): 1.754e-05, MSE(pi3): 7.617e-04\n",
      "Epoch 24900, Train loss: 3.180e+02, Test loss: 7.657e+02, MSE(e): 2.142e-05, MSE(pi1): 2.757e-03, MSE(pi2): 1.750e-05, MSE(pi3): 7.618e-04\n",
      "Epoch 25000, Train loss: 3.175e+02, Test loss: 7.673e+02, MSE(e): 2.137e-05, MSE(pi1): 2.755e-03, MSE(pi2): 1.746e-05, MSE(pi3): 7.619e-04\n",
      "Epoch 25100, Train loss: 3.170e+02, Test loss: 7.690e+02, MSE(e): 2.133e-05, MSE(pi1): 2.753e-03, MSE(pi2): 1.742e-05, MSE(pi3): 7.620e-04\n",
      "Epoch 25200, Train loss: 3.166e+02, Test loss: 7.706e+02, MSE(e): 2.128e-05, MSE(pi1): 2.751e-03, MSE(pi2): 1.738e-05, MSE(pi3): 7.622e-04\n",
      "Epoch 25300, Train loss: 3.161e+02, Test loss: 7.722e+02, MSE(e): 2.124e-05, MSE(pi1): 2.750e-03, MSE(pi2): 1.734e-05, MSE(pi3): 7.623e-04\n",
      "Epoch 25400, Train loss: 3.157e+02, Test loss: 7.738e+02, MSE(e): 2.119e-05, MSE(pi1): 2.748e-03, MSE(pi2): 1.731e-05, MSE(pi3): 7.624e-04\n",
      "Epoch 25500, Train loss: 3.152e+02, Test loss: 7.754e+02, MSE(e): 2.115e-05, MSE(pi1): 2.746e-03, MSE(pi2): 1.727e-05, MSE(pi3): 7.625e-04\n",
      "Epoch 25600, Train loss: 3.148e+02, Test loss: 7.770e+02, MSE(e): 2.111e-05, MSE(pi1): 2.744e-03, MSE(pi2): 1.723e-05, MSE(pi3): 7.626e-04\n",
      "Epoch 25700, Train loss: 3.144e+02, Test loss: 7.787e+02, MSE(e): 2.107e-05, MSE(pi1): 2.743e-03, MSE(pi2): 1.720e-05, MSE(pi3): 7.627e-04\n",
      "Epoch 25800, Train loss: 3.140e+02, Test loss: 7.804e+02, MSE(e): 2.102e-05, MSE(pi1): 2.741e-03, MSE(pi2): 1.716e-05, MSE(pi3): 7.628e-04\n",
      "Epoch 25900, Train loss: 3.135e+02, Test loss: 7.820e+02, MSE(e): 2.098e-05, MSE(pi1): 2.739e-03, MSE(pi2): 1.713e-05, MSE(pi3): 7.630e-04\n",
      "Epoch 26000, Train loss: 3.131e+02, Test loss: 7.836e+02, MSE(e): 2.094e-05, MSE(pi1): 2.737e-03, MSE(pi2): 1.709e-05, MSE(pi3): 7.631e-04\n",
      "Epoch 26100, Train loss: 3.127e+02, Test loss: 7.853e+02, MSE(e): 2.090e-05, MSE(pi1): 2.736e-03, MSE(pi2): 1.706e-05, MSE(pi3): 7.632e-04\n",
      "Epoch 26200, Train loss: 3.123e+02, Test loss: 7.870e+02, MSE(e): 2.086e-05, MSE(pi1): 2.734e-03, MSE(pi2): 1.703e-05, MSE(pi3): 7.633e-04\n",
      "Epoch 26300, Train loss: 3.119e+02, Test loss: 7.887e+02, MSE(e): 2.082e-05, MSE(pi1): 2.732e-03, MSE(pi2): 1.699e-05, MSE(pi3): 7.634e-04\n",
      "Epoch 26400, Train loss: 3.115e+02, Test loss: 7.904e+02, MSE(e): 2.078e-05, MSE(pi1): 2.731e-03, MSE(pi2): 1.696e-05, MSE(pi3): 7.635e-04\n",
      "Epoch 26500, Train loss: 3.111e+02, Test loss: 7.921e+02, MSE(e): 2.074e-05, MSE(pi1): 2.729e-03, MSE(pi2): 1.693e-05, MSE(pi3): 7.636e-04\n",
      "Epoch 26600, Train loss: 3.107e+02, Test loss: 7.938e+02, MSE(e): 2.070e-05, MSE(pi1): 2.727e-03, MSE(pi2): 1.690e-05, MSE(pi3): 7.637e-04\n",
      "Epoch 26700, Train loss: 3.103e+02, Test loss: 7.955e+02, MSE(e): 2.066e-05, MSE(pi1): 2.725e-03, MSE(pi2): 1.687e-05, MSE(pi3): 7.638e-04\n",
      "Epoch 26800, Train loss: 3.099e+02, Test loss: 7.971e+02, MSE(e): 2.063e-05, MSE(pi1): 2.724e-03, MSE(pi2): 1.684e-05, MSE(pi3): 7.640e-04\n",
      "Epoch 26900, Train loss: 3.095e+02, Test loss: 7.989e+02, MSE(e): 2.059e-05, MSE(pi1): 2.722e-03, MSE(pi2): 1.680e-05, MSE(pi3): 7.640e-04\n",
      "Epoch 27000, Train loss: 3.091e+02, Test loss: 8.006e+02, MSE(e): 2.055e-05, MSE(pi1): 2.720e-03, MSE(pi2): 1.677e-05, MSE(pi3): 7.641e-04\n",
      "Epoch 27100, Train loss: 3.088e+02, Test loss: 8.022e+02, MSE(e): 2.052e-05, MSE(pi1): 2.719e-03, MSE(pi2): 1.675e-05, MSE(pi3): 7.642e-04\n",
      "Epoch 27200, Train loss: 3.084e+02, Test loss: 8.038e+02, MSE(e): 2.048e-05, MSE(pi1): 2.717e-03, MSE(pi2): 1.672e-05, MSE(pi3): 7.643e-04\n",
      "Epoch 27300, Train loss: 3.081e+02, Test loss: 8.054e+02, MSE(e): 2.045e-05, MSE(pi1): 2.716e-03, MSE(pi2): 1.669e-05, MSE(pi3): 7.644e-04\n",
      "Epoch 27400, Train loss: 3.077e+02, Test loss: 8.070e+02, MSE(e): 2.041e-05, MSE(pi1): 2.714e-03, MSE(pi2): 1.666e-05, MSE(pi3): 7.645e-04\n",
      "Epoch 27500, Train loss: 3.074e+02, Test loss: 8.086e+02, MSE(e): 2.038e-05, MSE(pi1): 2.713e-03, MSE(pi2): 1.664e-05, MSE(pi3): 7.646e-04\n",
      "Epoch 27600, Train loss: 3.071e+02, Test loss: 8.101e+02, MSE(e): 2.035e-05, MSE(pi1): 2.712e-03, MSE(pi2): 1.661e-05, MSE(pi3): 7.646e-04\n",
      "Epoch 27700, Train loss: 3.068e+02, Test loss: 8.117e+02, MSE(e): 2.032e-05, MSE(pi1): 2.711e-03, MSE(pi2): 1.658e-05, MSE(pi3): 7.647e-04\n",
      "Epoch 27800, Train loss: 3.064e+02, Test loss: 8.134e+02, MSE(e): 2.028e-05, MSE(pi1): 2.709e-03, MSE(pi2): 1.656e-05, MSE(pi3): 7.648e-04\n",
      "Epoch 27900, Train loss: 3.061e+02, Test loss: 8.151e+02, MSE(e): 2.025e-05, MSE(pi1): 2.708e-03, MSE(pi2): 1.654e-05, MSE(pi3): 7.648e-04\n",
      "Epoch 28000, Train loss: 3.058e+02, Test loss: 8.167e+02, MSE(e): 2.022e-05, MSE(pi1): 2.707e-03, MSE(pi2): 1.651e-05, MSE(pi3): 7.649e-04\n",
      "Epoch 28100, Train loss: 3.055e+02, Test loss: 8.183e+02, MSE(e): 2.019e-05, MSE(pi1): 2.706e-03, MSE(pi2): 1.649e-05, MSE(pi3): 7.650e-04\n",
      "Epoch 28200, Train loss: 3.052e+02, Test loss: 8.199e+02, MSE(e): 2.016e-05, MSE(pi1): 2.705e-03, MSE(pi2): 1.646e-05, MSE(pi3): 7.650e-04\n",
      "Epoch 28300, Train loss: 3.049e+02, Test loss: 8.214e+02, MSE(e): 2.013e-05, MSE(pi1): 2.704e-03, MSE(pi2): 1.644e-05, MSE(pi3): 7.651e-04\n",
      "Epoch 28400, Train loss: 3.046e+02, Test loss: 8.230e+02, MSE(e): 2.011e-05, MSE(pi1): 2.702e-03, MSE(pi2): 1.642e-05, MSE(pi3): 7.651e-04\n",
      "Epoch 28500, Train loss: 3.043e+02, Test loss: 8.245e+02, MSE(e): 2.008e-05, MSE(pi1): 2.702e-03, MSE(pi2): 1.640e-05, MSE(pi3): 7.652e-04\n",
      "Epoch 28600, Train loss: 3.040e+02, Test loss: 8.260e+02, MSE(e): 2.005e-05, MSE(pi1): 2.701e-03, MSE(pi2): 1.638e-05, MSE(pi3): 7.652e-04\n",
      "Epoch 28700, Train loss: 3.038e+02, Test loss: 8.275e+02, MSE(e): 2.002e-05, MSE(pi1): 2.700e-03, MSE(pi2): 1.635e-05, MSE(pi3): 7.652e-04\n",
      "Epoch 28800, Train loss: 3.035e+02, Test loss: 8.290e+02, MSE(e): 2.000e-05, MSE(pi1): 2.699e-03, MSE(pi2): 1.633e-05, MSE(pi3): 7.653e-04\n",
      "Epoch 28900, Train loss: 3.032e+02, Test loss: 8.305e+02, MSE(e): 1.997e-05, MSE(pi1): 2.698e-03, MSE(pi2): 1.631e-05, MSE(pi3): 7.653e-04\n",
      "Epoch 29000, Train loss: 3.029e+02, Test loss: 8.319e+02, MSE(e): 1.994e-05, MSE(pi1): 2.697e-03, MSE(pi2): 1.629e-05, MSE(pi3): 7.654e-04\n",
      "Epoch 29100, Train loss: 3.027e+02, Test loss: 8.334e+02, MSE(e): 1.992e-05, MSE(pi1): 2.696e-03, MSE(pi2): 1.627e-05, MSE(pi3): 7.654e-04\n",
      "Epoch 29200, Train loss: 3.024e+02, Test loss: 8.348e+02, MSE(e): 1.989e-05, MSE(pi1): 2.695e-03, MSE(pi2): 1.625e-05, MSE(pi3): 7.654e-04\n",
      "Epoch 29300, Train loss: 3.022e+02, Test loss: 8.363e+02, MSE(e): 1.987e-05, MSE(pi1): 2.694e-03, MSE(pi2): 1.623e-05, MSE(pi3): 7.655e-04\n",
      "Epoch 29400, Train loss: 3.019e+02, Test loss: 8.377e+02, MSE(e): 1.984e-05, MSE(pi1): 2.694e-03, MSE(pi2): 1.622e-05, MSE(pi3): 7.655e-04\n",
      "Epoch 29500, Train loss: 3.017e+02, Test loss: 8.392e+02, MSE(e): 1.982e-05, MSE(pi1): 2.693e-03, MSE(pi2): 1.620e-05, MSE(pi3): 7.656e-04\n",
      "Epoch 29600, Train loss: 3.014e+02, Test loss: 8.406e+02, MSE(e): 1.979e-05, MSE(pi1): 2.692e-03, MSE(pi2): 1.618e-05, MSE(pi3): 7.656e-04\n",
      "Epoch 29700, Train loss: 3.012e+02, Test loss: 8.419e+02, MSE(e): 1.977e-05, MSE(pi1): 2.691e-03, MSE(pi2): 1.616e-05, MSE(pi3): 7.656e-04\n",
      "Epoch 29800, Train loss: 3.009e+02, Test loss: 8.433e+02, MSE(e): 1.975e-05, MSE(pi1): 2.690e-03, MSE(pi2): 1.614e-05, MSE(pi3): 7.657e-04\n",
      "Epoch 29900, Train loss: 3.007e+02, Test loss: 8.445e+02, MSE(e): 1.972e-05, MSE(pi1): 2.689e-03, MSE(pi2): 1.612e-05, MSE(pi3): 7.657e-04\n",
      "Epoch 30000, Train loss: 3.005e+02, Test loss: 8.459e+02, MSE(e): 1.970e-05, MSE(pi1): 2.689e-03, MSE(pi2): 1.611e-05, MSE(pi3): 7.657e-04\n",
      "Epoch 30100, Train loss: 3.002e+02, Test loss: 8.471e+02, MSE(e): 1.968e-05, MSE(pi1): 2.688e-03, MSE(pi2): 1.609e-05, MSE(pi3): 7.658e-04\n",
      "Epoch 30200, Train loss: 3.000e+02, Test loss: 8.483e+02, MSE(e): 1.965e-05, MSE(pi1): 2.687e-03, MSE(pi2): 1.607e-05, MSE(pi3): 7.658e-04\n",
      "Epoch 30300, Train loss: 2.998e+02, Test loss: 8.495e+02, MSE(e): 1.963e-05, MSE(pi1): 2.686e-03, MSE(pi2): 1.605e-05, MSE(pi3): 7.658e-04\n",
      "Epoch 30400, Train loss: 2.996e+02, Test loss: 8.507e+02, MSE(e): 1.961e-05, MSE(pi1): 2.685e-03, MSE(pi2): 1.604e-05, MSE(pi3): 7.659e-04\n",
      "Epoch 30500, Train loss: 2.993e+02, Test loss: 8.519e+02, MSE(e): 1.959e-05, MSE(pi1): 2.685e-03, MSE(pi2): 1.602e-05, MSE(pi3): 7.659e-04\n",
      "Epoch 30600, Train loss: 2.991e+02, Test loss: 8.531e+02, MSE(e): 1.957e-05, MSE(pi1): 2.684e-03, MSE(pi2): 1.600e-05, MSE(pi3): 7.659e-04\n",
      "Epoch 30700, Train loss: 2.989e+02, Test loss: 8.543e+02, MSE(e): 1.954e-05, MSE(pi1): 2.683e-03, MSE(pi2): 1.599e-05, MSE(pi3): 7.660e-04\n",
      "Epoch 30800, Train loss: 2.987e+02, Test loss: 8.554e+02, MSE(e): 1.952e-05, MSE(pi1): 2.682e-03, MSE(pi2): 1.597e-05, MSE(pi3): 7.660e-04\n",
      "Epoch 30900, Train loss: 2.985e+02, Test loss: 8.565e+02, MSE(e): 1.950e-05, MSE(pi1): 2.682e-03, MSE(pi2): 1.596e-05, MSE(pi3): 7.660e-04\n",
      "Epoch 31000, Train loss: 2.982e+02, Test loss: 8.575e+02, MSE(e): 1.948e-05, MSE(pi1): 2.681e-03, MSE(pi2): 1.594e-05, MSE(pi3): 7.660e-04\n",
      "Epoch 31100, Train loss: 2.980e+02, Test loss: 8.586e+02, MSE(e): 1.946e-05, MSE(pi1): 2.680e-03, MSE(pi2): 1.593e-05, MSE(pi3): 7.660e-04\n",
      "Epoch 31200, Train loss: 2.978e+02, Test loss: 8.597e+02, MSE(e): 1.944e-05, MSE(pi1): 2.680e-03, MSE(pi2): 1.591e-05, MSE(pi3): 7.660e-04\n",
      "Epoch 31300, Train loss: 2.976e+02, Test loss: 8.607e+02, MSE(e): 1.942e-05, MSE(pi1): 2.680e-03, MSE(pi2): 1.590e-05, MSE(pi3): 7.660e-04\n",
      "Epoch 31400, Train loss: 2.974e+02, Test loss: 8.617e+02, MSE(e): 1.940e-05, MSE(pi1): 2.680e-03, MSE(pi2): 1.588e-05, MSE(pi3): 7.660e-04\n",
      "Epoch 31500, Train loss: 2.972e+02, Test loss: 8.627e+02, MSE(e): 1.938e-05, MSE(pi1): 2.681e-03, MSE(pi2): 1.587e-05, MSE(pi3): 7.658e-04\n",
      "Epoch 31600, Train loss: 2.970e+02, Test loss: 8.637e+02, MSE(e): 1.936e-05, MSE(pi1): 2.682e-03, MSE(pi2): 1.585e-05, MSE(pi3): 7.657e-04\n",
      "Epoch 31700, Train loss: 2.968e+02, Test loss: 8.646e+02, MSE(e): 1.934e-05, MSE(pi1): 2.680e-03, MSE(pi2): 1.584e-05, MSE(pi3): 7.658e-04\n",
      "Epoch 31800, Train loss: 2.966e+02, Test loss: 8.656e+02, MSE(e): 1.932e-05, MSE(pi1): 2.680e-03, MSE(pi2): 1.582e-05, MSE(pi3): 7.658e-04\n",
      "Epoch 31900, Train loss: 2.964e+02, Test loss: 8.666e+02, MSE(e): 1.930e-05, MSE(pi1): 2.679e-03, MSE(pi2): 1.581e-05, MSE(pi3): 7.658e-04\n",
      "Epoch 32000, Train loss: 2.962e+02, Test loss: 8.675e+02, MSE(e): 1.928e-05, MSE(pi1): 2.678e-03, MSE(pi2): 1.579e-05, MSE(pi3): 7.659e-04\n",
      "Epoch 32100, Train loss: 2.960e+02, Test loss: 8.685e+02, MSE(e): 1.926e-05, MSE(pi1): 2.678e-03, MSE(pi2): 1.578e-05, MSE(pi3): 7.659e-04\n",
      "Epoch 32200, Train loss: 2.958e+02, Test loss: 8.695e+02, MSE(e): 1.924e-05, MSE(pi1): 2.677e-03, MSE(pi2): 1.577e-05, MSE(pi3): 7.659e-04\n",
      "Epoch 32300, Train loss: 2.956e+02, Test loss: 8.704e+02, MSE(e): 1.923e-05, MSE(pi1): 2.676e-03, MSE(pi2): 1.575e-05, MSE(pi3): 7.659e-04\n",
      "Epoch 32400, Train loss: 2.954e+02, Test loss: 8.713e+02, MSE(e): 1.921e-05, MSE(pi1): 2.676e-03, MSE(pi2): 1.574e-05, MSE(pi3): 7.660e-04\n",
      "Epoch 32500, Train loss: 2.952e+02, Test loss: 8.723e+02, MSE(e): 1.919e-05, MSE(pi1): 2.675e-03, MSE(pi2): 1.573e-05, MSE(pi3): 7.660e-04\n",
      "Epoch 32600, Train loss: 2.951e+02, Test loss: 8.731e+02, MSE(e): 1.917e-05, MSE(pi1): 2.674e-03, MSE(pi2): 1.571e-05, MSE(pi3): 7.660e-04\n",
      "Epoch 32700, Train loss: 2.949e+02, Test loss: 8.740e+02, MSE(e): 1.915e-05, MSE(pi1): 2.673e-03, MSE(pi2): 1.570e-05, MSE(pi3): 7.661e-04\n",
      "Epoch 32800, Train loss: 2.947e+02, Test loss: 8.749e+02, MSE(e): 1.913e-05, MSE(pi1): 2.673e-03, MSE(pi2): 1.569e-05, MSE(pi3): 7.661e-04\n",
      "Epoch 32900, Train loss: 2.945e+02, Test loss: 8.757e+02, MSE(e): 1.912e-05, MSE(pi1): 2.672e-03, MSE(pi2): 1.567e-05, MSE(pi3): 7.661e-04\n",
      "Epoch 33000, Train loss: 2.943e+02, Test loss: 8.767e+02, MSE(e): 1.910e-05, MSE(pi1): 2.672e-03, MSE(pi2): 1.566e-05, MSE(pi3): 7.662e-04\n",
      "Epoch 33100, Train loss: 2.942e+02, Test loss: 8.776e+02, MSE(e): 1.908e-05, MSE(pi1): 2.671e-03, MSE(pi2): 1.565e-05, MSE(pi3): 7.662e-04\n",
      "Epoch 33200, Train loss: 2.940e+02, Test loss: 8.784e+02, MSE(e): 1.906e-05, MSE(pi1): 2.670e-03, MSE(pi2): 1.563e-05, MSE(pi3): 7.662e-04\n",
      "Epoch 33300, Train loss: 2.938e+02, Test loss: 8.791e+02, MSE(e): 1.905e-05, MSE(pi1): 2.669e-03, MSE(pi2): 1.562e-05, MSE(pi3): 7.662e-04\n",
      "Epoch 33400, Train loss: 2.936e+02, Test loss: 8.799e+02, MSE(e): 1.903e-05, MSE(pi1): 2.669e-03, MSE(pi2): 1.561e-05, MSE(pi3): 7.663e-04\n",
      "Epoch 33500, Train loss: 2.934e+02, Test loss: 8.807e+02, MSE(e): 1.901e-05, MSE(pi1): 2.668e-03, MSE(pi2): 1.560e-05, MSE(pi3): 7.663e-04\n",
      "Epoch 33600, Train loss: 2.933e+02, Test loss: 8.815e+02, MSE(e): 1.900e-05, MSE(pi1): 2.667e-03, MSE(pi2): 1.558e-05, MSE(pi3): 7.664e-04\n",
      "Epoch 33700, Train loss: 2.931e+02, Test loss: 8.823e+02, MSE(e): 1.898e-05, MSE(pi1): 2.666e-03, MSE(pi2): 1.557e-05, MSE(pi3): 7.664e-04\n",
      "Epoch 33800, Train loss: 2.929e+02, Test loss: 8.831e+02, MSE(e): 1.896e-05, MSE(pi1): 2.666e-03, MSE(pi2): 1.556e-05, MSE(pi3): 7.664e-04\n",
      "Epoch 33900, Train loss: 2.928e+02, Test loss: 8.839e+02, MSE(e): 1.894e-05, MSE(pi1): 2.665e-03, MSE(pi2): 1.555e-05, MSE(pi3): 7.664e-04\n",
      "Epoch 34000, Train loss: 2.926e+02, Test loss: 8.846e+02, MSE(e): 1.893e-05, MSE(pi1): 2.664e-03, MSE(pi2): 1.554e-05, MSE(pi3): 7.665e-04\n",
      "Epoch 34100, Train loss: 2.924e+02, Test loss: 8.854e+02, MSE(e): 1.891e-05, MSE(pi1): 2.664e-03, MSE(pi2): 1.552e-05, MSE(pi3): 7.665e-04\n",
      "Epoch 34200, Train loss: 2.923e+02, Test loss: 8.861e+02, MSE(e): 1.890e-05, MSE(pi1): 2.664e-03, MSE(pi2): 1.551e-05, MSE(pi3): 7.665e-04\n",
      "Epoch 34300, Train loss: 2.921e+02, Test loss: 8.868e+02, MSE(e): 1.888e-05, MSE(pi1): 2.662e-03, MSE(pi2): 1.550e-05, MSE(pi3): 7.666e-04\n",
      "Epoch 34400, Train loss: 2.919e+02, Test loss: 8.876e+02, MSE(e): 1.886e-05, MSE(pi1): 2.662e-03, MSE(pi2): 1.549e-05, MSE(pi3): 7.666e-04\n",
      "Epoch 34500, Train loss: 2.918e+02, Test loss: 8.883e+02, MSE(e): 1.885e-05, MSE(pi1): 2.662e-03, MSE(pi2): 1.548e-05, MSE(pi3): 7.666e-04\n",
      "Epoch 34600, Train loss: 2.916e+02, Test loss: 8.891e+02, MSE(e): 1.883e-05, MSE(pi1): 2.661e-03, MSE(pi2): 1.546e-05, MSE(pi3): 7.666e-04\n",
      "Epoch 34700, Train loss: 2.914e+02, Test loss: 8.899e+02, MSE(e): 1.882e-05, MSE(pi1): 2.660e-03, MSE(pi2): 1.545e-05, MSE(pi3): 7.667e-04\n",
      "Epoch 34800, Train loss: 2.913e+02, Test loss: 8.907e+02, MSE(e): 1.880e-05, MSE(pi1): 2.660e-03, MSE(pi2): 1.544e-05, MSE(pi3): 7.667e-04\n",
      "Epoch 34900, Train loss: 2.911e+02, Test loss: 8.914e+02, MSE(e): 1.879e-05, MSE(pi1): 2.659e-03, MSE(pi2): 1.543e-05, MSE(pi3): 7.667e-04\n",
      "Epoch 35000, Train loss: 2.910e+02, Test loss: 8.920e+02, MSE(e): 1.877e-05, MSE(pi1): 2.659e-03, MSE(pi2): 1.542e-05, MSE(pi3): 7.667e-04\n",
      "Epoch 35100, Train loss: 2.908e+02, Test loss: 8.927e+02, MSE(e): 1.876e-05, MSE(pi1): 2.658e-03, MSE(pi2): 1.541e-05, MSE(pi3): 7.668e-04\n",
      "Epoch 35200, Train loss: 2.907e+02, Test loss: 8.935e+02, MSE(e): 1.874e-05, MSE(pi1): 2.657e-03, MSE(pi2): 1.540e-05, MSE(pi3): 7.668e-04\n",
      "Epoch 35300, Train loss: 2.905e+02, Test loss: 8.941e+02, MSE(e): 1.873e-05, MSE(pi1): 2.657e-03, MSE(pi2): 1.539e-05, MSE(pi3): 7.668e-04\n",
      "Epoch 35400, Train loss: 2.904e+02, Test loss: 8.949e+02, MSE(e): 1.871e-05, MSE(pi1): 2.656e-03, MSE(pi2): 1.538e-05, MSE(pi3): 7.669e-04\n",
      "Epoch 35500, Train loss: 2.902e+02, Test loss: 8.955e+02, MSE(e): 1.870e-05, MSE(pi1): 2.656e-03, MSE(pi2): 1.537e-05, MSE(pi3): 7.669e-04\n",
      "Epoch 35600, Train loss: 2.901e+02, Test loss: 8.962e+02, MSE(e): 1.868e-05, MSE(pi1): 2.655e-03, MSE(pi2): 1.536e-05, MSE(pi3): 7.669e-04\n",
      "Epoch 35700, Train loss: 2.899e+02, Test loss: 8.968e+02, MSE(e): 1.867e-05, MSE(pi1): 2.654e-03, MSE(pi2): 1.535e-05, MSE(pi3): 7.669e-04\n",
      "Epoch 35800, Train loss: 2.898e+02, Test loss: 8.975e+02, MSE(e): 1.866e-05, MSE(pi1): 2.654e-03, MSE(pi2): 1.533e-05, MSE(pi3): 7.670e-04\n",
      "Epoch 35900, Train loss: 2.897e+02, Test loss: 8.982e+02, MSE(e): 1.864e-05, MSE(pi1): 2.653e-03, MSE(pi2): 1.532e-05, MSE(pi3): 7.670e-04\n",
      "Epoch 36000, Train loss: 2.895e+02, Test loss: 8.989e+02, MSE(e): 1.863e-05, MSE(pi1): 2.653e-03, MSE(pi2): 1.531e-05, MSE(pi3): 7.670e-04\n",
      "Epoch 36100, Train loss: 2.894e+02, Test loss: 8.995e+02, MSE(e): 1.861e-05, MSE(pi1): 2.652e-03, MSE(pi2): 1.530e-05, MSE(pi3): 7.671e-04\n",
      "Epoch 36200, Train loss: 2.892e+02, Test loss: 9.002e+02, MSE(e): 1.860e-05, MSE(pi1): 2.651e-03, MSE(pi2): 1.529e-05, MSE(pi3): 7.671e-04\n",
      "Epoch 36300, Train loss: 2.891e+02, Test loss: 9.009e+02, MSE(e): 1.858e-05, MSE(pi1): 2.651e-03, MSE(pi2): 1.528e-05, MSE(pi3): 7.671e-04\n",
      "Epoch 36400, Train loss: 2.890e+02, Test loss: 9.016e+02, MSE(e): 1.857e-05, MSE(pi1): 2.650e-03, MSE(pi2): 1.527e-05, MSE(pi3): 7.672e-04\n",
      "Epoch 36500, Train loss: 2.888e+02, Test loss: 9.023e+02, MSE(e): 1.856e-05, MSE(pi1): 2.649e-03, MSE(pi2): 1.526e-05, MSE(pi3): 7.672e-04\n",
      "Epoch 36600, Train loss: 2.887e+02, Test loss: 9.030e+02, MSE(e): 1.854e-05, MSE(pi1): 2.649e-03, MSE(pi2): 1.525e-05, MSE(pi3): 7.672e-04\n",
      "Epoch 36700, Train loss: 2.885e+02, Test loss: 9.036e+02, MSE(e): 1.853e-05, MSE(pi1): 2.648e-03, MSE(pi2): 1.524e-05, MSE(pi3): 7.672e-04\n",
      "Epoch 36800, Train loss: 2.884e+02, Test loss: 9.042e+02, MSE(e): 1.852e-05, MSE(pi1): 2.648e-03, MSE(pi2): 1.523e-05, MSE(pi3): 7.673e-04\n",
      "Epoch 36900, Train loss: 2.883e+02, Test loss: 9.048e+02, MSE(e): 1.850e-05, MSE(pi1): 2.647e-03, MSE(pi2): 1.523e-05, MSE(pi3): 7.673e-04\n",
      "Epoch 37000, Train loss: 2.881e+02, Test loss: 9.054e+02, MSE(e): 1.849e-05, MSE(pi1): 2.647e-03, MSE(pi2): 1.522e-05, MSE(pi3): 7.673e-04\n",
      "Epoch 37100, Train loss: 2.880e+02, Test loss: 9.062e+02, MSE(e): 1.848e-05, MSE(pi1): 2.646e-03, MSE(pi2): 1.521e-05, MSE(pi3): 7.673e-04\n",
      "Epoch 37200, Train loss: 2.879e+02, Test loss: 9.069e+02, MSE(e): 1.847e-05, MSE(pi1): 2.646e-03, MSE(pi2): 1.520e-05, MSE(pi3): 7.674e-04\n",
      "Epoch 37300, Train loss: 2.877e+02, Test loss: 9.076e+02, MSE(e): 1.845e-05, MSE(pi1): 2.645e-03, MSE(pi2): 1.519e-05, MSE(pi3): 7.674e-04\n",
      "Epoch 37400, Train loss: 2.876e+02, Test loss: 9.083e+02, MSE(e): 1.844e-05, MSE(pi1): 2.644e-03, MSE(pi2): 1.518e-05, MSE(pi3): 7.674e-04\n",
      "Epoch 37500, Train loss: 2.875e+02, Test loss: 9.090e+02, MSE(e): 1.843e-05, MSE(pi1): 2.644e-03, MSE(pi2): 1.517e-05, MSE(pi3): 7.675e-04\n",
      "Epoch 37600, Train loss: 2.873e+02, Test loss: 9.097e+02, MSE(e): 1.841e-05, MSE(pi1): 2.643e-03, MSE(pi2): 1.516e-05, MSE(pi3): 7.675e-04\n",
      "Epoch 37700, Train loss: 2.872e+02, Test loss: 9.103e+02, MSE(e): 1.840e-05, MSE(pi1): 2.643e-03, MSE(pi2): 1.515e-05, MSE(pi3): 7.675e-04\n",
      "Epoch 37800, Train loss: 2.871e+02, Test loss: 9.110e+02, MSE(e): 1.839e-05, MSE(pi1): 2.642e-03, MSE(pi2): 1.514e-05, MSE(pi3): 7.675e-04\n",
      "Epoch 37900, Train loss: 2.870e+02, Test loss: 9.117e+02, MSE(e): 1.838e-05, MSE(pi1): 2.641e-03, MSE(pi2): 1.513e-05, MSE(pi3): 7.676e-04\n",
      "Epoch 38000, Train loss: 2.868e+02, Test loss: 9.125e+02, MSE(e): 1.837e-05, MSE(pi1): 2.641e-03, MSE(pi2): 1.512e-05, MSE(pi3): 7.676e-04\n",
      "Epoch 38100, Train loss: 2.867e+02, Test loss: 9.132e+02, MSE(e): 1.835e-05, MSE(pi1): 2.640e-03, MSE(pi2): 1.512e-05, MSE(pi3): 7.676e-04\n",
      "Epoch 38200, Train loss: 2.866e+02, Test loss: 9.141e+02, MSE(e): 1.834e-05, MSE(pi1): 2.640e-03, MSE(pi2): 1.511e-05, MSE(pi3): 7.677e-04\n",
      "Epoch 38300, Train loss: 2.865e+02, Test loss: 9.148e+02, MSE(e): 1.833e-05, MSE(pi1): 2.639e-03, MSE(pi2): 1.510e-05, MSE(pi3): 7.677e-04\n",
      "Epoch 38400, Train loss: 2.864e+02, Test loss: 9.157e+02, MSE(e): 1.832e-05, MSE(pi1): 2.638e-03, MSE(pi2): 1.509e-05, MSE(pi3): 7.677e-04\n",
      "Epoch 38500, Train loss: 2.863e+02, Test loss: 9.166e+02, MSE(e): 1.831e-05, MSE(pi1): 2.638e-03, MSE(pi2): 1.508e-05, MSE(pi3): 7.677e-04\n",
      "Epoch 38600, Train loss: 2.861e+02, Test loss: 9.174e+02, MSE(e): 1.830e-05, MSE(pi1): 2.637e-03, MSE(pi2): 1.507e-05, MSE(pi3): 7.678e-04\n",
      "Epoch 38700, Train loss: 2.860e+02, Test loss: 9.182e+02, MSE(e): 1.829e-05, MSE(pi1): 2.637e-03, MSE(pi2): 1.507e-05, MSE(pi3): 7.678e-04\n",
      "Epoch 38800, Train loss: 2.859e+02, Test loss: 9.190e+02, MSE(e): 1.827e-05, MSE(pi1): 2.636e-03, MSE(pi2): 1.506e-05, MSE(pi3): 7.678e-04\n",
      "Epoch 38900, Train loss: 2.858e+02, Test loss: 9.198e+02, MSE(e): 1.826e-05, MSE(pi1): 2.636e-03, MSE(pi2): 1.505e-05, MSE(pi3): 7.679e-04\n",
      "Epoch 39000, Train loss: 2.857e+02, Test loss: 9.206e+02, MSE(e): 1.825e-05, MSE(pi1): 2.635e-03, MSE(pi2): 1.504e-05, MSE(pi3): 7.679e-04\n",
      "Epoch 39100, Train loss: 2.856e+02, Test loss: 9.215e+02, MSE(e): 1.824e-05, MSE(pi1): 2.634e-03, MSE(pi2): 1.503e-05, MSE(pi3): 7.679e-04\n",
      "Epoch 39200, Train loss: 2.854e+02, Test loss: 9.224e+02, MSE(e): 1.823e-05, MSE(pi1): 2.634e-03, MSE(pi2): 1.502e-05, MSE(pi3): 7.679e-04\n",
      "Epoch 39300, Train loss: 2.853e+02, Test loss: 9.233e+02, MSE(e): 1.822e-05, MSE(pi1): 2.634e-03, MSE(pi2): 1.502e-05, MSE(pi3): 7.679e-04\n",
      "Epoch 39400, Train loss: 2.852e+02, Test loss: 9.241e+02, MSE(e): 1.821e-05, MSE(pi1): 2.634e-03, MSE(pi2): 1.501e-05, MSE(pi3): 7.679e-04\n",
      "Epoch 39500, Train loss: 2.851e+02, Test loss: 9.250e+02, MSE(e): 1.820e-05, MSE(pi1): 2.634e-03, MSE(pi2): 1.500e-05, MSE(pi3): 7.679e-04\n",
      "Epoch 39600, Train loss: 2.850e+02, Test loss: 9.259e+02, MSE(e): 1.819e-05, MSE(pi1): 2.634e-03, MSE(pi2): 1.499e-05, MSE(pi3): 7.679e-04\n",
      "Epoch 39700, Train loss: 2.849e+02, Test loss: 9.268e+02, MSE(e): 1.818e-05, MSE(pi1): 2.632e-03, MSE(pi2): 1.499e-05, MSE(pi3): 7.681e-04\n",
      "Epoch 39800, Train loss: 2.848e+02, Test loss: 9.278e+02, MSE(e): 1.817e-05, MSE(pi1): 2.632e-03, MSE(pi2): 1.498e-05, MSE(pi3): 7.681e-04\n",
      "Epoch 39900, Train loss: 2.847e+02, Test loss: 9.288e+02, MSE(e): 1.816e-05, MSE(pi1): 2.631e-03, MSE(pi2): 1.497e-05, MSE(pi3): 7.682e-04\n",
      "Epoch 40000, Train loss: 2.846e+02, Test loss: 9.298e+02, MSE(e): 1.814e-05, MSE(pi1): 2.630e-03, MSE(pi2): 1.496e-05, MSE(pi3): 7.682e-04\n",
      "Epoch 40100, Train loss: 2.845e+02, Test loss: 9.307e+02, MSE(e): 1.814e-05, MSE(pi1): 2.630e-03, MSE(pi2): 1.496e-05, MSE(pi3): 7.682e-04\n",
      "Epoch 40200, Train loss: 2.844e+02, Test loss: 9.318e+02, MSE(e): 1.813e-05, MSE(pi1): 2.629e-03, MSE(pi2): 1.495e-05, MSE(pi3): 7.682e-04\n",
      "Epoch 40300, Train loss: 2.843e+02, Test loss: 9.330e+02, MSE(e): 1.812e-05, MSE(pi1): 2.629e-03, MSE(pi2): 1.494e-05, MSE(pi3): 7.683e-04\n",
      "Epoch 40400, Train loss: 2.842e+02, Test loss: 9.344e+02, MSE(e): 1.810e-05, MSE(pi1): 2.629e-03, MSE(pi2): 1.493e-05, MSE(pi3): 7.683e-04\n",
      "Epoch 40500, Train loss: 2.841e+02, Test loss: 9.358e+02, MSE(e): 1.809e-05, MSE(pi1): 2.628e-03, MSE(pi2): 1.493e-05, MSE(pi3): 7.683e-04\n",
      "Epoch 40600, Train loss: 2.840e+02, Test loss: 9.371e+02, MSE(e): 1.809e-05, MSE(pi1): 2.628e-03, MSE(pi2): 1.492e-05, MSE(pi3): 7.683e-04\n",
      "Epoch 40700, Train loss: 2.839e+02, Test loss: 9.385e+02, MSE(e): 1.808e-05, MSE(pi1): 2.627e-03, MSE(pi2): 1.491e-05, MSE(pi3): 7.683e-04\n",
      "Epoch 40800, Train loss: 2.838e+02, Test loss: 9.400e+02, MSE(e): 1.807e-05, MSE(pi1): 2.627e-03, MSE(pi2): 1.491e-05, MSE(pi3): 7.684e-04\n",
      "Epoch 40900, Train loss: 2.837e+02, Test loss: 9.415e+02, MSE(e): 1.806e-05, MSE(pi1): 2.626e-03, MSE(pi2): 1.490e-05, MSE(pi3): 7.684e-04\n",
      "Epoch 41000, Train loss: 2.836e+02, Test loss: 9.430e+02, MSE(e): 1.805e-05, MSE(pi1): 2.626e-03, MSE(pi2): 1.489e-05, MSE(pi3): 7.684e-04\n",
      "Epoch 41100, Train loss: 2.835e+02, Test loss: 9.446e+02, MSE(e): 1.804e-05, MSE(pi1): 2.626e-03, MSE(pi2): 1.489e-05, MSE(pi3): 7.684e-04\n",
      "Epoch 41200, Train loss: 2.834e+02, Test loss: 9.462e+02, MSE(e): 1.803e-05, MSE(pi1): 2.625e-03, MSE(pi2): 1.488e-05, MSE(pi3): 7.684e-04\n",
      "Epoch 41300, Train loss: 2.833e+02, Test loss: 9.480e+02, MSE(e): 1.802e-05, MSE(pi1): 2.625e-03, MSE(pi2): 1.487e-05, MSE(pi3): 7.684e-04\n",
      "Epoch 41400, Train loss: 2.832e+02, Test loss: 9.498e+02, MSE(e): 1.801e-05, MSE(pi1): 2.625e-03, MSE(pi2): 1.487e-05, MSE(pi3): 7.684e-04\n",
      "Epoch 41500, Train loss: 2.831e+02, Test loss: 9.517e+02, MSE(e): 1.800e-05, MSE(pi1): 2.624e-03, MSE(pi2): 1.486e-05, MSE(pi3): 7.684e-04\n",
      "Epoch 41600, Train loss: 2.830e+02, Test loss: 9.537e+02, MSE(e): 1.799e-05, MSE(pi1): 2.624e-03, MSE(pi2): 1.485e-05, MSE(pi3): 7.685e-04\n",
      "Epoch 41700, Train loss: 2.829e+02, Test loss: 9.560e+02, MSE(e): 1.798e-05, MSE(pi1): 2.624e-03, MSE(pi2): 1.485e-05, MSE(pi3): 7.685e-04\n",
      "Epoch 41800, Train loss: 2.828e+02, Test loss: 9.584e+02, MSE(e): 1.797e-05, MSE(pi1): 2.623e-03, MSE(pi2): 1.484e-05, MSE(pi3): 7.685e-04\n",
      "Epoch 41900, Train loss: 2.827e+02, Test loss: 9.608e+02, MSE(e): 1.796e-05, MSE(pi1): 2.622e-03, MSE(pi2): 1.483e-05, MSE(pi3): 7.686e-04\n",
      "Epoch 42000, Train loss: 2.826e+02, Test loss: 9.635e+02, MSE(e): 1.796e-05, MSE(pi1): 2.615e-03, MSE(pi2): 1.483e-05, MSE(pi3): 7.691e-04\n",
      "Epoch 42100, Train loss: 2.826e+02, Test loss: 9.661e+02, MSE(e): 1.795e-05, MSE(pi1): 2.618e-03, MSE(pi2): 1.482e-05, MSE(pi3): 7.688e-04\n",
      "Epoch 42200, Train loss: 2.825e+02, Test loss: 9.690e+02, MSE(e): 1.794e-05, MSE(pi1): 2.618e-03, MSE(pi2): 1.481e-05, MSE(pi3): 7.687e-04\n",
      "Epoch 42300, Train loss: 2.824e+02, Test loss: 9.721e+02, MSE(e): 1.793e-05, MSE(pi1): 2.619e-03, MSE(pi2): 1.481e-05, MSE(pi3): 7.687e-04\n",
      "Epoch 42400, Train loss: 2.823e+02, Test loss: 9.748e+02, MSE(e): 1.792e-05, MSE(pi1): 2.615e-03, MSE(pi2): 1.480e-05, MSE(pi3): 7.690e-04\n",
      "Epoch 42500, Train loss: 2.822e+02, Test loss: 9.775e+02, MSE(e): 1.791e-05, MSE(pi1): 2.615e-03, MSE(pi2): 1.480e-05, MSE(pi3): 7.689e-04\n",
      "Epoch 42600, Train loss: 2.821e+02, Test loss: 9.802e+02, MSE(e): 1.790e-05, MSE(pi1): 2.616e-03, MSE(pi2): 1.479e-05, MSE(pi3): 7.689e-04\n",
      "Epoch 42700, Train loss: 2.820e+02, Test loss: 9.828e+02, MSE(e): 1.790e-05, MSE(pi1): 2.614e-03, MSE(pi2): 1.478e-05, MSE(pi3): 7.691e-04\n",
      "Epoch 42800, Train loss: 2.819e+02, Test loss: 9.853e+02, MSE(e): 1.789e-05, MSE(pi1): 2.616e-03, MSE(pi2): 1.478e-05, MSE(pi3): 7.688e-04\n",
      "Epoch 42900, Train loss: 2.819e+02, Test loss: 9.878e+02, MSE(e): 1.788e-05, MSE(pi1): 2.614e-03, MSE(pi2): 1.477e-05, MSE(pi3): 7.690e-04\n",
      "Epoch 43000, Train loss: 2.818e+02, Test loss: 9.902e+02, MSE(e): 1.787e-05, MSE(pi1): 2.614e-03, MSE(pi2): 1.477e-05, MSE(pi3): 7.689e-04\n",
      "Epoch 43100, Train loss: 2.817e+02, Test loss: 9.925e+02, MSE(e): 1.786e-05, MSE(pi1): 2.614e-03, MSE(pi2): 1.476e-05, MSE(pi3): 7.690e-04\n",
      "Epoch 43200, Train loss: 2.816e+02, Test loss: 9.946e+02, MSE(e): 1.786e-05, MSE(pi1): 2.615e-03, MSE(pi2): 1.475e-05, MSE(pi3): 7.688e-04\n",
      "Epoch 43300, Train loss: 2.815e+02, Test loss: 9.970e+02, MSE(e): 1.785e-05, MSE(pi1): 2.615e-03, MSE(pi2): 1.475e-05, MSE(pi3): 7.688e-04\n",
      "Epoch 43400, Train loss: 2.815e+02, Test loss: 9.991e+02, MSE(e): 1.784e-05, MSE(pi1): 2.616e-03, MSE(pi2): 1.474e-05, MSE(pi3): 7.686e-04\n",
      "Epoch 43500, Train loss: 2.814e+02, Test loss: 1.001e+03, MSE(e): 1.784e-05, MSE(pi1): 2.616e-03, MSE(pi2): 1.474e-05, MSE(pi3): 7.686e-04\n",
      "Epoch 43600, Train loss: 2.813e+02, Test loss: 1.003e+03, MSE(e): 1.783e-05, MSE(pi1): 2.616e-03, MSE(pi2): 1.473e-05, MSE(pi3): 7.685e-04\n",
      "Epoch 43700, Train loss: 2.813e+02, Test loss: 1.006e+03, MSE(e): 1.782e-05, MSE(pi1): 2.616e-03, MSE(pi2): 1.473e-05, MSE(pi3): 7.686e-04\n",
      "Epoch 43800, Train loss: 2.812e+02, Test loss: 1.008e+03, MSE(e): 1.782e-05, MSE(pi1): 2.615e-03, MSE(pi2): 1.472e-05, MSE(pi3): 7.686e-04\n",
      "Epoch 43900, Train loss: 2.811e+02, Test loss: 1.010e+03, MSE(e): 1.781e-05, MSE(pi1): 2.615e-03, MSE(pi2): 1.472e-05, MSE(pi3): 7.686e-04\n",
      "Epoch 44000, Train loss: 2.811e+02, Test loss: 1.012e+03, MSE(e): 1.780e-05, MSE(pi1): 2.616e-03, MSE(pi2): 1.471e-05, MSE(pi3): 7.685e-04\n",
      "Epoch 44100, Train loss: 2.810e+02, Test loss: 1.014e+03, MSE(e): 1.780e-05, MSE(pi1): 2.615e-03, MSE(pi2): 1.471e-05, MSE(pi3): 7.686e-04\n",
      "Epoch 44200, Train loss: 2.809e+02, Test loss: 1.016e+03, MSE(e): 1.779e-05, MSE(pi1): 2.615e-03, MSE(pi2): 1.470e-05, MSE(pi3): 7.685e-04\n",
      "Epoch 44300, Train loss: 2.809e+02, Test loss: 1.018e+03, MSE(e): 1.779e-05, MSE(pi1): 2.614e-03, MSE(pi2): 1.470e-05, MSE(pi3): 7.686e-04\n",
      "Epoch 44400, Train loss: 2.808e+02, Test loss: 1.021e+03, MSE(e): 1.778e-05, MSE(pi1): 2.614e-03, MSE(pi2): 1.470e-05, MSE(pi3): 7.686e-04\n",
      "Epoch 44500, Train loss: 2.807e+02, Test loss: 1.023e+03, MSE(e): 1.777e-05, MSE(pi1): 2.615e-03, MSE(pi2): 1.469e-05, MSE(pi3): 7.685e-04\n",
      "Epoch 44600, Train loss: 2.807e+02, Test loss: 1.025e+03, MSE(e): 1.777e-05, MSE(pi1): 2.614e-03, MSE(pi2): 1.469e-05, MSE(pi3): 7.686e-04\n",
      "Epoch 44700, Train loss: 2.806e+02, Test loss: 1.027e+03, MSE(e): 1.776e-05, MSE(pi1): 2.614e-03, MSE(pi2): 1.468e-05, MSE(pi3): 7.685e-04\n",
      "Epoch 44800, Train loss: 2.805e+02, Test loss: 1.029e+03, MSE(e): 1.775e-05, MSE(pi1): 2.613e-03, MSE(pi2): 1.468e-05, MSE(pi3): 7.686e-04\n",
      "Epoch 44900, Train loss: 2.805e+02, Test loss: 1.032e+03, MSE(e): 1.774e-05, MSE(pi1): 2.613e-03, MSE(pi2): 1.467e-05, MSE(pi3): 7.685e-04\n",
      "Epoch 45000, Train loss: 2.804e+02, Test loss: 1.034e+03, MSE(e): 1.774e-05, MSE(pi1): 2.613e-03, MSE(pi2): 1.467e-05, MSE(pi3): 7.685e-04\n",
      "Epoch 45100, Train loss: 2.803e+02, Test loss: 1.036e+03, MSE(e): 1.773e-05, MSE(pi1): 2.613e-03, MSE(pi2): 1.466e-05, MSE(pi3): 7.686e-04\n",
      "Epoch 45200, Train loss: 2.802e+02, Test loss: 1.038e+03, MSE(e): 1.772e-05, MSE(pi1): 2.613e-03, MSE(pi2): 1.465e-05, MSE(pi3): 7.685e-04\n",
      "Epoch 45300, Train loss: 2.802e+02, Test loss: 1.041e+03, MSE(e): 1.772e-05, MSE(pi1): 2.612e-03, MSE(pi2): 1.465e-05, MSE(pi3): 7.686e-04\n",
      "Epoch 45400, Train loss: 2.801e+02, Test loss: 1.043e+03, MSE(e): 1.771e-05, MSE(pi1): 2.613e-03, MSE(pi2): 1.464e-05, MSE(pi3): 7.685e-04\n",
      "Epoch 45500, Train loss: 2.800e+02, Test loss: 1.045e+03, MSE(e): 1.770e-05, MSE(pi1): 2.612e-03, MSE(pi2): 1.464e-05, MSE(pi3): 7.686e-04\n",
      "Epoch 45600, Train loss: 2.799e+02, Test loss: 1.048e+03, MSE(e): 1.769e-05, MSE(pi1): 2.613e-03, MSE(pi2): 1.463e-05, MSE(pi3): 7.685e-04\n",
      "Epoch 45700, Train loss: 2.799e+02, Test loss: 1.050e+03, MSE(e): 1.769e-05, MSE(pi1): 2.615e-03, MSE(pi2): 1.463e-05, MSE(pi3): 7.683e-04\n",
      "Epoch 45800, Train loss: 2.798e+02, Test loss: 1.052e+03, MSE(e): 1.768e-05, MSE(pi1): 2.615e-03, MSE(pi2): 1.462e-05, MSE(pi3): 7.682e-04\n",
      "Epoch 45900, Train loss: 2.797e+02, Test loss: 1.055e+03, MSE(e): 1.767e-05, MSE(pi1): 2.615e-03, MSE(pi2): 1.462e-05, MSE(pi3): 7.682e-04\n",
      "Epoch 46000, Train loss: 2.796e+02, Test loss: 1.057e+03, MSE(e): 1.766e-05, MSE(pi1): 2.615e-03, MSE(pi2): 1.461e-05, MSE(pi3): 7.682e-04\n",
      "Epoch 46100, Train loss: 2.795e+02, Test loss: 1.060e+03, MSE(e): 1.766e-05, MSE(pi1): 2.616e-03, MSE(pi2): 1.461e-05, MSE(pi3): 7.681e-04\n",
      "Epoch 46200, Train loss: 2.794e+02, Test loss: 1.062e+03, MSE(e): 1.765e-05, MSE(pi1): 2.615e-03, MSE(pi2): 1.460e-05, MSE(pi3): 7.682e-04\n",
      "Epoch 46300, Train loss: 2.794e+02, Test loss: 1.065e+03, MSE(e): 1.764e-05, MSE(pi1): 2.615e-03, MSE(pi2): 1.459e-05, MSE(pi3): 7.682e-04\n",
      "Epoch 46400, Train loss: 2.793e+02, Test loss: 1.068e+03, MSE(e): 1.763e-05, MSE(pi1): 2.616e-03, MSE(pi2): 1.459e-05, MSE(pi3): 7.681e-04\n",
      "Epoch 46500, Train loss: 2.792e+02, Test loss: 1.070e+03, MSE(e): 1.763e-05, MSE(pi1): 2.615e-03, MSE(pi2): 1.458e-05, MSE(pi3): 7.681e-04\n",
      "Epoch 46600, Train loss: 2.791e+02, Test loss: 1.073e+03, MSE(e): 1.762e-05, MSE(pi1): 2.615e-03, MSE(pi2): 1.458e-05, MSE(pi3): 7.681e-04\n",
      "Epoch 46700, Train loss: 2.791e+02, Test loss: 1.076e+03, MSE(e): 1.761e-05, MSE(pi1): 2.616e-03, MSE(pi2): 1.457e-05, MSE(pi3): 7.680e-04\n",
      "Epoch 46800, Train loss: 2.790e+02, Test loss: 1.079e+03, MSE(e): 1.760e-05, MSE(pi1): 2.615e-03, MSE(pi2): 1.457e-05, MSE(pi3): 7.681e-04\n",
      "Epoch 46900, Train loss: 2.789e+02, Test loss: 1.082e+03, MSE(e): 1.759e-05, MSE(pi1): 2.615e-03, MSE(pi2): 1.456e-05, MSE(pi3): 7.681e-04\n",
      "Epoch 47000, Train loss: 2.788e+02, Test loss: 1.084e+03, MSE(e): 1.758e-05, MSE(pi1): 2.616e-03, MSE(pi2): 1.455e-05, MSE(pi3): 7.680e-04\n",
      "Epoch 47100, Train loss: 2.787e+02, Test loss: 1.087e+03, MSE(e): 1.757e-05, MSE(pi1): 2.615e-03, MSE(pi2): 1.455e-05, MSE(pi3): 7.680e-04\n",
      "Epoch 47200, Train loss: 2.786e+02, Test loss: 1.090e+03, MSE(e): 1.757e-05, MSE(pi1): 2.615e-03, MSE(pi2): 1.454e-05, MSE(pi3): 7.680e-04\n",
      "Epoch 47300, Train loss: 2.785e+02, Test loss: 1.093e+03, MSE(e): 1.756e-05, MSE(pi1): 2.616e-03, MSE(pi2): 1.454e-05, MSE(pi3): 7.679e-04\n",
      "Epoch 47400, Train loss: 2.785e+02, Test loss: 1.096e+03, MSE(e): 1.755e-05, MSE(pi1): 2.615e-03, MSE(pi2): 1.453e-05, MSE(pi3): 7.680e-04\n",
      "Epoch 47500, Train loss: 2.784e+02, Test loss: 1.099e+03, MSE(e): 1.754e-05, MSE(pi1): 2.616e-03, MSE(pi2): 1.453e-05, MSE(pi3): 7.679e-04\n",
      "Epoch 47600, Train loss: 2.783e+02, Test loss: 1.102e+03, MSE(e): 1.753e-05, MSE(pi1): 2.615e-03, MSE(pi2): 1.452e-05, MSE(pi3): 7.679e-04\n",
      "Epoch 47700, Train loss: 2.782e+02, Test loss: 1.106e+03, MSE(e): 1.752e-05, MSE(pi1): 2.615e-03, MSE(pi2): 1.451e-05, MSE(pi3): 7.679e-04\n",
      "Epoch 47800, Train loss: 2.781e+02, Test loss: 1.109e+03, MSE(e): 1.752e-05, MSE(pi1): 2.616e-03, MSE(pi2): 1.451e-05, MSE(pi3): 7.678e-04\n",
      "Epoch 47900, Train loss: 2.780e+02, Test loss: 1.112e+03, MSE(e): 1.751e-05, MSE(pi1): 2.615e-03, MSE(pi2): 1.450e-05, MSE(pi3): 7.679e-04\n",
      "Epoch 48000, Train loss: 2.779e+02, Test loss: 1.115e+03, MSE(e): 1.750e-05, MSE(pi1): 2.616e-03, MSE(pi2): 1.450e-05, MSE(pi3): 7.678e-04\n",
      "Epoch 48100, Train loss: 2.778e+02, Test loss: 1.118e+03, MSE(e): 1.749e-05, MSE(pi1): 2.616e-03, MSE(pi2): 1.449e-05, MSE(pi3): 7.678e-04\n",
      "Epoch 48200, Train loss: 2.778e+02, Test loss: 1.122e+03, MSE(e): 1.748e-05, MSE(pi1): 2.616e-03, MSE(pi2): 1.449e-05, MSE(pi3): 7.678e-04\n",
      "Epoch 48300, Train loss: 2.777e+02, Test loss: 1.125e+03, MSE(e): 1.747e-05, MSE(pi1): 2.616e-03, MSE(pi2): 1.448e-05, MSE(pi3): 7.677e-04\n",
      "Epoch 48400, Train loss: 2.776e+02, Test loss: 1.129e+03, MSE(e): 1.746e-05, MSE(pi1): 2.615e-03, MSE(pi2): 1.447e-05, MSE(pi3): 7.678e-04\n",
      "Epoch 48500, Train loss: 2.775e+02, Test loss: 1.132e+03, MSE(e): 1.746e-05, MSE(pi1): 2.616e-03, MSE(pi2): 1.447e-05, MSE(pi3): 7.677e-04\n",
      "Epoch 48600, Train loss: 2.774e+02, Test loss: 1.136e+03, MSE(e): 1.745e-05, MSE(pi1): 2.616e-03, MSE(pi2): 1.446e-05, MSE(pi3): 7.677e-04\n",
      "Epoch 48700, Train loss: 2.773e+02, Test loss: 1.140e+03, MSE(e): 1.744e-05, MSE(pi1): 2.616e-03, MSE(pi2): 1.446e-05, MSE(pi3): 7.677e-04\n",
      "Epoch 48800, Train loss: 2.772e+02, Test loss: 1.144e+03, MSE(e): 1.743e-05, MSE(pi1): 2.616e-03, MSE(pi2): 1.445e-05, MSE(pi3): 7.677e-04\n",
      "Epoch 48900, Train loss: 2.771e+02, Test loss: 1.148e+03, MSE(e): 1.742e-05, MSE(pi1): 2.616e-03, MSE(pi2): 1.444e-05, MSE(pi3): 7.677e-04\n",
      "Epoch 49000, Train loss: 2.771e+02, Test loss: 1.151e+03, MSE(e): 1.741e-05, MSE(pi1): 2.616e-03, MSE(pi2): 1.444e-05, MSE(pi3): 7.676e-04\n",
      "Epoch 49100, Train loss: 2.770e+02, Test loss: 1.155e+03, MSE(e): 1.740e-05, MSE(pi1): 2.616e-03, MSE(pi2): 1.443e-05, MSE(pi3): 7.676e-04\n",
      "Epoch 49200, Train loss: 2.769e+02, Test loss: 1.159e+03, MSE(e): 1.739e-05, MSE(pi1): 2.617e-03, MSE(pi2): 1.443e-05, MSE(pi3): 7.675e-04\n",
      "Epoch 49300, Train loss: 2.768e+02, Test loss: 1.163e+03, MSE(e): 1.739e-05, MSE(pi1): 2.616e-03, MSE(pi2): 1.442e-05, MSE(pi3): 7.676e-04\n",
      "Epoch 49400, Train loss: 2.767e+02, Test loss: 1.168e+03, MSE(e): 1.738e-05, MSE(pi1): 2.616e-03, MSE(pi2): 1.442e-05, MSE(pi3): 7.675e-04\n",
      "Epoch 49500, Train loss: 2.766e+02, Test loss: 1.172e+03, MSE(e): 1.737e-05, MSE(pi1): 2.616e-03, MSE(pi2): 1.441e-05, MSE(pi3): 7.676e-04\n",
      "Epoch 49600, Train loss: 2.765e+02, Test loss: 1.176e+03, MSE(e): 1.736e-05, MSE(pi1): 2.616e-03, MSE(pi2): 1.440e-05, MSE(pi3): 7.675e-04\n",
      "Epoch 49700, Train loss: 2.764e+02, Test loss: 1.181e+03, MSE(e): 1.735e-05, MSE(pi1): 2.616e-03, MSE(pi2): 1.440e-05, MSE(pi3): 7.675e-04\n",
      "Epoch 49800, Train loss: 2.763e+02, Test loss: 1.185e+03, MSE(e): 1.734e-05, MSE(pi1): 2.616e-03, MSE(pi2): 1.439e-05, MSE(pi3): 7.675e-04\n",
      "Epoch 49900, Train loss: 2.763e+02, Test loss: 1.190e+03, MSE(e): 1.733e-05, MSE(pi1): 2.616e-03, MSE(pi2): 1.438e-05, MSE(pi3): 7.675e-04\n",
      "Epoch 50000, Train loss: 2.762e+02, Test loss: 1.195e+03, MSE(e): 1.732e-05, MSE(pi1): 2.616e-03, MSE(pi2): 1.438e-05, MSE(pi3): 7.675e-04\n",
      "Epoch 50100, Train loss: 2.761e+02, Test loss: 1.199e+03, MSE(e): 1.732e-05, MSE(pi1): 2.617e-03, MSE(pi2): 1.437e-05, MSE(pi3): 7.674e-04\n",
      "Epoch 50200, Train loss: 2.760e+02, Test loss: 1.204e+03, MSE(e): 1.731e-05, MSE(pi1): 2.616e-03, MSE(pi2): 1.437e-05, MSE(pi3): 7.675e-04\n",
      "Epoch 50300, Train loss: 2.759e+02, Test loss: 1.210e+03, MSE(e): 1.730e-05, MSE(pi1): 2.617e-03, MSE(pi2): 1.436e-05, MSE(pi3): 7.674e-04\n",
      "Epoch 50400, Train loss: 2.758e+02, Test loss: 1.215e+03, MSE(e): 1.729e-05, MSE(pi1): 2.616e-03, MSE(pi2): 1.435e-05, MSE(pi3): 7.674e-04\n",
      "Epoch 50500, Train loss: 2.757e+02, Test loss: 1.220e+03, MSE(e): 1.728e-05, MSE(pi1): 2.616e-03, MSE(pi2): 1.435e-05, MSE(pi3): 7.674e-04\n",
      "Epoch 50600, Train loss: 2.756e+02, Test loss: 1.225e+03, MSE(e): 1.727e-05, MSE(pi1): 2.616e-03, MSE(pi2): 1.434e-05, MSE(pi3): 7.674e-04\n",
      "Epoch 50700, Train loss: 2.756e+02, Test loss: 1.230e+03, MSE(e): 1.726e-05, MSE(pi1): 2.616e-03, MSE(pi2): 1.434e-05, MSE(pi3): 7.674e-04\n",
      "Epoch 50800, Train loss: 2.755e+02, Test loss: 1.235e+03, MSE(e): 1.726e-05, MSE(pi1): 2.616e-03, MSE(pi2): 1.433e-05, MSE(pi3): 7.674e-04\n",
      "Epoch 50900, Train loss: 2.754e+02, Test loss: 1.241e+03, MSE(e): 1.725e-05, MSE(pi1): 2.615e-03, MSE(pi2): 1.432e-05, MSE(pi3): 7.674e-04\n",
      "Epoch 51000, Train loss: 2.753e+02, Test loss: 1.246e+03, MSE(e): 1.724e-05, MSE(pi1): 2.617e-03, MSE(pi2): 1.432e-05, MSE(pi3): 7.673e-04\n",
      "Epoch 51100, Train loss: 2.752e+02, Test loss: 1.252e+03, MSE(e): 1.723e-05, MSE(pi1): 2.615e-03, MSE(pi2): 1.431e-05, MSE(pi3): 7.674e-04\n",
      "Epoch 51200, Train loss: 2.751e+02, Test loss: 1.258e+03, MSE(e): 1.722e-05, MSE(pi1): 2.615e-03, MSE(pi2): 1.431e-05, MSE(pi3): 7.673e-04\n",
      "Epoch 51300, Train loss: 2.751e+02, Test loss: 1.264e+03, MSE(e): 1.722e-05, MSE(pi1): 2.616e-03, MSE(pi2): 1.430e-05, MSE(pi3): 7.673e-04\n",
      "Epoch 51400, Train loss: 2.750e+02, Test loss: 1.270e+03, MSE(e): 1.721e-05, MSE(pi1): 2.615e-03, MSE(pi2): 1.429e-05, MSE(pi3): 7.674e-04\n",
      "Epoch 51500, Train loss: 2.749e+02, Test loss: 1.276e+03, MSE(e): 1.720e-05, MSE(pi1): 2.615e-03, MSE(pi2): 1.429e-05, MSE(pi3): 7.673e-04\n",
      "Epoch 51600, Train loss: 2.748e+02, Test loss: 1.283e+03, MSE(e): 1.719e-05, MSE(pi1): 2.616e-03, MSE(pi2): 1.428e-05, MSE(pi3): 7.672e-04\n",
      "Epoch 51700, Train loss: 2.747e+02, Test loss: 1.289e+03, MSE(e): 1.718e-05, MSE(pi1): 2.616e-03, MSE(pi2): 1.428e-05, MSE(pi3): 7.672e-04\n",
      "Epoch 51800, Train loss: 2.747e+02, Test loss: 1.295e+03, MSE(e): 1.718e-05, MSE(pi1): 2.616e-03, MSE(pi2): 1.427e-05, MSE(pi3): 7.673e-04\n",
      "Epoch 51900, Train loss: 2.746e+02, Test loss: 1.302e+03, MSE(e): 1.717e-05, MSE(pi1): 2.613e-03, MSE(pi2): 1.427e-05, MSE(pi3): 7.675e-04\n",
      "Epoch 52000, Train loss: 2.745e+02, Test loss: 1.309e+03, MSE(e): 1.716e-05, MSE(pi1): 2.617e-03, MSE(pi2): 1.426e-05, MSE(pi3): 7.671e-04\n",
      "Epoch 52100, Train loss: 2.744e+02, Test loss: 1.315e+03, MSE(e): 1.715e-05, MSE(pi1): 2.619e-03, MSE(pi2): 1.425e-05, MSE(pi3): 7.669e-04\n",
      "Epoch 52200, Train loss: 2.744e+02, Test loss: 1.323e+03, MSE(e): 1.715e-05, MSE(pi1): 2.617e-03, MSE(pi2): 1.425e-05, MSE(pi3): 7.672e-04\n",
      "Epoch 52300, Train loss: 2.743e+02, Test loss: 1.330e+03, MSE(e): 1.714e-05, MSE(pi1): 2.614e-03, MSE(pi2): 1.424e-05, MSE(pi3): 7.673e-04\n",
      "Epoch 52400, Train loss: 2.742e+02, Test loss: 1.337e+03, MSE(e): 1.713e-05, MSE(pi1): 2.617e-03, MSE(pi2): 1.424e-05, MSE(pi3): 7.671e-04\n",
      "Epoch 52500, Train loss: 2.742e+02, Test loss: 1.344e+03, MSE(e): 1.713e-05, MSE(pi1): 2.614e-03, MSE(pi2): 1.423e-05, MSE(pi3): 7.674e-04\n",
      "Epoch 52600, Train loss: 2.741e+02, Test loss: 1.352e+03, MSE(e): 1.712e-05, MSE(pi1): 2.613e-03, MSE(pi2): 1.423e-05, MSE(pi3): 7.676e-04\n",
      "Epoch 52700, Train loss: 2.740e+02, Test loss: 1.359e+03, MSE(e): 1.711e-05, MSE(pi1): 2.614e-03, MSE(pi2): 1.422e-05, MSE(pi3): 7.673e-04\n",
      "Epoch 52800, Train loss: 2.740e+02, Test loss: 1.367e+03, MSE(e): 1.711e-05, MSE(pi1): 2.614e-03, MSE(pi2): 1.422e-05, MSE(pi3): 7.674e-04\n",
      "Epoch 52900, Train loss: 2.739e+02, Test loss: 1.375e+03, MSE(e): 1.710e-05, MSE(pi1): 2.614e-03, MSE(pi2): 1.421e-05, MSE(pi3): 7.673e-04\n",
      "Epoch 53000, Train loss: 2.738e+02, Test loss: 1.383e+03, MSE(e): 1.709e-05, MSE(pi1): 2.615e-03, MSE(pi2): 1.421e-05, MSE(pi3): 7.672e-04\n",
      "Epoch 53100, Train loss: 2.738e+02, Test loss: 1.391e+03, MSE(e): 1.709e-05, MSE(pi1): 2.616e-03, MSE(pi2): 1.420e-05, MSE(pi3): 7.671e-04\n",
      "Epoch 53200, Train loss: 2.737e+02, Test loss: 1.399e+03, MSE(e): 1.708e-05, MSE(pi1): 2.613e-03, MSE(pi2): 1.420e-05, MSE(pi3): 7.674e-04\n",
      "Epoch 53300, Train loss: 2.736e+02, Test loss: 1.408e+03, MSE(e): 1.708e-05, MSE(pi1): 2.613e-03, MSE(pi2): 1.419e-05, MSE(pi3): 7.673e-04\n",
      "Epoch 53400, Train loss: 2.736e+02, Test loss: 1.417e+03, MSE(e): 1.707e-05, MSE(pi1): 2.616e-03, MSE(pi2): 1.419e-05, MSE(pi3): 7.671e-04\n",
      "Epoch 53500, Train loss: 2.735e+02, Test loss: 1.426e+03, MSE(e): 1.706e-05, MSE(pi1): 2.614e-03, MSE(pi2): 1.418e-05, MSE(pi3): 7.672e-04\n",
      "Epoch 53600, Train loss: 2.734e+02, Test loss: 1.436e+03, MSE(e): 1.706e-05, MSE(pi1): 2.615e-03, MSE(pi2): 1.418e-05, MSE(pi3): 7.671e-04\n",
      "Epoch 53700, Train loss: 2.734e+02, Test loss: 1.446e+03, MSE(e): 1.705e-05, MSE(pi1): 2.615e-03, MSE(pi2): 1.417e-05, MSE(pi3): 7.671e-04\n",
      "Epoch 53800, Train loss: 2.733e+02, Test loss: 1.455e+03, MSE(e): 1.705e-05, MSE(pi1): 2.614e-03, MSE(pi2): 1.417e-05, MSE(pi3): 7.672e-04\n",
      "Epoch 53900, Train loss: 2.732e+02, Test loss: 1.463e+03, MSE(e): 1.704e-05, MSE(pi1): 2.614e-03, MSE(pi2): 1.416e-05, MSE(pi3): 7.671e-04\n",
      "Epoch 54000, Train loss: 2.732e+02, Test loss: 1.471e+03, MSE(e): 1.703e-05, MSE(pi1): 2.614e-03, MSE(pi2): 1.416e-05, MSE(pi3): 7.672e-04\n",
      "Epoch 54100, Train loss: 2.731e+02, Test loss: 1.478e+03, MSE(e): 1.703e-05, MSE(pi1): 2.614e-03, MSE(pi2): 1.415e-05, MSE(pi3): 7.672e-04\n",
      "Epoch 54200, Train loss: 2.731e+02, Test loss: 1.487e+03, MSE(e): 1.702e-05, MSE(pi1): 2.614e-03, MSE(pi2): 1.415e-05, MSE(pi3): 7.672e-04\n",
      "Epoch 54300, Train loss: 2.730e+02, Test loss: 1.496e+03, MSE(e): 1.701e-05, MSE(pi1): 2.614e-03, MSE(pi2): 1.414e-05, MSE(pi3): 7.672e-04\n",
      "Epoch 54400, Train loss: 2.729e+02, Test loss: 1.506e+03, MSE(e): 1.701e-05, MSE(pi1): 2.613e-03, MSE(pi2): 1.414e-05, MSE(pi3): 7.672e-04\n",
      "Epoch 54500, Train loss: 2.729e+02, Test loss: 1.516e+03, MSE(e): 1.700e-05, MSE(pi1): 2.613e-03, MSE(pi2): 1.413e-05, MSE(pi3): 7.672e-04\n",
      "Epoch 54600, Train loss: 2.728e+02, Test loss: 1.525e+03, MSE(e): 1.700e-05, MSE(pi1): 2.613e-03, MSE(pi2): 1.413e-05, MSE(pi3): 7.672e-04\n",
      "Epoch 54700, Train loss: 2.728e+02, Test loss: 1.535e+03, MSE(e): 1.699e-05, MSE(pi1): 2.613e-03, MSE(pi2): 1.413e-05, MSE(pi3): 7.672e-04\n",
      "Epoch 54800, Train loss: 2.728e+02, Test loss: 1.544e+03, MSE(e): 1.699e-05, MSE(pi1): 2.613e-03, MSE(pi2): 1.412e-05, MSE(pi3): 7.672e-04\n",
      "Epoch 54900, Train loss: 2.727e+02, Test loss: 1.552e+03, MSE(e): 1.699e-05, MSE(pi1): 2.613e-03, MSE(pi2): 1.412e-05, MSE(pi3): 7.672e-04\n",
      "Epoch 55000, Train loss: 2.727e+02, Test loss: 1.560e+03, MSE(e): 1.698e-05, MSE(pi1): 2.613e-03, MSE(pi2): 1.411e-05, MSE(pi3): 7.672e-04\n",
      "Epoch 55100, Train loss: 2.726e+02, Test loss: 1.568e+03, MSE(e): 1.698e-05, MSE(pi1): 2.613e-03, MSE(pi2): 1.411e-05, MSE(pi3): 7.672e-04\n",
      "Epoch 55200, Train loss: 2.726e+02, Test loss: 1.575e+03, MSE(e): 1.697e-05, MSE(pi1): 2.613e-03, MSE(pi2): 1.411e-05, MSE(pi3): 7.672e-04\n",
      "Epoch 55300, Train loss: 2.725e+02, Test loss: 1.582e+03, MSE(e): 1.697e-05, MSE(pi1): 2.612e-03, MSE(pi2): 1.410e-05, MSE(pi3): 7.672e-04\n",
      "Epoch 55400, Train loss: 2.725e+02, Test loss: 1.588e+03, MSE(e): 1.696e-05, MSE(pi1): 2.612e-03, MSE(pi2): 1.410e-05, MSE(pi3): 7.672e-04\n",
      "Epoch 55500, Train loss: 2.724e+02, Test loss: 1.594e+03, MSE(e): 1.696e-05, MSE(pi1): 2.612e-03, MSE(pi2): 1.409e-05, MSE(pi3): 7.672e-04\n",
      "Epoch 55600, Train loss: 2.724e+02, Test loss: 1.598e+03, MSE(e): 1.695e-05, MSE(pi1): 2.612e-03, MSE(pi2): 1.409e-05, MSE(pi3): 7.672e-04\n",
      "Epoch 55700, Train loss: 2.723e+02, Test loss: 1.603e+03, MSE(e): 1.695e-05, MSE(pi1): 2.612e-03, MSE(pi2): 1.409e-05, MSE(pi3): 7.672e-04\n",
      "Epoch 55800, Train loss: 2.723e+02, Test loss: 1.607e+03, MSE(e): 1.694e-05, MSE(pi1): 2.612e-03, MSE(pi2): 1.408e-05, MSE(pi3): 7.672e-04\n",
      "Epoch 55900, Train loss: 2.722e+02, Test loss: 1.610e+03, MSE(e): 1.694e-05, MSE(pi1): 2.612e-03, MSE(pi2): 1.408e-05, MSE(pi3): 7.672e-04\n",
      "Epoch 56000, Train loss: 2.722e+02, Test loss: 1.613e+03, MSE(e): 1.693e-05, MSE(pi1): 2.612e-03, MSE(pi2): 1.407e-05, MSE(pi3): 7.672e-04\n",
      "Epoch 56100, Train loss: 2.721e+02, Test loss: 1.616e+03, MSE(e): 1.693e-05, MSE(pi1): 2.611e-03, MSE(pi2): 1.407e-05, MSE(pi3): 7.672e-04\n",
      "Epoch 56200, Train loss: 2.721e+02, Test loss: 1.618e+03, MSE(e): 1.692e-05, MSE(pi1): 2.611e-03, MSE(pi2): 1.407e-05, MSE(pi3): 7.672e-04\n",
      "Epoch 56300, Train loss: 2.720e+02, Test loss: 1.620e+03, MSE(e): 1.692e-05, MSE(pi1): 2.611e-03, MSE(pi2): 1.406e-05, MSE(pi3): 7.672e-04\n",
      "Epoch 56400, Train loss: 2.720e+02, Test loss: 1.622e+03, MSE(e): 1.691e-05, MSE(pi1): 2.611e-03, MSE(pi2): 1.406e-05, MSE(pi3): 7.672e-04\n",
      "Epoch 56500, Train loss: 2.719e+02, Test loss: 1.623e+03, MSE(e): 1.691e-05, MSE(pi1): 2.611e-03, MSE(pi2): 1.405e-05, MSE(pi3): 7.672e-04\n",
      "Epoch 56600, Train loss: 2.719e+02, Test loss: 1.624e+03, MSE(e): 1.690e-05, MSE(pi1): 2.611e-03, MSE(pi2): 1.405e-05, MSE(pi3): 7.672e-04\n",
      "Epoch 56700, Train loss: 2.718e+02, Test loss: 1.625e+03, MSE(e): 1.690e-05, MSE(pi1): 2.611e-03, MSE(pi2): 1.404e-05, MSE(pi3): 7.672e-04\n",
      "Epoch 56800, Train loss: 2.717e+02, Test loss: 1.625e+03, MSE(e): 1.689e-05, MSE(pi1): 2.610e-03, MSE(pi2): 1.404e-05, MSE(pi3): 7.672e-04\n",
      "Epoch 56900, Train loss: 2.717e+02, Test loss: 1.625e+03, MSE(e): 1.688e-05, MSE(pi1): 2.610e-03, MSE(pi2): 1.403e-05, MSE(pi3): 7.672e-04\n",
      "Epoch 57000, Train loss: 2.716e+02, Test loss: 1.625e+03, MSE(e): 1.688e-05, MSE(pi1): 2.610e-03, MSE(pi2): 1.403e-05, MSE(pi3): 7.672e-04\n",
      "Epoch 57100, Train loss: 2.716e+02, Test loss: 1.624e+03, MSE(e): 1.688e-05, MSE(pi1): 2.610e-03, MSE(pi2): 1.403e-05, MSE(pi3): 7.672e-04\n",
      "Epoch 57200, Train loss: 2.716e+02, Test loss: 1.624e+03, MSE(e): 1.687e-05, MSE(pi1): 2.610e-03, MSE(pi2): 1.402e-05, MSE(pi3): 7.673e-04\n",
      "Epoch 57300, Train loss: 2.715e+02, Test loss: 1.623e+03, MSE(e): 1.687e-05, MSE(pi1): 2.610e-03, MSE(pi2): 1.402e-05, MSE(pi3): 7.673e-04\n",
      "Epoch 57400, Train loss: 2.714e+02, Test loss: 1.622e+03, MSE(e): 1.686e-05, MSE(pi1): 2.609e-03, MSE(pi2): 1.401e-05, MSE(pi3): 7.673e-04\n",
      "Epoch 57500, Train loss: 2.714e+02, Test loss: 1.621e+03, MSE(e): 1.686e-05, MSE(pi1): 2.609e-03, MSE(pi2): 1.401e-05, MSE(pi3): 7.673e-04\n",
      "Epoch 57600, Train loss: 2.714e+02, Test loss: 1.620e+03, MSE(e): 1.685e-05, MSE(pi1): 2.609e-03, MSE(pi2): 1.401e-05, MSE(pi3): 7.673e-04\n",
      "Epoch 57700, Train loss: 2.713e+02, Test loss: 1.618e+03, MSE(e): 1.685e-05, MSE(pi1): 2.609e-03, MSE(pi2): 1.400e-05, MSE(pi3): 7.673e-04\n",
      "Epoch 57800, Train loss: 2.712e+02, Test loss: 1.616e+03, MSE(e): 1.684e-05, MSE(pi1): 2.609e-03, MSE(pi2): 1.400e-05, MSE(pi3): 7.673e-04\n",
      "Epoch 57900, Train loss: 2.712e+02, Test loss: 1.615e+03, MSE(e): 1.684e-05, MSE(pi1): 2.608e-03, MSE(pi2): 1.399e-05, MSE(pi3): 7.673e-04\n",
      "Epoch 58000, Train loss: 2.711e+02, Test loss: 1.612e+03, MSE(e): 1.683e-05, MSE(pi1): 2.608e-03, MSE(pi2): 1.399e-05, MSE(pi3): 7.673e-04\n",
      "Epoch 58100, Train loss: 2.711e+02, Test loss: 1.610e+03, MSE(e): 1.683e-05, MSE(pi1): 2.608e-03, MSE(pi2): 1.399e-05, MSE(pi3): 7.673e-04\n",
      "Epoch 58200, Train loss: 2.710e+02, Test loss: 1.608e+03, MSE(e): 1.682e-05, MSE(pi1): 2.608e-03, MSE(pi2): 1.398e-05, MSE(pi3): 7.673e-04\n",
      "Epoch 58300, Train loss: 2.710e+02, Test loss: 1.606e+03, MSE(e): 1.682e-05, MSE(pi1): 2.608e-03, MSE(pi2): 1.398e-05, MSE(pi3): 7.673e-04\n",
      "Epoch 58400, Train loss: 2.709e+02, Test loss: 1.603e+03, MSE(e): 1.681e-05, MSE(pi1): 2.608e-03, MSE(pi2): 1.397e-05, MSE(pi3): 7.673e-04\n",
      "Epoch 58500, Train loss: 2.709e+02, Test loss: 1.601e+03, MSE(e): 1.680e-05, MSE(pi1): 2.607e-03, MSE(pi2): 1.397e-05, MSE(pi3): 7.674e-04\n",
      "Epoch 58600, Train loss: 2.708e+02, Test loss: 1.598e+03, MSE(e): 1.680e-05, MSE(pi1): 2.607e-03, MSE(pi2): 1.396e-05, MSE(pi3): 7.674e-04\n",
      "Epoch 58700, Train loss: 2.708e+02, Test loss: 1.596e+03, MSE(e): 1.679e-05, MSE(pi1): 2.607e-03, MSE(pi2): 1.396e-05, MSE(pi3): 7.674e-04\n",
      "Epoch 58800, Train loss: 2.707e+02, Test loss: 1.593e+03, MSE(e): 1.679e-05, MSE(pi1): 2.607e-03, MSE(pi2): 1.396e-05, MSE(pi3): 7.674e-04\n",
      "Epoch 58900, Train loss: 2.707e+02, Test loss: 1.591e+03, MSE(e): 1.678e-05, MSE(pi1): 2.607e-03, MSE(pi2): 1.395e-05, MSE(pi3): 7.674e-04\n",
      "Epoch 59000, Train loss: 2.706e+02, Test loss: 1.588e+03, MSE(e): 1.678e-05, MSE(pi1): 2.607e-03, MSE(pi2): 1.395e-05, MSE(pi3): 7.674e-04\n",
      "Epoch 59100, Train loss: 2.706e+02, Test loss: 1.586e+03, MSE(e): 1.677e-05, MSE(pi1): 2.606e-03, MSE(pi2): 1.394e-05, MSE(pi3): 7.674e-04\n",
      "Epoch 59200, Train loss: 2.705e+02, Test loss: 1.584e+03, MSE(e): 1.677e-05, MSE(pi1): 2.606e-03, MSE(pi2): 1.394e-05, MSE(pi3): 7.674e-04\n",
      "Epoch 59300, Train loss: 2.704e+02, Test loss: 1.581e+03, MSE(e): 1.676e-05, MSE(pi1): 2.606e-03, MSE(pi2): 1.393e-05, MSE(pi3): 7.674e-04\n",
      "Epoch 59400, Train loss: 2.704e+02, Test loss: 1.579e+03, MSE(e): 1.676e-05, MSE(pi1): 2.606e-03, MSE(pi2): 1.393e-05, MSE(pi3): 7.674e-04\n",
      "Epoch 59500, Train loss: 2.703e+02, Test loss: 1.576e+03, MSE(e): 1.675e-05, MSE(pi1): 2.606e-03, MSE(pi2): 1.393e-05, MSE(pi3): 7.674e-04\n",
      "Epoch 59600, Train loss: 2.703e+02, Test loss: 1.574e+03, MSE(e): 1.675e-05, MSE(pi1): 2.606e-03, MSE(pi2): 1.392e-05, MSE(pi3): 7.674e-04\n",
      "Epoch 59700, Train loss: 2.702e+02, Test loss: 1.571e+03, MSE(e): 1.674e-05, MSE(pi1): 2.606e-03, MSE(pi2): 1.392e-05, MSE(pi3): 7.674e-04\n",
      "Epoch 59800, Train loss: 2.702e+02, Test loss: 1.568e+03, MSE(e): 1.674e-05, MSE(pi1): 2.605e-03, MSE(pi2): 1.391e-05, MSE(pi3): 7.674e-04\n",
      "Epoch 59900, Train loss: 2.701e+02, Test loss: 1.564e+03, MSE(e): 1.673e-05, MSE(pi1): 2.605e-03, MSE(pi2): 1.391e-05, MSE(pi3): 7.675e-04\n",
      "Epoch 60000, Train loss: 2.701e+02, Test loss: 1.562e+03, MSE(e): 1.673e-05, MSE(pi1): 2.605e-03, MSE(pi2): 1.391e-05, MSE(pi3): 7.674e-04\n",
      "Epoch 60100, Train loss: 2.700e+02, Test loss: 1.559e+03, MSE(e): 1.672e-05, MSE(pi1): 2.605e-03, MSE(pi2): 1.390e-05, MSE(pi3): 7.675e-04\n",
      "Epoch 60200, Train loss: 2.700e+02, Test loss: 1.556e+03, MSE(e): 1.672e-05, MSE(pi1): 2.605e-03, MSE(pi2): 1.390e-05, MSE(pi3): 7.674e-04\n",
      "Epoch 60300, Train loss: 2.699e+02, Test loss: 1.553e+03, MSE(e): 1.671e-05, MSE(pi1): 2.605e-03, MSE(pi2): 1.389e-05, MSE(pi3): 7.675e-04\n",
      "Epoch 60400, Train loss: 2.699e+02, Test loss: 1.550e+03, MSE(e): 1.671e-05, MSE(pi1): 2.604e-03, MSE(pi2): 1.389e-05, MSE(pi3): 7.675e-04\n",
      "Epoch 60500, Train loss: 2.699e+02, Test loss: 1.547e+03, MSE(e): 1.670e-05, MSE(pi1): 2.604e-03, MSE(pi2): 1.389e-05, MSE(pi3): 7.675e-04\n",
      "Epoch 60600, Train loss: 2.698e+02, Test loss: 1.544e+03, MSE(e): 1.670e-05, MSE(pi1): 2.604e-03, MSE(pi2): 1.388e-05, MSE(pi3): 7.675e-04\n",
      "Epoch 60700, Train loss: 2.697e+02, Test loss: 1.541e+03, MSE(e): 1.669e-05, MSE(pi1): 2.604e-03, MSE(pi2): 1.388e-05, MSE(pi3): 7.675e-04\n",
      "Epoch 60800, Train loss: 2.697e+02, Test loss: 1.538e+03, MSE(e): 1.669e-05, MSE(pi1): 2.604e-03, MSE(pi2): 1.387e-05, MSE(pi3): 7.675e-04\n",
      "Epoch 60900, Train loss: 2.697e+02, Test loss: 1.535e+03, MSE(e): 1.669e-05, MSE(pi1): 2.604e-03, MSE(pi2): 1.387e-05, MSE(pi3): 7.675e-04\n",
      "Epoch 61000, Train loss: 2.696e+02, Test loss: 1.532e+03, MSE(e): 1.668e-05, MSE(pi1): 2.604e-03, MSE(pi2): 1.387e-05, MSE(pi3): 7.675e-04\n",
      "Epoch 61100, Train loss: 2.696e+02, Test loss: 1.529e+03, MSE(e): 1.668e-05, MSE(pi1): 2.603e-03, MSE(pi2): 1.386e-05, MSE(pi3): 7.675e-04\n",
      "Epoch 61200, Train loss: 2.695e+02, Test loss: 1.526e+03, MSE(e): 1.667e-05, MSE(pi1): 2.603e-03, MSE(pi2): 1.386e-05, MSE(pi3): 7.675e-04\n",
      "Epoch 61300, Train loss: 2.695e+02, Test loss: 1.523e+03, MSE(e): 1.667e-05, MSE(pi1): 2.603e-03, MSE(pi2): 1.386e-05, MSE(pi3): 7.675e-04\n",
      "Epoch 61400, Train loss: 2.694e+02, Test loss: 1.520e+03, MSE(e): 1.666e-05, MSE(pi1): 2.603e-03, MSE(pi2): 1.385e-05, MSE(pi3): 7.676e-04\n",
      "Epoch 61500, Train loss: 2.694e+02, Test loss: 1.516e+03, MSE(e): 1.666e-05, MSE(pi1): 2.603e-03, MSE(pi2): 1.385e-05, MSE(pi3): 7.676e-04\n",
      "Epoch 61600, Train loss: 2.693e+02, Test loss: 1.513e+03, MSE(e): 1.665e-05, MSE(pi1): 2.602e-03, MSE(pi2): 1.384e-05, MSE(pi3): 7.676e-04\n",
      "Epoch 61700, Train loss: 2.693e+02, Test loss: 1.510e+03, MSE(e): 1.665e-05, MSE(pi1): 2.602e-03, MSE(pi2): 1.384e-05, MSE(pi3): 7.676e-04\n",
      "Epoch 61800, Train loss: 2.692e+02, Test loss: 1.508e+03, MSE(e): 1.664e-05, MSE(pi1): 2.602e-03, MSE(pi2): 1.384e-05, MSE(pi3): 7.676e-04\n",
      "Epoch 61900, Train loss: 2.692e+02, Test loss: 1.504e+03, MSE(e): 1.664e-05, MSE(pi1): 2.602e-03, MSE(pi2): 1.383e-05, MSE(pi3): 7.676e-04\n",
      "Epoch 62000, Train loss: 2.691e+02, Test loss: 1.501e+03, MSE(e): 1.663e-05, MSE(pi1): 2.602e-03, MSE(pi2): 1.383e-05, MSE(pi3): 7.676e-04\n",
      "Epoch 62100, Train loss: 2.691e+02, Test loss: 1.498e+03, MSE(e): 1.663e-05, MSE(pi1): 2.602e-03, MSE(pi2): 1.382e-05, MSE(pi3): 7.676e-04\n",
      "Epoch 62200, Train loss: 2.690e+02, Test loss: 1.495e+03, MSE(e): 1.662e-05, MSE(pi1): 2.601e-03, MSE(pi2): 1.382e-05, MSE(pi3): 7.676e-04\n",
      "Epoch 62300, Train loss: 2.690e+02, Test loss: 1.492e+03, MSE(e): 1.662e-05, MSE(pi1): 2.601e-03, MSE(pi2): 1.382e-05, MSE(pi3): 7.676e-04\n",
      "Epoch 62400, Train loss: 2.689e+02, Test loss: 1.489e+03, MSE(e): 1.661e-05, MSE(pi1): 2.601e-03, MSE(pi2): 1.381e-05, MSE(pi3): 7.676e-04\n",
      "Epoch 62500, Train loss: 2.688e+02, Test loss: 1.486e+03, MSE(e): 1.661e-05, MSE(pi1): 2.601e-03, MSE(pi2): 1.381e-05, MSE(pi3): 7.676e-04\n",
      "Epoch 62600, Train loss: 2.688e+02, Test loss: 1.483e+03, MSE(e): 1.660e-05, MSE(pi1): 2.601e-03, MSE(pi2): 1.380e-05, MSE(pi3): 7.677e-04\n",
      "Epoch 62700, Train loss: 2.687e+02, Test loss: 1.480e+03, MSE(e): 1.660e-05, MSE(pi1): 2.601e-03, MSE(pi2): 1.380e-05, MSE(pi3): 7.677e-04\n",
      "Epoch 62800, Train loss: 2.687e+02, Test loss: 1.477e+03, MSE(e): 1.659e-05, MSE(pi1): 2.600e-03, MSE(pi2): 1.380e-05, MSE(pi3): 7.677e-04\n",
      "Epoch 62900, Train loss: 2.686e+02, Test loss: 1.475e+03, MSE(e): 1.659e-05, MSE(pi1): 2.600e-03, MSE(pi2): 1.379e-05, MSE(pi3): 7.677e-04\n",
      "Epoch 63000, Train loss: 2.686e+02, Test loss: 1.472e+03, MSE(e): 1.658e-05, MSE(pi1): 2.600e-03, MSE(pi2): 1.379e-05, MSE(pi3): 7.677e-04\n",
      "Epoch 63100, Train loss: 2.685e+02, Test loss: 1.469e+03, MSE(e): 1.658e-05, MSE(pi1): 2.600e-03, MSE(pi2): 1.379e-05, MSE(pi3): 7.677e-04\n",
      "Epoch 63200, Train loss: 2.685e+02, Test loss: 1.466e+03, MSE(e): 1.657e-05, MSE(pi1): 2.600e-03, MSE(pi2): 1.378e-05, MSE(pi3): 7.677e-04\n",
      "Epoch 63300, Train loss: 2.684e+02, Test loss: 1.464e+03, MSE(e): 1.657e-05, MSE(pi1): 2.600e-03, MSE(pi2): 1.378e-05, MSE(pi3): 7.677e-04\n",
      "Epoch 63400, Train loss: 2.684e+02, Test loss: 1.461e+03, MSE(e): 1.656e-05, MSE(pi1): 2.599e-03, MSE(pi2): 1.377e-05, MSE(pi3): 7.677e-04\n",
      "Epoch 63500, Train loss: 2.684e+02, Test loss: 1.458e+03, MSE(e): 1.656e-05, MSE(pi1): 2.599e-03, MSE(pi2): 1.377e-05, MSE(pi3): 7.677e-04\n",
      "Epoch 63600, Train loss: 2.683e+02, Test loss: 1.455e+03, MSE(e): 1.655e-05, MSE(pi1): 2.599e-03, MSE(pi2): 1.377e-05, MSE(pi3): 7.677e-04\n",
      "Epoch 63700, Train loss: 2.683e+02, Test loss: 1.453e+03, MSE(e): 1.655e-05, MSE(pi1): 2.599e-03, MSE(pi2): 1.376e-05, MSE(pi3): 7.678e-04\n",
      "Epoch 63800, Train loss: 2.682e+02, Test loss: 1.450e+03, MSE(e): 1.654e-05, MSE(pi1): 2.599e-03, MSE(pi2): 1.376e-05, MSE(pi3): 7.677e-04\n",
      "Epoch 63900, Train loss: 2.682e+02, Test loss: 1.447e+03, MSE(e): 1.654e-05, MSE(pi1): 2.598e-03, MSE(pi2): 1.376e-05, MSE(pi3): 7.678e-04\n",
      "Epoch 64000, Train loss: 2.681e+02, Test loss: 1.444e+03, MSE(e): 1.653e-05, MSE(pi1): 2.598e-03, MSE(pi2): 1.375e-05, MSE(pi3): 7.678e-04\n",
      "Epoch 64100, Train loss: 2.681e+02, Test loss: 1.442e+03, MSE(e): 1.653e-05, MSE(pi1): 2.598e-03, MSE(pi2): 1.375e-05, MSE(pi3): 7.678e-04\n",
      "Epoch 64200, Train loss: 2.680e+02, Test loss: 1.439e+03, MSE(e): 1.653e-05, MSE(pi1): 2.598e-03, MSE(pi2): 1.375e-05, MSE(pi3): 7.678e-04\n",
      "Epoch 64300, Train loss: 2.680e+02, Test loss: 1.436e+03, MSE(e): 1.652e-05, MSE(pi1): 2.598e-03, MSE(pi2): 1.374e-05, MSE(pi3): 7.678e-04\n",
      "Epoch 64400, Train loss: 2.679e+02, Test loss: 1.432e+03, MSE(e): 1.652e-05, MSE(pi1): 2.598e-03, MSE(pi2): 1.374e-05, MSE(pi3): 7.678e-04\n",
      "Epoch 64500, Train loss: 2.679e+02, Test loss: 1.429e+03, MSE(e): 1.651e-05, MSE(pi1): 2.598e-03, MSE(pi2): 1.373e-05, MSE(pi3): 7.678e-04\n",
      "Epoch 64600, Train loss: 2.678e+02, Test loss: 1.426e+03, MSE(e): 1.651e-05, MSE(pi1): 2.597e-03, MSE(pi2): 1.373e-05, MSE(pi3): 7.678e-04\n",
      "Epoch 64700, Train loss: 2.678e+02, Test loss: 1.423e+03, MSE(e): 1.650e-05, MSE(pi1): 2.597e-03, MSE(pi2): 1.373e-05, MSE(pi3): 7.678e-04\n",
      "Epoch 64800, Train loss: 2.677e+02, Test loss: 1.420e+03, MSE(e): 1.650e-05, MSE(pi1): 2.597e-03, MSE(pi2): 1.372e-05, MSE(pi3): 7.678e-04\n",
      "Epoch 64900, Train loss: 2.677e+02, Test loss: 1.417e+03, MSE(e): 1.649e-05, MSE(pi1): 2.597e-03, MSE(pi2): 1.372e-05, MSE(pi3): 7.679e-04\n",
      "Epoch 65000, Train loss: 2.676e+02, Test loss: 1.414e+03, MSE(e): 1.649e-05, MSE(pi1): 2.597e-03, MSE(pi2): 1.372e-05, MSE(pi3): 7.678e-04\n",
      "Epoch 65100, Train loss: 2.676e+02, Test loss: 1.412e+03, MSE(e): 1.648e-05, MSE(pi1): 2.596e-03, MSE(pi2): 1.371e-05, MSE(pi3): 7.679e-04\n",
      "Epoch 65200, Train loss: 2.675e+02, Test loss: 1.409e+03, MSE(e): 1.648e-05, MSE(pi1): 2.596e-03, MSE(pi2): 1.371e-05, MSE(pi3): 7.679e-04\n",
      "Epoch 65300, Train loss: 2.675e+02, Test loss: 1.406e+03, MSE(e): 1.647e-05, MSE(pi1): 2.596e-03, MSE(pi2): 1.371e-05, MSE(pi3): 7.679e-04\n",
      "Epoch 65400, Train loss: 2.674e+02, Test loss: 1.403e+03, MSE(e): 1.647e-05, MSE(pi1): 2.596e-03, MSE(pi2): 1.370e-05, MSE(pi3): 7.679e-04\n",
      "Epoch 65500, Train loss: 2.674e+02, Test loss: 1.400e+03, MSE(e): 1.646e-05, MSE(pi1): 2.596e-03, MSE(pi2): 1.370e-05, MSE(pi3): 7.678e-04\n",
      "Epoch 65600, Train loss: 2.674e+02, Test loss: 1.397e+03, MSE(e): 1.646e-05, MSE(pi1): 2.595e-03, MSE(pi2): 1.369e-05, MSE(pi3): 7.680e-04\n",
      "Epoch 65700, Train loss: 2.673e+02, Test loss: 1.395e+03, MSE(e): 1.645e-05, MSE(pi1): 2.596e-03, MSE(pi2): 1.369e-05, MSE(pi3): 7.679e-04\n",
      "Epoch 65800, Train loss: 2.673e+02, Test loss: 1.392e+03, MSE(e): 1.645e-05, MSE(pi1): 2.595e-03, MSE(pi2): 1.369e-05, MSE(pi3): 7.679e-04\n",
      "Epoch 65900, Train loss: 2.672e+02, Test loss: 1.389e+03, MSE(e): 1.645e-05, MSE(pi1): 2.595e-03, MSE(pi2): 1.368e-05, MSE(pi3): 7.681e-04\n",
      "Epoch 66000, Train loss: 2.671e+02, Test loss: 1.386e+03, MSE(e): 1.644e-05, MSE(pi1): 2.595e-03, MSE(pi2): 1.368e-05, MSE(pi3): 7.679e-04\n",
      "Epoch 66100, Train loss: 2.671e+02, Test loss: 1.384e+03, MSE(e): 1.643e-05, MSE(pi1): 2.594e-03, MSE(pi2): 1.368e-05, MSE(pi3): 7.680e-04\n",
      "Epoch 66200, Train loss: 2.671e+02, Test loss: 1.381e+03, MSE(e): 1.643e-05, MSE(pi1): 2.594e-03, MSE(pi2): 1.367e-05, MSE(pi3): 7.681e-04\n",
      "Epoch 66300, Train loss: 2.670e+02, Test loss: 1.378e+03, MSE(e): 1.642e-05, MSE(pi1): 2.595e-03, MSE(pi2): 1.367e-05, MSE(pi3): 7.679e-04\n",
      "Epoch 66400, Train loss: 2.669e+02, Test loss: 1.375e+03, MSE(e): 1.642e-05, MSE(pi1): 2.594e-03, MSE(pi2): 1.366e-05, MSE(pi3): 7.680e-04\n",
      "Epoch 66500, Train loss: 2.669e+02, Test loss: 1.372e+03, MSE(e): 1.641e-05, MSE(pi1): 2.593e-03, MSE(pi2): 1.366e-05, MSE(pi3): 7.682e-04\n",
      "Epoch 66600, Train loss: 2.669e+02, Test loss: 1.370e+03, MSE(e): 1.641e-05, MSE(pi1): 2.594e-03, MSE(pi2): 1.366e-05, MSE(pi3): 7.679e-04\n",
      "Epoch 66700, Train loss: 2.668e+02, Test loss: 1.367e+03, MSE(e): 1.641e-05, MSE(pi1): 2.594e-03, MSE(pi2): 1.365e-05, MSE(pi3): 7.679e-04\n",
      "Epoch 66800, Train loss: 2.668e+02, Test loss: 1.364e+03, MSE(e): 1.640e-05, MSE(pi1): 2.593e-03, MSE(pi2): 1.365e-05, MSE(pi3): 7.682e-04\n",
      "Epoch 66900, Train loss: 2.667e+02, Test loss: 1.362e+03, MSE(e): 1.640e-05, MSE(pi1): 2.593e-03, MSE(pi2): 1.365e-05, MSE(pi3): 7.680e-04\n",
      "Epoch 67000, Train loss: 2.667e+02, Test loss: 1.359e+03, MSE(e): 1.639e-05, MSE(pi1): 2.594e-03, MSE(pi2): 1.364e-05, MSE(pi3): 7.679e-04\n",
      "Epoch 67100, Train loss: 2.666e+02, Test loss: 1.356e+03, MSE(e): 1.639e-05, MSE(pi1): 2.593e-03, MSE(pi2): 1.364e-05, MSE(pi3): 7.680e-04\n",
      "Epoch 67200, Train loss: 2.666e+02, Test loss: 1.353e+03, MSE(e): 1.638e-05, MSE(pi1): 2.592e-03, MSE(pi2): 1.364e-05, MSE(pi3): 7.683e-04\n",
      "Epoch 67300, Train loss: 2.665e+02, Test loss: 1.350e+03, MSE(e): 1.638e-05, MSE(pi1): 2.593e-03, MSE(pi2): 1.363e-05, MSE(pi3): 7.680e-04\n",
      "Epoch 67400, Train loss: 2.665e+02, Test loss: 1.348e+03, MSE(e): 1.638e-05, MSE(pi1): 2.594e-03, MSE(pi2): 1.363e-05, MSE(pi3): 7.678e-04\n",
      "Epoch 67500, Train loss: 2.665e+02, Test loss: 1.345e+03, MSE(e): 1.637e-05, MSE(pi1): 2.595e-03, MSE(pi2): 1.363e-05, MSE(pi3): 7.677e-04\n",
      "Epoch 67600, Train loss: 2.664e+02, Test loss: 1.342e+03, MSE(e): 1.637e-05, MSE(pi1): 2.594e-03, MSE(pi2): 1.362e-05, MSE(pi3): 7.679e-04\n",
      "Epoch 67700, Train loss: 2.664e+02, Test loss: 1.340e+03, MSE(e): 1.636e-05, MSE(pi1): 2.594e-03, MSE(pi2): 1.362e-05, MSE(pi3): 7.680e-04\n",
      "Epoch 67800, Train loss: 2.663e+02, Test loss: 1.337e+03, MSE(e): 1.636e-05, MSE(pi1): 2.596e-03, MSE(pi2): 1.362e-05, MSE(pi3): 7.677e-04\n",
      "Epoch 67900, Train loss: 2.663e+02, Test loss: 1.335e+03, MSE(e): 1.635e-05, MSE(pi1): 2.596e-03, MSE(pi2): 1.361e-05, MSE(pi3): 7.676e-04\n",
      "Epoch 68000, Train loss: 2.662e+02, Test loss: 1.332e+03, MSE(e): 1.635e-05, MSE(pi1): 2.596e-03, MSE(pi2): 1.361e-05, MSE(pi3): 7.675e-04\n",
      "Epoch 68100, Train loss: 2.662e+02, Test loss: 1.330e+03, MSE(e): 1.634e-05, MSE(pi1): 2.596e-03, MSE(pi2): 1.361e-05, MSE(pi3): 7.676e-04\n",
      "Epoch 68200, Train loss: 2.661e+02, Test loss: 1.327e+03, MSE(e): 1.634e-05, MSE(pi1): 2.594e-03, MSE(pi2): 1.360e-05, MSE(pi3): 7.678e-04\n",
      "Epoch 68300, Train loss: 2.661e+02, Test loss: 1.324e+03, MSE(e): 1.634e-05, MSE(pi1): 2.593e-03, MSE(pi2): 1.360e-05, MSE(pi3): 7.681e-04\n",
      "Epoch 68400, Train loss: 2.661e+02, Test loss: 1.322e+03, MSE(e): 1.633e-05, MSE(pi1): 2.593e-03, MSE(pi2): 1.360e-05, MSE(pi3): 7.681e-04\n",
      "Epoch 68500, Train loss: 2.660e+02, Test loss: 1.319e+03, MSE(e): 1.633e-05, MSE(pi1): 2.594e-03, MSE(pi2): 1.359e-05, MSE(pi3): 7.680e-04\n",
      "Epoch 68600, Train loss: 2.660e+02, Test loss: 1.317e+03, MSE(e): 1.632e-05, MSE(pi1): 2.594e-03, MSE(pi2): 1.359e-05, MSE(pi3): 7.678e-04\n",
      "Epoch 68700, Train loss: 2.659e+02, Test loss: 1.315e+03, MSE(e): 1.632e-05, MSE(pi1): 2.595e-03, MSE(pi2): 1.359e-05, MSE(pi3): 7.677e-04\n",
      "Epoch 68800, Train loss: 2.659e+02, Test loss: 1.312e+03, MSE(e): 1.631e-05, MSE(pi1): 2.595e-03, MSE(pi2): 1.358e-05, MSE(pi3): 7.676e-04\n",
      "Epoch 68900, Train loss: 2.658e+02, Test loss: 1.310e+03, MSE(e): 1.631e-05, MSE(pi1): 2.595e-03, MSE(pi2): 1.358e-05, MSE(pi3): 7.676e-04\n",
      "Epoch 69000, Train loss: 2.658e+02, Test loss: 1.307e+03, MSE(e): 1.631e-05, MSE(pi1): 2.595e-03, MSE(pi2): 1.358e-05, MSE(pi3): 7.676e-04\n",
      "Epoch 69100, Train loss: 2.657e+02, Test loss: 1.305e+03, MSE(e): 1.630e-05, MSE(pi1): 2.595e-03, MSE(pi2): 1.357e-05, MSE(pi3): 7.676e-04\n",
      "Epoch 69200, Train loss: 2.657e+02, Test loss: 1.303e+03, MSE(e): 1.630e-05, MSE(pi1): 2.595e-03, MSE(pi2): 1.357e-05, MSE(pi3): 7.676e-04\n",
      "Epoch 69300, Train loss: 2.657e+02, Test loss: 1.300e+03, MSE(e): 1.629e-05, MSE(pi1): 2.594e-03, MSE(pi2): 1.357e-05, MSE(pi3): 7.676e-04\n",
      "Epoch 69400, Train loss: 2.656e+02, Test loss: 1.298e+03, MSE(e): 1.629e-05, MSE(pi1): 2.594e-03, MSE(pi2): 1.356e-05, MSE(pi3): 7.676e-04\n",
      "Epoch 69500, Train loss: 2.656e+02, Test loss: 1.296e+03, MSE(e): 1.629e-05, MSE(pi1): 2.594e-03, MSE(pi2): 1.356e-05, MSE(pi3): 7.677e-04\n",
      "Epoch 69600, Train loss: 2.655e+02, Test loss: 1.293e+03, MSE(e): 1.628e-05, MSE(pi1): 2.594e-03, MSE(pi2): 1.356e-05, MSE(pi3): 7.677e-04\n",
      "Epoch 69700, Train loss: 2.655e+02, Test loss: 1.291e+03, MSE(e): 1.628e-05, MSE(pi1): 2.594e-03, MSE(pi2): 1.355e-05, MSE(pi3): 7.677e-04\n",
      "Epoch 69800, Train loss: 2.654e+02, Test loss: 1.289e+03, MSE(e): 1.627e-05, MSE(pi1): 2.594e-03, MSE(pi2): 1.355e-05, MSE(pi3): 7.677e-04\n",
      "Epoch 69900, Train loss: 2.654e+02, Test loss: 1.286e+03, MSE(e): 1.627e-05, MSE(pi1): 2.593e-03, MSE(pi2): 1.355e-05, MSE(pi3): 7.677e-04\n",
      "Epoch 70000, Train loss: 2.653e+02, Test loss: 1.284e+03, MSE(e): 1.626e-05, MSE(pi1): 2.593e-03, MSE(pi2): 1.354e-05, MSE(pi3): 7.677e-04\n",
      "Epoch 70100, Train loss: 2.653e+02, Test loss: 1.282e+03, MSE(e): 1.626e-05, MSE(pi1): 2.593e-03, MSE(pi2): 1.354e-05, MSE(pi3): 7.677e-04\n",
      "Epoch 70200, Train loss: 2.652e+02, Test loss: 1.279e+03, MSE(e): 1.625e-05, MSE(pi1): 2.592e-03, MSE(pi2): 1.354e-05, MSE(pi3): 7.678e-04\n",
      "Epoch 70300, Train loss: 2.652e+02, Test loss: 1.277e+03, MSE(e): 1.625e-05, MSE(pi1): 2.591e-03, MSE(pi2): 1.353e-05, MSE(pi3): 7.680e-04\n",
      "Epoch 70400, Train loss: 2.652e+02, Test loss: 1.275e+03, MSE(e): 1.624e-05, MSE(pi1): 2.590e-03, MSE(pi2): 1.353e-05, MSE(pi3): 7.682e-04\n",
      "Epoch 70500, Train loss: 2.651e+02, Test loss: 1.272e+03, MSE(e): 1.624e-05, MSE(pi1): 2.589e-03, MSE(pi2): 1.353e-05, MSE(pi3): 7.684e-04\n",
      "Epoch 70600, Train loss: 2.651e+02, Test loss: 1.270e+03, MSE(e): 1.623e-05, MSE(pi1): 2.589e-03, MSE(pi2): 1.352e-05, MSE(pi3): 7.684e-04\n",
      "Epoch 70700, Train loss: 2.650e+02, Test loss: 1.268e+03, MSE(e): 1.623e-05, MSE(pi1): 2.590e-03, MSE(pi2): 1.352e-05, MSE(pi3): 7.682e-04\n",
      "Epoch 70800, Train loss: 2.650e+02, Test loss: 1.266e+03, MSE(e): 1.623e-05, MSE(pi1): 2.591e-03, MSE(pi2): 1.352e-05, MSE(pi3): 7.680e-04\n",
      "Epoch 70900, Train loss: 2.649e+02, Test loss: 1.263e+03, MSE(e): 1.622e-05, MSE(pi1): 2.591e-03, MSE(pi2): 1.351e-05, MSE(pi3): 7.678e-04\n",
      "Epoch 71000, Train loss: 2.649e+02, Test loss: 1.261e+03, MSE(e): 1.622e-05, MSE(pi1): 2.592e-03, MSE(pi2): 1.351e-05, MSE(pi3): 7.678e-04\n",
      "Epoch 71100, Train loss: 2.649e+02, Test loss: 1.259e+03, MSE(e): 1.622e-05, MSE(pi1): 2.591e-03, MSE(pi2): 1.351e-05, MSE(pi3): 7.678e-04\n",
      "Epoch 71200, Train loss: 2.648e+02, Test loss: 1.257e+03, MSE(e): 1.621e-05, MSE(pi1): 2.590e-03, MSE(pi2): 1.350e-05, MSE(pi3): 7.679e-04\n",
      "Epoch 71300, Train loss: 2.648e+02, Test loss: 1.255e+03, MSE(e): 1.621e-05, MSE(pi1): 2.588e-03, MSE(pi2): 1.350e-05, MSE(pi3): 7.686e-04\n",
      "Epoch 71400, Train loss: 2.647e+02, Test loss: 1.253e+03, MSE(e): 1.620e-05, MSE(pi1): 2.589e-03, MSE(pi2): 1.350e-05, MSE(pi3): 7.682e-04\n",
      "Epoch 71500, Train loss: 2.647e+02, Test loss: 1.251e+03, MSE(e): 1.620e-05, MSE(pi1): 2.590e-03, MSE(pi2): 1.349e-05, MSE(pi3): 7.679e-04\n",
      "Epoch 71600, Train loss: 2.646e+02, Test loss: 1.249e+03, MSE(e): 1.619e-05, MSE(pi1): 2.591e-03, MSE(pi2): 1.349e-05, MSE(pi3): 7.678e-04\n",
      "Epoch 71700, Train loss: 2.646e+02, Test loss: 1.246e+03, MSE(e): 1.619e-05, MSE(pi1): 2.590e-03, MSE(pi2): 1.349e-05, MSE(pi3): 7.679e-04\n",
      "Epoch 71800, Train loss: 2.646e+02, Test loss: 1.244e+03, MSE(e): 1.619e-05, MSE(pi1): 2.589e-03, MSE(pi2): 1.348e-05, MSE(pi3): 7.682e-04\n",
      "Epoch 71900, Train loss: 2.646e+02, Test loss: 1.242e+03, MSE(e): 1.618e-05, MSE(pi1): 2.587e-03, MSE(pi2): 1.348e-05, MSE(pi3): 7.685e-04\n",
      "Epoch 72000, Train loss: 2.645e+02, Test loss: 1.240e+03, MSE(e): 1.618e-05, MSE(pi1): 2.590e-03, MSE(pi2): 1.348e-05, MSE(pi3): 7.679e-04\n",
      "Epoch 72100, Train loss: 2.644e+02, Test loss: 1.238e+03, MSE(e): 1.617e-05, MSE(pi1): 2.590e-03, MSE(pi2): 1.347e-05, MSE(pi3): 7.679e-04\n",
      "Epoch 72200, Train loss: 2.644e+02, Test loss: 1.236e+03, MSE(e): 1.617e-05, MSE(pi1): 2.589e-03, MSE(pi2): 1.347e-05, MSE(pi3): 7.680e-04\n",
      "Epoch 72300, Train loss: 2.644e+02, Test loss: 1.233e+03, MSE(e): 1.616e-05, MSE(pi1): 2.586e-03, MSE(pi2): 1.347e-05, MSE(pi3): 7.687e-04\n",
      "Epoch 72400, Train loss: 2.643e+02, Test loss: 1.231e+03, MSE(e): 1.616e-05, MSE(pi1): 2.589e-03, MSE(pi2): 1.347e-05, MSE(pi3): 7.680e-04\n",
      "Epoch 72500, Train loss: 2.643e+02, Test loss: 1.229e+03, MSE(e): 1.616e-05, MSE(pi1): 2.589e-03, MSE(pi2): 1.346e-05, MSE(pi3): 7.679e-04\n",
      "Epoch 72600, Train loss: 2.642e+02, Test loss: 1.227e+03, MSE(e): 1.615e-05, MSE(pi1): 2.587e-03, MSE(pi2): 1.346e-05, MSE(pi3): 7.683e-04\n",
      "Epoch 72700, Train loss: 2.642e+02, Test loss: 1.225e+03, MSE(e): 1.615e-05, MSE(pi1): 2.587e-03, MSE(pi2): 1.346e-05, MSE(pi3): 7.683e-04\n",
      "Epoch 72800, Train loss: 2.641e+02, Test loss: 1.223e+03, MSE(e): 1.614e-05, MSE(pi1): 2.589e-03, MSE(pi2): 1.345e-05, MSE(pi3): 7.679e-04\n",
      "Epoch 72900, Train loss: 2.641e+02, Test loss: 1.221e+03, MSE(e): 1.614e-05, MSE(pi1): 2.588e-03, MSE(pi2): 1.345e-05, MSE(pi3): 7.680e-04\n",
      "Epoch 73000, Train loss: 2.641e+02, Test loss: 1.218e+03, MSE(e): 1.614e-05, MSE(pi1): 2.586e-03, MSE(pi2): 1.345e-05, MSE(pi3): 7.686e-04\n",
      "Epoch 73100, Train loss: 2.640e+02, Test loss: 1.217e+03, MSE(e): 1.613e-05, MSE(pi1): 2.588e-03, MSE(pi2): 1.344e-05, MSE(pi3): 7.679e-04\n",
      "Epoch 73200, Train loss: 2.640e+02, Test loss: 1.214e+03, MSE(e): 1.613e-05, MSE(pi1): 2.587e-03, MSE(pi2): 1.344e-05, MSE(pi3): 7.682e-04\n",
      "Epoch 73300, Train loss: 2.639e+02, Test loss: 1.212e+03, MSE(e): 1.612e-05, MSE(pi1): 2.588e-03, MSE(pi2): 1.344e-05, MSE(pi3): 7.681e-04\n",
      "Epoch 73400, Train loss: 2.639e+02, Test loss: 1.210e+03, MSE(e): 1.612e-05, MSE(pi1): 2.588e-03, MSE(pi2): 1.343e-05, MSE(pi3): 7.680e-04\n",
      "Epoch 73500, Train loss: 2.639e+02, Test loss: 1.208e+03, MSE(e): 1.611e-05, MSE(pi1): 2.584e-03, MSE(pi2): 1.343e-05, MSE(pi3): 7.689e-04\n",
      "Epoch 73600, Train loss: 2.638e+02, Test loss: 1.206e+03, MSE(e): 1.611e-05, MSE(pi1): 2.587e-03, MSE(pi2): 1.343e-05, MSE(pi3): 7.680e-04\n",
      "Epoch 73700, Train loss: 2.638e+02, Test loss: 1.204e+03, MSE(e): 1.611e-05, MSE(pi1): 2.587e-03, MSE(pi2): 1.342e-05, MSE(pi3): 7.681e-04\n",
      "Epoch 73800, Train loss: 2.637e+02, Test loss: 1.202e+03, MSE(e): 1.610e-05, MSE(pi1): 2.586e-03, MSE(pi2): 1.342e-05, MSE(pi3): 7.683e-04\n",
      "Epoch 73900, Train loss: 2.637e+02, Test loss: 1.200e+03, MSE(e): 1.610e-05, MSE(pi1): 2.587e-03, MSE(pi2): 1.342e-05, MSE(pi3): 7.680e-04\n",
      "Epoch 74000, Train loss: 2.637e+02, Test loss: 1.198e+03, MSE(e): 1.610e-05, MSE(pi1): 2.583e-03, MSE(pi2): 1.342e-05, MSE(pi3): 7.690e-04\n",
      "Epoch 74100, Train loss: 2.636e+02, Test loss: 1.196e+03, MSE(e): 1.609e-05, MSE(pi1): 2.587e-03, MSE(pi2): 1.341e-05, MSE(pi3): 7.680e-04\n",
      "Epoch 74200, Train loss: 2.636e+02, Test loss: 1.194e+03, MSE(e): 1.609e-05, MSE(pi1): 2.584e-03, MSE(pi2): 1.341e-05, MSE(pi3): 7.686e-04\n",
      "Epoch 74300, Train loss: 2.635e+02, Test loss: 1.192e+03, MSE(e): 1.608e-05, MSE(pi1): 2.587e-03, MSE(pi2): 1.341e-05, MSE(pi3): 7.681e-04\n",
      "Epoch 74400, Train loss: 2.635e+02, Test loss: 1.190e+03, MSE(e): 1.608e-05, MSE(pi1): 2.585e-03, MSE(pi2): 1.340e-05, MSE(pi3): 7.684e-04\n",
      "Epoch 74500, Train loss: 2.635e+02, Test loss: 1.188e+03, MSE(e): 1.608e-05, MSE(pi1): 2.586e-03, MSE(pi2): 1.340e-05, MSE(pi3): 7.681e-04\n",
      "Epoch 74600, Train loss: 2.634e+02, Test loss: 1.186e+03, MSE(e): 1.607e-05, MSE(pi1): 2.584e-03, MSE(pi2): 1.340e-05, MSE(pi3): 7.684e-04\n",
      "Epoch 74700, Train loss: 2.634e+02, Test loss: 1.185e+03, MSE(e): 1.607e-05, MSE(pi1): 2.586e-03, MSE(pi2): 1.339e-05, MSE(pi3): 7.681e-04\n",
      "Epoch 74800, Train loss: 2.634e+02, Test loss: 1.182e+03, MSE(e): 1.606e-05, MSE(pi1): 2.582e-03, MSE(pi2): 1.339e-05, MSE(pi3): 7.690e-04\n",
      "Epoch 74900, Train loss: 2.633e+02, Test loss: 1.181e+03, MSE(e): 1.606e-05, MSE(pi1): 2.585e-03, MSE(pi2): 1.339e-05, MSE(pi3): 7.681e-04\n",
      "Epoch 75000, Train loss: 2.633e+02, Test loss: 1.179e+03, MSE(e): 1.606e-05, MSE(pi1): 2.581e-03, MSE(pi2): 1.339e-05, MSE(pi3): 7.692e-04\n",
      "Epoch 75100, Train loss: 2.632e+02, Test loss: 1.177e+03, MSE(e): 1.605e-05, MSE(pi1): 2.585e-03, MSE(pi2): 1.338e-05, MSE(pi3): 7.681e-04\n",
      "Epoch 75200, Train loss: 2.632e+02, Test loss: 1.175e+03, MSE(e): 1.605e-05, MSE(pi1): 2.582e-03, MSE(pi2): 1.338e-05, MSE(pi3): 7.690e-04\n",
      "Epoch 75300, Train loss: 2.631e+02, Test loss: 1.173e+03, MSE(e): 1.604e-05, MSE(pi1): 2.585e-03, MSE(pi2): 1.338e-05, MSE(pi3): 7.682e-04\n",
      "Epoch 75400, Train loss: 2.631e+02, Test loss: 1.171e+03, MSE(e): 1.604e-05, MSE(pi1): 2.584e-03, MSE(pi2): 1.337e-05, MSE(pi3): 7.684e-04\n",
      "Epoch 75500, Train loss: 2.630e+02, Test loss: 1.169e+03, MSE(e): 1.603e-05, MSE(pi1): 2.584e-03, MSE(pi2): 1.337e-05, MSE(pi3): 7.682e-04\n",
      "Epoch 75600, Train loss: 2.630e+02, Test loss: 1.167e+03, MSE(e): 1.603e-05, MSE(pi1): 2.584e-03, MSE(pi2): 1.337e-05, MSE(pi3): 7.682e-04\n",
      "Epoch 75700, Train loss: 2.630e+02, Test loss: 1.165e+03, MSE(e): 1.603e-05, MSE(pi1): 2.581e-03, MSE(pi2): 1.336e-05, MSE(pi3): 7.690e-04\n",
      "Epoch 75800, Train loss: 2.629e+02, Test loss: 1.164e+03, MSE(e): 1.602e-05, MSE(pi1): 2.584e-03, MSE(pi2): 1.336e-05, MSE(pi3): 7.682e-04\n",
      "Epoch 75900, Train loss: 2.629e+02, Test loss: 1.162e+03, MSE(e): 1.602e-05, MSE(pi1): 2.584e-03, MSE(pi2): 1.336e-05, MSE(pi3): 7.684e-04\n",
      "Epoch 76000, Train loss: 2.628e+02, Test loss: 1.160e+03, MSE(e): 1.601e-05, MSE(pi1): 2.582e-03, MSE(pi2): 1.335e-05, MSE(pi3): 7.684e-04\n",
      "Epoch 76100, Train loss: 2.628e+02, Test loss: 1.158e+03, MSE(e): 1.601e-05, MSE(pi1): 2.583e-03, MSE(pi2): 1.335e-05, MSE(pi3): 7.682e-04\n",
      "Epoch 76200, Train loss: 2.628e+02, Test loss: 1.156e+03, MSE(e): 1.601e-05, MSE(pi1): 2.580e-03, MSE(pi2): 1.335e-05, MSE(pi3): 7.692e-04\n",
      "Epoch 76300, Train loss: 2.627e+02, Test loss: 1.154e+03, MSE(e): 1.600e-05, MSE(pi1): 2.583e-03, MSE(pi2): 1.334e-05, MSE(pi3): 7.683e-04\n",
      "Epoch 76400, Train loss: 2.627e+02, Test loss: 1.152e+03, MSE(e): 1.600e-05, MSE(pi1): 2.583e-03, MSE(pi2): 1.334e-05, MSE(pi3): 7.683e-04\n",
      "Epoch 76500, Train loss: 2.627e+02, Test loss: 1.150e+03, MSE(e): 1.599e-05, MSE(pi1): 2.578e-03, MSE(pi2): 1.334e-05, MSE(pi3): 7.695e-04\n",
      "Epoch 76600, Train loss: 2.626e+02, Test loss: 1.148e+03, MSE(e): 1.599e-05, MSE(pi1): 2.582e-03, MSE(pi2): 1.334e-05, MSE(pi3): 7.683e-04\n",
      "Epoch 76700, Train loss: 2.625e+02, Test loss: 1.146e+03, MSE(e): 1.599e-05, MSE(pi1): 2.583e-03, MSE(pi2): 1.333e-05, MSE(pi3): 7.683e-04\n",
      "Epoch 76800, Train loss: 2.625e+02, Test loss: 1.144e+03, MSE(e): 1.598e-05, MSE(pi1): 2.578e-03, MSE(pi2): 1.333e-05, MSE(pi3): 7.695e-04\n",
      "Epoch 76900, Train loss: 2.624e+02, Test loss: 1.143e+03, MSE(e): 1.598e-05, MSE(pi1): 2.582e-03, MSE(pi2): 1.333e-05, MSE(pi3): 7.684e-04\n",
      "Epoch 77000, Train loss: 2.624e+02, Test loss: 1.141e+03, MSE(e): 1.597e-05, MSE(pi1): 2.582e-03, MSE(pi2): 1.332e-05, MSE(pi3): 7.684e-04\n",
      "Epoch 77100, Train loss: 2.624e+02, Test loss: 1.139e+03, MSE(e): 1.597e-05, MSE(pi1): 2.577e-03, MSE(pi2): 1.332e-05, MSE(pi3): 7.695e-04\n",
      "Epoch 77200, Train loss: 2.623e+02, Test loss: 1.138e+03, MSE(e): 1.597e-05, MSE(pi1): 2.581e-03, MSE(pi2): 1.332e-05, MSE(pi3): 7.684e-04\n",
      "Epoch 77300, Train loss: 2.623e+02, Test loss: 1.136e+03, MSE(e): 1.596e-05, MSE(pi1): 2.581e-03, MSE(pi2): 1.332e-05, MSE(pi3): 7.684e-04\n",
      "Epoch 77400, Train loss: 2.623e+02, Test loss: 1.134e+03, MSE(e): 1.596e-05, MSE(pi1): 2.579e-03, MSE(pi2): 1.331e-05, MSE(pi3): 7.690e-04\n",
      "Epoch 77500, Train loss: 2.622e+02, Test loss: 1.133e+03, MSE(e): 1.595e-05, MSE(pi1): 2.579e-03, MSE(pi2): 1.331e-05, MSE(pi3): 7.686e-04\n",
      "Epoch 77600, Train loss: 2.622e+02, Test loss: 1.131e+03, MSE(e): 1.595e-05, MSE(pi1): 2.581e-03, MSE(pi2): 1.331e-05, MSE(pi3): 7.684e-04\n",
      "Epoch 77700, Train loss: 2.621e+02, Test loss: 1.129e+03, MSE(e): 1.595e-05, MSE(pi1): 2.581e-03, MSE(pi2): 1.330e-05, MSE(pi3): 7.684e-04\n",
      "Epoch 77800, Train loss: 2.621e+02, Test loss: 1.127e+03, MSE(e): 1.594e-05, MSE(pi1): 2.578e-03, MSE(pi2): 1.330e-05, MSE(pi3): 7.692e-04\n",
      "Epoch 77900, Train loss: 2.621e+02, Test loss: 1.126e+03, MSE(e): 1.594e-05, MSE(pi1): 2.579e-03, MSE(pi2): 1.330e-05, MSE(pi3): 7.688e-04\n",
      "Epoch 78000, Train loss: 2.620e+02, Test loss: 1.124e+03, MSE(e): 1.593e-05, MSE(pi1): 2.580e-03, MSE(pi2): 1.330e-05, MSE(pi3): 7.685e-04\n",
      "Epoch 78100, Train loss: 2.620e+02, Test loss: 1.123e+03, MSE(e): 1.593e-05, MSE(pi1): 2.580e-03, MSE(pi2): 1.329e-05, MSE(pi3): 7.684e-04\n",
      "Epoch 78200, Train loss: 2.620e+02, Test loss: 1.121e+03, MSE(e): 1.593e-05, MSE(pi1): 2.579e-03, MSE(pi2): 1.329e-05, MSE(pi3): 7.689e-04\n",
      "Epoch 78300, Train loss: 2.619e+02, Test loss: 1.119e+03, MSE(e): 1.592e-05, MSE(pi1): 2.575e-03, MSE(pi2): 1.329e-05, MSE(pi3): 7.695e-04\n",
      "Epoch 78400, Train loss: 2.619e+02, Test loss: 1.117e+03, MSE(e): 1.592e-05, MSE(pi1): 2.579e-03, MSE(pi2): 1.328e-05, MSE(pi3): 7.686e-04\n",
      "Epoch 78500, Train loss: 2.618e+02, Test loss: 1.116e+03, MSE(e): 1.592e-05, MSE(pi1): 2.579e-03, MSE(pi2): 1.328e-05, MSE(pi3): 7.685e-04\n",
      "Epoch 78600, Train loss: 2.618e+02, Test loss: 1.114e+03, MSE(e): 1.591e-05, MSE(pi1): 2.579e-03, MSE(pi2): 1.328e-05, MSE(pi3): 7.685e-04\n",
      "Epoch 78700, Train loss: 2.618e+02, Test loss: 1.112e+03, MSE(e): 1.591e-05, MSE(pi1): 2.580e-03, MSE(pi2): 1.328e-05, MSE(pi3): 7.685e-04\n",
      "Epoch 78800, Train loss: 2.618e+02, Test loss: 1.110e+03, MSE(e): 1.591e-05, MSE(pi1): 2.576e-03, MSE(pi2): 1.327e-05, MSE(pi3): 7.694e-04\n",
      "Epoch 78900, Train loss: 2.617e+02, Test loss: 1.109e+03, MSE(e): 1.590e-05, MSE(pi1): 2.575e-03, MSE(pi2): 1.327e-05, MSE(pi3): 7.694e-04\n",
      "Epoch 79000, Train loss: 2.616e+02, Test loss: 1.107e+03, MSE(e): 1.590e-05, MSE(pi1): 2.578e-03, MSE(pi2): 1.327e-05, MSE(pi3): 7.687e-04\n",
      "Epoch 79100, Train loss: 2.616e+02, Test loss: 1.106e+03, MSE(e): 1.589e-05, MSE(pi1): 2.578e-03, MSE(pi2): 1.327e-05, MSE(pi3): 7.686e-04\n",
      "Epoch 79200, Train loss: 2.615e+02, Test loss: 1.104e+03, MSE(e): 1.589e-05, MSE(pi1): 2.578e-03, MSE(pi2): 1.326e-05, MSE(pi3): 7.686e-04\n",
      "Epoch 79300, Train loss: 2.615e+02, Test loss: 1.103e+03, MSE(e): 1.589e-05, MSE(pi1): 2.578e-03, MSE(pi2): 1.326e-05, MSE(pi3): 7.686e-04\n",
      "Epoch 79400, Train loss: 2.615e+02, Test loss: 1.101e+03, MSE(e): 1.588e-05, MSE(pi1): 2.578e-03, MSE(pi2): 1.326e-05, MSE(pi3): 7.686e-04\n",
      "Epoch 79500, Train loss: 2.614e+02, Test loss: 1.099e+03, MSE(e): 1.588e-05, MSE(pi1): 2.578e-03, MSE(pi2): 1.325e-05, MSE(pi3): 7.686e-04\n",
      "Epoch 79600, Train loss: 2.614e+02, Test loss: 1.098e+03, MSE(e): 1.587e-05, MSE(pi1): 2.578e-03, MSE(pi2): 1.325e-05, MSE(pi3): 7.688e-04\n",
      "Epoch 79700, Train loss: 2.614e+02, Test loss: 1.096e+03, MSE(e): 1.587e-05, MSE(pi1): 2.575e-03, MSE(pi2): 1.325e-05, MSE(pi3): 7.694e-04\n",
      "Epoch 79800, Train loss: 2.614e+02, Test loss: 1.094e+03, MSE(e): 1.586e-05, MSE(pi1): 2.572e-03, MSE(pi2): 1.324e-05, MSE(pi3): 7.700e-04\n",
      "Epoch 79900, Train loss: 2.614e+02, Test loss: 1.093e+03, MSE(e): 1.586e-05, MSE(pi1): 2.572e-03, MSE(pi2): 1.324e-05, MSE(pi3): 7.700e-04\n",
      "Epoch 80000, Train loss: 2.613e+02, Test loss: 1.091e+03, MSE(e): 1.586e-05, MSE(pi1): 2.572e-03, MSE(pi2): 1.324e-05, MSE(pi3): 7.698e-04\n",
      "Epoch 80100, Train loss: 2.613e+02, Test loss: 1.090e+03, MSE(e): 1.586e-05, MSE(pi1): 2.573e-03, MSE(pi2): 1.324e-05, MSE(pi3): 7.696e-04\n",
      "Epoch 80200, Train loss: 2.612e+02, Test loss: 1.088e+03, MSE(e): 1.585e-05, MSE(pi1): 2.574e-03, MSE(pi2): 1.323e-05, MSE(pi3): 7.693e-04\n",
      "Epoch 80300, Train loss: 2.611e+02, Test loss: 1.087e+03, MSE(e): 1.585e-05, MSE(pi1): 2.575e-03, MSE(pi2): 1.323e-05, MSE(pi3): 7.690e-04\n",
      "Epoch 80400, Train loss: 2.611e+02, Test loss: 1.085e+03, MSE(e): 1.584e-05, MSE(pi1): 2.575e-03, MSE(pi2): 1.323e-05, MSE(pi3): 7.689e-04\n",
      "Epoch 80500, Train loss: 2.610e+02, Test loss: 1.084e+03, MSE(e): 1.584e-05, MSE(pi1): 2.576e-03, MSE(pi2): 1.323e-05, MSE(pi3): 7.688e-04\n",
      "Epoch 80600, Train loss: 2.610e+02, Test loss: 1.082e+03, MSE(e): 1.583e-05, MSE(pi1): 2.576e-03, MSE(pi2): 1.322e-05, MSE(pi3): 7.687e-04\n",
      "Epoch 80700, Train loss: 2.610e+02, Test loss: 1.081e+03, MSE(e): 1.583e-05, MSE(pi1): 2.576e-03, MSE(pi2): 1.322e-05, MSE(pi3): 7.687e-04\n",
      "Epoch 80800, Train loss: 2.609e+02, Test loss: 1.079e+03, MSE(e): 1.583e-05, MSE(pi1): 2.576e-03, MSE(pi2): 1.322e-05, MSE(pi3): 7.687e-04\n",
      "Epoch 80900, Train loss: 2.609e+02, Test loss: 1.078e+03, MSE(e): 1.582e-05, MSE(pi1): 2.575e-03, MSE(pi2): 1.321e-05, MSE(pi3): 7.687e-04\n",
      "Epoch 81000, Train loss: 2.608e+02, Test loss: 1.076e+03, MSE(e): 1.582e-05, MSE(pi1): 2.575e-03, MSE(pi2): 1.321e-05, MSE(pi3): 7.688e-04\n",
      "Epoch 81100, Train loss: 2.608e+02, Test loss: 1.075e+03, MSE(e): 1.582e-05, MSE(pi1): 2.575e-03, MSE(pi2): 1.321e-05, MSE(pi3): 7.688e-04\n",
      "Epoch 81200, Train loss: 2.608e+02, Test loss: 1.074e+03, MSE(e): 1.581e-05, MSE(pi1): 2.575e-03, MSE(pi2): 1.321e-05, MSE(pi3): 7.688e-04\n",
      "Epoch 81300, Train loss: 2.607e+02, Test loss: 1.072e+03, MSE(e): 1.581e-05, MSE(pi1): 2.575e-03, MSE(pi2): 1.320e-05, MSE(pi3): 7.688e-04\n",
      "Epoch 81400, Train loss: 2.607e+02, Test loss: 1.071e+03, MSE(e): 1.580e-05, MSE(pi1): 2.574e-03, MSE(pi2): 1.320e-05, MSE(pi3): 7.688e-04\n",
      "Epoch 81500, Train loss: 2.606e+02, Test loss: 1.069e+03, MSE(e): 1.580e-05, MSE(pi1): 2.574e-03, MSE(pi2): 1.320e-05, MSE(pi3): 7.689e-04\n",
      "Epoch 81600, Train loss: 2.606e+02, Test loss: 1.068e+03, MSE(e): 1.580e-05, MSE(pi1): 2.574e-03, MSE(pi2): 1.319e-05, MSE(pi3): 7.689e-04\n",
      "Epoch 81700, Train loss: 2.606e+02, Test loss: 1.066e+03, MSE(e): 1.579e-05, MSE(pi1): 2.573e-03, MSE(pi2): 1.319e-05, MSE(pi3): 7.690e-04\n",
      "Epoch 81800, Train loss: 2.606e+02, Test loss: 1.065e+03, MSE(e): 1.579e-05, MSE(pi1): 2.572e-03, MSE(pi2): 1.319e-05, MSE(pi3): 7.693e-04\n",
      "Epoch 81900, Train loss: 2.606e+02, Test loss: 1.063e+03, MSE(e): 1.579e-05, MSE(pi1): 2.569e-03, MSE(pi2): 1.319e-05, MSE(pi3): 7.702e-04\n",
      "Epoch 82000, Train loss: 2.605e+02, Test loss: 1.062e+03, MSE(e): 1.578e-05, MSE(pi1): 2.572e-03, MSE(pi2): 1.318e-05, MSE(pi3): 7.694e-04\n",
      "Epoch 82100, Train loss: 2.604e+02, Test loss: 1.061e+03, MSE(e): 1.578e-05, MSE(pi1): 2.574e-03, MSE(pi2): 1.318e-05, MSE(pi3): 7.689e-04\n",
      "Epoch 82200, Train loss: 2.604e+02, Test loss: 1.060e+03, MSE(e): 1.577e-05, MSE(pi1): 2.574e-03, MSE(pi2): 1.318e-05, MSE(pi3): 7.689e-04\n",
      "Epoch 82300, Train loss: 2.603e+02, Test loss: 1.058e+03, MSE(e): 1.577e-05, MSE(pi1): 2.574e-03, MSE(pi2): 1.318e-05, MSE(pi3): 7.689e-04\n",
      "Epoch 82400, Train loss: 2.603e+02, Test loss: 1.057e+03, MSE(e): 1.577e-05, MSE(pi1): 2.573e-03, MSE(pi2): 1.317e-05, MSE(pi3): 7.689e-04\n",
      "Epoch 82500, Train loss: 2.603e+02, Test loss: 1.055e+03, MSE(e): 1.576e-05, MSE(pi1): 2.573e-03, MSE(pi2): 1.317e-05, MSE(pi3): 7.689e-04\n",
      "Epoch 82600, Train loss: 2.602e+02, Test loss: 1.054e+03, MSE(e): 1.576e-05, MSE(pi1): 2.572e-03, MSE(pi2): 1.317e-05, MSE(pi3): 7.691e-04\n",
      "Epoch 82700, Train loss: 2.602e+02, Test loss: 1.052e+03, MSE(e): 1.576e-05, MSE(pi1): 2.569e-03, MSE(pi2): 1.317e-05, MSE(pi3): 7.698e-04\n",
      "Epoch 82800, Train loss: 2.602e+02, Test loss: 1.051e+03, MSE(e): 1.575e-05, MSE(pi1): 2.575e-03, MSE(pi2): 1.316e-05, MSE(pi3): 7.689e-04\n",
      "Epoch 82900, Train loss: 2.601e+02, Test loss: 1.050e+03, MSE(e): 1.575e-05, MSE(pi1): 2.572e-03, MSE(pi2): 1.316e-05, MSE(pi3): 7.689e-04\n",
      "Epoch 83000, Train loss: 2.601e+02, Test loss: 1.048e+03, MSE(e): 1.575e-05, MSE(pi1): 2.572e-03, MSE(pi2): 1.316e-05, MSE(pi3): 7.690e-04\n",
      "Epoch 83100, Train loss: 2.601e+02, Test loss: 1.047e+03, MSE(e): 1.574e-05, MSE(pi1): 2.572e-03, MSE(pi2): 1.316e-05, MSE(pi3): 7.693e-04\n",
      "Epoch 83200, Train loss: 2.600e+02, Test loss: 1.046e+03, MSE(e): 1.574e-05, MSE(pi1): 2.572e-03, MSE(pi2): 1.315e-05, MSE(pi3): 7.690e-04\n",
      "Epoch 83300, Train loss: 2.600e+02, Test loss: 1.045e+03, MSE(e): 1.573e-05, MSE(pi1): 2.571e-03, MSE(pi2): 1.315e-05, MSE(pi3): 7.691e-04\n",
      "Epoch 83400, Train loss: 2.599e+02, Test loss: 1.044e+03, MSE(e): 1.573e-05, MSE(pi1): 2.576e-03, MSE(pi2): 1.315e-05, MSE(pi3): 7.687e-04\n",
      "Epoch 83500, Train loss: 2.599e+02, Test loss: 1.042e+03, MSE(e): 1.573e-05, MSE(pi1): 2.572e-03, MSE(pi2): 1.315e-05, MSE(pi3): 7.690e-04\n",
      "Epoch 83600, Train loss: 2.599e+02, Test loss: 1.041e+03, MSE(e): 1.572e-05, MSE(pi1): 2.571e-03, MSE(pi2): 1.314e-05, MSE(pi3): 7.691e-04\n",
      "Epoch 83700, Train loss: 2.598e+02, Test loss: 1.040e+03, MSE(e): 1.572e-05, MSE(pi1): 2.569e-03, MSE(pi2): 1.314e-05, MSE(pi3): 7.695e-04\n",
      "Epoch 83800, Train loss: 2.598e+02, Test loss: 1.039e+03, MSE(e): 1.572e-05, MSE(pi1): 2.571e-03, MSE(pi2): 1.314e-05, MSE(pi3): 7.690e-04\n",
      "Epoch 83900, Train loss: 2.597e+02, Test loss: 1.037e+03, MSE(e): 1.571e-05, MSE(pi1): 2.570e-03, MSE(pi2): 1.314e-05, MSE(pi3): 7.691e-04\n",
      "Epoch 84000, Train loss: 2.597e+02, Test loss: 1.036e+03, MSE(e): 1.571e-05, MSE(pi1): 2.573e-03, MSE(pi2): 1.313e-05, MSE(pi3): 7.689e-04\n",
      "Epoch 84100, Train loss: 2.597e+02, Test loss: 1.035e+03, MSE(e): 1.571e-05, MSE(pi1): 2.570e-03, MSE(pi2): 1.313e-05, MSE(pi3): 7.691e-04\n",
      "Epoch 84200, Train loss: 2.597e+02, Test loss: 1.033e+03, MSE(e): 1.570e-05, MSE(pi1): 2.568e-03, MSE(pi2): 1.313e-05, MSE(pi3): 7.694e-04\n",
      "Epoch 84300, Train loss: 2.596e+02, Test loss: 1.032e+03, MSE(e): 1.570e-05, MSE(pi1): 2.570e-03, MSE(pi2): 1.312e-05, MSE(pi3): 7.691e-04\n",
      "Epoch 84400, Train loss: 2.596e+02, Test loss: 1.031e+03, MSE(e): 1.569e-05, MSE(pi1): 2.568e-03, MSE(pi2): 1.312e-05, MSE(pi3): 7.694e-04\n",
      "Epoch 84500, Train loss: 2.595e+02, Test loss: 1.030e+03, MSE(e): 1.569e-05, MSE(pi1): 2.568e-03, MSE(pi2): 1.312e-05, MSE(pi3): 7.694e-04\n",
      "Epoch 84600, Train loss: 2.595e+02, Test loss: 1.029e+03, MSE(e): 1.569e-05, MSE(pi1): 2.576e-03, MSE(pi2): 1.312e-05, MSE(pi3): 7.685e-04\n",
      "Epoch 84700, Train loss: 2.595e+02, Test loss: 1.028e+03, MSE(e): 1.568e-05, MSE(pi1): 2.568e-03, MSE(pi2): 1.311e-05, MSE(pi3): 7.693e-04\n",
      "Epoch 84800, Train loss: 2.594e+02, Test loss: 1.026e+03, MSE(e): 1.568e-05, MSE(pi1): 2.567e-03, MSE(pi2): 1.311e-05, MSE(pi3): 7.695e-04\n",
      "Epoch 84900, Train loss: 2.594e+02, Test loss: 1.025e+03, MSE(e): 1.568e-05, MSE(pi1): 2.572e-03, MSE(pi2): 1.311e-05, MSE(pi3): 7.690e-04\n",
      "Epoch 85000, Train loss: 2.594e+02, Test loss: 1.024e+03, MSE(e): 1.567e-05, MSE(pi1): 2.568e-03, MSE(pi2): 1.311e-05, MSE(pi3): 7.693e-04\n",
      "Epoch 85100, Train loss: 2.593e+02, Test loss: 1.023e+03, MSE(e): 1.567e-05, MSE(pi1): 2.567e-03, MSE(pi2): 1.310e-05, MSE(pi3): 7.695e-04\n",
      "Epoch 85200, Train loss: 2.593e+02, Test loss: 1.022e+03, MSE(e): 1.567e-05, MSE(pi1): 2.567e-03, MSE(pi2): 1.310e-05, MSE(pi3): 7.695e-04\n",
      "Epoch 85300, Train loss: 2.592e+02, Test loss: 1.021e+03, MSE(e): 1.566e-05, MSE(pi1): 2.576e-03, MSE(pi2): 1.310e-05, MSE(pi3): 7.685e-04\n",
      "Epoch 85400, Train loss: 2.592e+02, Test loss: 1.019e+03, MSE(e): 1.566e-05, MSE(pi1): 2.567e-03, MSE(pi2): 1.310e-05, MSE(pi3): 7.694e-04\n",
      "Epoch 85500, Train loss: 2.592e+02, Test loss: 1.018e+03, MSE(e): 1.565e-05, MSE(pi1): 2.569e-03, MSE(pi2): 1.309e-05, MSE(pi3): 7.695e-04\n",
      "Epoch 85600, Train loss: 2.591e+02, Test loss: 1.017e+03, MSE(e): 1.565e-05, MSE(pi1): 2.571e-03, MSE(pi2): 1.309e-05, MSE(pi3): 7.689e-04\n",
      "Epoch 85700, Train loss: 2.591e+02, Test loss: 1.015e+03, MSE(e): 1.565e-05, MSE(pi1): 2.564e-03, MSE(pi2): 1.309e-05, MSE(pi3): 7.704e-04\n",
      "Epoch 85800, Train loss: 2.591e+02, Test loss: 1.014e+03, MSE(e): 1.564e-05, MSE(pi1): 2.566e-03, MSE(pi2): 1.309e-05, MSE(pi3): 7.696e-04\n",
      "Epoch 85900, Train loss: 2.590e+02, Test loss: 1.013e+03, MSE(e): 1.564e-05, MSE(pi1): 2.566e-03, MSE(pi2): 1.308e-05, MSE(pi3): 7.695e-04\n",
      "Epoch 86000, Train loss: 2.590e+02, Test loss: 1.012e+03, MSE(e): 1.564e-05, MSE(pi1): 2.575e-03, MSE(pi2): 1.308e-05, MSE(pi3): 7.686e-04\n",
      "Epoch 86100, Train loss: 2.589e+02, Test loss: 1.010e+03, MSE(e): 1.563e-05, MSE(pi1): 2.566e-03, MSE(pi2): 1.308e-05, MSE(pi3): 7.695e-04\n",
      "Epoch 86200, Train loss: 2.589e+02, Test loss: 1.009e+03, MSE(e): 1.563e-05, MSE(pi1): 2.565e-03, MSE(pi2): 1.308e-05, MSE(pi3): 7.696e-04\n",
      "Epoch 86300, Train loss: 2.589e+02, Test loss: 1.008e+03, MSE(e): 1.563e-05, MSE(pi1): 2.569e-03, MSE(pi2): 1.307e-05, MSE(pi3): 7.691e-04\n",
      "Epoch 86400, Train loss: 2.588e+02, Test loss: 1.007e+03, MSE(e): 1.562e-05, MSE(pi1): 2.566e-03, MSE(pi2): 1.307e-05, MSE(pi3): 7.694e-04\n",
      "Epoch 86500, Train loss: 2.588e+02, Test loss: 1.005e+03, MSE(e): 1.562e-05, MSE(pi1): 2.564e-03, MSE(pi2): 1.307e-05, MSE(pi3): 7.697e-04\n",
      "Epoch 86600, Train loss: 2.588e+02, Test loss: 1.004e+03, MSE(e): 1.561e-05, MSE(pi1): 2.564e-03, MSE(pi2): 1.307e-05, MSE(pi3): 7.698e-04\n",
      "Epoch 86700, Train loss: 2.587e+02, Test loss: 1.003e+03, MSE(e): 1.561e-05, MSE(pi1): 2.563e-03, MSE(pi2): 1.306e-05, MSE(pi3): 7.702e-04\n",
      "Epoch 86800, Train loss: 2.587e+02, Test loss: 1.001e+03, MSE(e): 1.561e-05, MSE(pi1): 2.563e-03, MSE(pi2): 1.306e-05, MSE(pi3): 7.703e-04\n",
      "Epoch 86900, Train loss: 2.586e+02, Test loss: 1.000e+03, MSE(e): 1.560e-05, MSE(pi1): 2.570e-03, MSE(pi2): 1.306e-05, MSE(pi3): 7.689e-04\n",
      "Epoch 87000, Train loss: 2.587e+02, Test loss: 9.988e+02, MSE(e): 1.560e-05, MSE(pi1): 2.561e-03, MSE(pi2): 1.306e-05, MSE(pi3): 7.709e-04\n",
      "Epoch 87100, Train loss: 2.586e+02, Test loss: 9.979e+02, MSE(e): 1.560e-05, MSE(pi1): 2.563e-03, MSE(pi2): 1.305e-05, MSE(pi3): 7.699e-04\n",
      "Epoch 87200, Train loss: 2.585e+02, Test loss: 9.968e+02, MSE(e): 1.559e-05, MSE(pi1): 2.564e-03, MSE(pi2): 1.305e-05, MSE(pi3): 7.697e-04\n",
      "Epoch 87300, Train loss: 2.585e+02, Test loss: 9.959e+02, MSE(e): 1.559e-05, MSE(pi1): 2.576e-03, MSE(pi2): 1.305e-05, MSE(pi3): 7.684e-04\n",
      "Epoch 87400, Train loss: 2.585e+02, Test loss: 9.947e+02, MSE(e): 1.559e-05, MSE(pi1): 2.564e-03, MSE(pi2): 1.305e-05, MSE(pi3): 7.696e-04\n",
      "Epoch 87500, Train loss: 2.584e+02, Test loss: 9.935e+02, MSE(e): 1.558e-05, MSE(pi1): 2.563e-03, MSE(pi2): 1.305e-05, MSE(pi3): 7.698e-04\n",
      "Epoch 87600, Train loss: 2.584e+02, Test loss: 9.924e+02, MSE(e): 1.558e-05, MSE(pi1): 2.564e-03, MSE(pi2): 1.304e-05, MSE(pi3): 7.699e-04\n",
      "Epoch 87700, Train loss: 2.584e+02, Test loss: 9.914e+02, MSE(e): 1.558e-05, MSE(pi1): 2.564e-03, MSE(pi2): 1.304e-05, MSE(pi3): 7.695e-04\n",
      "Epoch 87800, Train loss: 2.583e+02, Test loss: 9.904e+02, MSE(e): 1.557e-05, MSE(pi1): 2.571e-03, MSE(pi2): 1.304e-05, MSE(pi3): 7.689e-04\n",
      "Epoch 87900, Train loss: 2.583e+02, Test loss: 9.892e+02, MSE(e): 1.557e-05, MSE(pi1): 2.562e-03, MSE(pi2): 1.304e-05, MSE(pi3): 7.698e-04\n",
      "Epoch 88000, Train loss: 2.583e+02, Test loss: 9.881e+02, MSE(e): 1.556e-05, MSE(pi1): 2.563e-03, MSE(pi2): 1.303e-05, MSE(pi3): 7.697e-04\n",
      "Epoch 88100, Train loss: 2.582e+02, Test loss: 9.870e+02, MSE(e): 1.556e-05, MSE(pi1): 2.567e-03, MSE(pi2): 1.303e-05, MSE(pi3): 7.692e-04\n",
      "Epoch 88200, Train loss: 2.582e+02, Test loss: 9.860e+02, MSE(e): 1.556e-05, MSE(pi1): 2.563e-03, MSE(pi2): 1.303e-05, MSE(pi3): 7.696e-04\n",
      "Epoch 88300, Train loss: 2.582e+02, Test loss: 9.849e+02, MSE(e): 1.555e-05, MSE(pi1): 2.559e-03, MSE(pi2): 1.303e-05, MSE(pi3): 7.702e-04\n",
      "Epoch 88400, Train loss: 2.582e+02, Test loss: 9.839e+02, MSE(e): 1.555e-05, MSE(pi1): 2.561e-03, MSE(pi2): 1.302e-05, MSE(pi3): 7.705e-04\n",
      "Epoch 88500, Train loss: 2.580e+02, Test loss: 9.832e+02, MSE(e): 1.554e-05, MSE(pi1): 2.567e-03, MSE(pi2): 1.302e-05, MSE(pi3): 7.691e-04\n",
      "Epoch 88600, Train loss: 2.581e+02, Test loss: 9.820e+02, MSE(e): 1.554e-05, MSE(pi1): 2.560e-03, MSE(pi2): 1.302e-05, MSE(pi3): 7.707e-04\n",
      "Epoch 88700, Train loss: 2.580e+02, Test loss: 9.812e+02, MSE(e): 1.554e-05, MSE(pi1): 2.561e-03, MSE(pi2): 1.302e-05, MSE(pi3): 7.700e-04\n",
      "Epoch 88800, Train loss: 2.579e+02, Test loss: 9.803e+02, MSE(e): 1.553e-05, MSE(pi1): 2.562e-03, MSE(pi2): 1.301e-05, MSE(pi3): 7.698e-04\n",
      "Epoch 88900, Train loss: 2.579e+02, Test loss: 9.794e+02, MSE(e): 1.553e-05, MSE(pi1): 2.575e-03, MSE(pi2): 1.301e-05, MSE(pi3): 7.685e-04\n",
      "Epoch 89000, Train loss: 2.579e+02, Test loss: 9.783e+02, MSE(e): 1.553e-05, MSE(pi1): 2.561e-03, MSE(pi2): 1.301e-05, MSE(pi3): 7.698e-04\n",
      "Epoch 89100, Train loss: 2.579e+02, Test loss: 9.772e+02, MSE(e): 1.552e-05, MSE(pi1): 2.560e-03, MSE(pi2): 1.301e-05, MSE(pi3): 7.704e-04\n",
      "Epoch 89200, Train loss: 2.579e+02, Test loss: 9.763e+02, MSE(e): 1.552e-05, MSE(pi1): 2.557e-03, MSE(pi2): 1.301e-05, MSE(pi3): 7.710e-04\n",
      "Epoch 89300, Train loss: 2.578e+02, Test loss: 9.756e+02, MSE(e): 1.552e-05, MSE(pi1): 2.564e-03, MSE(pi2): 1.300e-05, MSE(pi3): 7.695e-04\n",
      "Epoch 89400, Train loss: 2.578e+02, Test loss: 9.745e+02, MSE(e): 1.551e-05, MSE(pi1): 2.559e-03, MSE(pi2): 1.300e-05, MSE(pi3): 7.709e-04\n",
      "Epoch 89500, Train loss: 2.577e+02, Test loss: 9.737e+02, MSE(e): 1.551e-05, MSE(pi1): 2.562e-03, MSE(pi2): 1.300e-05, MSE(pi3): 7.699e-04\n",
      "Epoch 89600, Train loss: 2.577e+02, Test loss: 9.728e+02, MSE(e): 1.551e-05, MSE(pi1): 2.562e-03, MSE(pi2): 1.300e-05, MSE(pi3): 7.697e-04\n",
      "Epoch 89700, Train loss: 2.576e+02, Test loss: 9.720e+02, MSE(e): 1.550e-05, MSE(pi1): 2.575e-03, MSE(pi2): 1.299e-05, MSE(pi3): 7.684e-04\n",
      "Epoch 89800, Train loss: 2.576e+02, Test loss: 9.709e+02, MSE(e): 1.550e-05, MSE(pi1): 2.560e-03, MSE(pi2): 1.299e-05, MSE(pi3): 7.699e-04\n",
      "Epoch 89900, Train loss: 2.576e+02, Test loss: 9.699e+02, MSE(e): 1.550e-05, MSE(pi1): 2.561e-03, MSE(pi2): 1.299e-05, MSE(pi3): 7.702e-04\n",
      "Epoch 90000, Train loss: 2.576e+02, Test loss: 9.688e+02, MSE(e): 1.549e-05, MSE(pi1): 2.557e-03, MSE(pi2): 1.299e-05, MSE(pi3): 7.709e-04\n",
      "Epoch 90100, Train loss: 2.575e+02, Test loss: 9.681e+02, MSE(e): 1.549e-05, MSE(pi1): 2.565e-03, MSE(pi2): 1.299e-05, MSE(pi3): 7.692e-04\n",
      "Epoch 90200, Train loss: 2.575e+02, Test loss: 9.670e+02, MSE(e): 1.549e-05, MSE(pi1): 2.559e-03, MSE(pi2): 1.298e-05, MSE(pi3): 7.701e-04\n",
      "Epoch 90300, Train loss: 2.575e+02, Test loss: 9.659e+02, MSE(e): 1.548e-05, MSE(pi1): 2.560e-03, MSE(pi2): 1.298e-05, MSE(pi3): 7.705e-04\n",
      "Epoch 90400, Train loss: 2.575e+02, Test loss: 9.648e+02, MSE(e): 1.548e-05, MSE(pi1): 2.556e-03, MSE(pi2): 1.298e-05, MSE(pi3): 7.715e-04\n",
      "Epoch 90500, Train loss: 2.574e+02, Test loss: 9.640e+02, MSE(e): 1.548e-05, MSE(pi1): 2.561e-03, MSE(pi2): 1.298e-05, MSE(pi3): 7.700e-04\n",
      "Epoch 90600, Train loss: 2.573e+02, Test loss: 9.631e+02, MSE(e): 1.547e-05, MSE(pi1): 2.559e-03, MSE(pi2): 1.297e-05, MSE(pi3): 7.701e-04\n",
      "Epoch 90700, Train loss: 2.573e+02, Test loss: 9.621e+02, MSE(e): 1.547e-05, MSE(pi1): 2.560e-03, MSE(pi2): 1.297e-05, MSE(pi3): 7.698e-04\n",
      "Epoch 90800, Train loss: 2.573e+02, Test loss: 9.612e+02, MSE(e): 1.547e-05, MSE(pi1): 2.558e-03, MSE(pi2): 1.297e-05, MSE(pi3): 7.704e-04\n",
      "Epoch 90900, Train loss: 2.573e+02, Test loss: 9.603e+02, MSE(e): 1.546e-05, MSE(pi1): 2.559e-03, MSE(pi2): 1.297e-05, MSE(pi3): 7.704e-04\n",
      "Epoch 91000, Train loss: 2.573e+02, Test loss: 9.593e+02, MSE(e): 1.546e-05, MSE(pi1): 2.557e-03, MSE(pi2): 1.297e-05, MSE(pi3): 7.716e-04\n",
      "Epoch 91100, Train loss: 2.572e+02, Test loss: 9.587e+02, MSE(e): 1.546e-05, MSE(pi1): 2.560e-03, MSE(pi2): 1.296e-05, MSE(pi3): 7.701e-04\n",
      "Epoch 91200, Train loss: 2.571e+02, Test loss: 9.579e+02, MSE(e): 1.545e-05, MSE(pi1): 2.558e-03, MSE(pi2): 1.296e-05, MSE(pi3): 7.701e-04\n",
      "Epoch 91300, Train loss: 2.571e+02, Test loss: 9.573e+02, MSE(e): 1.545e-05, MSE(pi1): 2.571e-03, MSE(pi2): 1.296e-05, MSE(pi3): 7.687e-04\n",
      "Epoch 91400, Train loss: 2.571e+02, Test loss: 9.564e+02, MSE(e): 1.545e-05, MSE(pi1): 2.562e-03, MSE(pi2): 1.296e-05, MSE(pi3): 7.695e-04\n",
      "Epoch 91500, Train loss: 2.571e+02, Test loss: 9.554e+02, MSE(e): 1.544e-05, MSE(pi1): 2.555e-03, MSE(pi2): 1.296e-05, MSE(pi3): 7.711e-04\n",
      "Epoch 91600, Train loss: 2.571e+02, Test loss: 9.545e+02, MSE(e): 1.544e-05, MSE(pi1): 2.555e-03, MSE(pi2): 1.295e-05, MSE(pi3): 7.712e-04\n",
      "Epoch 91700, Train loss: 2.569e+02, Test loss: 9.538e+02, MSE(e): 1.544e-05, MSE(pi1): 2.563e-03, MSE(pi2): 1.295e-05, MSE(pi3): 7.693e-04\n",
      "Epoch 91800, Train loss: 2.569e+02, Test loss: 9.528e+02, MSE(e): 1.543e-05, MSE(pi1): 2.558e-03, MSE(pi2): 1.295e-05, MSE(pi3): 7.701e-04\n",
      "Epoch 91900, Train loss: 2.569e+02, Test loss: 9.519e+02, MSE(e): 1.543e-05, MSE(pi1): 2.558e-03, MSE(pi2): 1.295e-05, MSE(pi3): 7.702e-04\n",
      "Epoch 92000, Train loss: 2.569e+02, Test loss: 9.509e+02, MSE(e): 1.543e-05, MSE(pi1): 2.557e-03, MSE(pi2): 1.294e-05, MSE(pi3): 7.705e-04\n",
      "Epoch 92100, Train loss: 2.568e+02, Test loss: 9.501e+02, MSE(e): 1.542e-05, MSE(pi1): 2.567e-03, MSE(pi2): 1.294e-05, MSE(pi3): 7.691e-04\n",
      "Epoch 92200, Train loss: 2.568e+02, Test loss: 9.491e+02, MSE(e): 1.542e-05, MSE(pi1): 2.557e-03, MSE(pi2): 1.294e-05, MSE(pi3): 7.703e-04\n",
      "Epoch 92300, Train loss: 2.567e+02, Test loss: 9.483e+02, MSE(e): 1.542e-05, MSE(pi1): 2.562e-03, MSE(pi2): 1.294e-05, MSE(pi3): 7.695e-04\n",
      "Epoch 92400, Train loss: 2.567e+02, Test loss: 9.476e+02, MSE(e): 1.541e-05, MSE(pi1): 2.574e-03, MSE(pi2): 1.294e-05, MSE(pi3): 7.684e-04\n",
      "Epoch 92500, Train loss: 2.567e+02, Test loss: 9.465e+02, MSE(e): 1.541e-05, MSE(pi1): 2.553e-03, MSE(pi2): 1.293e-05, MSE(pi3): 7.706e-04\n",
      "Epoch 92600, Train loss: 2.567e+02, Test loss: 9.455e+02, MSE(e): 1.540e-05, MSE(pi1): 2.553e-03, MSE(pi2): 1.293e-05, MSE(pi3): 7.710e-04\n",
      "Epoch 92700, Train loss: 2.566e+02, Test loss: 9.449e+02, MSE(e): 1.540e-05, MSE(pi1): 2.558e-03, MSE(pi2): 1.293e-05, MSE(pi3): 7.699e-04\n",
      "Epoch 92800, Train loss: 2.565e+02, Test loss: 9.441e+02, MSE(e): 1.540e-05, MSE(pi1): 2.562e-03, MSE(pi2): 1.293e-05, MSE(pi3): 7.695e-04\n",
      "Epoch 92900, Train loss: 2.565e+02, Test loss: 9.433e+02, MSE(e): 1.539e-05, MSE(pi1): 2.569e-03, MSE(pi2): 1.292e-05, MSE(pi3): 7.689e-04\n",
      "Epoch 93000, Train loss: 2.565e+02, Test loss: 9.424e+02, MSE(e): 1.539e-05, MSE(pi1): 2.557e-03, MSE(pi2): 1.292e-05, MSE(pi3): 7.703e-04\n",
      "Epoch 93100, Train loss: 2.565e+02, Test loss: 9.415e+02, MSE(e): 1.539e-05, MSE(pi1): 2.555e-03, MSE(pi2): 1.292e-05, MSE(pi3): 7.704e-04\n",
      "Epoch 93200, Train loss: 2.564e+02, Test loss: 9.410e+02, MSE(e): 1.538e-05, MSE(pi1): 2.572e-03, MSE(pi2): 1.292e-05, MSE(pi3): 7.685e-04\n",
      "Epoch 93300, Train loss: 2.564e+02, Test loss: 9.402e+02, MSE(e): 1.538e-05, MSE(pi1): 2.570e-03, MSE(pi2): 1.292e-05, MSE(pi3): 7.689e-04\n",
      "Epoch 93400, Train loss: 2.564e+02, Test loss: 9.391e+02, MSE(e): 1.537e-05, MSE(pi1): 2.557e-03, MSE(pi2): 1.291e-05, MSE(pi3): 7.704e-04\n",
      "Epoch 93500, Train loss: 2.563e+02, Test loss: 9.383e+02, MSE(e): 1.537e-05, MSE(pi1): 2.555e-03, MSE(pi2): 1.291e-05, MSE(pi3): 7.704e-04\n",
      "Epoch 93600, Train loss: 2.563e+02, Test loss: 9.375e+02, MSE(e): 1.537e-05, MSE(pi1): 2.557e-03, MSE(pi2): 1.291e-05, MSE(pi3): 7.700e-04\n",
      "Epoch 93700, Train loss: 2.562e+02, Test loss: 9.367e+02, MSE(e): 1.537e-05, MSE(pi1): 2.562e-03, MSE(pi2): 1.291e-05, MSE(pi3): 7.695e-04\n",
      "Epoch 93800, Train loss: 2.563e+02, Test loss: 9.356e+02, MSE(e): 1.536e-05, MSE(pi1): 2.556e-03, MSE(pi2): 1.291e-05, MSE(pi3): 7.707e-04\n",
      "Epoch 93900, Train loss: 2.562e+02, Test loss: 9.347e+02, MSE(e): 1.536e-05, MSE(pi1): 2.555e-03, MSE(pi2): 1.290e-05, MSE(pi3): 7.704e-04\n",
      "Epoch 94000, Train loss: 2.561e+02, Test loss: 9.339e+02, MSE(e): 1.536e-05, MSE(pi1): 2.555e-03, MSE(pi2): 1.290e-05, MSE(pi3): 7.701e-04\n",
      "Epoch 94100, Train loss: 2.561e+02, Test loss: 9.330e+02, MSE(e): 1.535e-05, MSE(pi1): 2.555e-03, MSE(pi2): 1.290e-05, MSE(pi3): 7.703e-04\n",
      "Epoch 94200, Train loss: 2.561e+02, Test loss: 9.321e+02, MSE(e): 1.535e-05, MSE(pi1): 2.554e-03, MSE(pi2): 1.290e-05, MSE(pi3): 7.704e-04\n",
      "Epoch 94300, Train loss: 2.560e+02, Test loss: 9.312e+02, MSE(e): 1.535e-05, MSE(pi1): 2.555e-03, MSE(pi2): 1.290e-05, MSE(pi3): 7.703e-04\n",
      "Epoch 94400, Train loss: 2.561e+02, Test loss: 9.303e+02, MSE(e): 1.534e-05, MSE(pi1): 2.552e-03, MSE(pi2): 1.289e-05, MSE(pi3): 7.711e-04\n",
      "Epoch 94500, Train loss: 2.560e+02, Test loss: 9.295e+02, MSE(e): 1.534e-05, MSE(pi1): 2.554e-03, MSE(pi2): 1.289e-05, MSE(pi3): 7.706e-04\n",
      "Epoch 94600, Train loss: 2.559e+02, Test loss: 9.288e+02, MSE(e): 1.534e-05, MSE(pi1): 2.556e-03, MSE(pi2): 1.289e-05, MSE(pi3): 7.700e-04\n",
      "Epoch 94700, Train loss: 2.559e+02, Test loss: 9.281e+02, MSE(e): 1.533e-05, MSE(pi1): 2.555e-03, MSE(pi2): 1.289e-05, MSE(pi3): 7.703e-04\n",
      "Epoch 94800, Train loss: 2.559e+02, Test loss: 9.273e+02, MSE(e): 1.533e-05, MSE(pi1): 2.554e-03, MSE(pi2): 1.289e-05, MSE(pi3): 7.702e-04\n",
      "Epoch 94900, Train loss: 2.558e+02, Test loss: 9.266e+02, MSE(e): 1.533e-05, MSE(pi1): 2.563e-03, MSE(pi2): 1.288e-05, MSE(pi3): 7.693e-04\n",
      "Epoch 95000, Train loss: 2.558e+02, Test loss: 9.260e+02, MSE(e): 1.532e-05, MSE(pi1): 2.574e-03, MSE(pi2): 1.288e-05, MSE(pi3): 7.684e-04\n",
      "Epoch 95100, Train loss: 2.558e+02, Test loss: 9.249e+02, MSE(e): 1.532e-05, MSE(pi1): 2.554e-03, MSE(pi2): 1.288e-05, MSE(pi3): 7.703e-04\n",
      "Epoch 95200, Train loss: 2.558e+02, Test loss: 9.240e+02, MSE(e): 1.532e-05, MSE(pi1): 2.554e-03, MSE(pi2): 1.288e-05, MSE(pi3): 7.705e-04\n",
      "Epoch 95300, Train loss: 2.557e+02, Test loss: 9.232e+02, MSE(e): 1.531e-05, MSE(pi1): 2.554e-03, MSE(pi2): 1.288e-05, MSE(pi3): 7.702e-04\n",
      "Epoch 95400, Train loss: 2.557e+02, Test loss: 9.223e+02, MSE(e): 1.531e-05, MSE(pi1): 2.554e-03, MSE(pi2): 1.287e-05, MSE(pi3): 7.702e-04\n",
      "Epoch 95500, Train loss: 2.556e+02, Test loss: 9.216e+02, MSE(e): 1.531e-05, MSE(pi1): 2.566e-03, MSE(pi2): 1.287e-05, MSE(pi3): 7.690e-04\n",
      "Epoch 95600, Train loss: 2.556e+02, Test loss: 9.209e+02, MSE(e): 1.530e-05, MSE(pi1): 2.561e-03, MSE(pi2): 1.287e-05, MSE(pi3): 7.696e-04\n",
      "Epoch 95700, Train loss: 2.556e+02, Test loss: 9.200e+02, MSE(e): 1.530e-05, MSE(pi1): 2.551e-03, MSE(pi2): 1.287e-05, MSE(pi3): 7.711e-04\n",
      "Epoch 95800, Train loss: 2.556e+02, Test loss: 9.195e+02, MSE(e): 1.530e-05, MSE(pi1): 2.553e-03, MSE(pi2): 1.287e-05, MSE(pi3): 7.705e-04\n",
      "Epoch 95900, Train loss: 2.555e+02, Test loss: 9.189e+02, MSE(e): 1.529e-05, MSE(pi1): 2.553e-03, MSE(pi2): 1.286e-05, MSE(pi3): 7.703e-04\n",
      "Epoch 96000, Train loss: 2.555e+02, Test loss: 9.182e+02, MSE(e): 1.529e-05, MSE(pi1): 2.557e-03, MSE(pi2): 1.286e-05, MSE(pi3): 7.700e-04\n",
      "Epoch 96100, Train loss: 2.554e+02, Test loss: 9.176e+02, MSE(e): 1.529e-05, MSE(pi1): 2.555e-03, MSE(pi2): 1.286e-05, MSE(pi3): 7.699e-04\n",
      "Epoch 96200, Train loss: 2.554e+02, Test loss: 9.169e+02, MSE(e): 1.528e-05, MSE(pi1): 2.553e-03, MSE(pi2): 1.286e-05, MSE(pi3): 7.705e-04\n",
      "Epoch 96300, Train loss: 2.554e+02, Test loss: 9.162e+02, MSE(e): 1.528e-05, MSE(pi1): 2.553e-03, MSE(pi2): 1.286e-05, MSE(pi3): 7.704e-04\n",
      "Epoch 96400, Train loss: 2.553e+02, Test loss: 9.155e+02, MSE(e): 1.528e-05, MSE(pi1): 2.553e-03, MSE(pi2): 1.285e-05, MSE(pi3): 7.702e-04\n",
      "Epoch 96500, Train loss: 2.553e+02, Test loss: 9.149e+02, MSE(e): 1.528e-05, MSE(pi1): 2.553e-03, MSE(pi2): 1.285e-05, MSE(pi3): 7.703e-04\n",
      "Epoch 96600, Train loss: 2.553e+02, Test loss: 9.143e+02, MSE(e): 1.527e-05, MSE(pi1): 2.565e-03, MSE(pi2): 1.285e-05, MSE(pi3): 7.691e-04\n",
      "Epoch 96700, Train loss: 2.553e+02, Test loss: 9.137e+02, MSE(e): 1.527e-05, MSE(pi1): 2.565e-03, MSE(pi2): 1.285e-05, MSE(pi3): 7.691e-04\n",
      "Epoch 96800, Train loss: 2.553e+02, Test loss: 9.128e+02, MSE(e): 1.526e-05, MSE(pi1): 2.550e-03, MSE(pi2): 1.285e-05, MSE(pi3): 7.711e-04\n",
      "Epoch 96900, Train loss: 2.552e+02, Test loss: 9.123e+02, MSE(e): 1.526e-05, MSE(pi1): 2.552e-03, MSE(pi2): 1.284e-05, MSE(pi3): 7.704e-04\n",
      "Epoch 97000, Train loss: 2.551e+02, Test loss: 9.117e+02, MSE(e): 1.526e-05, MSE(pi1): 2.551e-03, MSE(pi2): 1.284e-05, MSE(pi3): 7.705e-04\n",
      "Epoch 97100, Train loss: 2.551e+02, Test loss: 9.110e+02, MSE(e): 1.525e-05, MSE(pi1): 2.557e-03, MSE(pi2): 1.284e-05, MSE(pi3): 7.699e-04\n",
      "Epoch 97200, Train loss: 2.551e+02, Test loss: 9.105e+02, MSE(e): 1.525e-05, MSE(pi1): 2.562e-03, MSE(pi2): 1.284e-05, MSE(pi3): 7.694e-04\n",
      "Epoch 97300, Train loss: 2.551e+02, Test loss: 9.097e+02, MSE(e): 1.525e-05, MSE(pi1): 2.553e-03, MSE(pi2): 1.284e-05, MSE(pi3): 7.711e-04\n",
      "Epoch 97400, Train loss: 2.550e+02, Test loss: 9.091e+02, MSE(e): 1.524e-05, MSE(pi1): 2.552e-03, MSE(pi2): 1.283e-05, MSE(pi3): 7.705e-04\n",
      "Epoch 97500, Train loss: 2.550e+02, Test loss: 9.085e+02, MSE(e): 1.524e-05, MSE(pi1): 2.551e-03, MSE(pi2): 1.283e-05, MSE(pi3): 7.704e-04\n",
      "Epoch 97600, Train loss: 2.549e+02, Test loss: 9.079e+02, MSE(e): 1.524e-05, MSE(pi1): 2.550e-03, MSE(pi2): 1.283e-05, MSE(pi3): 7.705e-04\n",
      "Epoch 97700, Train loss: 2.550e+02, Test loss: 9.072e+02, MSE(e): 1.523e-05, MSE(pi1): 2.550e-03, MSE(pi2): 1.283e-05, MSE(pi3): 7.711e-04\n",
      "Epoch 97800, Train loss: 2.549e+02, Test loss: 9.069e+02, MSE(e): 1.523e-05, MSE(pi1): 2.559e-03, MSE(pi2): 1.283e-05, MSE(pi3): 7.695e-04\n",
      "Epoch 97900, Train loss: 2.548e+02, Test loss: 9.062e+02, MSE(e): 1.523e-05, MSE(pi1): 2.557e-03, MSE(pi2): 1.282e-05, MSE(pi3): 7.698e-04\n",
      "Epoch 98000, Train loss: 2.548e+02, Test loss: 9.057e+02, MSE(e): 1.522e-05, MSE(pi1): 2.555e-03, MSE(pi2): 1.282e-05, MSE(pi3): 7.701e-04\n",
      "Epoch 98100, Train loss: 2.548e+02, Test loss: 9.051e+02, MSE(e): 1.522e-05, MSE(pi1): 2.555e-03, MSE(pi2): 1.282e-05, MSE(pi3): 7.699e-04\n",
      "Epoch 98200, Train loss: 2.547e+02, Test loss: 9.046e+02, MSE(e): 1.522e-05, MSE(pi1): 2.560e-03, MSE(pi2): 1.282e-05, MSE(pi3): 7.694e-04\n",
      "Epoch 98300, Train loss: 2.547e+02, Test loss: 9.040e+02, MSE(e): 1.521e-05, MSE(pi1): 2.560e-03, MSE(pi2): 1.281e-05, MSE(pi3): 7.693e-04\n",
      "Epoch 98400, Train loss: 2.546e+02, Test loss: 9.034e+02, MSE(e): 1.521e-05, MSE(pi1): 2.553e-03, MSE(pi2): 1.281e-05, MSE(pi3): 7.700e-04\n",
      "Epoch 98500, Train loss: 2.546e+02, Test loss: 9.028e+02, MSE(e): 1.521e-05, MSE(pi1): 2.555e-03, MSE(pi2): 1.281e-05, MSE(pi3): 7.698e-04\n",
      "Epoch 98600, Train loss: 2.546e+02, Test loss: 9.022e+02, MSE(e): 1.520e-05, MSE(pi1): 2.553e-03, MSE(pi2): 1.281e-05, MSE(pi3): 7.700e-04\n",
      "Epoch 98700, Train loss: 2.546e+02, Test loss: 9.015e+02, MSE(e): 1.520e-05, MSE(pi1): 2.551e-03, MSE(pi2): 1.281e-05, MSE(pi3): 7.706e-04\n",
      "Epoch 98800, Train loss: 2.545e+02, Test loss: 9.010e+02, MSE(e): 1.520e-05, MSE(pi1): 2.549e-03, MSE(pi2): 1.281e-05, MSE(pi3): 7.706e-04\n",
      "Epoch 98900, Train loss: 2.545e+02, Test loss: 9.005e+02, MSE(e): 1.519e-05, MSE(pi1): 2.547e-03, MSE(pi2): 1.280e-05, MSE(pi3): 7.712e-04\n",
      "Epoch 99000, Train loss: 2.546e+02, Test loss: 9.000e+02, MSE(e): 1.519e-05, MSE(pi1): 2.552e-03, MSE(pi2): 1.280e-05, MSE(pi3): 7.712e-04\n",
      "Epoch 99100, Train loss: 2.544e+02, Test loss: 8.997e+02, MSE(e): 1.519e-05, MSE(pi1): 2.560e-03, MSE(pi2): 1.280e-05, MSE(pi3): 7.694e-04\n",
      "Epoch 99200, Train loss: 2.544e+02, Test loss: 8.991e+02, MSE(e): 1.519e-05, MSE(pi1): 2.557e-03, MSE(pi2): 1.280e-05, MSE(pi3): 7.697e-04\n",
      "Epoch 99300, Train loss: 2.544e+02, Test loss: 8.986e+02, MSE(e): 1.518e-05, MSE(pi1): 2.559e-03, MSE(pi2): 1.280e-05, MSE(pi3): 7.695e-04\n",
      "Epoch 99400, Train loss: 2.544e+02, Test loss: 8.980e+02, MSE(e): 1.518e-05, MSE(pi1): 2.555e-03, MSE(pi2): 1.280e-05, MSE(pi3): 7.700e-04\n",
      "Epoch 99500, Train loss: 2.544e+02, Test loss: 8.973e+02, MSE(e): 1.517e-05, MSE(pi1): 2.548e-03, MSE(pi2): 1.279e-05, MSE(pi3): 7.715e-04\n",
      "Epoch 99600, Train loss: 2.543e+02, Test loss: 8.968e+02, MSE(e): 1.517e-05, MSE(pi1): 2.549e-03, MSE(pi2): 1.279e-05, MSE(pi3): 7.707e-04\n",
      "Epoch 99700, Train loss: 2.543e+02, Test loss: 8.962e+02, MSE(e): 1.517e-05, MSE(pi1): 2.548e-03, MSE(pi2): 1.279e-05, MSE(pi3): 7.708e-04\n",
      "Epoch 99800, Train loss: 2.542e+02, Test loss: 8.956e+02, MSE(e): 1.517e-05, MSE(pi1): 2.546e-03, MSE(pi2): 1.279e-05, MSE(pi3): 7.711e-04\n",
      "Epoch 99900, Train loss: 2.542e+02, Test loss: 8.950e+02, MSE(e): 1.516e-05, MSE(pi1): 2.549e-03, MSE(pi2): 1.278e-05, MSE(pi3): 7.710e-04\n",
      "\n",
      "Training process finished after 100000 epochs\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Parametros de entrenamiento\n",
    "start_epoch = 9000\n",
    "n_epochs = 100000\n",
    "\n",
    "batch_size = 64 \n",
    "n_checkpoints = 100\n",
    "\n",
    "second_lr = 1e-4\n",
    "\n",
    "train_loop(model, optimizer, X_train_NN, y_train_NN, f_train_NN, X_test_NN, y_test_NN, f_test_NN,\n",
    "           D, n_checkpoints, start_epoch=start_epoch, n_epochs=n_epochs, batch_size=batch_size, \n",
    "           model_results_path=MODEL_RESULTS_PGNNIV_PATH, device=DEVICE, new_lr=second_lr)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SciML_test_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
