{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import GPUtil\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Imports de la libreria propia\n",
    "from vecopsciml.kernels.derivative import DerivativeKernels\n",
    "from vecopsciml.utils import TensOps\n",
    "\n",
    "# Imports de las funciones creadas para este programa\n",
    "# from models.POD import PODNonlinearModel\n",
    "from utils.folders import create_folder\n",
    "from utils.load_data import load_data\n",
    "from trainers.train import train_loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from vecopsciml.operators.zero_order import Mx, My"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Folder already exists at: /home/rmunoz/Escritorio/rmunozTMELab/Physically-Guided-Machine-Learning/results/non_linear\n",
      "Folder already exists at: /home/rmunoz/Escritorio/rmunozTMELab/Physically-Guided-Machine-Learning/results/non_linear/fourier_model\n"
     ]
    }
   ],
   "source": [
    "# Creamos los paths para las distintas carpetas\n",
    "ROOT_PATH = r'/home/rmunoz/Escritorio/rmunozTMELab/Physically-Guided-Machine-Learning'\n",
    "DATA_PATH = os.path.join(ROOT_PATH, r'data/non_linear/non_linear.pkl')\n",
    "RESULTS_FOLDER_PATH = os.path.join(ROOT_PATH, r'results/non_linear')\n",
    "MODEL_RESULTS_PATH = os.path.join(ROOT_PATH, r'results/non_linear/fourier_model')\n",
    "\n",
    "# Creamos las carpetas que sean necesarias (si ya están creadas se avisará de ello)\n",
    "create_folder(RESULTS_FOLDER_PATH)\n",
    "create_folder(MODEL_RESULTS_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully loaded from: /home/rmunoz/Escritorio/rmunozTMELab/Physically-Guided-Machine-Learning/data/non_linear/non_linear.pkl\n"
     ]
    }
   ],
   "source": [
    "# Load dataset\n",
    "dataset = load_data(DATA_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convolutional filters to derivate\n",
    "dx = dataset['x_step_size']\n",
    "dy = dataset['y_step_size']\n",
    "D = DerivativeKernels(dx, dy, 0).grad_kernels_two_dimensions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "print(f\"Using device: {DEVICE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train data splitting in train/test\n",
    "X = torch.tensor(dataset['X_train'], dtype=torch.float32).unsqueeze(1)\n",
    "y = torch.tensor(dataset['y_train'], dtype=torch.float32).unsqueeze(1)\n",
    "K = torch.tensor(dataset['k_train'], dtype=torch.float32).unsqueeze(1)\n",
    "f = torch.tensor(dataset['f_train'], dtype=torch.float32).unsqueeze(1)\n",
    "\n",
    "X_train, X_test, y_train, y_test, K_train, K_test, f_train, f_test = train_test_split(X, y, K, f, test_size=0.3, random_state=42)\n",
    "\n",
    "# Data processing and adequacy with our TensOps library\n",
    "X_train = X_train.to(DEVICE)\n",
    "X_test = X_test.to(DEVICE)\n",
    "\n",
    "y_train = TensOps(y_train.to(DEVICE).requires_grad_(True), space_dimension=2, contravariance=0, covariance=0)\n",
    "y_test = TensOps(y_test.to(DEVICE).requires_grad_(True), space_dimension=2, contravariance=0, covariance=0)\n",
    "\n",
    "K_train = TensOps(K_train.to(DEVICE).requires_grad_(True), space_dimension=2, contravariance=0, covariance=0)\n",
    "K_test = TensOps(K_test.to(DEVICE).requires_grad_(True), space_dimension=2, contravariance=0, covariance=0)\n",
    "\n",
    "f_train = TensOps(f_train.to(DEVICE).requires_grad_(True), space_dimension=2, contravariance=0, covariance=0)\n",
    "f_test = TensOps(f_test.to(DEVICE).requires_grad_(True), space_dimension=2, contravariance=0, covariance=0)\n",
    "\n",
    "# Loading and processing validation data\n",
    "X_val = torch.tensor(dataset['X_val'], dtype=torch.float32).unsqueeze(1)\n",
    "y_val = TensOps(torch.tensor(dataset['y_val'], dtype=torch.float32, requires_grad=True).unsqueeze(1), space_dimension=2, contravariance=0, covariance=0)\n",
    "K_val = TensOps(torch.tensor(dataset['k_val'], dtype=torch.float32, requires_grad=True).unsqueeze(1), space_dimension=2, contravariance=0, covariance=0)\n",
    "f_val = TensOps(torch.tensor(dataset['f_val'], dtype=torch.float32, requires_grad=True).unsqueeze(1), space_dimension=2, contravariance=0, covariance=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def modes_base(data, n_modes):\n",
    "\n",
    "    # FFT decomposition and obtain energy of each mode\n",
    "    fft_data = torch.fft.fft2(data)\n",
    "    fft_data_shifted = torch.fft.fftshift(fft_data)\n",
    "    energy = torch.abs(fft_data_shifted)\n",
    "    energy_flattened = energy.flatten(1, 3)\n",
    "\n",
    "    # Get the n_modes more energetic modes and their indices\n",
    "    top_energetic = torch.topk(energy_flattened, n_modes).indices\n",
    "    top_energetic_indices, _ = zip(*Counter(top_energetic.flatten()).most_common(n_modes))\n",
    "    top_energetic_indices = list(map(int, top_energetic_indices))\n",
    "\n",
    "    # Create an empty template to include the modes\n",
    "    filtered_modes = torch.zeros_like(energy, dtype=torch.complex64)\n",
    "    filtered_modes.flatten(1, 3)[:, top_energetic_indices] = fft_data_shifted.flatten(1, 3)[:, top_energetic_indices]\n",
    "\n",
    "    # Return the base with the 'n_modes' most energetic modes\n",
    "    return top_energetic_indices, filtered_modes\n",
    "\n",
    "def reconstruct_data(coefficients_shifted):\n",
    "    \n",
    "    # Compute inverse FFT and reconstruct data\n",
    "    filtered_modes_base = torch.fft.ifftshift(coefficients_shifted)\n",
    "    reconstructed_data = torch.real(torch.fft.ifft2(filtered_modes_base))\n",
    "\n",
    "    return reconstructed_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_modes = 100\n",
    "\n",
    "indices_base, _ = modes_base(data=y_train.values, n_modes=num_modes)\n",
    "reconstructed_data = reconstruct_data(_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predictive network architecture\n",
    "input_shape = X_train[0].shape\n",
    "predictive_layers = [20, 10, num_modes]\n",
    "predictive_output = y_train.values[0].shape\n",
    "\n",
    "# Explanatory network architecture\n",
    "explanatory_input = Mx(My(y_train)).values[0].shape\n",
    "explanatory_layers = [10, 10]\n",
    "explanatory_output = Mx(My(f_train)).values[0].shape\n",
    "\n",
    "# Other parameters\n",
    "n_filters_explanatory = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from vecopsciml.utils import TensOps\n",
    "from vecopsciml.operators.zero_order import Mx, My\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "\n",
    "    def __init__(self, input_size, hidden_layer_1_size, hidden_layer_2_size, latent_space_size):\n",
    "        super(Encoder, self).__init__()\n",
    "\n",
    "        # Parameters\n",
    "        self.in_size = torch.tensor(input_size)\n",
    "        self.h1_size = hidden_layer_1_size\n",
    "        self.h2_size = hidden_layer_2_size\n",
    "        self.ls_size = latent_space_size\n",
    "\n",
    "        # Architecture\n",
    "        self.flatten_layer = nn.Flatten(start_dim=1, end_dim=-1)\n",
    "        self.hidden1_layer = nn.Linear(torch.prod(self.in_size), self.h1_size)\n",
    "        self.hidden2_layer = nn.Linear(self.h1_size, self.h2_size)\n",
    "        self.latent_space_layer = nn.Linear(self.h2_size, self.ls_size)\n",
    "    \n",
    "    def forward(self, X):\n",
    "        \n",
    "        X = self.flatten_layer(X)\n",
    "        X = torch.sigmoid(self.hidden1_layer(X))\n",
    "        X = torch.sigmoid(self.hidden2_layer(X))\n",
    "        latent_space_output = (self.latent_space_layer(X))\n",
    "\n",
    "        return latent_space_output\n",
    "    \n",
    "class Explanatory(nn.Module):\n",
    "\n",
    "    def __init__(self, input_size, n_filters, hidden_layer_size, output_size):\n",
    "        super(Explanatory, self).__init__()\n",
    "\n",
    "        # Parameters\n",
    "        self.in_size = torch.tensor(input_size)\n",
    "        self.n_filters = n_filters\n",
    "        self.h_layer = hidden_layer_size\n",
    "        self.out_size = torch.tensor(output_size)\n",
    "\n",
    "        # Architecture\n",
    "        self.conv_expand_layer = nn.Conv2d(in_channels=1, out_channels=self.n_filters, kernel_size=1)\n",
    "        self.flatten_layer = nn.Flatten(start_dim=1, end_dim=-1)\n",
    "        self.hidden_layer = nn.Linear(n_filters*torch.prod(self.in_size), n_filters*torch.prod(self.out_size))\n",
    "        self.conv_converge_layer = nn.Conv2d(in_channels=n_filters, out_channels=1, kernel_size=1)\n",
    "        \n",
    "    def forward(self, X):\n",
    "        \n",
    "        X = torch.sigmoid(self.conv_expand_layer(X))\n",
    "        X = self.flatten_layer(X)\n",
    "        X = self.hidden_layer(X)\n",
    "        X = X.view(X.size(0), self.n_filters, self.out_size[1], self.out_size[2])\n",
    "        explanatory_output = self.conv_converge_layer(X)\n",
    "\n",
    "        return explanatory_output\n",
    "    \n",
    "class FFTNonlinearModel(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_size, predictive_layers, FFT_modes_base, output_predictive_size, explanatory_input_size, explanatory_layers, output_explanatory_size, n_filters):\n",
    "        \n",
    "        super(FFTNonlinearModel, self).__init__()\n",
    "\n",
    "        # Parameters\n",
    "        self.in_size = input_size\n",
    "        self.pred_size = predictive_layers\n",
    "        self.out_pred_size = output_predictive_size\n",
    "        \n",
    "        self.in_exp_size = explanatory_input_size\n",
    "        self.exp_size = explanatory_layers\n",
    "        self.out_exp_size = output_explanatory_size\n",
    "\n",
    "        self.n_filters = n_filters\n",
    "\n",
    "        # Architecture\n",
    "        self.encoder = Encoder(self.in_size, self.pred_size[0], self.pred_size[1], 2*self.pred_size[2])\n",
    "        self.base_indices = FFT_modes_base\n",
    "        self.explanatory = Explanatory(self.in_exp_size, self.n_filters, self.exp_size[0], self.out_exp_size)\n",
    "        \n",
    "    def forward(self, X):\n",
    "\n",
    "        # Predictive network\n",
    "        X = self.encoder(X)\n",
    "\n",
    "        # Manipulating output to obtain real and complex part\n",
    "        output_predictive = X.view(X.size(0), self.pred_size[2], 2)\n",
    "        real = output_predictive[..., 0]\n",
    "        imag = output_predictive[..., 1]\n",
    "    \n",
    "        # Reconstruction with FFT and manipulation of prediction output\n",
    "        base = torch.zeros((X.size(0), *self.out_pred_size), dtype=torch.complex64).to(DEVICE)\n",
    "        base.flatten(1, 3)[:, self.base_indices] = torch.complex(real, imag)\n",
    "\n",
    "        u = reconstruct_data(base).to(DEVICE)        \n",
    "        um = Mx(My(TensOps(u, space_dimension=2, contravariance=0, covariance=0))).values\n",
    "\n",
    "        # Explanatory network\n",
    "        K = self.explanatory(um)\n",
    "        \n",
    "        return u, K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training from scratch.\n",
      "Epoch 0, Train loss: 8.477e+08, Test loss: 1.077e+09, MSE(e): 8.474e+01, MSE(pi1): 2.331e+01, MSE(pi2): 3.413e+01, MSE(pi3): 3.005e-01\n",
      "Epoch 100, Train loss: 6.649e+08, Test loss: 8.340e+08, MSE(e): 6.648e+01, MSE(pi1): 8.822e-01, MSE(pi2): 2.673e+01, MSE(pi3): 1.838e-01\n",
      "Epoch 200, Train loss: 4.876e+08, Test loss: 6.207e+08, MSE(e): 4.875e+01, MSE(pi1): 5.801e-01, MSE(pi2): 1.968e+01, MSE(pi3): 1.359e-01\n",
      "Epoch 300, Train loss: 3.649e+08, Test loss: 4.718e+08, MSE(e): 3.649e+01, MSE(pi1): 5.757e-01, MSE(pi2): 1.478e+01, MSE(pi3): 1.325e-01\n",
      "Epoch 400, Train loss: 2.721e+08, Test loss: 3.580e+08, MSE(e): 2.721e+01, MSE(pi1): 5.756e-01, MSE(pi2): 1.106e+01, MSE(pi3): 1.325e-01\n",
      "Epoch 500, Train loss: 2.017e+08, Test loss: 2.704e+08, MSE(e): 2.017e+01, MSE(pi1): 5.756e-01, MSE(pi2): 8.249e+00, MSE(pi3): 1.325e-01\n",
      "Epoch 600, Train loss: 1.490e+08, Test loss: 2.039e+08, MSE(e): 1.490e+01, MSE(pi1): 5.757e-01, MSE(pi2): 6.141e+00, MSE(pi3): 1.325e-01\n",
      "Epoch 700, Train loss: 1.104e+08, Test loss: 1.541e+08, MSE(e): 1.103e+01, MSE(pi1): 5.756e-01, MSE(pi2): 4.595e+00, MSE(pi3): 1.325e-01\n",
      "Epoch 800, Train loss: 8.271e+07, Test loss: 1.176e+08, MSE(e): 8.269e+00, MSE(pi1): 5.759e-01, MSE(pi2): 3.489e+00, MSE(pi3): 1.325e-01\n",
      "Epoch 900, Train loss: 6.350e+07, Test loss: 9.135e+07, MSE(e): 6.348e+00, MSE(pi1): 5.756e-01, MSE(pi2): 2.721e+00, MSE(pi3): 1.325e-01\n",
      "Epoch 1000, Train loss: 5.057e+07, Test loss: 7.293e+07, MSE(e): 5.055e+00, MSE(pi1): 5.758e-01, MSE(pi2): 2.204e+00, MSE(pi3): 1.325e-01\n",
      "Epoch 1100, Train loss: 4.217e+07, Test loss: 6.029e+07, MSE(e): 4.215e+00, MSE(pi1): 5.912e-01, MSE(pi2): 1.868e+00, MSE(pi3): 1.326e-01\n",
      "Epoch 1200, Train loss: 3.691e+07, Test loss: 5.177e+07, MSE(e): 3.689e+00, MSE(pi1): 5.756e-01, MSE(pi2): 1.657e+00, MSE(pi3): 1.325e-01\n",
      "Epoch 1300, Train loss: 3.374e+07, Test loss: 4.615e+07, MSE(e): 3.372e+00, MSE(pi1): 5.756e-01, MSE(pi2): 1.531e+00, MSE(pi3): 1.325e-01\n",
      "Epoch 1400, Train loss: 3.192e+07, Test loss: 4.248e+07, MSE(e): 3.190e+00, MSE(pi1): 5.803e-01, MSE(pi2): 1.458e+00, MSE(pi3): 1.325e-01\n",
      "Epoch 1500, Train loss: 3.092e+07, Test loss: 4.012e+07, MSE(e): 3.089e+00, MSE(pi1): 5.756e-01, MSE(pi2): 1.418e+00, MSE(pi3): 1.325e-01\n",
      "Epoch 1600, Train loss: 3.039e+07, Test loss: 3.860e+07, MSE(e): 3.037e+00, MSE(pi1): 5.758e-01, MSE(pi2): 1.397e+00, MSE(pi3): 1.325e-01\n",
      "Epoch 1700, Train loss: 3.012e+07, Test loss: 3.763e+07, MSE(e): 3.010e+00, MSE(pi1): 5.756e-01, MSE(pi2): 1.386e+00, MSE(pi3): 1.325e-01\n",
      "Epoch 1800, Train loss: 3.000e+07, Test loss: 3.701e+07, MSE(e): 2.998e+00, MSE(pi1): 5.776e-01, MSE(pi2): 1.381e+00, MSE(pi3): 1.325e-01\n",
      "Epoch 1900, Train loss: 2.994e+07, Test loss: 3.662e+07, MSE(e): 2.992e+00, MSE(pi1): 5.756e-01, MSE(pi2): 1.379e+00, MSE(pi3): 1.325e-01\n",
      "Epoch 2000, Train loss: 2.991e+07, Test loss: 3.637e+07, MSE(e): 2.989e+00, MSE(pi1): 5.758e-01, MSE(pi2): 1.378e+00, MSE(pi3): 1.325e-01\n",
      "Epoch 2100, Train loss: 2.990e+07, Test loss: 3.622e+07, MSE(e): 2.988e+00, MSE(pi1): 5.757e-01, MSE(pi2): 1.377e+00, MSE(pi3): 1.325e-01\n",
      "Epoch 2200, Train loss: 2.990e+07, Test loss: 3.612e+07, MSE(e): 2.988e+00, MSE(pi1): 5.756e-01, MSE(pi2): 1.377e+00, MSE(pi3): 1.325e-01\n",
      "Epoch 2300, Train loss: 2.990e+07, Test loss: 3.606e+07, MSE(e): 2.988e+00, MSE(pi1): 5.756e-01, MSE(pi2): 1.377e+00, MSE(pi3): 1.325e-01\n",
      "Epoch 2400, Train loss: 2.990e+07, Test loss: 3.603e+07, MSE(e): 2.988e+00, MSE(pi1): 5.758e-01, MSE(pi2): 1.377e+00, MSE(pi3): 1.325e-01\n",
      "Epoch 2500, Train loss: 2.990e+07, Test loss: 3.601e+07, MSE(e): 2.988e+00, MSE(pi1): 5.756e-01, MSE(pi2): 1.377e+00, MSE(pi3): 1.325e-01\n",
      "Epoch 2600, Train loss: 2.990e+07, Test loss: 3.600e+07, MSE(e): 2.988e+00, MSE(pi1): 5.756e-01, MSE(pi2): 1.377e+00, MSE(pi3): 1.325e-01\n",
      "Epoch 2700, Train loss: 2.990e+07, Test loss: 3.599e+07, MSE(e): 2.988e+00, MSE(pi1): 5.756e-01, MSE(pi2): 1.377e+00, MSE(pi3): 1.325e-01\n",
      "Epoch 2800, Train loss: 2.990e+07, Test loss: 3.599e+07, MSE(e): 2.988e+00, MSE(pi1): 5.757e-01, MSE(pi2): 1.377e+00, MSE(pi3): 1.325e-01\n",
      "Epoch 2900, Train loss: 2.990e+07, Test loss: 3.599e+07, MSE(e): 2.988e+00, MSE(pi1): 5.756e-01, MSE(pi2): 1.377e+00, MSE(pi3): 1.325e-01\n",
      "Epoch 3000, Train loss: 2.990e+07, Test loss: 3.599e+07, MSE(e): 2.988e+00, MSE(pi1): 5.756e-01, MSE(pi2): 1.377e+00, MSE(pi3): 1.325e-01\n",
      "Epoch 3100, Train loss: 2.990e+07, Test loss: 3.599e+07, MSE(e): 2.988e+00, MSE(pi1): 5.928e-01, MSE(pi2): 1.377e+00, MSE(pi3): 1.325e-01\n",
      "Epoch 3200, Train loss: 2.990e+07, Test loss: 3.599e+07, MSE(e): 2.988e+00, MSE(pi1): 5.756e-01, MSE(pi2): 1.377e+00, MSE(pi3): 1.325e-01\n",
      "Epoch 3300, Train loss: 2.990e+07, Test loss: 3.599e+07, MSE(e): 2.988e+00, MSE(pi1): 5.756e-01, MSE(pi2): 1.377e+00, MSE(pi3): 1.325e-01\n",
      "Epoch 3400, Train loss: 2.990e+07, Test loss: 3.599e+07, MSE(e): 2.988e+00, MSE(pi1): 5.757e-01, MSE(pi2): 1.377e+00, MSE(pi3): 1.325e-01\n",
      "Epoch 3500, Train loss: 2.990e+07, Test loss: 3.599e+07, MSE(e): 2.988e+00, MSE(pi1): 5.756e-01, MSE(pi2): 1.377e+00, MSE(pi3): 1.325e-01\n",
      "Epoch 3600, Train loss: 2.990e+07, Test loss: 3.599e+07, MSE(e): 2.988e+00, MSE(pi1): 5.756e-01, MSE(pi2): 1.377e+00, MSE(pi3): 1.325e-01\n",
      "Epoch 3700, Train loss: 2.990e+07, Test loss: 3.599e+07, MSE(e): 2.988e+00, MSE(pi1): 5.756e-01, MSE(pi2): 1.377e+00, MSE(pi3): 1.325e-01\n",
      "Epoch 3800, Train loss: 2.990e+07, Test loss: 3.599e+07, MSE(e): 2.988e+00, MSE(pi1): 5.758e-01, MSE(pi2): 1.377e+00, MSE(pi3): 1.325e-01\n",
      "Epoch 3900, Train loss: 2.990e+07, Test loss: 3.599e+07, MSE(e): 2.988e+00, MSE(pi1): 5.756e-01, MSE(pi2): 1.377e+00, MSE(pi3): 1.325e-01\n",
      "Epoch 4000, Train loss: 2.990e+07, Test loss: 3.599e+07, MSE(e): 2.988e+00, MSE(pi1): 5.756e-01, MSE(pi2): 1.377e+00, MSE(pi3): 1.325e-01\n",
      "Epoch 4100, Train loss: 2.990e+07, Test loss: 3.599e+07, MSE(e): 2.988e+00, MSE(pi1): 5.757e-01, MSE(pi2): 1.377e+00, MSE(pi3): 1.325e-01\n",
      "Epoch 4200, Train loss: 2.990e+07, Test loss: 3.599e+07, MSE(e): 2.988e+00, MSE(pi1): 5.756e-01, MSE(pi2): 1.377e+00, MSE(pi3): 1.325e-01\n",
      "Epoch 4300, Train loss: 2.990e+07, Test loss: 3.599e+07, MSE(e): 2.988e+00, MSE(pi1): 5.756e-01, MSE(pi2): 1.377e+00, MSE(pi3): 1.325e-01\n",
      "Epoch 4400, Train loss: 2.990e+07, Test loss: 3.599e+07, MSE(e): 2.988e+00, MSE(pi1): 5.757e-01, MSE(pi2): 1.377e+00, MSE(pi3): 1.325e-01\n",
      "Epoch 4500, Train loss: 2.990e+07, Test loss: 3.599e+07, MSE(e): 2.988e+00, MSE(pi1): 5.756e-01, MSE(pi2): 1.377e+00, MSE(pi3): 1.325e-01\n",
      "Epoch 4600, Train loss: 2.990e+07, Test loss: 3.599e+07, MSE(e): 2.988e+00, MSE(pi1): 5.759e-01, MSE(pi2): 1.377e+00, MSE(pi3): 1.325e-01\n",
      "Epoch 4700, Train loss: 2.990e+07, Test loss: 3.599e+07, MSE(e): 2.988e+00, MSE(pi1): 5.756e-01, MSE(pi2): 1.377e+00, MSE(pi3): 1.325e-01\n",
      "Epoch 4800, Train loss: 2.990e+07, Test loss: 3.599e+07, MSE(e): 2.988e+00, MSE(pi1): 5.757e-01, MSE(pi2): 1.377e+00, MSE(pi3): 1.325e-01\n",
      "Epoch 4900, Train loss: 2.990e+07, Test loss: 3.599e+07, MSE(e): 2.988e+00, MSE(pi1): 5.827e-01, MSE(pi2): 1.377e+00, MSE(pi3): 1.325e-01\n",
      "Epoch 5000, Train loss: 2.990e+07, Test loss: 3.599e+07, MSE(e): 2.988e+00, MSE(pi1): 5.806e-01, MSE(pi2): 1.377e+00, MSE(pi3): 1.325e-01\n",
      "Epoch 5100, Train loss: 2.990e+07, Test loss: 3.599e+07, MSE(e): 2.988e+00, MSE(pi1): 5.784e-01, MSE(pi2): 1.377e+00, MSE(pi3): 1.325e-01\n",
      "Epoch 5200, Train loss: 2.990e+07, Test loss: 3.599e+07, MSE(e): 2.988e+00, MSE(pi1): 5.784e-01, MSE(pi2): 1.377e+00, MSE(pi3): 1.325e-01\n",
      "Epoch 5300, Train loss: 2.990e+07, Test loss: 3.599e+07, MSE(e): 2.988e+00, MSE(pi1): 5.797e-01, MSE(pi2): 1.377e+00, MSE(pi3): 1.325e-01\n",
      "Epoch 5400, Train loss: 2.990e+07, Test loss: 3.599e+07, MSE(e): 2.988e+00, MSE(pi1): 5.877e-01, MSE(pi2): 1.377e+00, MSE(pi3): 1.325e-01\n",
      "Epoch 5500, Train loss: 2.990e+07, Test loss: 3.599e+07, MSE(e): 2.988e+00, MSE(pi1): 5.888e-01, MSE(pi2): 1.377e+00, MSE(pi3): 1.326e-01\n",
      "Epoch 5600, Train loss: 2.990e+07, Test loss: 3.599e+07, MSE(e): 2.988e+00, MSE(pi1): 5.907e-01, MSE(pi2): 1.377e+00, MSE(pi3): 1.326e-01\n",
      "Epoch 5700, Train loss: 2.990e+07, Test loss: 3.599e+07, MSE(e): 2.988e+00, MSE(pi1): 5.792e-01, MSE(pi2): 1.377e+00, MSE(pi3): 1.325e-01\n",
      "Epoch 5800, Train loss: 2.990e+07, Test loss: 3.599e+07, MSE(e): 2.988e+00, MSE(pi1): 5.882e-01, MSE(pi2): 1.377e+00, MSE(pi3): 1.325e-01\n",
      "Epoch 5900, Train loss: 2.990e+07, Test loss: 3.599e+07, MSE(e): 2.988e+00, MSE(pi1): 6.152e-01, MSE(pi2): 1.377e+00, MSE(pi3): 1.326e-01\n",
      "Epoch 6000, Train loss: 2.990e+07, Test loss: 3.599e+07, MSE(e): 2.988e+00, MSE(pi1): 6.061e-01, MSE(pi2): 1.377e+00, MSE(pi3): 1.326e-01\n",
      "Epoch 6100, Train loss: 2.990e+07, Test loss: 3.599e+07, MSE(e): 2.988e+00, MSE(pi1): 5.874e-01, MSE(pi2): 1.377e+00, MSE(pi3): 1.326e-01\n",
      "Epoch 6200, Train loss: 2.990e+07, Test loss: 3.599e+07, MSE(e): 2.988e+00, MSE(pi1): 5.872e-01, MSE(pi2): 1.377e+00, MSE(pi3): 1.325e-01\n",
      "Epoch 6300, Train loss: 2.990e+07, Test loss: 3.599e+07, MSE(e): 2.988e+00, MSE(pi1): 6.496e-01, MSE(pi2): 1.377e+00, MSE(pi3): 1.328e-01\n",
      "Epoch 6400, Train loss: 2.990e+07, Test loss: 3.599e+07, MSE(e): 2.988e+00, MSE(pi1): 5.963e-01, MSE(pi2): 1.377e+00, MSE(pi3): 1.325e-01\n",
      "Epoch 6500, Train loss: 2.990e+07, Test loss: 3.599e+07, MSE(e): 2.988e+00, MSE(pi1): 5.832e-01, MSE(pi2): 1.377e+00, MSE(pi3): 1.325e-01\n",
      "Epoch 6600, Train loss: 2.990e+07, Test loss: 3.599e+07, MSE(e): 2.988e+00, MSE(pi1): 5.839e-01, MSE(pi2): 1.377e+00, MSE(pi3): 1.326e-01\n",
      "Epoch 6700, Train loss: 2.990e+07, Test loss: 3.599e+07, MSE(e): 2.988e+00, MSE(pi1): 6.399e-01, MSE(pi2): 1.377e+00, MSE(pi3): 1.327e-01\n",
      "Epoch 6800, Train loss: 2.990e+07, Test loss: 3.599e+07, MSE(e): 2.988e+00, MSE(pi1): 5.871e-01, MSE(pi2): 1.377e+00, MSE(pi3): 1.324e-01\n",
      "Epoch 6900, Train loss: 2.990e+07, Test loss: 3.599e+07, MSE(e): 2.988e+00, MSE(pi1): 5.999e-01, MSE(pi2): 1.377e+00, MSE(pi3): 1.325e-01\n",
      "Epoch 7000, Train loss: 2.990e+07, Test loss: 3.599e+07, MSE(e): 2.988e+00, MSE(pi1): 6.459e-01, MSE(pi2): 1.377e+00, MSE(pi3): 1.327e-01\n",
      "Epoch 7100, Train loss: 2.990e+07, Test loss: 3.599e+07, MSE(e): 2.988e+00, MSE(pi1): 5.798e-01, MSE(pi2): 1.377e+00, MSE(pi3): 1.325e-01\n",
      "Epoch 7200, Train loss: 2.990e+07, Test loss: 3.599e+07, MSE(e): 2.988e+00, MSE(pi1): 5.820e-01, MSE(pi2): 1.377e+00, MSE(pi3): 1.325e-01\n",
      "Epoch 7300, Train loss: 2.983e+07, Test loss: 3.605e+07, MSE(e): 2.981e+00, MSE(pi1): 6.000e-01, MSE(pi2): 1.374e+00, MSE(pi3): 1.332e-01\n",
      "Epoch 7400, Train loss: 2.599e+07, Test loss: 3.541e+07, MSE(e): 2.597e+00, MSE(pi1): 5.922e-01, MSE(pi2): 1.215e+00, MSE(pi3): 1.310e-01\n",
      "Epoch 7500, Train loss: 1.657e+07, Test loss: 5.779e+07, MSE(e): 1.655e+00, MSE(pi1): 6.785e-01, MSE(pi2): 8.151e-01, MSE(pi3): 1.244e-01\n",
      "Epoch 7600, Train loss: 1.112e+07, Test loss: 5.905e+07, MSE(e): 1.110e+00, MSE(pi1): 6.079e-01, MSE(pi2): 5.821e-01, MSE(pi3): 1.190e-01\n",
      "Epoch 7700, Train loss: 9.436e+06, Test loss: 6.136e+07, MSE(e): 9.419e-01, MSE(pi1): 6.400e-01, MSE(pi2): 4.979e-01, MSE(pi3): 1.072e-01\n",
      "Epoch 7800, Train loss: 7.716e+06, Test loss: 6.509e+07, MSE(e): 7.697e-01, MSE(pi1): 8.731e-01, MSE(pi2): 4.063e-01, MSE(pi3): 9.269e-02\n",
      "Epoch 7900, Train loss: 6.469e+06, Test loss: 6.986e+07, MSE(e): 6.456e-01, MSE(pi1): 5.184e-01, MSE(pi2): 3.386e-01, MSE(pi3): 7.492e-02\n",
      "Epoch 8000, Train loss: 5.706e+06, Test loss: 7.524e+07, MSE(e): 5.694e-01, MSE(pi1): 5.129e-01, MSE(pi2): 2.972e-01, MSE(pi3): 6.409e-02\n",
      "Epoch 8100, Train loss: 5.075e+06, Test loss: 7.870e+07, MSE(e): 5.065e-01, MSE(pi1): 3.988e-01, MSE(pi2): 2.650e-01, MSE(pi3): 5.801e-02\n",
      "Epoch 8200, Train loss: 4.531e+06, Test loss: 8.419e+07, MSE(e): 4.522e-01, MSE(pi1): 2.980e-01, MSE(pi2): 2.358e-01, MSE(pi3): 5.271e-02\n",
      "Epoch 8300, Train loss: 3.676e+06, Test loss: 8.687e+07, MSE(e): 3.667e-01, MSE(pi1): 3.333e-01, MSE(pi2): 2.019e-01, MSE(pi3): 5.141e-02\n",
      "Epoch 8400, Train loss: 3.152e+06, Test loss: 8.954e+07, MSE(e): 3.144e-01, MSE(pi1): 3.020e-01, MSE(pi2): 1.767e-01, MSE(pi3): 4.825e-02\n",
      "Epoch 8500, Train loss: 2.663e+06, Test loss: 9.257e+07, MSE(e): 2.656e-01, MSE(pi1): 2.945e-01, MSE(pi2): 1.516e-01, MSE(pi3): 4.396e-02\n",
      "Epoch 8600, Train loss: 2.195e+06, Test loss: 9.624e+07, MSE(e): 2.189e-01, MSE(pi1): 2.592e-01, MSE(pi2): 1.261e-01, MSE(pi3): 3.942e-02\n",
      "Epoch 8700, Train loss: 1.926e+06, Test loss: 9.766e+07, MSE(e): 1.919e-01, MSE(pi1): 2.773e-01, MSE(pi2): 1.118e-01, MSE(pi3): 3.755e-02\n",
      "Epoch 8800, Train loss: 1.777e+06, Test loss: 9.898e+07, MSE(e): 1.771e-01, MSE(pi1): 2.540e-01, MSE(pi2): 1.041e-01, MSE(pi3): 3.569e-02\n",
      "Epoch 8900, Train loss: 1.673e+06, Test loss: 9.986e+07, MSE(e): 1.667e-01, MSE(pi1): 3.285e-01, MSE(pi2): 9.899e-02, MSE(pi3): 3.487e-02\n",
      "Epoch 9000, Train loss: 1.436e+06, Test loss: 1.045e+08, MSE(e): 1.429e-01, MSE(pi1): 2.615e-01, MSE(pi2): 8.863e-02, MSE(pi3): 3.451e-02\n",
      "Epoch 9100, Train loss: 1.229e+06, Test loss: 1.086e+08, MSE(e): 1.224e-01, MSE(pi1): 2.360e-01, MSE(pi2): 7.781e-02, MSE(pi3): 3.174e-02\n",
      "Epoch 9200, Train loss: 1.094e+06, Test loss: 1.131e+08, MSE(e): 1.088e-01, MSE(pi1): 2.323e-01, MSE(pi2): 7.037e-02, MSE(pi3): 2.893e-02\n",
      "Epoch 9300, Train loss: 1.009e+06, Test loss: 1.153e+08, MSE(e): 1.003e-01, MSE(pi1): 2.461e-01, MSE(pi2): 6.527e-02, MSE(pi3): 2.949e-02\n",
      "Epoch 9400, Train loss: 9.159e+05, Test loss: 1.153e+08, MSE(e): 9.109e-02, MSE(pi1): 2.392e-01, MSE(pi2): 5.995e-02, MSE(pi3): 2.606e-02\n",
      "Epoch 9500, Train loss: 8.538e+05, Test loss: 1.151e+08, MSE(e): 8.493e-02, MSE(pi1): 2.002e-01, MSE(pi2): 5.635e-02, MSE(pi3): 2.420e-02\n",
      "Epoch 9600, Train loss: 8.049e+05, Test loss: 1.149e+08, MSE(e): 8.004e-02, MSE(pi1): 2.119e-01, MSE(pi2): 5.349e-02, MSE(pi3): 2.386e-02\n",
      "Epoch 9700, Train loss: 7.682e+05, Test loss: 1.148e+08, MSE(e): 7.638e-02, MSE(pi1): 2.073e-01, MSE(pi2): 5.136e-02, MSE(pi3): 2.250e-02\n",
      "Epoch 9800, Train loss: 7.591e+05, Test loss: 1.152e+08, MSE(e): 7.548e-02, MSE(pi1): 2.014e-01, MSE(pi2): 5.069e-02, MSE(pi3): 2.291e-02\n",
      "Epoch 9900, Train loss: 7.244e+05, Test loss: 1.155e+08, MSE(e): 7.201e-02, MSE(pi1): 1.956e-01, MSE(pi2): 4.868e-02, MSE(pi3): 2.225e-02\n",
      "Epoch 10000, Train loss: 6.911e+05, Test loss: 1.153e+08, MSE(e): 6.871e-02, MSE(pi1): 1.923e-01, MSE(pi2): 4.678e-02, MSE(pi3): 2.035e-02\n",
      "Epoch 10100, Train loss: 6.720e+05, Test loss: 1.155e+08, MSE(e): 6.682e-02, MSE(pi1): 1.732e-01, MSE(pi2): 4.562e-02, MSE(pi3): 1.999e-02\n",
      "Epoch 10200, Train loss: 6.559e+05, Test loss: 1.159e+08, MSE(e): 6.522e-02, MSE(pi1): 1.770e-01, MSE(pi2): 4.461e-02, MSE(pi3): 1.946e-02\n",
      "Epoch 10300, Train loss: 6.395e+05, Test loss: 1.160e+08, MSE(e): 6.357e-02, MSE(pi1): 1.899e-01, MSE(pi2): 4.355e-02, MSE(pi3): 1.924e-02\n",
      "Epoch 10400, Train loss: 6.249e+05, Test loss: 1.163e+08, MSE(e): 6.212e-02, MSE(pi1): 1.833e-01, MSE(pi2): 4.260e-02, MSE(pi3): 1.915e-02\n",
      "Epoch 10500, Train loss: 6.154e+05, Test loss: 1.167e+08, MSE(e): 6.117e-02, MSE(pi1): 1.783e-01, MSE(pi2): 4.191e-02, MSE(pi3): 1.918e-02\n",
      "Epoch 10600, Train loss: 5.882e+05, Test loss: 1.166e+08, MSE(e): 5.843e-02, MSE(pi1): 2.198e-01, MSE(pi2): 4.004e-02, MSE(pi3): 1.759e-02\n",
      "Epoch 10700, Train loss: 6.447e+05, Test loss: 1.183e+08, MSE(e): 6.409e-02, MSE(pi1): 1.832e-01, MSE(pi2): 4.277e-02, MSE(pi3): 1.981e-02\n",
      "Epoch 10800, Train loss: 5.569e+05, Test loss: 1.180e+08, MSE(e): 5.517e-02, MSE(pi1): 3.238e-01, MSE(pi2): 3.816e-02, MSE(pi3): 1.856e-02\n",
      "Epoch 10900, Train loss: 5.417e+05, Test loss: 1.182e+08, MSE(e): 5.378e-02, MSE(pi1): 2.155e-01, MSE(pi2): 3.732e-02, MSE(pi3): 1.698e-02\n",
      "Epoch 11000, Train loss: 2.364e+06, Test loss: 1.206e+08, MSE(e): 1.568e-01, MSE(pi1): 7.547e+01, MSE(pi2): 7.022e-02, MSE(pi3): 4.187e-01\n",
      "Epoch 11100, Train loss: 5.411e+05, Test loss: 1.178e+08, MSE(e): 5.225e-02, MSE(pi1): 8.238e-01, MSE(pi2): 3.629e-02, MSE(pi3): 1.029e-01\n",
      "Epoch 11200, Train loss: 5.281e+05, Test loss: 1.177e+08, MSE(e): 5.160e-02, MSE(pi1): 4.770e-01, MSE(pi2): 3.583e-02, MSE(pi3): 7.310e-02\n",
      "Epoch 11300, Train loss: 5.204e+05, Test loss: 1.176e+08, MSE(e): 5.095e-02, MSE(pi1): 4.352e-01, MSE(pi2): 3.537e-02, MSE(pi3): 6.455e-02\n",
      "Epoch 11400, Train loss: 5.124e+05, Test loss: 1.175e+08, MSE(e): 5.028e-02, MSE(pi1): 3.901e-01, MSE(pi2): 3.488e-02, MSE(pi3): 5.674e-02\n",
      "Epoch 11500, Train loss: 5.034e+05, Test loss: 1.175e+08, MSE(e): 4.955e-02, MSE(pi1): 3.253e-01, MSE(pi2): 3.436e-02, MSE(pi3): 4.655e-02\n",
      "Epoch 11600, Train loss: 4.943e+05, Test loss: 1.175e+08, MSE(e): 4.880e-02, MSE(pi1): 2.563e-01, MSE(pi2): 3.382e-02, MSE(pi3): 3.681e-02\n",
      "Epoch 11700, Train loss: 4.858e+05, Test loss: 1.175e+08, MSE(e): 4.808e-02, MSE(pi1): 1.969e-01, MSE(pi2): 3.325e-02, MSE(pi3): 3.030e-02\n",
      "Epoch 11800, Train loss: 4.742e+05, Test loss: 1.177e+08, MSE(e): 4.700e-02, MSE(pi1): 1.607e-01, MSE(pi2): 3.251e-02, MSE(pi3): 2.608e-02\n",
      "Epoch 11900, Train loss: 5.409e+05, Test loss: 1.182e+08, MSE(e): 5.370e-02, MSE(pi1): 1.491e-01, MSE(pi2): 3.549e-02, MSE(pi3): 2.388e-02\n",
      "Epoch 12000, Train loss: 4.497e+05, Test loss: 1.179e+08, MSE(e): 4.462e-02, MSE(pi1): 1.310e-01, MSE(pi2): 3.082e-02, MSE(pi3): 2.134e-02\n",
      "Epoch 12100, Train loss: 5.830e+05, Test loss: 1.188e+08, MSE(e): 5.796e-02, MSE(pi1): 1.328e-01, MSE(pi2): 3.686e-02, MSE(pi3): 2.124e-02\n",
      "Epoch 12200, Train loss: 4.209e+05, Test loss: 1.178e+08, MSE(e): 4.178e-02, MSE(pi1): 1.166e-01, MSE(pi2): 2.895e-02, MSE(pi3): 1.896e-02\n",
      "Epoch 12300, Train loss: 3.913e+05, Test loss: 1.186e+08, MSE(e): 3.881e-02, MSE(pi1): 1.380e-01, MSE(pi2): 2.705e-02, MSE(pi3): 1.849e-02\n",
      "Epoch 12400, Train loss: 3.741e+05, Test loss: 1.178e+08, MSE(e): 3.707e-02, MSE(pi1): 1.620e-01, MSE(pi2): 2.562e-02, MSE(pi3): 1.771e-02\n",
      "Epoch 12500, Train loss: 3.304e+05, Test loss: 1.181e+08, MSE(e): 3.273e-02, MSE(pi1): 1.383e-01, MSE(pi2): 2.309e-02, MSE(pi3): 1.703e-02\n",
      "Epoch 12600, Train loss: 3.110e+05, Test loss: 1.184e+08, MSE(e): 3.083e-02, MSE(pi1): 1.119e-01, MSE(pi2): 2.182e-02, MSE(pi3): 1.610e-02\n",
      "Epoch 12700, Train loss: 3.015e+05, Test loss: 1.185e+08, MSE(e): 2.990e-02, MSE(pi1): 9.877e-02, MSE(pi2): 2.113e-02, MSE(pi3): 1.524e-02\n",
      "Epoch 12800, Train loss: 2.859e+05, Test loss: 1.188e+08, MSE(e): 2.828e-02, MSE(pi1): 1.538e-01, MSE(pi2): 2.013e-02, MSE(pi3): 1.482e-02\n",
      "Epoch 12900, Train loss: 2.765e+05, Test loss: 1.190e+08, MSE(e): 2.714e-02, MSE(pi1): 3.591e-01, MSE(pi2): 1.938e-02, MSE(pi3): 1.485e-02\n",
      "Epoch 13000, Train loss: 2.691e+05, Test loss: 1.208e+08, MSE(e): 2.666e-02, MSE(pi1): 1.056e-01, MSE(pi2): 1.882e-02, MSE(pi3): 1.385e-02\n",
      "Epoch 13100, Train loss: 2.435e+05, Test loss: 1.217e+08, MSE(e): 2.412e-02, MSE(pi1): 9.609e-02, MSE(pi2): 1.729e-02, MSE(pi3): 1.345e-02\n",
      "Epoch 13200, Train loss: 2.433e+05, Test loss: 1.225e+08, MSE(e): 2.401e-02, MSE(pi1): 1.712e-01, MSE(pi2): 1.691e-02, MSE(pi3): 1.419e-02\n",
      "Epoch 13300, Train loss: 2.211e+05, Test loss: 1.231e+08, MSE(e): 2.189e-02, MSE(pi1): 9.149e-02, MSE(pi2): 1.572e-02, MSE(pi3): 1.247e-02\n",
      "Epoch 13400, Train loss: 2.186e+05, Test loss: 1.235e+08, MSE(e): 2.165e-02, MSE(pi1): 8.032e-02, MSE(pi2): 1.544e-02, MSE(pi3): 1.224e-02\n",
      "Epoch 13500, Train loss: 2.315e+05, Test loss: 1.240e+08, MSE(e): 2.297e-02, MSE(pi1): 6.284e-02, MSE(pi2): 1.591e-02, MSE(pi3): 1.179e-02\n",
      "Epoch 13600, Train loss: 1.952e+05, Test loss: 1.247e+08, MSE(e): 1.934e-02, MSE(pi1): 6.152e-02, MSE(pi2): 1.388e-02, MSE(pi3): 1.187e-02\n",
      "Epoch 13700, Train loss: 1.876e+05, Test loss: 1.251e+08, MSE(e): 1.858e-02, MSE(pi1): 6.157e-02, MSE(pi2): 1.336e-02, MSE(pi3): 1.157e-02\n",
      "Epoch 13800, Train loss: 1.807e+05, Test loss: 1.254e+08, MSE(e): 1.790e-02, MSE(pi1): 5.518e-02, MSE(pi2): 1.289e-02, MSE(pi3): 1.115e-02\n",
      "Epoch 13900, Train loss: 1.748e+05, Test loss: 1.256e+08, MSE(e): 1.731e-02, MSE(pi1): 5.275e-02, MSE(pi2): 1.249e-02, MSE(pi3): 1.085e-02\n",
      "Epoch 14000, Train loss: 1.737e+05, Test loss: 1.259e+08, MSE(e): 1.721e-02, MSE(pi1): 5.267e-02, MSE(pi2): 1.224e-02, MSE(pi3): 1.092e-02\n",
      "Epoch 14100, Train loss: 1.726e+05, Test loss: 1.260e+08, MSE(e): 1.709e-02, MSE(pi1): 6.214e-02, MSE(pi2): 1.220e-02, MSE(pi3): 1.062e-02\n",
      "Epoch 14200, Train loss: 1.976e+05, Test loss: 1.259e+08, MSE(e): 1.954e-02, MSE(pi1): 1.078e-01, MSE(pi2): 1.332e-02, MSE(pi3): 1.080e-02\n",
      "Epoch 14300, Train loss: 1.539e+05, Test loss: 1.262e+08, MSE(e): 1.524e-02, MSE(pi1): 4.618e-02, MSE(pi2): 1.104e-02, MSE(pi3): 1.012e-02\n",
      "Epoch 14400, Train loss: 1.501e+05, Test loss: 1.262e+08, MSE(e): 1.482e-02, MSE(pi1): 8.809e-02, MSE(pi2): 1.075e-02, MSE(pi3): 1.049e-02\n",
      "Epoch 14500, Train loss: 1.457e+05, Test loss: 1.262e+08, MSE(e): 1.443e-02, MSE(pi1): 4.800e-02, MSE(pi2): 1.045e-02, MSE(pi3): 9.655e-03\n",
      "Epoch 14600, Train loss: 1.491e+05, Test loss: 1.264e+08, MSE(e): 1.443e-02, MSE(pi1): 3.593e-01, MSE(pi2): 1.032e-02, MSE(pi3): 1.184e-02\n",
      "Epoch 14700, Train loss: 1.379e+05, Test loss: 1.263e+08, MSE(e): 1.364e-02, MSE(pi1): 5.701e-02, MSE(pi2): 9.923e-03, MSE(pi3): 9.413e-03\n",
      "Epoch 14800, Train loss: 1.347e+05, Test loss: 1.263e+08, MSE(e): 1.331e-02, MSE(pi1): 6.908e-02, MSE(pi2): 9.684e-03, MSE(pi3): 9.204e-03\n",
      "Epoch 14900, Train loss: 1.311e+05, Test loss: 1.263e+08, MSE(e): 1.297e-02, MSE(pi1): 4.839e-02, MSE(pi2): 9.457e-03, MSE(pi3): 8.850e-03\n",
      "Epoch 15000, Train loss: 1.286e+05, Test loss: 1.263e+08, MSE(e): 1.271e-02, MSE(pi1): 6.380e-02, MSE(pi2): 9.238e-03, MSE(pi3): 8.756e-03\n",
      "Epoch 15100, Train loss: 1.441e+05, Test loss: 1.262e+08, MSE(e): 1.417e-02, MSE(pi1): 1.412e-01, MSE(pi2): 1.004e-02, MSE(pi3): 9.134e-03\n",
      "Epoch 15200, Train loss: 1.238e+05, Test loss: 1.263e+08, MSE(e): 1.224e-02, MSE(pi1): 4.807e-02, MSE(pi2): 8.874e-03, MSE(pi3): 8.344e-03\n",
      "Epoch 15300, Train loss: 1.200e+05, Test loss: 1.262e+08, MSE(e): 1.186e-02, MSE(pi1): 6.434e-02, MSE(pi2): 8.657e-03, MSE(pi3): 7.991e-03\n",
      "Epoch 15400, Train loss: 1.182e+05, Test loss: 1.262e+08, MSE(e): 1.162e-02, MSE(pi1): 1.236e-01, MSE(pi2): 8.493e-03, MSE(pi3): 7.714e-03\n",
      "Epoch 15500, Train loss: 1.154e+05, Test loss: 1.261e+08, MSE(e): 1.139e-02, MSE(pi1): 7.233e-02, MSE(pi2): 8.355e-03, MSE(pi3): 7.575e-03\n",
      "Epoch 15600, Train loss: 1.185e+05, Test loss: 1.261e+08, MSE(e): 1.170e-02, MSE(pi1): 7.649e-02, MSE(pi2): 8.366e-03, MSE(pi3): 7.542e-03\n",
      "Epoch 15700, Train loss: 1.116e+05, Test loss: 1.261e+08, MSE(e): 1.097e-02, MSE(pi1): 1.116e-01, MSE(pi2): 8.047e-03, MSE(pi3): 7.839e-03\n",
      "Epoch 15800, Train loss: 1.097e+05, Test loss: 1.261e+08, MSE(e): 1.077e-02, MSE(pi1): 1.183e-01, MSE(pi2): 7.912e-03, MSE(pi3): 7.771e-03\n",
      "Epoch 15900, Train loss: 1.513e+05, Test loss: 1.261e+08, MSE(e): 1.492e-02, MSE(pi1): 1.400e-01, MSE(pi2): 1.007e-02, MSE(pi3): 7.033e-03\n",
      "Epoch 16000, Train loss: 1.048e+05, Test loss: 1.262e+08, MSE(e): 1.037e-02, MSE(pi1): 4.328e-02, MSE(pi2): 7.640e-03, MSE(pi3): 6.677e-03\n",
      "Epoch 16100, Train loss: 1.036e+05, Test loss: 1.262e+08, MSE(e): 1.021e-02, MSE(pi1): 8.747e-02, MSE(pi2): 7.501e-03, MSE(pi3): 6.403e-03\n",
      "Epoch 16200, Train loss: 1.158e+05, Test loss: 1.263e+08, MSE(e): 1.144e-02, MSE(pi1): 7.826e-02, MSE(pi2): 8.202e-03, MSE(pi3): 6.466e-03\n",
      "Epoch 16300, Train loss: 9.991e+04, Test loss: 1.265e+08, MSE(e): 9.868e-03, MSE(pi1): 6.093e-02, MSE(pi2): 7.251e-03, MSE(pi3): 6.133e-03\n",
      "Epoch 16400, Train loss: 9.715e+04, Test loss: 1.267e+08, MSE(e): 9.624e-03, MSE(pi1): 3.634e-02, MSE(pi2): 7.125e-03, MSE(pi3): 5.393e-03\n",
      "Epoch 16500, Train loss: 9.581e+04, Test loss: 1.269e+08, MSE(e): 9.464e-03, MSE(pi1): 5.764e-02, MSE(pi2): 6.994e-03, MSE(pi3): 5.841e-03\n",
      "Epoch 16600, Train loss: 9.540e+04, Test loss: 1.271e+08, MSE(e): 9.377e-03, MSE(pi1): 1.047e-01, MSE(pi2): 6.883e-03, MSE(pi3): 5.820e-03\n",
      "Epoch 16700, Train loss: 9.962e+04, Test loss: 1.274e+08, MSE(e): 9.807e-03, MSE(pi1): 9.693e-02, MSE(pi2): 7.192e-03, MSE(pi3): 5.762e-03\n",
      "Epoch 16800, Train loss: 9.062e+04, Test loss: 1.277e+08, MSE(e): 8.966e-03, MSE(pi1): 4.351e-02, MSE(pi2): 6.664e-03, MSE(pi3): 5.152e-03\n",
      "Epoch 16900, Train loss: 8.909e+04, Test loss: 1.279e+08, MSE(e): 8.827e-03, MSE(pi1): 3.646e-02, MSE(pi2): 6.576e-03, MSE(pi3): 4.578e-03\n",
      "Epoch 17000, Train loss: 8.870e+04, Test loss: 1.282e+08, MSE(e): 8.759e-03, MSE(pi1): 6.336e-02, MSE(pi2): 6.529e-03, MSE(pi3): 4.706e-03\n",
      "Epoch 17100, Train loss: 8.686e+04, Test loss: 1.284e+08, MSE(e): 8.592e-03, MSE(pi1): 4.787e-02, MSE(pi2): 6.368e-03, MSE(pi3): 4.619e-03\n",
      "Epoch 17200, Train loss: 8.573e+04, Test loss: 1.287e+08, MSE(e): 8.454e-03, MSE(pi1): 7.503e-02, MSE(pi2): 6.313e-03, MSE(pi3): 4.354e-03\n",
      "Epoch 17300, Train loss: 8.484e+04, Test loss: 1.289e+08, MSE(e): 8.327e-03, MSE(pi1): 1.057e-01, MSE(pi2): 6.185e-03, MSE(pi3): 5.076e-03\n",
      "Epoch 17400, Train loss: 8.261e+04, Test loss: 1.291e+08, MSE(e): 8.178e-03, MSE(pi1): 4.280e-02, MSE(pi2): 6.112e-03, MSE(pi3): 3.916e-03\n",
      "Epoch 17500, Train loss: 8.149e+04, Test loss: 1.293e+08, MSE(e): 8.066e-03, MSE(pi1): 4.575e-02, MSE(pi2): 6.027e-03, MSE(pi3): 3.611e-03\n",
      "Epoch 17600, Train loss: 8.050e+04, Test loss: 1.295e+08, MSE(e): 7.967e-03, MSE(pi1): 4.299e-02, MSE(pi2): 5.969e-03, MSE(pi3): 3.941e-03\n",
      "Epoch 17700, Train loss: 7.967e+04, Test loss: 1.297e+08, MSE(e): 7.886e-03, MSE(pi1): 4.288e-02, MSE(pi2): 5.878e-03, MSE(pi3): 3.798e-03\n",
      "Epoch 17800, Train loss: 7.906e+04, Test loss: 1.299e+08, MSE(e): 7.832e-03, MSE(pi1): 3.818e-02, MSE(pi2): 5.876e-03, MSE(pi3): 3.481e-03\n",
      "Epoch 17900, Train loss: 1.005e+05, Test loss: 1.301e+08, MSE(e): 9.919e-03, MSE(pi1): 9.380e-02, MSE(pi2): 6.704e-03, MSE(pi3): 3.518e-03\n",
      "Epoch 18000, Train loss: 1.087e+05, Test loss: 1.304e+08, MSE(e): 1.074e-02, MSE(pi1): 8.859e-02, MSE(pi2): 7.367e-03, MSE(pi3): 4.256e-03\n",
      "Epoch 18100, Train loss: 7.561e+04, Test loss: 1.305e+08, MSE(e): 7.486e-03, MSE(pi1): 3.736e-02, MSE(pi2): 5.610e-03, MSE(pi3): 3.676e-03\n",
      "Epoch 18200, Train loss: 7.475e+04, Test loss: 1.307e+08, MSE(e): 7.384e-03, MSE(pi1): 5.948e-02, MSE(pi2): 5.551e-03, MSE(pi3): 3.073e-03\n",
      "Epoch 18300, Train loss: 7.362e+04, Test loss: 1.308e+08, MSE(e): 7.295e-03, MSE(pi1): 3.623e-02, MSE(pi2): 5.497e-03, MSE(pi3): 3.039e-03\n",
      "Epoch 18400, Train loss: 7.361e+04, Test loss: 1.310e+08, MSE(e): 7.275e-03, MSE(pi1): 5.351e-02, MSE(pi2): 5.486e-03, MSE(pi3): 3.164e-03\n",
      "Epoch 18500, Train loss: 7.354e+04, Test loss: 1.311e+08, MSE(e): 7.279e-03, MSE(pi1): 4.182e-02, MSE(pi2): 5.418e-03, MSE(pi3): 3.290e-03\n",
      "Epoch 18600, Train loss: 7.581e+04, Test loss: 1.312e+08, MSE(e): 7.498e-03, MSE(pi1): 4.946e-02, MSE(pi2): 5.469e-03, MSE(pi3): 3.356e-03\n",
      "Epoch 18700, Train loss: 7.042e+04, Test loss: 1.313e+08, MSE(e): 6.987e-03, MSE(pi1): 2.408e-02, MSE(pi2): 5.263e-03, MSE(pi3): 3.053e-03\n",
      "Epoch 18800, Train loss: 6.993e+04, Test loss: 1.315e+08, MSE(e): 6.901e-03, MSE(pi1): 6.137e-02, MSE(pi2): 5.219e-03, MSE(pi3): 3.010e-03\n",
      "Epoch 18900, Train loss: 6.914e+04, Test loss: 1.315e+08, MSE(e): 6.828e-03, MSE(pi1): 5.631e-02, MSE(pi2): 5.173e-03, MSE(pi3): 2.857e-03\n",
      "Epoch 19000, Train loss: 6.821e+04, Test loss: 1.316e+08, MSE(e): 6.756e-03, MSE(pi1): 3.405e-02, MSE(pi2): 5.118e-03, MSE(pi3): 3.077e-03\n",
      "Epoch 19100, Train loss: 6.846e+04, Test loss: 1.317e+08, MSE(e): 6.695e-03, MSE(pi1): 1.176e-01, MSE(pi2): 5.064e-03, MSE(pi3): 3.283e-03\n",
      "Epoch 19200, Train loss: 6.871e+04, Test loss: 1.317e+08, MSE(e): 6.813e-03, MSE(pi1): 2.684e-02, MSE(pi2): 5.073e-03, MSE(pi3): 3.039e-03\n",
      "Epoch 19300, Train loss: 6.602e+04, Test loss: 1.318e+08, MSE(e): 6.540e-03, MSE(pi1): 3.389e-02, MSE(pi2): 4.970e-03, MSE(pi3): 2.810e-03\n",
      "Epoch 19400, Train loss: 6.712e+04, Test loss: 1.319e+08, MSE(e): 6.644e-03, MSE(pi1): 3.437e-02, MSE(pi2): 5.044e-03, MSE(pi3): 3.309e-03\n",
      "Epoch 19500, Train loss: 6.474e+04, Test loss: 1.319e+08, MSE(e): 6.408e-03, MSE(pi1): 3.425e-02, MSE(pi2): 4.881e-03, MSE(pi3): 3.179e-03\n",
      "Epoch 19600, Train loss: 6.442e+04, Test loss: 1.319e+08, MSE(e): 6.357e-03, MSE(pi1): 5.885e-02, MSE(pi2): 4.847e-03, MSE(pi3): 2.523e-03\n",
      "Epoch 19700, Train loss: 6.450e+04, Test loss: 1.320e+08, MSE(e): 6.311e-03, MSE(pi1): 1.097e-01, MSE(pi2): 4.795e-03, MSE(pi3): 2.914e-03\n",
      "Epoch 19800, Train loss: 6.322e+04, Test loss: 1.320e+08, MSE(e): 6.243e-03, MSE(pi1): 5.058e-02, MSE(pi2): 4.750e-03, MSE(pi3): 2.837e-03\n",
      "Epoch 19900, Train loss: 6.915e+04, Test loss: 1.321e+08, MSE(e): 6.810e-03, MSE(pi1): 7.429e-02, MSE(pi2): 5.085e-03, MSE(pi3): 3.077e-03\n",
      "\n",
      "Training process finished after 20000 epochs\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load model and the optimizer\n",
    "model = FFTNonlinearModel(input_shape, predictive_layers, indices_base, predictive_output, explanatory_input, explanatory_layers, explanatory_output, n_filters_explanatory).to(DEVICE)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-2)\n",
    "\n",
    "# Parametros de entrenamiento\n",
    "start_epoch = 0\n",
    "n_epochs = 20000\n",
    "\n",
    "batch_size = 64\n",
    "n_checkpoints = 10\n",
    "\n",
    "train_loop(model, optimizer, X_train, y_train, f_train, X_test, y_test, f_test,\n",
    "           D,  n_checkpoints, start_epoch=start_epoch, n_epochs=n_epochs, batch_size=batch_size, \n",
    "           model_results_path=MODEL_RESULTS_PATH, device=DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training from a checkpoint. Epoch 18000.\n",
      "Epoch 18000, Train loss: 1.194e+05, Test loss: 1.303e+08, MSE(e): 1.183e-02, MSE(pi1): 6.554e-02, MSE(pi2): 7.527e-03, MSE(pi3): 4.168e-03\n",
      "Epoch 18100, Train loss: 7.554e+04, Test loss: 1.305e+08, MSE(e): 7.487e-03, MSE(pi1): 3.323e-02, MSE(pi2): 5.639e-03, MSE(pi3): 3.330e-03\n",
      "Epoch 18200, Train loss: 7.488e+04, Test loss: 1.307e+08, MSE(e): 7.383e-03, MSE(pi1): 6.205e-02, MSE(pi2): 5.557e-03, MSE(pi3): 4.163e-03\n",
      "Epoch 18300, Train loss: 7.368e+04, Test loss: 1.308e+08, MSE(e): 7.296e-03, MSE(pi1): 3.793e-02, MSE(pi2): 5.494e-03, MSE(pi3): 3.406e-03\n",
      "Epoch 18400, Train loss: 7.493e+04, Test loss: 1.309e+08, MSE(e): 7.410e-03, MSE(pi1): 4.709e-02, MSE(pi2): 5.484e-03, MSE(pi3): 3.617e-03\n",
      "Epoch 18500, Train loss: 8.057e+04, Test loss: 1.311e+08, MSE(e): 7.986e-03, MSE(pi1): 3.761e-02, MSE(pi2): 5.881e-03, MSE(pi3): 3.220e-03\n",
      "Epoch 18600, Train loss: 7.742e+04, Test loss: 1.313e+08, MSE(e): 7.658e-03, MSE(pi1): 5.321e-02, MSE(pi2): 5.687e-03, MSE(pi3): 3.022e-03\n",
      "Epoch 18700, Train loss: 7.048e+04, Test loss: 1.314e+08, MSE(e): 6.993e-03, MSE(pi1): 2.334e-02, MSE(pi2): 5.297e-03, MSE(pi3): 3.153e-03\n",
      "Epoch 18800, Train loss: 6.973e+04, Test loss: 1.314e+08, MSE(e): 6.896e-03, MSE(pi1): 4.274e-02, MSE(pi2): 5.210e-03, MSE(pi3): 3.390e-03\n",
      "Epoch 18900, Train loss: 6.928e+04, Test loss: 1.315e+08, MSE(e): 6.834e-03, MSE(pi1): 5.971e-02, MSE(pi2): 5.156e-03, MSE(pi3): 3.395e-03\n",
      "Epoch 19000, Train loss: 6.968e+04, Test loss: 1.316e+08, MSE(e): 6.906e-03, MSE(pi1): 2.917e-02, MSE(pi2): 5.151e-03, MSE(pi3): 3.223e-03\n",
      "Epoch 19100, Train loss: 6.859e+04, Test loss: 1.317e+08, MSE(e): 6.694e-03, MSE(pi1): 1.227e-01, MSE(pi2): 5.066e-03, MSE(pi3): 4.118e-03\n",
      "Epoch 19200, Train loss: 7.118e+04, Test loss: 1.317e+08, MSE(e): 7.056e-03, MSE(pi1): 3.056e-02, MSE(pi2): 5.299e-03, MSE(pi3): 3.102e-03\n",
      "Epoch 19300, Train loss: 6.607e+04, Test loss: 1.318e+08, MSE(e): 6.540e-03, MSE(pi1): 3.417e-02, MSE(pi2): 4.972e-03, MSE(pi3): 3.155e-03\n",
      "Epoch 19400, Train loss: 6.547e+04, Test loss: 1.318e+08, MSE(e): 6.488e-03, MSE(pi1): 2.906e-02, MSE(pi2): 4.923e-03, MSE(pi3): 3.004e-03\n",
      "Epoch 19500, Train loss: 6.489e+04, Test loss: 1.319e+08, MSE(e): 6.412e-03, MSE(pi1): 4.837e-02, MSE(pi2): 4.881e-03, MSE(pi3): 2.843e-03\n",
      "Epoch 19600, Train loss: 6.456e+04, Test loss: 1.319e+08, MSE(e): 6.358e-03, MSE(pi1): 6.380e-02, MSE(pi2): 4.843e-03, MSE(pi3): 3.411e-03\n",
      "Epoch 19700, Train loss: 6.414e+04, Test loss: 1.320e+08, MSE(e): 6.301e-03, MSE(pi1): 7.645e-02, MSE(pi2): 4.799e-03, MSE(pi3): 3.664e-03\n",
      "Epoch 19800, Train loss: 6.317e+04, Test loss: 1.320e+08, MSE(e): 6.243e-03, MSE(pi1): 4.097e-02, MSE(pi2): 4.773e-03, MSE(pi3): 3.207e-03\n",
      "Epoch 19900, Train loss: 9.458e+04, Test loss: 1.319e+08, MSE(e): 9.355e-03, MSE(pi1): 6.058e-02, MSE(pi2): 6.136e-03, MSE(pi3): 4.182e-03\n",
      "Epoch 20000, Train loss: 6.198e+04, Test loss: 1.320e+08, MSE(e): 6.113e-03, MSE(pi1): 5.186e-02, MSE(pi2): 4.675e-03, MSE(pi3): 3.225e-03\n",
      "Epoch 20100, Train loss: 6.741e+04, Test loss: 1.320e+08, MSE(e): 6.682e-03, MSE(pi1): 2.549e-02, MSE(pi2): 4.882e-03, MSE(pi3): 3.265e-03\n",
      "Epoch 20200, Train loss: 6.049e+04, Test loss: 1.320e+08, MSE(e): 5.987e-03, MSE(pi1): 3.192e-02, MSE(pi2): 4.585e-03, MSE(pi3): 2.901e-03\n",
      "Epoch 20300, Train loss: 6.029e+04, Test loss: 1.320e+08, MSE(e): 5.949e-03, MSE(pi1): 4.878e-02, MSE(pi2): 4.540e-03, MSE(pi3): 3.097e-03\n",
      "Epoch 20400, Train loss: 5.949e+04, Test loss: 1.320e+08, MSE(e): 5.878e-03, MSE(pi1): 4.145e-02, MSE(pi2): 4.515e-03, MSE(pi3): 2.879e-03\n",
      "Epoch 20500, Train loss: 6.132e+04, Test loss: 1.320e+08, MSE(e): 6.062e-03, MSE(pi1): 3.811e-02, MSE(pi2): 4.532e-03, MSE(pi3): 3.131e-03\n",
      "Epoch 20600, Train loss: 5.863e+04, Test loss: 1.321e+08, MSE(e): 5.806e-03, MSE(pi1): 2.381e-02, MSE(pi2): 4.468e-03, MSE(pi3): 3.195e-03\n",
      "Epoch 20700, Train loss: 5.748e+04, Test loss: 1.321e+08, MSE(e): 5.686e-03, MSE(pi1): 3.547e-02, MSE(pi2): 4.374e-03, MSE(pi3): 2.643e-03\n",
      "Epoch 20800, Train loss: 7.506e+04, Test loss: 1.317e+08, MSE(e): 6.315e-03, MSE(pi1): 6.539e-01, MSE(pi2): 4.689e-03, MSE(pi3): 5.367e-02\n",
      "Epoch 20900, Train loss: 6.329e+04, Test loss: 1.318e+08, MSE(e): 5.627e-03, MSE(pi1): 3.085e-01, MSE(pi2): 4.326e-03, MSE(pi3): 3.927e-02\n",
      "Epoch 21000, Train loss: 6.168e+04, Test loss: 1.318e+08, MSE(e): 5.581e-03, MSE(pi1): 2.528e-01, MSE(pi2): 4.295e-03, MSE(pi3): 3.347e-02\n",
      "Epoch 21100, Train loss: 6.030e+04, Test loss: 1.318e+08, MSE(e): 5.539e-03, MSE(pi1): 2.036e-01, MSE(pi2): 4.267e-03, MSE(pi3): 2.867e-02\n",
      "Epoch 21200, Train loss: 5.899e+04, Test loss: 1.317e+08, MSE(e): 5.499e-03, MSE(pi1): 1.565e-01, MSE(pi2): 4.239e-03, MSE(pi3): 2.430e-02\n",
      "Epoch 21300, Train loss: 5.776e+04, Test loss: 1.317e+08, MSE(e): 5.459e-03, MSE(pi1): 1.136e-01, MSE(pi2): 4.211e-03, MSE(pi3): 2.030e-02\n",
      "Epoch 21400, Train loss: 5.667e+04, Test loss: 1.317e+08, MSE(e): 5.419e-03, MSE(pi1): 8.075e-02, MSE(pi2): 4.183e-03, MSE(pi3): 1.672e-02\n",
      "Epoch 21500, Train loss: 2.123e+05, Test loss: 1.315e+08, MSE(e): 2.102e-02, MSE(pi1): 6.951e-02, MSE(pi2): 1.134e-02, MSE(pi3): 1.400e-02\n",
      "Epoch 21600, Train loss: 5.504e+04, Test loss: 1.316e+08, MSE(e): 5.341e-03, MSE(pi1): 4.943e-02, MSE(pi2): 4.128e-03, MSE(pi3): 1.132e-02\n",
      "Epoch 21700, Train loss: 5.637e+04, Test loss: 1.316e+08, MSE(e): 5.498e-03, MSE(pi1): 4.296e-02, MSE(pi2): 4.230e-03, MSE(pi3): 9.609e-03\n",
      "Epoch 21800, Train loss: 5.390e+04, Test loss: 1.315e+08, MSE(e): 5.267e-03, MSE(pi1): 3.878e-02, MSE(pi2): 4.079e-03, MSE(pi3): 8.330e-03\n",
      "Epoch 21900, Train loss: 1.617e+05, Test loss: 1.320e+08, MSE(e): 1.604e-02, MSE(pi1): 4.410e-02, MSE(pi2): 9.496e-03, MSE(pi3): 8.114e-03\n",
      "Epoch 22000, Train loss: 5.294e+04, Test loss: 1.315e+08, MSE(e): 5.192e-03, MSE(pi1): 3.298e-02, MSE(pi2): 4.020e-03, MSE(pi3): 6.823e-03\n",
      "Epoch 22100, Train loss: 9.802e+04, Test loss: 1.314e+08, MSE(e): 9.700e-03, MSE(pi1): 3.454e-02, MSE(pi2): 6.017e-03, MSE(pi3): 6.612e-03\n",
      "Epoch 22200, Train loss: 5.379e+04, Test loss: 1.314e+08, MSE(e): 5.119e-03, MSE(pi1): 1.957e-01, MSE(pi2): 3.970e-03, MSE(pi3): 6.356e-03\n",
      "Epoch 22300, Train loss: 5.655e+04, Test loss: 1.314e+08, MSE(e): 5.560e-03, MSE(pi1): 3.804e-02, MSE(pi2): 4.115e-03, MSE(pi3): 5.665e-03\n",
      "Epoch 22400, Train loss: 5.167e+04, Test loss: 1.315e+08, MSE(e): 5.045e-03, MSE(pi1): 6.482e-02, MSE(pi2): 3.917e-03, MSE(pi3): 5.660e-03\n",
      "Epoch 22500, Train loss: 5.088e+04, Test loss: 1.314e+08, MSE(e): 5.011e-03, MSE(pi1): 2.534e-02, MSE(pi2): 3.889e-03, MSE(pi3): 5.104e-03\n",
      "Epoch 22600, Train loss: 5.087e+04, Test loss: 1.315e+08, MSE(e): 5.013e-03, MSE(pi1): 2.369e-02, MSE(pi2): 3.868e-03, MSE(pi3): 4.916e-03\n",
      "Epoch 22700, Train loss: 5.376e+04, Test loss: 1.315e+08, MSE(e): 4.947e-03, MSE(pi1): 3.668e-01, MSE(pi2): 3.844e-03, MSE(pi3): 6.102e-03\n",
      "Epoch 22800, Train loss: 6.700e+04, Test loss: 1.316e+08, MSE(e): 6.607e-03, MSE(pi1): 4.493e-02, MSE(pi2): 4.749e-03, MSE(pi3): 4.768e-03\n",
      "Epoch 22900, Train loss: 4.932e+04, Test loss: 1.315e+08, MSE(e): 4.866e-03, MSE(pi1): 2.115e-02, MSE(pi2): 3.786e-03, MSE(pi3): 4.416e-03\n",
      "Epoch 23000, Train loss: 4.907e+04, Test loss: 1.316e+08, MSE(e): 4.844e-03, MSE(pi1): 2.005e-02, MSE(pi2): 3.780e-03, MSE(pi3): 4.222e-03\n",
      "Epoch 23100, Train loss: 4.859e+04, Test loss: 1.316e+08, MSE(e): 4.796e-03, MSE(pi1): 2.069e-02, MSE(pi2): 3.737e-03, MSE(pi3): 4.173e-03\n",
      "Epoch 23200, Train loss: 5.268e+04, Test loss: 1.317e+08, MSE(e): 5.201e-03, MSE(pi1): 2.440e-02, MSE(pi2): 3.888e-03, MSE(pi3): 4.205e-03\n",
      "Epoch 23300, Train loss: 4.803e+04, Test loss: 1.317e+08, MSE(e): 4.726e-03, MSE(pi1): 3.688e-02, MSE(pi2): 3.686e-03, MSE(pi3): 3.978e-03\n",
      "Epoch 23400, Train loss: 4.759e+04, Test loss: 1.317e+08, MSE(e): 4.702e-03, MSE(pi1): 1.816e-02, MSE(pi2): 3.678e-03, MSE(pi3): 3.809e-03\n",
      "Epoch 23500, Train loss: 5.086e+04, Test loss: 1.316e+08, MSE(e): 5.028e-03, MSE(pi1): 1.898e-02, MSE(pi2): 3.770e-03, MSE(pi3): 3.859e-03\n",
      "Epoch 23600, Train loss: 5.001e+04, Test loss: 1.317e+08, MSE(e): 4.946e-03, MSE(pi1): 1.785e-02, MSE(pi2): 3.728e-03, MSE(pi3): 3.730e-03\n",
      "Epoch 23700, Train loss: 4.658e+04, Test loss: 1.318e+08, MSE(e): 4.591e-03, MSE(pi1): 3.225e-02, MSE(pi2): 3.589e-03, MSE(pi3): 3.517e-03\n",
      "Epoch 23800, Train loss: 4.640e+04, Test loss: 1.318e+08, MSE(e): 4.563e-03, MSE(pi1): 4.037e-02, MSE(pi2): 3.575e-03, MSE(pi3): 3.643e-03\n",
      "Epoch 23900, Train loss: 6.095e+04, Test loss: 1.320e+08, MSE(e): 5.643e-03, MSE(pi1): 4.018e-01, MSE(pi2): 4.156e-03, MSE(pi3): 4.974e-03\n",
      "Epoch 24000, Train loss: 4.590e+04, Test loss: 1.319e+08, MSE(e): 4.492e-03, MSE(pi1): 6.354e-02, MSE(pi2): 3.522e-03, MSE(pi3): 3.429e-03\n",
      "Epoch 24100, Train loss: 4.553e+04, Test loss: 1.319e+08, MSE(e): 4.500e-03, MSE(pi1): 1.830e-02, MSE(pi2): 3.502e-03, MSE(pi3): 3.389e-03\n",
      "Epoch 24200, Train loss: 4.588e+04, Test loss: 1.320e+08, MSE(e): 4.519e-03, MSE(pi1): 3.472e-02, MSE(pi2): 3.542e-03, MSE(pi3): 3.330e-03\n",
      "Epoch 24300, Train loss: 4.665e+04, Test loss: 1.319e+08, MSE(e): 4.613e-03, MSE(pi1): 1.841e-02, MSE(pi2): 3.523e-03, MSE(pi3): 3.323e-03\n",
      "Epoch 24400, Train loss: 5.901e+04, Test loss: 1.320e+08, MSE(e): 5.829e-03, MSE(pi1): 3.453e-02, MSE(pi2): 4.299e-03, MSE(pi3): 3.639e-03\n",
      "Epoch 24500, Train loss: 4.396e+04, Test loss: 1.320e+08, MSE(e): 4.341e-03, MSE(pi1): 2.080e-02, MSE(pi2): 3.416e-03, MSE(pi3): 3.410e-03\n",
      "Epoch 24600, Train loss: 4.440e+04, Test loss: 1.320e+08, MSE(e): 4.313e-03, MSE(pi1): 9.212e-02, MSE(pi2): 3.391e-03, MSE(pi3): 3.547e-03\n",
      "Epoch 24700, Train loss: 4.562e+04, Test loss: 1.320e+08, MSE(e): 4.507e-03, MSE(pi1): 2.126e-02, MSE(pi2): 3.522e-03, MSE(pi3): 3.350e-03\n",
      "Epoch 24800, Train loss: 4.584e+04, Test loss: 1.322e+08, MSE(e): 4.429e-03, MSE(pi1): 1.197e-01, MSE(pi2): 3.450e-03, MSE(pi3): 3.492e-03\n",
      "Epoch 24900, Train loss: 7.964e+04, Test loss: 1.318e+08, MSE(e): 7.875e-03, MSE(pi1): 5.378e-02, MSE(pi2): 4.988e-03, MSE(pi3): 3.470e-03\n",
      "Epoch 25000, Train loss: 4.318e+04, Test loss: 1.320e+08, MSE(e): 4.237e-03, MSE(pi1): 4.474e-02, MSE(pi2): 3.345e-03, MSE(pi3): 3.567e-03\n",
      "Epoch 25100, Train loss: 4.391e+04, Test loss: 1.320e+08, MSE(e): 4.191e-03, MSE(pi1): 1.637e-01, MSE(pi2): 3.292e-03, MSE(pi3): 3.633e-03\n",
      "Epoch 25200, Train loss: 4.257e+04, Test loss: 1.320e+08, MSE(e): 4.170e-03, MSE(pi1): 5.050e-02, MSE(pi2): 3.293e-03, MSE(pi3): 3.691e-03\n",
      "Epoch 25300, Train loss: 4.216e+04, Test loss: 1.320e+08, MSE(e): 4.129e-03, MSE(pi1): 5.531e-02, MSE(pi2): 3.258e-03, MSE(pi3): 3.126e-03\n",
      "Epoch 25400, Train loss: 4.149e+04, Test loss: 1.320e+08, MSE(e): 4.099e-03, MSE(pi1): 1.803e-02, MSE(pi2): 3.237e-03, MSE(pi3): 3.118e-03\n",
      "Epoch 25500, Train loss: 4.172e+04, Test loss: 1.320e+08, MSE(e): 4.107e-03, MSE(pi1): 3.326e-02, MSE(pi2): 3.224e-03, MSE(pi3): 3.109e-03\n",
      "Epoch 25600, Train loss: 4.217e+04, Test loss: 1.319e+08, MSE(e): 4.125e-03, MSE(pi1): 5.910e-02, MSE(pi2): 3.261e-03, MSE(pi3): 3.182e-03\n",
      "Epoch 25700, Train loss: 7.128e+04, Test loss: 1.318e+08, MSE(e): 7.037e-03, MSE(pi1): 5.488e-02, MSE(pi2): 4.565e-03, MSE(pi3): 3.506e-03\n",
      "Epoch 25800, Train loss: 4.112e+04, Test loss: 1.319e+08, MSE(e): 4.012e-03, MSE(pi1): 6.589e-02, MSE(pi2): 3.180e-03, MSE(pi3): 3.406e-03\n",
      "Epoch 25900, Train loss: 4.093e+04, Test loss: 1.319e+08, MSE(e): 3.994e-03, MSE(pi1): 6.612e-02, MSE(pi2): 3.153e-03, MSE(pi3): 3.200e-03\n",
      "Epoch 26000, Train loss: 4.080e+04, Test loss: 1.319e+08, MSE(e): 3.974e-03, MSE(pi1): 7.095e-02, MSE(pi2): 3.149e-03, MSE(pi3): 3.527e-03\n",
      "Epoch 26100, Train loss: 4.000e+04, Test loss: 1.318e+08, MSE(e): 3.947e-03, MSE(pi1): 2.129e-02, MSE(pi2): 3.126e-03, MSE(pi3): 3.173e-03\n",
      "Epoch 26200, Train loss: 4.108e+04, Test loss: 1.319e+08, MSE(e): 4.008e-03, MSE(pi1): 6.359e-02, MSE(pi2): 3.173e-03, MSE(pi3): 3.574e-03\n",
      "Epoch 26300, Train loss: 6.261e+04, Test loss: 1.317e+08, MSE(e): 6.182e-03, MSE(pi1): 4.570e-02, MSE(pi2): 4.094e-03, MSE(pi3): 3.357e-03\n",
      "Epoch 26400, Train loss: 3.942e+04, Test loss: 1.318e+08, MSE(e): 3.878e-03, MSE(pi1): 3.123e-02, MSE(pi2): 3.078e-03, MSE(pi3): 3.276e-03\n",
      "Epoch 26500, Train loss: 3.949e+04, Test loss: 1.317e+08, MSE(e): 3.871e-03, MSE(pi1): 4.524e-02, MSE(pi2): 3.073e-03, MSE(pi3): 3.265e-03\n",
      "Epoch 26600, Train loss: 3.965e+04, Test loss: 1.317e+08, MSE(e): 3.866e-03, MSE(pi1): 6.445e-02, MSE(pi2): 3.051e-03, MSE(pi3): 3.406e-03\n",
      "Epoch 26700, Train loss: 4.548e+04, Test loss: 1.319e+08, MSE(e): 4.454e-03, MSE(pi1): 6.132e-02, MSE(pi2): 3.374e-03, MSE(pi3): 3.238e-03\n",
      "Epoch 26800, Train loss: 4.401e+04, Test loss: 1.316e+08, MSE(e): 4.310e-03, MSE(pi1): 5.871e-02, MSE(pi2): 3.315e-03, MSE(pi3): 3.213e-03\n",
      "Epoch 26900, Train loss: 5.380e+04, Test loss: 1.317e+08, MSE(e): 5.322e-03, MSE(pi1): 2.613e-02, MSE(pi2): 3.854e-03, MSE(pi3): 3.100e-03\n",
      "Epoch 27000, Train loss: 4.878e+04, Test loss: 1.315e+08, MSE(e): 4.816e-03, MSE(pi1): 2.597e-02, MSE(pi2): 3.429e-03, MSE(pi3): 3.510e-03\n",
      "Epoch 27100, Train loss: 3.841e+04, Test loss: 1.316e+08, MSE(e): 3.750e-03, MSE(pi1): 6.130e-02, MSE(pi2): 2.975e-03, MSE(pi3): 3.014e-03\n",
      "Epoch 27200, Train loss: 3.796e+04, Test loss: 1.315e+08, MSE(e): 3.742e-03, MSE(pi1): 1.944e-02, MSE(pi2): 2.982e-03, MSE(pi3): 3.431e-03\n",
      "Epoch 27300, Train loss: 4.280e+04, Test loss: 1.314e+08, MSE(e): 4.184e-03, MSE(pi1): 6.114e-02, MSE(pi2): 3.126e-03, MSE(pi3): 3.422e-03\n",
      "Epoch 27400, Train loss: 1.349e+05, Test loss: 1.313e+08, MSE(e): 1.338e-02, MSE(pi1): 6.893e-02, MSE(pi2): 7.501e-03, MSE(pi3): 3.982e-03\n",
      "Epoch 27500, Train loss: 3.758e+04, Test loss: 1.315e+08, MSE(e): 3.690e-03, MSE(pi1): 3.626e-02, MSE(pi2): 2.927e-03, MSE(pi3): 3.159e-03\n",
      "Epoch 27600, Train loss: 3.715e+04, Test loss: 1.314e+08, MSE(e): 3.655e-03, MSE(pi1): 2.965e-02, MSE(pi2): 2.900e-03, MSE(pi3): 3.014e-03\n",
      "Epoch 27700, Train loss: 3.682e+04, Test loss: 1.315e+08, MSE(e): 3.627e-03, MSE(pi1): 2.309e-02, MSE(pi2): 2.887e-03, MSE(pi3): 3.108e-03\n",
      "Epoch 27800, Train loss: 3.817e+04, Test loss: 1.314e+08, MSE(e): 3.652e-03, MSE(pi1): 1.206e-01, MSE(pi2): 2.872e-03, MSE(pi3): 4.402e-03\n",
      "Epoch 27900, Train loss: 4.200e+04, Test loss: 1.314e+08, MSE(e): 4.107e-03, MSE(pi1): 6.013e-02, MSE(pi2): 3.165e-03, MSE(pi3): 3.251e-03\n",
      "Epoch 28000, Train loss: 4.108e+04, Test loss: 1.312e+08, MSE(e): 3.927e-03, MSE(pi1): 1.442e-01, MSE(pi2): 3.012e-03, MSE(pi3): 3.598e-03\n",
      "Epoch 28100, Train loss: 3.628e+04, Test loss: 1.314e+08, MSE(e): 3.571e-03, MSE(pi1): 2.843e-02, MSE(pi2): 2.846e-03, MSE(pi3): 2.843e-03\n",
      "Epoch 28200, Train loss: 3.690e+04, Test loss: 1.314e+08, MSE(e): 3.548e-03, MSE(pi1): 1.046e-01, MSE(pi2): 2.816e-03, MSE(pi3): 3.643e-03\n",
      "Epoch 28300, Train loss: 3.661e+04, Test loss: 1.314e+08, MSE(e): 3.603e-03, MSE(pi1): 2.799e-02, MSE(pi2): 2.826e-03, MSE(pi3): 3.027e-03\n",
      "Epoch 28400, Train loss: 3.743e+04, Test loss: 1.315e+08, MSE(e): 3.623e-03, MSE(pi1): 8.517e-02, MSE(pi2): 2.849e-03, MSE(pi3): 3.417e-03\n",
      "Epoch 28500, Train loss: 5.124e+04, Test loss: 1.313e+08, MSE(e): 5.025e-03, MSE(pi1): 7.043e-02, MSE(pi2): 3.459e-03, MSE(pi3): 2.834e-03\n",
      "Epoch 28600, Train loss: 8.531e+04, Test loss: 1.314e+08, MSE(e): 8.442e-03, MSE(pi1): 4.546e-02, MSE(pi2): 5.294e-03, MSE(pi3): 4.361e-03\n",
      "Epoch 28700, Train loss: 4.016e+04, Test loss: 1.314e+08, MSE(e): 3.946e-03, MSE(pi1): 3.993e-02, MSE(pi2): 3.026e-03, MSE(pi3): 2.909e-03\n",
      "Epoch 28800, Train loss: 3.535e+04, Test loss: 1.313e+08, MSE(e): 3.449e-03, MSE(pi1): 5.868e-02, MSE(pi2): 2.734e-03, MSE(pi3): 2.761e-03\n",
      "Epoch 28900, Train loss: 3.549e+04, Test loss: 1.313e+08, MSE(e): 3.442e-03, MSE(pi1): 7.363e-02, MSE(pi2): 2.720e-03, MSE(pi3): 3.313e-03\n",
      "Epoch 29000, Train loss: 3.567e+04, Test loss: 1.313e+08, MSE(e): 3.501e-03, MSE(pi1): 3.262e-02, MSE(pi2): 2.769e-03, MSE(pi3): 3.303e-03\n",
      "Epoch 29100, Train loss: 3.456e+04, Test loss: 1.312e+08, MSE(e): 3.406e-03, MSE(pi1): 2.069e-02, MSE(pi2): 2.695e-03, MSE(pi3): 2.878e-03\n",
      "Epoch 29200, Train loss: 3.431e+04, Test loss: 1.312e+08, MSE(e): 3.384e-03, MSE(pi1): 1.723e-02, MSE(pi2): 2.680e-03, MSE(pi3): 2.906e-03\n",
      "Epoch 29300, Train loss: 3.502e+04, Test loss: 1.313e+08, MSE(e): 3.398e-03, MSE(pi1): 7.445e-02, MSE(pi2): 2.678e-03, MSE(pi3): 2.846e-03\n",
      "Epoch 29400, Train loss: 3.888e+04, Test loss: 1.313e+08, MSE(e): 3.827e-03, MSE(pi1): 3.106e-02, MSE(pi2): 2.909e-03, MSE(pi3): 2.907e-03\n",
      "Epoch 29500, Train loss: 3.427e+04, Test loss: 1.312e+08, MSE(e): 3.348e-03, MSE(pi1): 5.121e-02, MSE(pi2): 2.655e-03, MSE(pi3): 2.788e-03\n",
      "Epoch 29600, Train loss: 3.432e+04, Test loss: 1.312e+08, MSE(e): 3.348e-03, MSE(pi1): 5.147e-02, MSE(pi2): 2.647e-03, MSE(pi3): 3.184e-03\n",
      "Epoch 29700, Train loss: 3.523e+04, Test loss: 1.312e+08, MSE(e): 3.337e-03, MSE(pi1): 1.520e-01, MSE(pi2): 2.625e-03, MSE(pi3): 3.352e-03\n",
      "Epoch 29800, Train loss: 3.873e+04, Test loss: 1.311e+08, MSE(e): 3.814e-03, MSE(pi1): 2.762e-02, MSE(pi2): 2.833e-03, MSE(pi3): 3.153e-03\n",
      "Epoch 29900, Train loss: 3.431e+04, Test loss: 1.312e+08, MSE(e): 3.352e-03, MSE(pi1): 4.250e-02, MSE(pi2): 2.644e-03, MSE(pi3): 3.577e-03\n",
      "\n",
      "Training process finished after 30000 epochs\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Parametros de entrenamiento\n",
    "start_epoch = 18000\n",
    "n_epochs = 30000\n",
    "\n",
    "batch_size = 64\n",
    "n_checkpoints = 100\n",
    "\n",
    "second_lr = None\n",
    "\n",
    "train_loop(model, optimizer, X_train, y_train, f_train, X_test, y_test, f_test,\n",
    "           D,  n_checkpoints, start_epoch=start_epoch, n_epochs=n_epochs, batch_size=batch_size, \n",
    "           model_results_path=MODEL_RESULTS_PATH, device=DEVICE, new_lr=second_lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training from a checkpoint. Epoch 29700.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29700, Train loss: 3.493e+04, Test loss: 1.311e+08, MSE(e): 3.336e-03, MSE(pi1): 1.238e-01, MSE(pi2): 2.632e-03, MSE(pi3): 3.229e-03\n",
      "Epoch 29800, Train loss: 4.117e+04, Test loss: 1.311e+08, MSE(e): 4.064e-03, MSE(pi1): 2.317e-02, MSE(pi2): 3.055e-03, MSE(pi3): 2.893e-03\n",
      "Epoch 29900, Train loss: 3.430e+04, Test loss: 1.311e+08, MSE(e): 3.363e-03, MSE(pi1): 3.859e-02, MSE(pi2): 2.625e-03, MSE(pi3): 2.854e-03\n",
      "Epoch 30000, Train loss: 4.779e+04, Test loss: 1.310e+08, MSE(e): 4.723e-03, MSE(pi1): 2.588e-02, MSE(pi2): 3.225e-03, MSE(pi3): 3.002e-03\n",
      "Epoch 30100, Train loss: 3.860e+04, Test loss: 1.311e+08, MSE(e): 3.757e-03, MSE(pi1): 7.406e-02, MSE(pi2): 2.853e-03, MSE(pi3): 2.846e-03\n",
      "Epoch 30200, Train loss: 5.925e+04, Test loss: 1.313e+08, MSE(e): 5.850e-03, MSE(pi1): 4.385e-02, MSE(pi2): 3.892e-03, MSE(pi3): 3.081e-03\n",
      "Epoch 30300, Train loss: 3.675e+04, Test loss: 1.311e+08, MSE(e): 3.613e-03, MSE(pi1): 3.221e-02, MSE(pi2): 2.709e-03, MSE(pi3): 2.945e-03\n",
      "Epoch 30400, Train loss: 3.282e+04, Test loss: 1.311e+08, MSE(e): 3.227e-03, MSE(pi1): 2.731e-02, MSE(pi2): 2.548e-03, MSE(pi3): 2.781e-03\n",
      "Epoch 30500, Train loss: 3.386e+04, Test loss: 1.310e+08, MSE(e): 3.333e-03, MSE(pi1): 2.347e-02, MSE(pi2): 2.574e-03, MSE(pi3): 2.859e-03\n",
      "Epoch 30600, Train loss: 9.479e+04, Test loss: 1.309e+08, MSE(e): 9.397e-03, MSE(pi1): 4.866e-02, MSE(pi2): 5.449e-03, MSE(pi3): 3.268e-03\n",
      "Epoch 30700, Train loss: 3.318e+04, Test loss: 1.309e+08, MSE(e): 3.231e-03, MSE(pi1): 6.244e-02, MSE(pi2): 2.537e-03, MSE(pi3): 2.350e-03\n",
      "Epoch 30800, Train loss: 3.472e+04, Test loss: 1.309e+08, MSE(e): 3.417e-03, MSE(pi1): 2.366e-02, MSE(pi2): 2.600e-03, MSE(pi3): 3.083e-03\n",
      "Epoch 30900, Train loss: 3.218e+04, Test loss: 1.309e+08, MSE(e): 3.162e-03, MSE(pi1): 2.641e-02, MSE(pi2): 2.496e-03, MSE(pi3): 2.947e-03\n",
      "Epoch 31000, Train loss: 3.279e+04, Test loss: 1.308e+08, MSE(e): 3.166e-03, MSE(pi1): 8.740e-02, MSE(pi2): 2.488e-03, MSE(pi3): 2.543e-03\n",
      "Epoch 31100, Train loss: 1.026e+05, Test loss: 1.310e+08, MSE(e): 1.012e-02, MSE(pi1): 1.014e-01, MSE(pi2): 6.026e-03, MSE(pi3): 3.571e-03\n",
      "Epoch 31200, Train loss: 3.439e+04, Test loss: 1.306e+08, MSE(e): 3.368e-03, MSE(pi1): 4.766e-02, MSE(pi2): 2.662e-03, MSE(pi3): 2.283e-03\n",
      "Epoch 31300, Train loss: 3.221e+04, Test loss: 1.307e+08, MSE(e): 3.130e-03, MSE(pi1): 6.542e-02, MSE(pi2): 2.456e-03, MSE(pi3): 2.627e-03\n",
      "Epoch 31400, Train loss: 3.547e+04, Test loss: 1.308e+08, MSE(e): 3.494e-03, MSE(pi1): 2.793e-02, MSE(pi2): 2.655e-03, MSE(pi3): 2.521e-03\n",
      "Epoch 31500, Train loss: 3.330e+04, Test loss: 1.307e+08, MSE(e): 3.247e-03, MSE(pi1): 4.875e-02, MSE(pi2): 2.489e-03, MSE(pi3): 3.402e-03\n",
      "Epoch 31600, Train loss: 3.347e+04, Test loss: 1.306e+08, MSE(e): 3.286e-03, MSE(pi1): 3.220e-02, MSE(pi2): 2.566e-03, MSE(pi3): 2.809e-03\n",
      "Epoch 31700, Train loss: 3.197e+04, Test loss: 1.305e+08, MSE(e): 3.148e-03, MSE(pi1): 2.323e-02, MSE(pi2): 2.453e-03, MSE(pi3): 2.603e-03\n",
      "Epoch 31800, Train loss: 3.267e+04, Test loss: 1.306e+08, MSE(e): 3.200e-03, MSE(pi1): 4.061e-02, MSE(pi2): 2.492e-03, MSE(pi3): 2.591e-03\n",
      "Epoch 31900, Train loss: 3.429e+04, Test loss: 1.306e+08, MSE(e): 3.352e-03, MSE(pi1): 4.563e-02, MSE(pi2): 2.572e-03, MSE(pi3): 3.179e-03\n",
      "Epoch 32000, Train loss: 4.755e+04, Test loss: 1.305e+08, MSE(e): 4.686e-03, MSE(pi1): 3.960e-02, MSE(pi2): 3.148e-03, MSE(pi3): 2.882e-03\n",
      "Epoch 32100, Train loss: 3.122e+04, Test loss: 1.305e+08, MSE(e): 3.040e-03, MSE(pi1): 5.428e-02, MSE(pi2): 2.395e-03, MSE(pi3): 2.748e-03\n",
      "Epoch 32200, Train loss: 3.081e+04, Test loss: 1.305e+08, MSE(e): 3.028e-03, MSE(pi1): 2.677e-02, MSE(pi2): 2.379e-03, MSE(pi3): 2.572e-03\n",
      "Epoch 32300, Train loss: 3.115e+04, Test loss: 1.304e+08, MSE(e): 3.052e-03, MSE(pi1): 3.335e-02, MSE(pi2): 2.383e-03, MSE(pi3): 2.862e-03\n",
      "Epoch 32400, Train loss: 3.319e+04, Test loss: 1.304e+08, MSE(e): 3.275e-03, MSE(pi1): 2.119e-02, MSE(pi2): 2.535e-03, MSE(pi3): 2.261e-03\n",
      "Epoch 32500, Train loss: 3.038e+04, Test loss: 1.304e+08, MSE(e): 2.996e-03, MSE(pi1): 1.727e-02, MSE(pi2): 2.357e-03, MSE(pi3): 2.505e-03\n",
      "Epoch 32600, Train loss: 3.033e+04, Test loss: 1.303e+08, MSE(e): 2.989e-03, MSE(pi1): 2.080e-02, MSE(pi2): 2.350e-03, MSE(pi3): 2.293e-03\n",
      "Epoch 32700, Train loss: 3.153e+04, Test loss: 1.303e+08, MSE(e): 3.105e-03, MSE(pi1): 2.411e-02, MSE(pi2): 2.390e-03, MSE(pi3): 2.409e-03\n",
      "Epoch 32800, Train loss: 3.861e+04, Test loss: 1.303e+08, MSE(e): 3.813e-03, MSE(pi1): 2.011e-02, MSE(pi2): 2.786e-03, MSE(pi3): 2.772e-03\n",
      "Epoch 32900, Train loss: 4.168e+04, Test loss: 1.304e+08, MSE(e): 4.109e-03, MSE(pi1): 2.865e-02, MSE(pi2): 2.927e-03, MSE(pi3): 2.981e-03\n",
      "Epoch 33000, Train loss: 2.999e+04, Test loss: 1.302e+08, MSE(e): 2.958e-03, MSE(pi1): 1.559e-02, MSE(pi2): 2.331e-03, MSE(pi3): 2.433e-03\n",
      "Epoch 33100, Train loss: 3.069e+04, Test loss: 1.302e+08, MSE(e): 3.023e-03, MSE(pi1): 2.248e-02, MSE(pi2): 2.368e-03, MSE(pi3): 2.383e-03\n",
      "Epoch 33200, Train loss: 3.547e+04, Test loss: 1.301e+08, MSE(e): 3.490e-03, MSE(pi1): 3.216e-02, MSE(pi2): 2.627e-03, MSE(pi3): 2.492e-03\n",
      "Epoch 33300, Train loss: 6.343e+04, Test loss: 1.300e+08, MSE(e): 6.285e-03, MSE(pi1): 2.791e-02, MSE(pi2): 3.842e-03, MSE(pi3): 2.960e-03\n",
      "Epoch 33400, Train loss: 3.036e+04, Test loss: 1.300e+08, MSE(e): 2.942e-03, MSE(pi1): 6.872e-02, MSE(pi2): 2.303e-03, MSE(pi3): 2.486e-03\n",
      "Epoch 33500, Train loss: 3.128e+04, Test loss: 1.299e+08, MSE(e): 2.977e-03, MSE(pi1): 1.203e-01, MSE(pi2): 2.332e-03, MSE(pi3): 3.079e-03\n",
      "Epoch 33600, Train loss: 2.988e+04, Test loss: 1.300e+08, MSE(e): 2.928e-03, MSE(pi1): 3.097e-02, MSE(pi2): 2.301e-03, MSE(pi3): 2.857e-03\n",
      "Epoch 33700, Train loss: 2.957e+04, Test loss: 1.299e+08, MSE(e): 2.911e-03, MSE(pi1): 2.154e-02, MSE(pi2): 2.283e-03, MSE(pi3): 2.422e-03\n",
      "Epoch 33800, Train loss: 3.019e+04, Test loss: 1.299e+08, MSE(e): 2.916e-03, MSE(pi1): 7.615e-02, MSE(pi2): 2.282e-03, MSE(pi3): 2.625e-03\n",
      "Epoch 33900, Train loss: 3.802e+04, Test loss: 1.298e+08, MSE(e): 3.750e-03, MSE(pi1): 2.522e-02, MSE(pi2): 2.637e-03, MSE(pi3): 2.639e-03\n",
      "Epoch 34000, Train loss: 3.037e+04, Test loss: 1.297e+08, MSE(e): 2.967e-03, MSE(pi1): 4.747e-02, MSE(pi2): 2.316e-03, MSE(pi3): 2.242e-03\n",
      "Epoch 34100, Train loss: 2.927e+04, Test loss: 1.298e+08, MSE(e): 2.880e-03, MSE(pi1): 2.315e-02, MSE(pi2): 2.263e-03, MSE(pi3): 2.362e-03\n",
      "Epoch 34200, Train loss: 3.038e+04, Test loss: 1.297e+08, MSE(e): 2.982e-03, MSE(pi1): 3.070e-02, MSE(pi2): 2.294e-03, MSE(pi3): 2.454e-03\n",
      "Epoch 34300, Train loss: 3.014e+04, Test loss: 1.298e+08, MSE(e): 2.965e-03, MSE(pi1): 2.389e-02, MSE(pi2): 2.311e-03, MSE(pi3): 2.450e-03\n",
      "Epoch 34400, Train loss: 2.940e+04, Test loss: 1.298e+08, MSE(e): 2.890e-03, MSE(pi1): 2.506e-02, MSE(pi2): 2.263e-03, MSE(pi3): 2.542e-03\n",
      "Epoch 34500, Train loss: 3.032e+04, Test loss: 1.295e+08, MSE(e): 2.986e-03, MSE(pi1): 2.532e-02, MSE(pi2): 2.336e-03, MSE(pi3): 2.113e-03\n",
      "Epoch 34600, Train loss: 1.118e+05, Test loss: 1.297e+08, MSE(e): 1.106e-02, MSE(pi1): 6.054e-02, MSE(pi2): 6.216e-03, MSE(pi3): 5.245e-03\n",
      "Epoch 34700, Train loss: 2.900e+04, Test loss: 1.297e+08, MSE(e): 2.843e-03, MSE(pi1): 3.455e-02, MSE(pi2): 2.232e-03, MSE(pi3): 2.236e-03\n",
      "Epoch 34800, Train loss: 2.890e+04, Test loss: 1.296e+08, MSE(e): 2.841e-03, MSE(pi1): 2.563e-02, MSE(pi2): 2.226e-03, MSE(pi3): 2.288e-03\n",
      "Epoch 34900, Train loss: 2.919e+04, Test loss: 1.296e+08, MSE(e): 2.859e-03, MSE(pi1): 3.580e-02, MSE(pi2): 2.235e-03, MSE(pi3): 2.311e-03\n",
      "Epoch 35000, Train loss: 3.464e+04, Test loss: 1.301e+08, MSE(e): 3.358e-03, MSE(pi1): 5.601e-02, MSE(pi2): 2.582e-03, MSE(pi3): 4.970e-03\n",
      "Epoch 35100, Train loss: 7.276e+05, Test loss: 1.287e+08, MSE(e): 5.994e-02, MSE(pi1): 1.134e+01, MSE(pi2): 2.672e-02, MSE(pi3): 1.478e-01\n",
      "Epoch 35200, Train loss: 3.325e+04, Test loss: 1.292e+08, MSE(e): 2.844e-03, MSE(pi1): 2.105e-01, MSE(pi2): 2.226e-03, MSE(pi3): 2.698e-02\n",
      "Epoch 35300, Train loss: 3.196e+04, Test loss: 1.292e+08, MSE(e): 2.821e-03, MSE(pi1): 1.548e-01, MSE(pi2): 2.217e-03, MSE(pi3): 2.198e-02\n",
      "Epoch 35400, Train loss: 3.115e+04, Test loss: 1.292e+08, MSE(e): 2.812e-03, MSE(pi1): 1.200e-01, MSE(pi2): 2.212e-03, MSE(pi3): 1.825e-02\n",
      "Epoch 35500, Train loss: 3.057e+04, Test loss: 1.292e+08, MSE(e): 2.806e-03, MSE(pi1): 9.623e-02, MSE(pi2): 2.207e-03, MSE(pi3): 1.537e-02\n",
      "Epoch 35600, Train loss: 3.012e+04, Test loss: 1.292e+08, MSE(e): 2.801e-03, MSE(pi1): 7.957e-02, MSE(pi2): 2.202e-03, MSE(pi3): 1.312e-02\n",
      "Epoch 35700, Train loss: 2.977e+04, Test loss: 1.292e+08, MSE(e): 2.796e-03, MSE(pi1): 6.750e-02, MSE(pi2): 2.198e-03, MSE(pi3): 1.134e-02\n",
      "Epoch 35800, Train loss: 2.949e+04, Test loss: 1.292e+08, MSE(e): 2.791e-03, MSE(pi1): 5.844e-02, MSE(pi2): 2.194e-03, MSE(pi3): 9.911e-03\n",
      "Epoch 35900, Train loss: 2.926e+04, Test loss: 1.292e+08, MSE(e): 2.786e-03, MSE(pi1): 5.144e-02, MSE(pi2): 2.190e-03, MSE(pi3): 8.764e-03\n",
      "Epoch 36000, Train loss: 2.906e+04, Test loss: 1.292e+08, MSE(e): 2.781e-03, MSE(pi1): 4.589e-02, MSE(pi2): 2.186e-03, MSE(pi3): 7.840e-03\n",
      "Epoch 36100, Train loss: 2.888e+04, Test loss: 1.292e+08, MSE(e): 2.776e-03, MSE(pi1): 4.142e-02, MSE(pi2): 2.182e-03, MSE(pi3): 7.093e-03\n",
      "Epoch 36200, Train loss: 2.895e+04, Test loss: 1.291e+08, MSE(e): 2.792e-03, MSE(pi1): 3.767e-02, MSE(pi2): 2.193e-03, MSE(pi3): 6.477e-03\n",
      "Epoch 36300, Train loss: 2.862e+04, Test loss: 1.291e+08, MSE(e): 2.767e-03, MSE(pi1): 3.462e-02, MSE(pi2): 2.175e-03, MSE(pi3): 5.980e-03\n",
      "Epoch 36400, Train loss: 2.869e+04, Test loss: 1.290e+08, MSE(e): 2.781e-03, MSE(pi1): 3.182e-02, MSE(pi2): 2.169e-03, MSE(pi3): 5.574e-03\n",
      "Epoch 36500, Train loss: 2.840e+04, Test loss: 1.290e+08, MSE(e): 2.758e-03, MSE(pi1): 2.958e-02, MSE(pi2): 2.168e-03, MSE(pi3): 5.189e-03\n",
      "Epoch 36600, Train loss: 3.811e+04, Test loss: 1.290e+08, MSE(e): 3.734e-03, MSE(pi1): 2.728e-02, MSE(pi2): 2.673e-03, MSE(pi3): 4.966e-03\n",
      "Epoch 36700, Train loss: 2.821e+04, Test loss: 1.289e+08, MSE(e): 2.750e-03, MSE(pi1): 2.558e-02, MSE(pi2): 2.161e-03, MSE(pi3): 4.579e-03\n",
      "Epoch 36800, Train loss: 1.911e+05, Test loss: 1.291e+08, MSE(e): 1.902e-02, MSE(pi1): 3.193e-02, MSE(pi2): 1.016e-02, MSE(pi3): 5.211e-03\n",
      "Epoch 36900, Train loss: 2.805e+04, Test loss: 1.289e+08, MSE(e): 2.742e-03, MSE(pi1): 2.233e-02, MSE(pi2): 2.154e-03, MSE(pi3): 4.079e-03\n",
      "Epoch 37000, Train loss: 2.799e+04, Test loss: 1.289e+08, MSE(e): 2.739e-03, MSE(pi1): 2.094e-02, MSE(pi2): 2.149e-03, MSE(pi3): 3.860e-03\n",
      "Epoch 37100, Train loss: 2.790e+04, Test loss: 1.288e+08, MSE(e): 2.733e-03, MSE(pi1): 1.961e-02, MSE(pi2): 2.148e-03, MSE(pi3): 3.661e-03\n",
      "Epoch 37200, Train loss: 3.043e+04, Test loss: 1.291e+08, MSE(e): 2.983e-03, MSE(pi1): 2.285e-02, MSE(pi2): 2.304e-03, MSE(pi3): 3.678e-03\n",
      "Epoch 37300, Train loss: 2.776e+04, Test loss: 1.288e+08, MSE(e): 2.725e-03, MSE(pi1): 1.756e-02, MSE(pi2): 2.141e-03, MSE(pi3): 3.324e-03\n",
      "Epoch 37400, Train loss: 2.861e+04, Test loss: 1.287e+08, MSE(e): 2.811e-03, MSE(pi1): 1.687e-02, MSE(pi2): 2.163e-03, MSE(pi3): 3.224e-03\n",
      "Epoch 37500, Train loss: 2.828e+04, Test loss: 1.286e+08, MSE(e): 2.781e-03, MSE(pi1): 1.731e-02, MSE(pi2): 2.189e-03, MSE(pi3): 3.005e-03\n",
      "Epoch 37600, Train loss: 2.768e+04, Test loss: 1.287e+08, MSE(e): 2.721e-03, MSE(pi1): 1.688e-02, MSE(pi2): 2.142e-03, MSE(pi3): 2.939e-03\n",
      "Epoch 37700, Train loss: 2.754e+04, Test loss: 1.286e+08, MSE(e): 2.710e-03, MSE(pi1): 1.541e-02, MSE(pi2): 2.130e-03, MSE(pi3): 2.846e-03\n",
      "Epoch 37800, Train loss: 3.278e+04, Test loss: 1.287e+08, MSE(e): 3.230e-03, MSE(pi1): 1.893e-02, MSE(pi2): 2.400e-03, MSE(pi3): 2.900e-03\n",
      "Epoch 37900, Train loss: 2.742e+04, Test loss: 1.286e+08, MSE(e): 2.701e-03, MSE(pi1): 1.401e-02, MSE(pi2): 2.123e-03, MSE(pi3): 2.712e-03\n",
      "Epoch 38000, Train loss: 2.747e+04, Test loss: 1.286e+08, MSE(e): 2.707e-03, MSE(pi1): 1.373e-02, MSE(pi2): 2.128e-03, MSE(pi3): 2.665e-03\n",
      "Epoch 38100, Train loss: 2.811e+04, Test loss: 1.285e+08, MSE(e): 2.767e-03, MSE(pi1): 1.786e-02, MSE(pi2): 2.159e-03, MSE(pi3): 2.592e-03\n",
      "Epoch 38200, Train loss: 2.781e+04, Test loss: 1.286e+08, MSE(e): 2.711e-03, MSE(pi1): 4.135e-02, MSE(pi2): 2.124e-03, MSE(pi3): 2.790e-03\n",
      "Epoch 38300, Train loss: 2.726e+04, Test loss: 1.285e+08, MSE(e): 2.687e-03, MSE(pi1): 1.410e-02, MSE(pi2): 2.111e-03, MSE(pi3): 2.542e-03\n",
      "Epoch 38400, Train loss: 2.755e+04, Test loss: 1.284e+08, MSE(e): 2.715e-03, MSE(pi1): 1.504e-02, MSE(pi2): 2.131e-03, MSE(pi3): 2.507e-03\n",
      "Epoch 38500, Train loss: 3.003e+04, Test loss: 1.284e+08, MSE(e): 2.964e-03, MSE(pi1): 1.377e-02, MSE(pi2): 2.271e-03, MSE(pi3): 2.524e-03\n",
      "Epoch 38600, Train loss: 3.041e+04, Test loss: 1.282e+08, MSE(e): 2.936e-03, MSE(pi1): 7.824e-02, MSE(pi2): 2.280e-03, MSE(pi3): 2.718e-03\n",
      "Epoch 38700, Train loss: 2.756e+04, Test loss: 1.283e+08, MSE(e): 2.716e-03, MSE(pi1): 1.383e-02, MSE(pi2): 2.108e-03, MSE(pi3): 2.540e-03\n",
      "Epoch 38800, Train loss: 2.816e+04, Test loss: 1.284e+08, MSE(e): 2.744e-03, MSE(pi1): 4.323e-02, MSE(pi2): 2.116e-03, MSE(pi3): 2.820e-03\n",
      "Epoch 38900, Train loss: 2.810e+04, Test loss: 1.284e+08, MSE(e): 2.769e-03, MSE(pi1): 1.513e-02, MSE(pi2): 2.157e-03, MSE(pi3): 2.517e-03\n",
      "Epoch 39000, Train loss: 2.705e+04, Test loss: 1.283e+08, MSE(e): 2.664e-03, MSE(pi1): 1.531e-02, MSE(pi2): 2.088e-03, MSE(pi3): 2.496e-03\n",
      "Epoch 39100, Train loss: 2.742e+04, Test loss: 1.282e+08, MSE(e): 2.688e-03, MSE(pi1): 2.911e-02, MSE(pi2): 2.106e-03, MSE(pi3): 2.488e-03\n",
      "Epoch 39200, Train loss: 4.022e+04, Test loss: 1.281e+08, MSE(e): 3.966e-03, MSE(pi1): 3.039e-02, MSE(pi2): 2.656e-03, MSE(pi3): 2.513e-03\n",
      "Epoch 39300, Train loss: 8.208e+04, Test loss: 1.282e+08, MSE(e): 8.118e-03, MSE(pi1): 5.993e-02, MSE(pi2): 4.830e-03, MSE(pi3): 2.904e-03\n",
      "Epoch 39400, Train loss: 3.610e+04, Test loss: 1.281e+08, MSE(e): 3.556e-03, MSE(pi1): 2.899e-02, MSE(pi2): 2.460e-03, MSE(pi3): 2.525e-03\n",
      "Epoch 39500, Train loss: 2.688e+04, Test loss: 1.282e+08, MSE(e): 2.647e-03, MSE(pi1): 1.576e-02, MSE(pi2): 2.077e-03, MSE(pi3): 2.548e-03\n",
      "Epoch 39600, Train loss: 2.704e+04, Test loss: 1.281e+08, MSE(e): 2.654e-03, MSE(pi1): 2.563e-02, MSE(pi2): 2.073e-03, MSE(pi3): 2.416e-03\n",
      "Epoch 39700, Train loss: 2.809e+04, Test loss: 1.281e+08, MSE(e): 2.766e-03, MSE(pi1): 1.761e-02, MSE(pi2): 2.149e-03, MSE(pi3): 2.558e-03\n",
      "Epoch 39800, Train loss: 7.264e+04, Test loss: 1.280e+08, MSE(e): 7.186e-03, MSE(pi1): 4.748e-02, MSE(pi2): 4.153e-03, MSE(pi3): 3.005e-03\n",
      "Epoch 39900, Train loss: 4.085e+04, Test loss: 1.281e+08, MSE(e): 4.014e-03, MSE(pi1): 4.098e-02, MSE(pi2): 2.792e-03, MSE(pi3): 2.996e-03\n",
      "Epoch 40000, Train loss: 2.749e+04, Test loss: 1.280e+08, MSE(e): 2.652e-03, MSE(pi1): 7.026e-02, MSE(pi2): 2.064e-03, MSE(pi3): 2.685e-03\n",
      "Epoch 40100, Train loss: 2.675e+04, Test loss: 1.280e+08, MSE(e): 2.636e-03, MSE(pi1): 1.447e-02, MSE(pi2): 2.063e-03, MSE(pi3): 2.423e-03\n",
      "Epoch 40200, Train loss: 2.679e+04, Test loss: 1.279e+08, MSE(e): 2.637e-03, MSE(pi1): 1.789e-02, MSE(pi2): 2.059e-03, MSE(pi3): 2.359e-03\n",
      "Epoch 40300, Train loss: 2.687e+04, Test loss: 1.279e+08, MSE(e): 2.649e-03, MSE(pi1): 1.379e-02, MSE(pi2): 2.074e-03, MSE(pi3): 2.412e-03\n",
      "Epoch 40400, Train loss: 2.857e+04, Test loss: 1.279e+08, MSE(e): 2.806e-03, MSE(pi1): 2.588e-02, MSE(pi2): 2.158e-03, MSE(pi3): 2.489e-03\n",
      "Epoch 40500, Train loss: 2.816e+04, Test loss: 1.279e+08, MSE(e): 2.746e-03, MSE(pi1): 4.406e-02, MSE(pi2): 2.126e-03, MSE(pi3): 2.550e-03\n",
      "Epoch 40600, Train loss: 2.670e+04, Test loss: 1.279e+08, MSE(e): 2.617e-03, MSE(pi1): 2.931e-02, MSE(pi2): 2.050e-03, MSE(pi3): 2.338e-03\n",
      "Epoch 40700, Train loss: 2.709e+04, Test loss: 1.278e+08, MSE(e): 2.626e-03, MSE(pi1): 5.893e-02, MSE(pi2): 2.050e-03, MSE(pi3): 2.390e-03\n",
      "Epoch 40800, Train loss: 3.846e+04, Test loss: 1.278e+08, MSE(e): 3.769e-03, MSE(pi1): 4.823e-02, MSE(pi2): 2.645e-03, MSE(pi3): 2.884e-03\n",
      "Epoch 40900, Train loss: 2.701e+04, Test loss: 1.278e+08, MSE(e): 2.645e-03, MSE(pi1): 2.798e-02, MSE(pi2): 2.054e-03, MSE(pi3): 2.806e-03\n",
      "Epoch 41000, Train loss: 2.648e+04, Test loss: 1.277e+08, MSE(e): 2.604e-03, MSE(pi1): 2.011e-02, MSE(pi2): 2.042e-03, MSE(pi3): 2.420e-03\n",
      "Epoch 41100, Train loss: 2.683e+04, Test loss: 1.277e+08, MSE(e): 2.644e-03, MSE(pi1): 1.569e-02, MSE(pi2): 2.047e-03, MSE(pi3): 2.365e-03\n",
      "Epoch 41200, Train loss: 5.680e+04, Test loss: 1.277e+08, MSE(e): 5.488e-03, MSE(pi1): 1.626e-01, MSE(pi2): 3.355e-03, MSE(pi3): 2.849e-03\n",
      "Epoch 41300, Train loss: 2.672e+04, Test loss: 1.277e+08, MSE(e): 2.598e-03, MSE(pi1): 4.555e-02, MSE(pi2): 2.032e-03, MSE(pi3): 2.833e-03\n",
      "Epoch 41400, Train loss: 2.696e+04, Test loss: 1.277e+08, MSE(e): 2.611e-03, MSE(pi1): 5.628e-02, MSE(pi2): 2.034e-03, MSE(pi3): 2.790e-03\n",
      "Epoch 41500, Train loss: 3.086e+04, Test loss: 1.276e+08, MSE(e): 3.021e-03, MSE(pi1): 4.344e-02, MSE(pi2): 2.205e-03, MSE(pi3): 2.201e-03\n",
      "Epoch 41600, Train loss: 3.204e+04, Test loss: 1.277e+08, MSE(e): 3.131e-03, MSE(pi1): 4.480e-02, MSE(pi2): 2.311e-03, MSE(pi3): 2.779e-03\n",
      "Epoch 41700, Train loss: 2.654e+04, Test loss: 1.275e+08, MSE(e): 2.590e-03, MSE(pi1): 3.951e-02, MSE(pi2): 2.025e-03, MSE(pi3): 2.409e-03\n",
      "Epoch 41800, Train loss: 3.647e+04, Test loss: 1.274e+08, MSE(e): 3.586e-03, MSE(pi1): 3.844e-02, MSE(pi2): 2.602e-03, MSE(pi3): 2.304e-03\n",
      "Epoch 41900, Train loss: 2.803e+04, Test loss: 1.275e+08, MSE(e): 2.752e-03, MSE(pi1): 2.450e-02, MSE(pi2): 2.126e-03, MSE(pi3): 2.552e-03\n",
      "Epoch 42000, Train loss: 2.633e+04, Test loss: 1.275e+08, MSE(e): 2.578e-03, MSE(pi1): 2.849e-02, MSE(pi2): 2.019e-03, MSE(pi3): 2.570e-03\n",
      "Epoch 42100, Train loss: 2.664e+04, Test loss: 1.274e+08, MSE(e): 2.622e-03, MSE(pi1): 1.757e-02, MSE(pi2): 2.050e-03, MSE(pi3): 2.361e-03\n",
      "Epoch 42200, Train loss: 6.714e+04, Test loss: 1.274e+08, MSE(e): 6.632e-03, MSE(pi1): 4.927e-02, MSE(pi2): 4.080e-03, MSE(pi3): 3.163e-03\n",
      "Epoch 42300, Train loss: 3.760e+04, Test loss: 1.273e+08, MSE(e): 3.701e-03, MSE(pi1): 3.714e-02, MSE(pi2): 2.497e-03, MSE(pi3): 2.124e-03\n",
      "Epoch 42400, Train loss: 1.331e+05, Test loss: 1.277e+08, MSE(e): 5.059e-03, MSE(pi1): 7.934e+00, MSE(pi2): 2.565e-03, MSE(pi3): 3.144e-02\n",
      "Epoch 42500, Train loss: 2.757e+04, Test loss: 1.273e+08, MSE(e): 2.631e-03, MSE(pi1): 3.517e-02, MSE(pi2): 2.053e-03, MSE(pi3): 9.053e-03\n",
      "Epoch 42600, Train loss: 2.857e+04, Test loss: 1.273e+08, MSE(e): 2.774e-03, MSE(pi1): 2.020e-02, MSE(pi2): 2.132e-03, MSE(pi3): 6.255e-03\n",
      "Epoch 42700, Train loss: 3.146e+04, Test loss: 1.273e+08, MSE(e): 3.079e-03, MSE(pi1): 1.715e-02, MSE(pi2): 2.211e-03, MSE(pi3): 4.911e-03\n",
      "Epoch 42800, Train loss: 3.211e+04, Test loss: 1.273e+08, MSE(e): 3.158e-03, MSE(pi1): 1.410e-02, MSE(pi2): 2.255e-03, MSE(pi3): 3.886e-03\n",
      "Epoch 42900, Train loss: 2.624e+04, Test loss: 1.272e+08, MSE(e): 2.580e-03, MSE(pi1): 1.218e-02, MSE(pi2): 2.004e-03, MSE(pi3): 3.178e-03\n",
      "Epoch 43000, Train loss: 2.589e+04, Test loss: 1.272e+08, MSE(e): 2.549e-03, MSE(pi1): 1.214e-02, MSE(pi2): 1.997e-03, MSE(pi3): 2.835e-03\n",
      "Epoch 43100, Train loss: 2.601e+04, Test loss: 1.271e+08, MSE(e): 2.561e-03, MSE(pi1): 1.188e-02, MSE(pi2): 1.994e-03, MSE(pi3): 2.781e-03\n",
      "Epoch 43200, Train loss: 2.778e+04, Test loss: 1.271e+08, MSE(e): 2.727e-03, MSE(pi1): 2.414e-02, MSE(pi2): 2.056e-03, MSE(pi3): 2.638e-03\n",
      "Epoch 43300, Train loss: 2.661e+04, Test loss: 1.271e+08, MSE(e): 2.620e-03, MSE(pi1): 1.244e-02, MSE(pi2): 2.041e-03, MSE(pi3): 2.870e-03\n",
      "Epoch 43400, Train loss: 3.434e+04, Test loss: 1.271e+08, MSE(e): 3.340e-03, MSE(pi1): 5.869e-02, MSE(pi2): 2.310e-03, MSE(pi3): 3.452e-03\n",
      "Epoch 43500, Train loss: 2.885e+04, Test loss: 1.268e+08, MSE(e): 2.785e-03, MSE(pi1): 7.696e-02, MSE(pi2): 2.159e-03, MSE(pi3): 2.267e-03\n",
      "Epoch 43600, Train loss: 2.694e+04, Test loss: 1.270e+08, MSE(e): 2.646e-03, MSE(pi1): 2.229e-02, MSE(pi2): 2.055e-03, MSE(pi3): 2.620e-03\n",
      "Epoch 43700, Train loss: 2.627e+04, Test loss: 1.269e+08, MSE(e): 2.583e-03, MSE(pi1): 1.966e-02, MSE(pi2): 2.013e-03, MSE(pi3): 2.422e-03\n",
      "Epoch 43800, Train loss: 2.593e+04, Test loss: 1.270e+08, MSE(e): 2.538e-03, MSE(pi1): 2.695e-02, MSE(pi2): 1.981e-03, MSE(pi3): 2.818e-03\n",
      "Epoch 43900, Train loss: 2.592e+04, Test loss: 1.270e+08, MSE(e): 2.552e-03, MSE(pi1): 1.409e-02, MSE(pi2): 1.999e-03, MSE(pi3): 2.663e-03\n",
      "Epoch 44000, Train loss: 2.840e+04, Test loss: 1.269e+08, MSE(e): 2.721e-03, MSE(pi1): 8.701e-02, MSE(pi2): 2.029e-03, MSE(pi3): 3.198e-03\n",
      "Epoch 44100, Train loss: 2.615e+04, Test loss: 1.269e+08, MSE(e): 2.538e-03, MSE(pi1): 5.013e-02, MSE(pi2): 1.978e-03, MSE(pi3): 2.676e-03\n",
      "Epoch 44200, Train loss: 3.242e+04, Test loss: 1.269e+08, MSE(e): 3.199e-03, MSE(pi1): 1.563e-02, MSE(pi2): 2.254e-03, MSE(pi3): 2.654e-03\n",
      "Epoch 44300, Train loss: 2.582e+04, Test loss: 1.269e+08, MSE(e): 2.536e-03, MSE(pi1): 2.059e-02, MSE(pi2): 1.983e-03, MSE(pi3): 2.551e-03\n",
      "Epoch 44400, Train loss: 2.560e+04, Test loss: 1.269e+08, MSE(e): 2.523e-03, MSE(pi1): 1.152e-02, MSE(pi2): 1.977e-03, MSE(pi3): 2.561e-03\n",
      "Epoch 44500, Train loss: 2.622e+04, Test loss: 1.268e+08, MSE(e): 2.534e-03, MSE(pi1): 6.616e-02, MSE(pi2): 1.971e-03, MSE(pi3): 2.197e-03\n",
      "Epoch 44600, Train loss: 2.624e+04, Test loss: 1.267e+08, MSE(e): 2.571e-03, MSE(pi1): 2.786e-02, MSE(pi2): 1.983e-03, MSE(pi3): 2.435e-03\n",
      "Epoch 44700, Train loss: 2.552e+04, Test loss: 1.268e+08, MSE(e): 2.513e-03, MSE(pi1): 1.330e-02, MSE(pi2): 1.965e-03, MSE(pi3): 2.466e-03\n",
      "Epoch 44800, Train loss: 2.569e+04, Test loss: 1.267e+08, MSE(e): 2.519e-03, MSE(pi1): 2.594e-02, MSE(pi2): 1.964e-03, MSE(pi3): 2.333e-03\n",
      "Epoch 44900, Train loss: 4.883e+04, Test loss: 1.267e+08, MSE(e): 4.823e-03, MSE(pi1): 3.381e-02, MSE(pi2): 2.978e-03, MSE(pi3): 2.642e-03\n",
      "Epoch 45000, Train loss: 2.564e+04, Test loss: 1.268e+08, MSE(e): 2.522e-03, MSE(pi1): 1.684e-02, MSE(pi2): 1.969e-03, MSE(pi3): 2.466e-03\n",
      "Epoch 45100, Train loss: 2.597e+04, Test loss: 1.267e+08, MSE(e): 2.520e-03, MSE(pi1): 5.363e-02, MSE(pi2): 1.959e-03, MSE(pi3): 2.296e-03\n",
      "Epoch 45200, Train loss: 2.670e+04, Test loss: 1.266e+08, MSE(e): 2.561e-03, MSE(pi1): 7.394e-02, MSE(pi2): 1.962e-03, MSE(pi3): 3.444e-03\n",
      "Epoch 45300, Train loss: 7.946e+04, Test loss: 1.267e+08, MSE(e): 7.886e-03, MSE(pi1): 3.044e-02, MSE(pi2): 4.442e-03, MSE(pi3): 2.913e-03\n",
      "Epoch 45400, Train loss: 2.658e+04, Test loss: 1.267e+08, MSE(e): 2.538e-03, MSE(pi1): 9.210e-02, MSE(pi2): 1.964e-03, MSE(pi3): 2.709e-03\n",
      "Epoch 45500, Train loss: 2.565e+04, Test loss: 1.265e+08, MSE(e): 2.526e-03, MSE(pi1): 1.661e-02, MSE(pi2): 1.980e-03, MSE(pi3): 2.220e-03\n",
      "Epoch 45600, Train loss: 1.250e+05, Test loss: 1.263e+08, MSE(e): 1.241e-02, MSE(pi1): 5.404e-02, MSE(pi2): 7.033e-03, MSE(pi3): 3.784e-03\n",
      "Epoch 45700, Train loss: 2.545e+04, Test loss: 1.265e+08, MSE(e): 2.498e-03, MSE(pi1): 2.087e-02, MSE(pi2): 1.950e-03, MSE(pi3): 2.622e-03\n",
      "Epoch 45800, Train loss: 4.459e+04, Test loss: 1.265e+08, MSE(e): 4.389e-03, MSE(pi1): 4.026e-02, MSE(pi2): 2.925e-03, MSE(pi3): 2.971e-03\n",
      "Epoch 45900, Train loss: 2.575e+04, Test loss: 1.265e+08, MSE(e): 2.502e-03, MSE(pi1): 4.771e-02, MSE(pi2): 1.949e-03, MSE(pi3): 2.470e-03\n",
      "Epoch 46000, Train loss: 2.991e+04, Test loss: 1.265e+08, MSE(e): 2.915e-03, MSE(pi1): 5.265e-02, MSE(pi2): 2.109e-03, MSE(pi3): 2.390e-03\n",
      "Epoch 46100, Train loss: 2.725e+04, Test loss: 1.264e+08, MSE(e): 2.680e-03, MSE(pi1): 2.096e-02, MSE(pi2): 2.066e-03, MSE(pi3): 2.356e-03\n",
      "Epoch 46200, Train loss: 2.674e+04, Test loss: 1.265e+08, MSE(e): 2.611e-03, MSE(pi1): 3.784e-02, MSE(pi2): 1.981e-03, MSE(pi3): 2.420e-03\n",
      "Epoch 46300, Train loss: 2.589e+04, Test loss: 1.264e+08, MSE(e): 2.517e-03, MSE(pi1): 4.445e-02, MSE(pi2): 1.958e-03, MSE(pi3): 2.767e-03\n",
      "Epoch 46400, Train loss: 2.554e+04, Test loss: 1.264e+08, MSE(e): 2.490e-03, MSE(pi1): 3.623e-02, MSE(pi2): 1.939e-03, MSE(pi3): 2.740e-03\n",
      "Epoch 46500, Train loss: 2.562e+04, Test loss: 1.264e+08, MSE(e): 2.494e-03, MSE(pi1): 4.633e-02, MSE(pi2): 1.939e-03, MSE(pi3): 2.143e-03\n",
      "Epoch 46600, Train loss: 2.529e+04, Test loss: 1.264e+08, MSE(e): 2.487e-03, MSE(pi1): 2.143e-02, MSE(pi2): 1.935e-03, MSE(pi3): 2.029e-03\n",
      "Epoch 46700, Train loss: 3.172e+04, Test loss: 1.264e+08, MSE(e): 3.103e-03, MSE(pi1): 4.465e-02, MSE(pi2): 2.263e-03, MSE(pi3): 2.446e-03\n",
      "Epoch 46800, Train loss: 2.611e+04, Test loss: 1.265e+08, MSE(e): 2.524e-03, MSE(pi1): 6.033e-02, MSE(pi2): 1.957e-03, MSE(pi3): 2.608e-03\n",
      "Epoch 46900, Train loss: 2.538e+04, Test loss: 1.264e+08, MSE(e): 2.490e-03, MSE(pi1): 2.758e-02, MSE(pi2): 1.942e-03, MSE(pi3): 2.112e-03\n",
      "Epoch 47000, Train loss: 2.535e+04, Test loss: 1.263e+08, MSE(e): 2.494e-03, MSE(pi1): 2.072e-02, MSE(pi2): 1.951e-03, MSE(pi3): 2.031e-03\n",
      "Epoch 47100, Train loss: 2.542e+04, Test loss: 1.263e+08, MSE(e): 2.498e-03, MSE(pi1): 1.901e-02, MSE(pi2): 1.936e-03, MSE(pi3): 2.521e-03\n",
      "Epoch 47200, Train loss: 2.811e+04, Test loss: 1.262e+08, MSE(e): 2.769e-03, MSE(pi1): 1.696e-02, MSE(pi2): 2.041e-03, MSE(pi3): 2.546e-03\n",
      "Epoch 47300, Train loss: 2.672e+04, Test loss: 1.261e+08, MSE(e): 2.577e-03, MSE(pi1): 7.156e-02, MSE(pi2): 2.005e-03, MSE(pi3): 2.316e-03\n",
      "Epoch 47400, Train loss: 2.595e+04, Test loss: 1.262e+08, MSE(e): 2.518e-03, MSE(pi1): 5.719e-02, MSE(pi2): 1.956e-03, MSE(pi3): 2.028e-03\n",
      "Epoch 47500, Train loss: 2.691e+04, Test loss: 1.263e+08, MSE(e): 2.571e-03, MSE(pi1): 7.861e-02, MSE(pi2): 1.964e-03, MSE(pi3): 4.054e-03\n",
      "Epoch 47600, Train loss: 2.565e+04, Test loss: 1.262e+08, MSE(e): 2.505e-03, MSE(pi1): 3.317e-02, MSE(pi2): 1.954e-03, MSE(pi3): 2.654e-03\n",
      "Epoch 47700, Train loss: 2.653e+04, Test loss: 1.261e+08, MSE(e): 2.503e-03, MSE(pi1): 1.142e-01, MSE(pi2): 1.928e-03, MSE(pi3): 3.518e-03\n",
      "Epoch 47800, Train loss: 3.038e+04, Test loss: 1.262e+08, MSE(e): 2.968e-03, MSE(pi1): 4.613e-02, MSE(pi2): 2.190e-03, MSE(pi3): 2.318e-03\n",
      "Epoch 47900, Train loss: 2.520e+04, Test loss: 1.261e+08, MSE(e): 2.471e-03, MSE(pi1): 2.823e-02, MSE(pi2): 1.931e-03, MSE(pi3): 2.004e-03\n",
      "Epoch 48000, Train loss: 2.517e+04, Test loss: 1.261e+08, MSE(e): 2.471e-03, MSE(pi1): 2.216e-02, MSE(pi2): 1.930e-03, MSE(pi3): 2.367e-03\n",
      "Epoch 48100, Train loss: 3.187e+04, Test loss: 1.261e+08, MSE(e): 3.121e-03, MSE(pi1): 4.445e-02, MSE(pi2): 2.299e-03, MSE(pi3): 2.068e-03\n",
      "Epoch 48200, Train loss: 2.528e+04, Test loss: 1.261e+08, MSE(e): 2.470e-03, MSE(pi1): 3.354e-02, MSE(pi2): 1.915e-03, MSE(pi3): 2.464e-03\n",
      "Epoch 48300, Train loss: 2.560e+04, Test loss: 1.261e+08, MSE(e): 2.517e-03, MSE(pi1): 1.972e-02, MSE(pi2): 1.959e-03, MSE(pi3): 2.334e-03\n",
      "Epoch 48400, Train loss: 2.571e+04, Test loss: 1.260e+08, MSE(e): 2.518e-03, MSE(pi1): 3.074e-02, MSE(pi2): 1.927e-03, MSE(pi3): 2.252e-03\n",
      "Epoch 48500, Train loss: 2.658e+04, Test loss: 1.261e+08, MSE(e): 2.543e-03, MSE(pi1): 8.733e-02, MSE(pi2): 1.927e-03, MSE(pi3): 2.697e-03\n",
      "Epoch 48600, Train loss: 2.516e+04, Test loss: 1.261e+08, MSE(e): 2.459e-03, MSE(pi1): 3.411e-02, MSE(pi2): 1.920e-03, MSE(pi3): 2.332e-03\n",
      "Epoch 48700, Train loss: 2.531e+04, Test loss: 1.261e+08, MSE(e): 2.479e-03, MSE(pi1): 2.414e-02, MSE(pi2): 1.931e-03, MSE(pi3): 2.785e-03\n",
      "Epoch 48800, Train loss: 2.990e+04, Test loss: 1.261e+08, MSE(e): 2.939e-03, MSE(pi1): 2.910e-02, MSE(pi2): 2.181e-03, MSE(pi3): 2.192e-03\n",
      "Epoch 48900, Train loss: 2.490e+04, Test loss: 1.260e+08, MSE(e): 2.453e-03, MSE(pi1): 1.382e-02, MSE(pi2): 1.918e-03, MSE(pi3): 2.273e-03\n",
      "Epoch 49000, Train loss: 2.486e+04, Test loss: 1.260e+08, MSE(e): 2.445e-03, MSE(pi1): 2.189e-02, MSE(pi2): 1.905e-03, MSE(pi3): 1.903e-03\n",
      "Epoch 49100, Train loss: 9.266e+05, Test loss: 1.274e+08, MSE(e): 5.797e-02, MSE(pi1): 3.173e+01, MSE(pi2): 3.067e-02, MSE(pi3): 2.957e-01\n",
      "Epoch 49200, Train loss: 3.100e+04, Test loss: 1.253e+08, MSE(e): 2.610e-03, MSE(pi1): 3.089e-01, MSE(pi2): 2.004e-03, MSE(pi3): 1.811e-02\n",
      "Epoch 49300, Train loss: 2.843e+04, Test loss: 1.254e+08, MSE(e): 2.492e-03, MSE(pi1): 2.052e-01, MSE(pi2): 1.938e-03, MSE(pi3): 1.454e-02\n",
      "Epoch 49400, Train loss: 2.757e+04, Test loss: 1.255e+08, MSE(e): 2.465e-03, MSE(pi1): 1.644e-01, MSE(pi2): 1.923e-03, MSE(pi3): 1.276e-02\n",
      "Epoch 49500, Train loss: 2.703e+04, Test loss: 1.255e+08, MSE(e): 2.453e-03, MSE(pi1): 1.354e-01, MSE(pi2): 1.917e-03, MSE(pi3): 1.141e-02\n",
      "Epoch 49600, Train loss: 2.663e+04, Test loss: 1.255e+08, MSE(e): 2.447e-03, MSE(pi1): 1.127e-01, MSE(pi2): 1.914e-03, MSE(pi3): 1.031e-02\n",
      "Epoch 49700, Train loss: 2.632e+04, Test loss: 1.255e+08, MSE(e): 2.443e-03, MSE(pi1): 9.445e-02, MSE(pi2): 1.911e-03, MSE(pi3): 9.386e-03\n",
      "Epoch 49800, Train loss: 2.606e+04, Test loss: 1.256e+08, MSE(e): 2.440e-03, MSE(pi1): 7.988e-02, MSE(pi2): 1.909e-03, MSE(pi3): 8.611e-03\n",
      "Epoch 49900, Train loss: 2.585e+04, Test loss: 1.256e+08, MSE(e): 2.437e-03, MSE(pi1): 6.831e-02, MSE(pi2): 1.907e-03, MSE(pi3): 7.956e-03\n",
      "\n",
      "Training process finished after 50000 epochs\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Parametros de entrenamiento\n",
    "start_epoch = 29700\n",
    "n_epochs = 50000\n",
    "\n",
    "batch_size = 64\n",
    "n_checkpoints = 100\n",
    "\n",
    "second_lr = None\n",
    "\n",
    "train_loop(model, optimizer, X_train, y_train, f_train, X_test, y_test, f_test,\n",
    "           D,  n_checkpoints, start_epoch=start_epoch, n_epochs=n_epochs, batch_size=batch_size, \n",
    "           model_results_path=MODEL_RESULTS_PATH, device=DEVICE, new_lr=second_lr)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SciML_test_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
