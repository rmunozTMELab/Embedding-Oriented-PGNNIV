{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from collections import Counter\n",
    "\n",
    "# Imports de la libreria propia\n",
    "from vecopsciml.kernels.derivative import DerivativeKernels\n",
    "from vecopsciml.utils import TensOps\n",
    "\n",
    "# Imports de las funciones creadas para este programa\n",
    "from utils.folders import create_folder\n",
    "from utils.load_data import load_data\n",
    "from trainers.train import train_loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "print(f\"Using device: {DEVICE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Folder already exists at: /home/rmunoz/Escritorio/rmunozTMELab/Physically-Guided-Machine-Learning/results/non_linear\n",
      "Folder already exists at: /home/rmunoz/Escritorio/rmunozTMELab/Physically-Guided-Machine-Learning/results/non_linear/model_POD\n"
     ]
    }
   ],
   "source": [
    "# Creamos los paths para las distintas carpetas\n",
    "ROOT_PATH = r'/home/rmunoz/Escritorio/rmunozTMELab/Physically-Guided-Machine-Learning'\n",
    "DATA_PATH = os.path.join(ROOT_PATH, r'data/non_linear/non_linear_decomposition.pkl')\n",
    "RESULTS_FOLDER_PATH = os.path.join(ROOT_PATH, r'results/non_linear')\n",
    "MODEL_RESULTS_PGNNIV_PATH = os.path.join(ROOT_PATH, r'results/non_linear/model_POD')\n",
    "\n",
    "\n",
    "# Creamos las carpetas que sean necesarias (si ya están creadas se avisará de ello)\n",
    "create_folder(RESULTS_FOLDER_PATH)\n",
    "create_folder(MODEL_RESULTS_PGNNIV_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully loaded from: /home/rmunoz/Escritorio/rmunozTMELab/Physically-Guided-Machine-Learning/data/non_linear/non_linear_decomposition.pkl\n"
     ]
    }
   ],
   "source": [
    "# Load dataset\n",
    "dataset = load_data(DATA_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convolutional filters to derivate\n",
    "dx = dataset['x_step_size']\n",
    "dy = dataset['y_step_size']\n",
    "D = DerivativeKernels(dx, dy, 0).grad_kernels_two_dimensions()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## División de los datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset length: 8000\n",
      "Validation dataset length: 2000\n"
     ]
    }
   ],
   "source": [
    "X_train = torch.Tensor(dataset['X_train']).unsqueeze(1).to(DEVICE)\n",
    "y_train = TensOps(torch.Tensor(dataset['y_train']).unsqueeze(1).requires_grad_(True).to(DEVICE), space_dimension=2, contravariance=0, covariance=0)\n",
    "K_train = TensOps(torch.tensor(dataset['k_train']).unsqueeze(1).requires_grad_(True).to(DEVICE), space_dimension=2, contravariance=0, covariance=0)\n",
    "f_train = TensOps(torch.tensor(dataset['f_train']).unsqueeze(1).to(torch.float32).requires_grad_(True).to(DEVICE), space_dimension=2, contravariance=0, covariance=0)\n",
    "\n",
    "X_val = torch.Tensor(dataset['X_val']).unsqueeze(1).to(DEVICE)\n",
    "y_val = TensOps(torch.Tensor(dataset['y_val']).unsqueeze(1).requires_grad_(True).to(DEVICE), space_dimension=2, contravariance=0, covariance=0)\n",
    "K_val = TensOps(torch.tensor(dataset['k_val']).unsqueeze(1).requires_grad_(True).to(DEVICE), space_dimension=2, contravariance=0, covariance=0)\n",
    "f_val = TensOps(torch.tensor(dataset['f_val']).to(torch.float32).unsqueeze(1).requires_grad_(True).to(DEVICE), space_dimension=2, contravariance=0, covariance=0)\n",
    "\n",
    "print(\"Train dataset length:\", len(X_train))\n",
    "print(\"Validation dataset length:\", len(X_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## POD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 --> 1.0\n",
      "1 --> 0.0983155369758606\n",
      "2 --> 0.05151933431625366\n",
      "3 --> 0.00887155532836914\n",
      "4 --> 0.005898416042327881\n",
      "5 --> 0.0036424994468688965\n",
      "6 --> 0.0016180872917175293\n",
      "7 --> 0.0011507868766784668\n",
      "8 --> 0.000786125659942627\n",
      "9 --> 0.0005230903625488281\n",
      "10 --> 0.0003058314323425293\n",
      "11 --> 0.00022345781326293945\n",
      "12 --> 0.0001596212387084961\n",
      "13 --> 0.00011056661605834961\n",
      "14 --> 8.106231689453125e-05\n",
      "15 --> 5.6684017181396484e-05\n",
      "16 --> 4.3272972106933594e-05\n",
      "17 --> 3.129243850708008e-05\n",
      "18 --> 2.372264862060547e-05\n",
      "19 --> 1.7762184143066406e-05\n"
     ]
    }
   ],
   "source": [
    "U_train, S_train, Vt_train = torch.linalg.svd(y_train.values.detach().squeeze().to('cpu').view(y_train.values.detach().shape[0], -1).T, full_matrices=False)\n",
    "\n",
    "error = []\n",
    "for mode_i in range(len(S_train)):\n",
    "    error.append(1-(sum(S_train[:mode_i])/sum(S_train)).numpy())\n",
    "    if mode_i < 20:\n",
    "        print(mode_i, '-->', error[mode_i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_modes = 14\n",
    "\n",
    "U_reduced_train = U_train[:, :num_modes]\n",
    "S_reduced_train = S_train[:num_modes]\n",
    "Vt_reduced_train = Vt_train[:num_modes, :]\n",
    "\n",
    "modes_base_train = torch.mm(U_reduced_train, torch.diag(S_reduced_train))\n",
    "# y_train = Vt_reduced_train.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "U_val, S_val, Vt_val = torch.linalg.svd(y_val.values.squeeze().view(y_val.values.shape[0], -1).T, full_matrices=False)\n",
    "\n",
    "num_modes = 14\n",
    "\n",
    "U_reduced_val = U_val[:, :num_modes]\n",
    "S_reduced_val = S_val[:num_modes]\n",
    "Vt_reduced_val = Vt_val[:num_modes, :]\n",
    "\n",
    "# data_reconstructed = torch.mm(torch.mm(U_reduced_val, torch.diag(S_reduced_val)), Vt_reduced_val)\n",
    "\n",
    "# y_val = Vt_reduced_val.T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Red neuronal general"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = X_train[0].shape\n",
    "POD_shape = num_modes\n",
    "output_shape = y_train.values[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from vecopsciml.utils import TensOps\n",
    "from vecopsciml.operators.zero_order import Mx, My\n",
    "\n",
    "class POD_PGNNIV(nn.Module):\n",
    "    def __init__(self, input_size, POD_output, explanatory_output_size, POD_base, device, **kwargs):\n",
    "        super(POD_PGNNIV, self).__init__()\n",
    "\n",
    "        self.input = input_size\n",
    "        self.POD_output = POD_output\n",
    "        self.output_expl = explanatory_output_size\n",
    "\n",
    "        self.hidden_units_pred = 10\n",
    "        self.hidden_units_exp = 15\n",
    "        self.filters_exp = 10\n",
    "\n",
    "        self.base = POD_base.to(device)\n",
    "\n",
    "        self.device = device\n",
    "\n",
    "        # # Predictive network\n",
    "        self.flatten_layer_pred = nn.Flatten(start_dim=1, end_dim=-1)\n",
    "        self.hidden1_layer_pred = nn.Linear(torch.prod(torch.tensor(self.input, device=self.device)), self.hidden_units_pred).to(self.device)\n",
    "        self.hidden2_layer_pred = nn.Linear(self.hidden_units_pred, self.hidden_units_pred).to(self.device)\n",
    "        self.output_layer_pred = nn.Linear(self.hidden_units_pred, self.POD_output).to(self.device)\n",
    "\n",
    "        # Explanatory network (commented out since they are not used in forward method)\n",
    "        self.conv1_exp = nn.Conv2d(in_channels=1, out_channels=self.filters_exp, kernel_size=1).to(self.device)\n",
    "        self.flatten_layer_exp = nn.Flatten().to(self.device)\n",
    "        self.hidden1_layer_exp = nn.LazyLinear(self.hidden_units_exp).to(self.device)\n",
    "        self.hidden2_layer_exp = nn.Linear(self.hidden_units_exp, self.hidden_units_exp).to(self.device)\n",
    "        self.output_layer_exp = nn.Linear(self.hidden_units_exp, self.filters_exp * (self.output_expl[1] - 1) * (self.output_expl[2] - 1)).to(self.device)\n",
    "        self.conv2_exp = nn.Conv2d(in_channels=self.filters_exp, out_channels=1, kernel_size=1).to(self.device)\n",
    "\n",
    "    def forward(self, X):\n",
    "\n",
    "        X = X.to(self.device)\n",
    "\n",
    "        # Predictive network\n",
    "        X = self.flatten_layer_pred(X)\n",
    "        X = torch.sigmoid(self.hidden1_layer_pred(X))\n",
    "        X = torch.sigmoid(self.hidden2_layer_pred(X))\n",
    "        output_predictive_net = self.output_layer_pred(X)\n",
    "\n",
    "        u_pred = torch.mm(self.base, output_predictive_net.T).T.reshape(output_predictive_net.shape[0], self.output_expl[0], self.output_expl[1], self.output_expl[2])\n",
    "        um_pred = My(Mx(TensOps(u_pred, space_dimension=2, contravariance=0, covariance=0))).values\n",
    "\n",
    "        x = torch.sigmoid(self.conv1_exp(um_pred))\n",
    "        x = self.flatten_layer_exp(x)\n",
    "        x = torch.sigmoid(self.hidden1_layer_exp(x))\n",
    "        x = torch.sigmoid(self.hidden2_layer_exp(x))\n",
    "        x = self.output_layer_exp(x)\n",
    "        x = x.view(x.size(0), self.filters_exp, self.output_expl[1] - 1, self.output_expl[2] - 1)\n",
    "        K_pred = self.conv2_exp(x)\n",
    "\n",
    "        return u_pred, K_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training\n",
      "Epoch 0, Train loss: 3.526e+11, Test loss: 3.499e+11, MSE(e): 3.525e+04, MSE(pi1): 2.518e+03, MSE(pi2): 1.417e+04, MSE(pi3): 9.845e+01\n",
      "Epoch 100, Train loss: 4.837e+06, Test loss: 4.577e+06, MSE(e): 4.815e-01, MSE(pi1): 6.363e-01, MSE(pi2): 3.477e-01, MSE(pi3): 1.486e-01\n",
      "Epoch 200, Train loss: 1.185e+05, Test loss: 1.338e+05, MSE(e): 1.161e-02, MSE(pi1): 8.716e-02, MSE(pi2): 8.434e-03, MSE(pi3): 1.472e-02\n",
      "Epoch 300, Train loss: 6.678e+04, Test loss: 7.549e+04, MSE(e): 6.580e-03, MSE(pi1): 3.730e-02, MSE(pi2): 5.293e-03, MSE(pi3): 5.939e-03\n",
      "Epoch 400, Train loss: 5.116e+04, Test loss: 4.584e+04, MSE(e): 5.087e-03, MSE(pi1): 8.889e-03, MSE(pi2): 4.211e-03, MSE(pi3): 1.967e-03\n",
      "Epoch 500, Train loss: 4.275e+04, Test loss: 3.957e+04, MSE(e): 4.250e-03, MSE(pi1): 9.456e-03, MSE(pi2): 3.668e-03, MSE(pi3): 1.559e-03\n",
      "Epoch 600, Train loss: 4.388e+04, Test loss: 3.322e+04, MSE(e): 4.364e-03, MSE(pi1): 9.133e-03, MSE(pi2): 3.569e-03, MSE(pi3): 1.442e-03\n",
      "Epoch 700, Train loss: 3.872e+04, Test loss: 3.752e+04, MSE(e): 3.850e-03, MSE(pi1): 9.212e-03, MSE(pi2): 3.274e-03, MSE(pi3): 1.297e-03\n",
      "Epoch 800, Train loss: 3.019e+04, Test loss: 3.100e+04, MSE(e): 2.997e-03, MSE(pi1): 9.946e-03, MSE(pi2): 2.860e-03, MSE(pi3): 1.119e-03\n",
      "Epoch 900, Train loss: 2.726e+04, Test loss: 2.732e+04, MSE(e): 2.706e-03, MSE(pi1): 9.174e-03, MSE(pi2): 2.684e-03, MSE(pi3): 1.092e-03\n",
      "Epoch 1000, Train loss: 2.588e+04, Test loss: 2.525e+04, MSE(e): 2.568e-03, MSE(pi1): 8.422e-03, MSE(pi2): 2.585e-03, MSE(pi3): 1.096e-03\n",
      "Epoch 1100, Train loss: 2.485e+04, Test loss: 2.381e+04, MSE(e): 2.466e-03, MSE(pi1): 8.050e-03, MSE(pi2): 2.506e-03, MSE(pi3): 1.084e-03\n",
      "Epoch 1200, Train loss: 2.413e+04, Test loss: 2.288e+04, MSE(e): 2.394e-03, MSE(pi1): 7.889e-03, MSE(pi2): 2.446e-03, MSE(pi3): 1.063e-03\n",
      "Epoch 1300, Train loss: 2.377e+04, Test loss: 2.218e+04, MSE(e): 2.358e-03, MSE(pi1): 8.005e-03, MSE(pi2): 2.408e-03, MSE(pi3): 1.031e-03\n",
      "Epoch 1400, Train loss: 2.366e+04, Test loss: 2.165e+04, MSE(e): 2.347e-03, MSE(pi1): 8.017e-03, MSE(pi2): 2.381e-03, MSE(pi3): 1.009e-03\n",
      "Epoch 1500, Train loss: 2.389e+04, Test loss: 2.136e+04, MSE(e): 2.371e-03, MSE(pi1): 7.922e-03, MSE(pi2): 2.368e-03, MSE(pi3): 9.967e-04\n",
      "Epoch 1600, Train loss: 2.435e+04, Test loss: 2.134e+04, MSE(e): 2.417e-03, MSE(pi1): 7.769e-03, MSE(pi2): 2.365e-03, MSE(pi3): 9.901e-04\n",
      "Epoch 1700, Train loss: 2.493e+04, Test loss: 2.171e+04, MSE(e): 2.476e-03, MSE(pi1): 7.620e-03, MSE(pi2): 2.367e-03, MSE(pi3): 9.838e-04\n",
      "Epoch 1800, Train loss: 2.518e+04, Test loss: 2.209e+04, MSE(e): 2.501e-03, MSE(pi1): 7.557e-03, MSE(pi2): 2.357e-03, MSE(pi3): 9.718e-04\n",
      "Epoch 1900, Train loss: 2.499e+04, Test loss: 2.255e+04, MSE(e): 2.482e-03, MSE(pi1): 7.477e-03, MSE(pi2): 2.331e-03, MSE(pi3): 9.579e-04\n",
      "Epoch 2000, Train loss: 2.740e+04, Test loss: 2.654e+04, MSE(e): 2.722e-03, MSE(pi1): 1.011e-02, MSE(pi2): 2.427e-03, MSE(pi3): 8.160e-04\n",
      "Epoch 2100, Train loss: 2.755e+04, Test loss: 2.600e+04, MSE(e): 2.736e-03, MSE(pi1): 9.767e-03, MSE(pi2): 2.405e-03, MSE(pi3): 8.220e-04\n",
      "Epoch 2200, Train loss: 2.673e+04, Test loss: 2.552e+04, MSE(e): 2.656e-03, MSE(pi1): 9.380e-03, MSE(pi2): 2.346e-03, MSE(pi3): 8.243e-04\n",
      "Epoch 2300, Train loss: 2.578e+04, Test loss: 2.488e+04, MSE(e): 2.561e-03, MSE(pi1): 9.003e-03, MSE(pi2): 2.281e-03, MSE(pi3): 8.257e-04\n",
      "Epoch 2400, Train loss: 2.439e+04, Test loss: 2.390e+04, MSE(e): 2.422e-03, MSE(pi1): 8.585e-03, MSE(pi2): 2.197e-03, MSE(pi3): 8.273e-04\n",
      "Epoch 2500, Train loss: 2.333e+04, Test loss: 2.306e+04, MSE(e): 2.316e-03, MSE(pi1): 8.211e-03, MSE(pi2): 2.124e-03, MSE(pi3): 8.290e-04\n",
      "Epoch 2600, Train loss: 2.191e+04, Test loss: 2.204e+04, MSE(e): 2.175e-03, MSE(pi1): 7.735e-03, MSE(pi2): 2.036e-03, MSE(pi3): 8.352e-04\n",
      "Epoch 2700, Train loss: 2.004e+04, Test loss: 2.049e+04, MSE(e): 1.988e-03, MSE(pi1): 7.308e-03, MSE(pi2): 1.929e-03, MSE(pi3): 8.363e-04\n",
      "Epoch 2800, Train loss: 1.863e+04, Test loss: 1.926e+04, MSE(e): 1.848e-03, MSE(pi1): 6.945e-03, MSE(pi2): 1.840e-03, MSE(pi3): 8.377e-04\n",
      "Epoch 2900, Train loss: 1.739e+04, Test loss: 1.803e+04, MSE(e): 1.724e-03, MSE(pi1): 6.542e-03, MSE(pi2): 1.759e-03, MSE(pi3): 8.455e-04\n",
      "Epoch 3000, Train loss: 1.650e+04, Test loss: 1.720e+04, MSE(e): 1.635e-03, MSE(pi1): 6.130e-03, MSE(pi2): 1.695e-03, MSE(pi3): 8.596e-04\n",
      "Epoch 3100, Train loss: 1.591e+04, Test loss: 1.660e+04, MSE(e): 1.576e-03, MSE(pi1): 5.847e-03, MSE(pi2): 1.645e-03, MSE(pi3): 8.698e-04\n",
      "Epoch 3200, Train loss: 1.522e+04, Test loss: 1.590e+04, MSE(e): 1.507e-03, MSE(pi1): 5.467e-03, MSE(pi2): 1.592e-03, MSE(pi3): 8.897e-04\n",
      "Epoch 3300, Train loss: 1.458e+04, Test loss: 1.542e+04, MSE(e): 1.444e-03, MSE(pi1): 5.035e-03, MSE(pi2): 1.542e-03, MSE(pi3): 9.181e-04\n",
      "Epoch 3400, Train loss: 1.401e+04, Test loss: 1.492e+04, MSE(e): 1.386e-03, MSE(pi1): 4.633e-03, MSE(pi2): 1.497e-03, MSE(pi3): 9.460e-04\n",
      "Epoch 3500, Train loss: 1.350e+04, Test loss: 1.449e+04, MSE(e): 1.336e-03, MSE(pi1): 4.808e-03, MSE(pi2): 1.456e-03, MSE(pi3): 9.161e-04\n",
      "Epoch 3600, Train loss: 1.315e+04, Test loss: 1.399e+04, MSE(e): 1.301e-03, MSE(pi1): 4.786e-03, MSE(pi2): 1.423e-03, MSE(pi3): 9.101e-04\n",
      "Epoch 3700, Train loss: 1.323e+04, Test loss: 1.492e+04, MSE(e): 1.309e-03, MSE(pi1): 4.845e-03, MSE(pi2): 1.411e-03, MSE(pi3): 9.008e-04\n",
      "Epoch 3800, Train loss: 1.254e+04, Test loss: 1.397e+04, MSE(e): 1.240e-03, MSE(pi1): 4.729e-03, MSE(pi2): 1.369e-03, MSE(pi3): 9.014e-04\n",
      "Epoch 3900, Train loss: 1.566e+04, Test loss: 1.695e+04, MSE(e): 1.552e-03, MSE(pi1): 5.157e-03, MSE(pi2): 1.482e-03, MSE(pi3): 8.821e-04\n",
      "Epoch 4000, Train loss: 1.527e+04, Test loss: 1.646e+04, MSE(e): 1.513e-03, MSE(pi1): 5.111e-03, MSE(pi2): 1.457e-03, MSE(pi3): 8.802e-04\n",
      "Epoch 4100, Train loss: 1.437e+04, Test loss: 1.610e+04, MSE(e): 1.423e-03, MSE(pi1): 5.005e-03, MSE(pi2): 1.411e-03, MSE(pi3): 8.788e-04\n",
      "Epoch 4200, Train loss: 1.433e+04, Test loss: 1.605e+04, MSE(e): 1.419e-03, MSE(pi1): 4.985e-03, MSE(pi2): 1.400e-03, MSE(pi3): 8.754e-04\n",
      "Epoch 4300, Train loss: 1.385e+04, Test loss: 1.583e+04, MSE(e): 1.371e-03, MSE(pi1): 4.919e-03, MSE(pi2): 1.373e-03, MSE(pi3): 8.732e-04\n",
      "Epoch 4400, Train loss: 1.367e+04, Test loss: 1.575e+04, MSE(e): 1.353e-03, MSE(pi1): 4.887e-03, MSE(pi2): 1.359e-03, MSE(pi3): 8.703e-04\n",
      "Epoch 4500, Train loss: 1.321e+04, Test loss: 1.544e+04, MSE(e): 1.307e-03, MSE(pi1): 4.825e-03, MSE(pi2): 1.333e-03, MSE(pi3): 8.685e-04\n",
      "Epoch 4600, Train loss: 1.315e+04, Test loss: 1.545e+04, MSE(e): 1.302e-03, MSE(pi1): 4.815e-03, MSE(pi2): 1.325e-03, MSE(pi3): 8.649e-04\n",
      "Epoch 4700, Train loss: 1.285e+04, Test loss: 1.532e+04, MSE(e): 1.271e-03, MSE(pi1): 4.774e-03, MSE(pi2): 1.306e-03, MSE(pi3): 8.625e-04\n",
      "Epoch 4800, Train loss: 1.071e+04, Test loss: 1.193e+04, MSE(e): 1.058e-03, MSE(pi1): 4.294e-03, MSE(pi2): 1.213e-03, MSE(pi3): 8.818e-04\n",
      "Epoch 4900, Train loss: 1.052e+04, Test loss: 1.176e+04, MSE(e): 1.038e-03, MSE(pi1): 4.223e-03, MSE(pi2): 1.198e-03, MSE(pi3): 8.821e-04\n",
      "Epoch 5000, Train loss: 1.058e+04, Test loss: 1.179e+04, MSE(e): 1.045e-03, MSE(pi1): 4.272e-03, MSE(pi2): 1.195e-03, MSE(pi3): 8.751e-04\n",
      "Epoch 5100, Train loss: 1.029e+04, Test loss: 1.155e+04, MSE(e): 1.016e-03, MSE(pi1): 4.163e-03, MSE(pi2): 1.177e-03, MSE(pi3): 8.784e-04\n",
      "Epoch 5200, Train loss: 1.015e+04, Test loss: 1.154e+04, MSE(e): 1.002e-03, MSE(pi1): 4.115e-03, MSE(pi2): 1.166e-03, MSE(pi3): 8.780e-04\n",
      "Epoch 5300, Train loss: 1.113e+04, Test loss: 1.227e+04, MSE(e): 1.100e-03, MSE(pi1): 4.434e-03, MSE(pi2): 1.199e-03, MSE(pi3): 8.552e-04\n",
      "Epoch 5400, Train loss: 1.036e+04, Test loss: 1.279e+04, MSE(e): 1.023e-03, MSE(pi1): 4.284e-03, MSE(pi2): 1.164e-03, MSE(pi3): 8.592e-04\n",
      "Epoch 5500, Train loss: 1.005e+04, Test loss: 1.217e+04, MSE(e): 9.922e-04, MSE(pi1): 4.202e-03, MSE(pi2): 1.145e-03, MSE(pi3): 8.611e-04\n",
      "Epoch 5600, Train loss: 9.896e+03, Test loss: 1.194e+04, MSE(e): 9.767e-04, MSE(pi1): 4.164e-03, MSE(pi2): 1.133e-03, MSE(pi3): 8.604e-04\n",
      "Epoch 5700, Train loss: 1.016e+04, Test loss: 1.137e+04, MSE(e): 1.003e-03, MSE(pi1): 4.257e-03, MSE(pi2): 1.136e-03, MSE(pi3): 8.523e-04\n",
      "Epoch 5800, Train loss: 9.960e+03, Test loss: 1.119e+04, MSE(e): 9.831e-04, MSE(pi1): 4.204e-03, MSE(pi2): 1.122e-03, MSE(pi3): 8.529e-04\n",
      "Epoch 5900, Train loss: 1.014e+04, Test loss: 1.134e+04, MSE(e): 1.001e-03, MSE(pi1): 4.270e-03, MSE(pi2): 1.123e-03, MSE(pi3): 8.461e-04\n",
      "Epoch 6000, Train loss: 9.624e+03, Test loss: 1.188e+04, MSE(e): 9.496e-04, MSE(pi1): 4.161e-03, MSE(pi2): 1.096e-03, MSE(pi3): 8.491e-04\n",
      "Epoch 6100, Train loss: 9.460e+03, Test loss: 1.077e+04, MSE(e): 9.333e-04, MSE(pi1): 4.092e-03, MSE(pi2): 1.082e-03, MSE(pi3): 8.515e-04\n",
      "Epoch 6200, Train loss: 9.254e+03, Test loss: 1.112e+04, MSE(e): 9.128e-04, MSE(pi1): 4.047e-03, MSE(pi2): 1.067e-03, MSE(pi3): 8.513e-04\n",
      "Epoch 6300, Train loss: 9.168e+03, Test loss: 1.106e+04, MSE(e): 9.042e-04, MSE(pi1): 4.044e-03, MSE(pi2): 1.055e-03, MSE(pi3): 8.487e-04\n",
      "Epoch 6400, Train loss: 9.059e+03, Test loss: 1.044e+04, MSE(e): 8.933e-04, MSE(pi1): 3.991e-03, MSE(pi2): 1.042e-03, MSE(pi3): 8.503e-04\n",
      "Epoch 6500, Train loss: 8.933e+03, Test loss: 1.033e+04, MSE(e): 8.808e-04, MSE(pi1): 3.914e-03, MSE(pi2): 1.028e-03, MSE(pi3): 8.532e-04\n",
      "Epoch 6600, Train loss: 8.812e+03, Test loss: 1.034e+04, MSE(e): 8.687e-04, MSE(pi1): 3.925e-03, MSE(pi2): 1.014e-03, MSE(pi3): 8.495e-04\n",
      "Epoch 6700, Train loss: 9.356e+03, Test loss: 1.064e+04, MSE(e): 9.230e-04, MSE(pi1): 4.214e-03, MSE(pi2): 1.027e-03, MSE(pi3): 8.279e-04\n",
      "Epoch 6800, Train loss: 8.611e+03, Test loss: 1.041e+04, MSE(e): 8.487e-04, MSE(pi1): 3.984e-03, MSE(pi2): 9.859e-04, MSE(pi3): 8.400e-04\n",
      "Epoch 6900, Train loss: 8.880e+03, Test loss: 1.026e+04, MSE(e): 8.754e-04, MSE(pi1): 4.137e-03, MSE(pi2): 9.848e-04, MSE(pi3): 8.278e-04\n",
      "Epoch 7000, Train loss: 8.444e+03, Test loss: 9.935e+03, MSE(e): 8.320e-04, MSE(pi1): 4.010e-03, MSE(pi2): 9.531e-04, MSE(pi3): 8.339e-04\n",
      "Epoch 7100, Train loss: 8.404e+03, Test loss: 9.911e+03, MSE(e): 8.280e-04, MSE(pi1): 4.046e-03, MSE(pi2): 9.362e-04, MSE(pi3): 8.295e-04\n",
      "Epoch 7200, Train loss: 8.110e+03, Test loss: 1.006e+04, MSE(e): 7.986e-04, MSE(pi1): 3.987e-03, MSE(pi2): 9.067e-04, MSE(pi3): 8.318e-04\n",
      "Epoch 7300, Train loss: 7.944e+03, Test loss: 9.908e+03, MSE(e): 7.820e-04, MSE(pi1): 3.981e-03, MSE(pi2): 8.785e-04, MSE(pi3): 8.313e-04\n",
      "Epoch 7400, Train loss: 7.727e+03, Test loss: 9.580e+03, MSE(e): 7.603e-04, MSE(pi1): 3.946e-03, MSE(pi2): 8.432e-04, MSE(pi3): 8.337e-04\n",
      "Epoch 7500, Train loss: 7.549e+03, Test loss: 9.566e+03, MSE(e): 7.426e-04, MSE(pi1): 3.977e-03, MSE(pi2): 8.046e-04, MSE(pi3): 8.324e-04\n",
      "Epoch 7600, Train loss: 7.419e+03, Test loss: 9.736e+03, MSE(e): 7.294e-04, MSE(pi1): 4.050e-03, MSE(pi2): 7.618e-04, MSE(pi3): 8.297e-04\n",
      "Epoch 7700, Train loss: 1.028e+04, Test loss: 1.258e+04, MSE(e): 1.015e-03, MSE(pi1): 4.592e-03, MSE(pi2): 8.397e-04, MSE(pi3): 8.071e-04\n",
      "Epoch 7800, Train loss: 9.443e+03, Test loss: 1.225e+04, MSE(e): 9.315e-04, MSE(pi1): 4.632e-03, MSE(pi2): 7.636e-04, MSE(pi3): 8.073e-04\n",
      "Epoch 7900, Train loss: 8.781e+03, Test loss: 1.194e+04, MSE(e): 8.653e-04, MSE(pi1): 4.592e-03, MSE(pi2): 6.997e-04, MSE(pi3): 8.133e-04\n",
      "Epoch 8000, Train loss: 6.292e+03, Test loss: 8.820e+03, MSE(e): 6.166e-04, MSE(pi1): 4.182e-03, MSE(pi2): 5.678e-04, MSE(pi3): 8.332e-04\n",
      "Epoch 8100, Train loss: 6.069e+03, Test loss: 8.625e+03, MSE(e): 5.944e-04, MSE(pi1): 4.122e-03, MSE(pi2): 5.386e-04, MSE(pi3): 8.364e-04\n",
      "Epoch 8200, Train loss: 6.001e+03, Test loss: 8.600e+03, MSE(e): 5.876e-04, MSE(pi1): 4.087e-03, MSE(pi2): 5.209e-04, MSE(pi3): 8.368e-04\n",
      "Epoch 8300, Train loss: 5.824e+03, Test loss: 8.421e+03, MSE(e): 5.700e-04, MSE(pi1): 4.025e-03, MSE(pi2): 5.021e-04, MSE(pi3): 8.367e-04\n",
      "Epoch 8400, Train loss: 5.669e+03, Test loss: 8.286e+03, MSE(e): 5.545e-04, MSE(pi1): 3.977e-03, MSE(pi2): 4.859e-04, MSE(pi3): 8.352e-04\n",
      "Epoch 8500, Train loss: 5.511e+03, Test loss: 8.150e+03, MSE(e): 5.388e-04, MSE(pi1): 3.939e-03, MSE(pi2): 4.704e-04, MSE(pi3): 8.327e-04\n",
      "Epoch 8600, Train loss: 5.293e+03, Test loss: 7.905e+03, MSE(e): 5.170e-04, MSE(pi1): 3.884e-03, MSE(pi2): 4.531e-04, MSE(pi3): 8.313e-04\n",
      "Epoch 8700, Train loss: 4.842e+03, Test loss: 7.470e+03, MSE(e): 4.720e-04, MSE(pi1): 3.780e-03, MSE(pi2): 4.265e-04, MSE(pi3): 8.323e-04\n",
      "Epoch 8800, Train loss: 5.041e+03, Test loss: 7.738e+03, MSE(e): 4.920e-04, MSE(pi1): 3.850e-03, MSE(pi2): 4.273e-04, MSE(pi3): 8.245e-04\n",
      "Epoch 8900, Train loss: 4.891e+03, Test loss: 7.539e+03, MSE(e): 4.770e-04, MSE(pi1): 3.803e-03, MSE(pi2): 4.143e-04, MSE(pi3): 8.237e-04\n",
      "Epoch 9000, Train loss: 4.322e+03, Test loss: 6.850e+03, MSE(e): 4.202e-04, MSE(pi1): 3.573e-03, MSE(pi2): 3.874e-04, MSE(pi3): 8.355e-04\n",
      "Epoch 9100, Train loss: 4.396e+03, Test loss: 6.908e+03, MSE(e): 4.276e-04, MSE(pi1): 3.681e-03, MSE(pi2): 3.823e-04, MSE(pi3): 8.241e-04\n",
      "Epoch 9200, Train loss: 6.366e+03, Test loss: 8.846e+03, MSE(e): 6.245e-04, MSE(pi1): 4.030e-03, MSE(pi2): 4.530e-04, MSE(pi3): 8.027e-04\n",
      "Epoch 9300, Train loss: 6.141e+03, Test loss: 8.664e+03, MSE(e): 6.020e-04, MSE(pi1): 3.989e-03, MSE(pi2): 4.374e-04, MSE(pi3): 8.020e-04\n",
      "Epoch 9400, Train loss: 7.874e+03, Test loss: 1.050e+04, MSE(e): 7.752e-04, MSE(pi1): 4.177e-03, MSE(pi2): 4.980e-04, MSE(pi3): 7.925e-04\n",
      "Epoch 9500, Train loss: 5.689e+03, Test loss: 8.166e+03, MSE(e): 5.570e-04, MSE(pi1): 3.905e-03, MSE(pi2): 4.071e-04, MSE(pi3): 8.020e-04\n",
      "Epoch 9600, Train loss: 6.151e+03, Test loss: 8.668e+03, MSE(e): 6.031e-04, MSE(pi1): 3.964e-03, MSE(pi2): 4.187e-04, MSE(pi3): 7.975e-04\n",
      "Epoch 9700, Train loss: 6.245e+03, Test loss: 8.738e+03, MSE(e): 6.125e-04, MSE(pi1): 3.969e-03, MSE(pi2): 4.164e-04, MSE(pi3): 7.959e-04\n",
      "Epoch 9800, Train loss: 4.084e+03, Test loss: 6.303e+03, MSE(e): 3.966e-04, MSE(pi1): 3.609e-03, MSE(pi2): 3.305e-04, MSE(pi3): 8.152e-04\n",
      "Epoch 9900, Train loss: 4.625e+03, Test loss: 6.864e+03, MSE(e): 4.507e-04, MSE(pi1): 3.729e-03, MSE(pi2): 3.450e-04, MSE(pi3): 8.065e-04\n",
      "\n",
      "Proceso finalizado después de 10000 épocas\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Se carga el modelo y el optimizador\n",
    "POD_model = POD_PGNNIV(input_size=input_shape, POD_output=POD_shape, explanatory_output_size=output_shape, POD_base=modes_base_train, device=DEVICE)\n",
    "optimizer = torch.optim.Adam(POD_model.parameters(), lr=1e-4)\n",
    "\n",
    "# Parametros de entrenamiento\n",
    "start_epoch = 0\n",
    "n_epochs = 10000\n",
    "\n",
    "batch_size = 64\n",
    "n_checkpoints = 10\n",
    "\n",
    "train_loop(POD_model, optimizer, n_checkpoints,\n",
    "           X_train.to(DEVICE), y_train, X_val, y_val, f_train, f_val,\n",
    "           D=D, start_epoch=start_epoch, n_epochs=n_epochs, batch_size=batch_size, \n",
    "           model_results_path=MODEL_RESULTS_PGNNIV_PATH, device=DEVICE,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SciML_test_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
