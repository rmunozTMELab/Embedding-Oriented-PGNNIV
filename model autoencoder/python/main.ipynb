{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from collections import Counter\n",
    "\n",
    "# Imports de la libreria propia\n",
    "from vecopsciml.kernels.derivative import DerivativeKernels\n",
    "from vecopsciml.utils import TensOps\n",
    "\n",
    "# Imports de las funciones creadas para este programa\n",
    "from utils.folders import create_folder\n",
    "from utils.load_data import load_data\n",
    "from trainers.train import train_loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "print(f\"Using device: {DEVICE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Folder already exists at: /home/rmunoz/Escritorio/rmunozTMELab/Physically-Guided-Machine-Learning/results/non_linear\n",
      "Folder already exists at: /home/rmunoz/Escritorio/rmunozTMELab/Physically-Guided-Machine-Learning/results/non_linear/model_autoencoder_AE\n",
      "Folder already exists at: /home/rmunoz/Escritorio/rmunozTMELab/Physically-Guided-Machine-Learning/results/non_linear/model_autoencoder_NN\n"
     ]
    }
   ],
   "source": [
    "# Creamos los paths para las distintas carpetas\n",
    "ROOT_PATH = r'/home/rmunoz/Escritorio/rmunozTMELab/Physically-Guided-Machine-Learning'\n",
    "DATA_PATH = os.path.join(ROOT_PATH, r'data/non_linear/non_linear_decomposition.pkl')\n",
    "RESULTS_FOLDER_PATH = os.path.join(ROOT_PATH, r'results/non_linear')\n",
    "MODEL_RESULTS_AE_PATH = os.path.join(ROOT_PATH, r'results/non_linear/model_autoencoder_AE')\n",
    "MODEL_RESULTS_PGNNIV_PATH = os.path.join(ROOT_PATH, r'results/non_linear/model_autoencoder_NN')\n",
    "\n",
    "\n",
    "# Creamos las carpetas que sean necesarias (si ya están creadas se avisará de ello)\n",
    "create_folder(RESULTS_FOLDER_PATH)\n",
    "create_folder(MODEL_RESULTS_AE_PATH)\n",
    "create_folder(MODEL_RESULTS_PGNNIV_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully loaded from: /home/rmunoz/Escritorio/rmunozTMELab/Physically-Guided-Machine-Learning/data/non_linear/non_linear_decomposition.pkl\n"
     ]
    }
   ],
   "source": [
    "# Load dataset\n",
    "dataset = load_data(DATA_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convolutional filters to derivate\n",
    "dx = dataset['x_step_size']\n",
    "dy = dataset['y_step_size']\n",
    "D = DerivativeKernels(dx, dy, 0).grad_kernels_two_dimensions()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## División de los datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset length: 8000\n",
      "Validation dataset length: 2000\n"
     ]
    }
   ],
   "source": [
    "X_train = torch.Tensor(dataset['X_train']).unsqueeze(1)\n",
    "y_train = torch.Tensor(dataset['y_train']).unsqueeze(1)\n",
    "K_train = torch.tensor(dataset['k_train']).unsqueeze(1)\n",
    "f_train = torch.tensor(dataset['f_train']).unsqueeze(1).to(torch.float32)\n",
    "\n",
    "X_val = torch.Tensor(dataset['X_val']).unsqueeze(1)\n",
    "y_val = TensOps(torch.Tensor(dataset['y_val']).unsqueeze(1).requires_grad_(True), space_dimension=2, contravariance=0, covariance=0)\n",
    "K_val = TensOps(torch.tensor(dataset['k_val']).unsqueeze(1).requires_grad_(True), space_dimension=2, contravariance=0, covariance=0)\n",
    "f_val = TensOps(torch.tensor(dataset['f_val']).to(torch.float32).unsqueeze(1).requires_grad_(True), space_dimension=2, contravariance=0, covariance=0)\n",
    "\n",
    "print(\"Train dataset length:\", len(X_train))\n",
    "print(\"Validation dataset length:\", len(X_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset length for the autoencoder: 2000\n",
      "Dataset length for the PGNNIV: 6000\n"
     ]
    }
   ],
   "source": [
    "N_data_AE = len(X_train)//4\n",
    "N_data_NN = len(X_train) - len(X_train)//4\n",
    "prop_data_NN = 1 - N_data_AE/(N_data_NN + N_data_AE)\n",
    "\n",
    "print(\"Dataset length for the autoencoder:\", N_data_AE)\n",
    "print(\"Dataset length for the PGNNIV:\", N_data_NN)\n",
    "\n",
    "X_AE, X_NN, y_AE, y_NN, K_AE, K_NN, f_AE, f_NN = train_test_split(X_train, y_train, K_train, f_train, test_size=prop_data_NN, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Datos para el autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_AE, y_test_AE = train_test_split(y_AE, test_size=0.2, random_state=42)\n",
    "\n",
    "y_train_AE = TensOps(y_train_AE.requires_grad_(True).to(DEVICE), space_dimension=2, contravariance=0, covariance=0)\n",
    "y_test_AE = TensOps(y_test_AE.requires_grad_(True).to(DEVICE), space_dimension=2, contravariance=0, covariance=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Datos para la PGNNIV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_NN, X_test_NN, y_train_NN, y_test_NN, K_train_NN, K_test_NN, f_train_NN, f_test_NN = train_test_split(X_NN, y_NN, K_NN, f_NN, test_size=0.2, random_state=42)\n",
    "\n",
    "X_train_NN = X_train_NN.to(DEVICE)\n",
    "X_test_NN = X_test_NN.to(DEVICE)\n",
    "\n",
    "y_train_NN = TensOps(y_train_NN.requires_grad_(True).to(DEVICE), space_dimension=2, contravariance=0, covariance=0)\n",
    "y_test_NN = TensOps(y_test_NN.requires_grad_(True).to(DEVICE), space_dimension=2, contravariance=0, covariance=0)\n",
    "\n",
    "K_train_NN = TensOps(K_train_NN.to(DEVICE), space_dimension=2, contravariance=0, covariance=0)\n",
    "K_test_NN = TensOps(K_test_NN.to(DEVICE), space_dimension=2, contravariance=0, covariance=0)\n",
    "\n",
    "f_train_NN = TensOps(f_train_NN.to(DEVICE), space_dimension=2, contravariance=0, covariance=0)\n",
    "f_test_NN = TensOps(f_test_NN.to(DEVICE), space_dimension=2, contravariance=0, covariance=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models import Autoencoder\n",
    "from trainers.eval import loss_function_autoencoder\n",
    "from utils.checkpoints import load_checkpoint, save_checkpoint\n",
    "from utils.checkpoints import load_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_autoencoder_epoch(model, optimizer, X_train, y_train):\n",
    "    model.train()\n",
    "    y_pred = model(X_train)\n",
    "    loss = loss_function_autoencoder(y_train, y_pred)\n",
    "\n",
    "    optimizer.zero_grad() \n",
    "    loss.backward(retain_graph=True)\n",
    "    optimizer.step()\n",
    "    \n",
    "    return loss\n",
    "\n",
    "\n",
    "def test_autoencoder_epoch(model, X_test, y_test):\n",
    "    y_pred = model(X_test)\n",
    "    loss = loss_function_autoencoder(y_test, y_pred)\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def autoencoder_train_loop(model, optimizer, X_train, y_train, X_test, y_test, start_epoch, n_epochs, batch_size, i_checkpoint, model_results_path, device, lr_updated=None):\n",
    "\n",
    "    if start_epoch > 0:\n",
    "        print(f'Starting training from a checkpoint. Epoch {start_epoch}.')\n",
    "\n",
    "        resume_epoch = start_epoch\n",
    "        model, optimizer, lists = load_checkpoint(model, optimizer, resume_epoch, model_results_path)\n",
    "        train_total_loss_list = lists['train_total_loss_list']\n",
    "        test_total_loss_list = lists['test_total_loss_list']\n",
    "\n",
    "        if lr_updated != None:\n",
    "            for param_group in optimizer.param_groups:\n",
    "                param_group['lr'] = lr_updated\n",
    "\n",
    "    else:\n",
    "        print(\"Starting training from the beginning\")\n",
    "\n",
    "        train_total_loss_list = []\n",
    "        test_total_loss_list = []\n",
    "\n",
    "    N_train = X_train.shape[0]\n",
    "    N_test = X_test.shape[0]\n",
    "\n",
    "    for epoch_i in range(start_epoch, n_epochs):\n",
    "\n",
    "        for batch_start in range(0, N_train, batch_size):\n",
    "            X_batch = X_train[batch_start:(batch_start+batch_size)].to(device)\n",
    "            y_batch = TensOps(y_train.values[batch_start:(batch_start+batch_size)].to(device), space_dimension=y_train.space_dim, contravariance=y_train.order[0], covariance=y_train.order[1])\n",
    "\n",
    "            loss_train = train_autoencoder_epoch(model, optimizer, X_batch, y_batch).item()\n",
    "            loss_test = test_autoencoder_epoch(model, X_test, y_test).item()\n",
    "\n",
    "        train_total_loss_list.append(loss_train/batch_size)\n",
    "        test_total_loss_list.append(loss_test/N_test)\n",
    "\n",
    "        if epoch_i % (1 if n_epochs < 100 else (10 if n_epochs <= 1000 else 1000)) == 0:\n",
    "            print(f'Epoch {epoch_i}, Train loss: {loss_train/batch_size:.3e}, Test loss: {loss_test/N_test:.3e}')\n",
    "\n",
    "        if epoch_i % (i_checkpoint) == 0:\n",
    "            save_checkpoint(model, optimizer, epoch_i, model_results_path, train_total_loss_list=train_total_loss_list, test_total_loss_list=test_total_loss_list)\n",
    "\n",
    "    save_checkpoint(model, optimizer, epoch_i, model_results_path, end_flag=True, train_total_loss_list=train_total_loss_list, test_total_loss_list=test_total_loss_list)   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "autoencoder_input_shape = y_train_AE.values[0].shape\n",
    "latent_space_dim = 20\n",
    "autoencoder_output_shape = y_train_AE.values[0].shape\n",
    "\n",
    "start_epoch = 0\n",
    "n_epochs = 100000\n",
    "batch_size = 64\n",
    "i_checkpoint = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = y_train_AE.values\n",
    "y_train = y_train_AE\n",
    "\n",
    "X_test = y_test_AE.values\n",
    "y_test = y_test_AE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = Autoencoder(input_size=autoencoder_input_shape, encoding_dim=latent_space_dim, output_size=autoencoder_output_shape, device=DEVICE)\n",
    "# optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "\n",
    "# autoencoder_train_loop(model, optimizer, X_train, y_train, X_test, y_test,  \n",
    "#                        start_epoch, n_epochs, batch_size, i_checkpoint, MODEL_RESULTS_AE_PATH, DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start_epoch = 90000\n",
    "# n_epochs = 150000\n",
    "# batch_size = 64\n",
    "# i_checkpoint = 10000\n",
    "# new_lr = 5e-5\n",
    "\n",
    "# autoencoder_train_loop(model, optimizer, X_train, y_train, X_test, y_test,  \n",
    "#                        start_epoch, n_epochs, batch_size, i_checkpoint, MODEL_RESULTS_AE_PATH, DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "autoencoder = Autoencoder(input_size=autoencoder_input_shape, encoding_dim=latent_space_dim, output_size=autoencoder_output_shape, device=DEVICE)\n",
    "optimizer = torch.optim.Adam(autoencoder.parameters(), lr=1e-4)\n",
    "\n",
    "autoencoder, optimizer, lists = load_results(autoencoder, optimizer, MODEL_RESULTS_AE_PATH, map_location=torch.device('cpu'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = autoencoder.encoder\n",
    "decoder = autoencoder.decoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Red neuronal general"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_input_shape = X_train_NN[0].shape\n",
    "latent_space_dim = 20\n",
    "nn_output_shape = y_train_NN.values[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from vecopsciml.utils import TensOps\n",
    "from vecopsciml.operators.zero_order import Mx, My\n",
    "\n",
    "class HiddenStatePGNNIV(nn.Module):\n",
    "    def __init__(self, input_size, latent_space_output, explanatory_output_size, decoder_model, device, **kwargs):\n",
    "        super(HiddenStatePGNNIV, self).__init__()\n",
    "\n",
    "        self.input = input_size\n",
    "        self.latent_space_output = latent_space_output\n",
    "        self.output_expl = explanatory_output_size\n",
    "\n",
    "        self.hidden_units_pred = 10\n",
    "        self.hidden_units_exp = 15\n",
    "        self.filters_exp = 10\n",
    "\n",
    "        self.decoder = decoder_model\n",
    "\n",
    "        self.device = device\n",
    "\n",
    "        # Predictive network\n",
    "        self.flatten_layer_pred = nn.Flatten(start_dim=1, end_dim=-1)\n",
    "        self.hidden1_layer_pred = nn.Linear(torch.prod(torch.tensor(self.input, device=self.device)), self.hidden_units_pred).to(self.device)\n",
    "        self.hidden2_layer_pred = nn.Linear(self.hidden_units_pred, self.hidden_units_pred).to(self.device)\n",
    "        self.output_layer_pred = nn.Linear(self.hidden_units_pred, self.latent_space_output).to(self.device)\n",
    "\n",
    "        # Explanatory network (commented out since they are not used in forward method)\n",
    "        self.conv1_exp = nn.Conv2d(in_channels=1, out_channels=self.filters_exp, kernel_size=1).to(self.device)\n",
    "        self.flatten_layer_exp = nn.Flatten().to(self.device)\n",
    "        self.hidden1_layer_exp = nn.LazyLinear(self.hidden_units_exp).to(self.device)\n",
    "        self.hidden2_layer_exp = nn.Linear(self.hidden_units_exp, self.hidden_units_exp).to(self.device)\n",
    "        self.output_layer_exp = nn.Linear(self.hidden_units_exp, self.filters_exp * (self.output_expl[1] - 1) * (self.output_expl[2] - 1)).to(self.device)\n",
    "        self.conv2_exp = nn.Conv2d(in_channels=self.filters_exp, out_channels=1, kernel_size=1).to(self.device)\n",
    "\n",
    "    def forward(self, X):\n",
    "\n",
    "        X = X.to(self.device)\n",
    "\n",
    "        # Predictive network\n",
    "        X = self.flatten_layer_pred(X)\n",
    "        X = torch.sigmoid(self.hidden1_layer_pred(X))\n",
    "        X = torch.sigmoid(self.hidden2_layer_pred(X))\n",
    "        output_predictive_net = self.output_layer_pred(X)\n",
    "\n",
    "        u_pred = decoder(output_predictive_net)\n",
    "        um_pred = My(Mx(TensOps(u_pred, space_dimension=2, contravariance=0, covariance=0))).values\n",
    "\n",
    "        x = torch.sigmoid(self.conv1_exp(um_pred))\n",
    "        x = self.flatten_layer_exp(x)\n",
    "        x = torch.sigmoid(self.hidden1_layer_exp(x))\n",
    "        x = torch.sigmoid(self.hidden2_layer_exp(x))\n",
    "        x = self.output_layer_exp(x)\n",
    "        x = x.view(x.size(0), self.filters_exp, self.output_expl[1] - 1, self.output_expl[2] - 1)\n",
    "        K_pred = self.conv2_exp(x)\n",
    "\n",
    "        return u_pred, K_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training\n",
      "Starting from a checkpoint. Epoch 900.\n",
      "Epoch 900, Train loss: 6.861e+03, Test loss: 1.137e+04, MSE(e): 6.625e-04, MSE(pi1): 1.131e-02, MSE(pi2): 5.967e-04, MSE(pi3): 1.228e-03\n",
      "Epoch 1000, Train loss: 5.060e+03, Test loss: 8.480e+03, MSE(e): 4.831e-04, MSE(pi1): 1.064e-02, MSE(pi2): 4.208e-04, MSE(pi3): 1.223e-03\n",
      "Epoch 1100, Train loss: 3.830e+03, Test loss: 6.656e+03, MSE(e): 3.614e-04, MSE(pi1): 9.455e-03, MSE(pi2): 3.006e-04, MSE(pi3): 1.215e-03\n",
      "Epoch 1200, Train loss: 3.190e+03, Test loss: 5.500e+03, MSE(e): 2.985e-04, MSE(pi1): 8.377e-03, MSE(pi2): 2.369e-04, MSE(pi3): 1.210e-03\n",
      "Epoch 1300, Train loss: 2.741e+03, Test loss: 4.441e+03, MSE(e): 2.544e-04, MSE(pi1): 8.266e-03, MSE(pi2): 2.003e-04, MSE(pi3): 1.148e-03\n",
      "Epoch 1400, Train loss: 2.433e+03, Test loss: 3.924e+03, MSE(e): 2.241e-04, MSE(pi1): 8.217e-03, MSE(pi2): 1.783e-04, MSE(pi3): 1.106e-03\n",
      "Epoch 1500, Train loss: 2.230e+03, Test loss: 3.523e+03, MSE(e): 2.037e-04, MSE(pi1): 8.695e-03, MSE(pi2): 1.632e-04, MSE(pi3): 1.061e-03\n",
      "Epoch 1600, Train loss: 2.067e+03, Test loss: 3.216e+03, MSE(e): 1.897e-04, MSE(pi1): 6.511e-03, MSE(pi2): 1.520e-04, MSE(pi3): 1.045e-03\n",
      "Epoch 1700, Train loss: 1.961e+03, Test loss: 2.973e+03, MSE(e): 1.796e-04, MSE(pi1): 6.579e-03, MSE(pi2): 1.434e-04, MSE(pi3): 9.937e-04\n",
      "Epoch 1800, Train loss: 1.884e+03, Test loss: 2.779e+03, MSE(e): 1.725e-04, MSE(pi1): 6.162e-03, MSE(pi2): 1.366e-04, MSE(pi3): 9.755e-04\n",
      "Epoch 1900, Train loss: 1.840e+03, Test loss: 2.622e+03, MSE(e): 1.685e-04, MSE(pi1): 5.959e-03, MSE(pi2): 1.318e-04, MSE(pi3): 9.531e-04\n",
      "Epoch 2000, Train loss: 1.817e+03, Test loss: 2.496e+03, MSE(e): 1.664e-04, MSE(pi1): 5.891e-03, MSE(pi2): 1.281e-04, MSE(pi3): 9.444e-04\n",
      "Epoch 2100, Train loss: 1.809e+03, Test loss: 2.396e+03, MSE(e): 1.658e-04, MSE(pi1): 5.913e-03, MSE(pi2): 1.257e-04, MSE(pi3): 9.146e-04\n",
      "Epoch 2200, Train loss: 1.808e+03, Test loss: 2.328e+03, MSE(e): 1.660e-04, MSE(pi1): 5.753e-03, MSE(pi2): 1.239e-04, MSE(pi3): 9.020e-04\n",
      "Epoch 2300, Train loss: 1.836e+03, Test loss: 2.472e+03, MSE(e): 1.689e-04, MSE(pi1): 5.786e-03, MSE(pi2): 1.243e-04, MSE(pi3): 8.890e-04\n",
      "Epoch 2400, Train loss: 1.840e+03, Test loss: 2.478e+03, MSE(e): 1.687e-04, MSE(pi1): 6.828e-03, MSE(pi2): 1.230e-04, MSE(pi3): 8.497e-04\n",
      "Epoch 2500, Train loss: 1.420e+03, Test loss: 2.889e+03, MSE(e): 1.266e-04, MSE(pi1): 6.232e-03, MSE(pi2): 1.028e-04, MSE(pi3): 9.110e-04\n",
      "Epoch 2600, Train loss: 1.368e+03, Test loss: 2.779e+03, MSE(e): 1.199e-04, MSE(pi1): 7.717e-03, MSE(pi2): 9.833e-05, MSE(pi3): 9.162e-04\n",
      "Epoch 2700, Train loss: 1.295e+03, Test loss: 2.669e+03, MSE(e): 1.142e-04, MSE(pi1): 6.108e-03, MSE(pi2): 9.448e-05, MSE(pi3): 9.098e-04\n",
      "Epoch 2800, Train loss: 1.250e+03, Test loss: 2.555e+03, MSE(e): 1.099e-04, MSE(pi1): 5.988e-03, MSE(pi2): 9.123e-05, MSE(pi3): 9.098e-04\n",
      "Epoch 2900, Train loss: 1.212e+03, Test loss: 2.431e+03, MSE(e): 1.062e-04, MSE(pi1): 5.839e-03, MSE(pi2): 8.831e-05, MSE(pi3): 9.099e-04\n",
      "Epoch 3000, Train loss: 1.181e+03, Test loss: 2.240e+03, MSE(e): 1.031e-04, MSE(pi1): 5.813e-03, MSE(pi2): 8.565e-05, MSE(pi3): 9.170e-04\n",
      "Epoch 3100, Train loss: 1.172e+03, Test loss: 2.113e+03, MSE(e): 1.021e-04, MSE(pi1): 5.732e-03, MSE(pi2): 8.395e-05, MSE(pi3): 9.348e-04\n",
      "Epoch 3200, Train loss: 1.183e+03, Test loss: 2.006e+03, MSE(e): 1.034e-04, MSE(pi1): 5.546e-03, MSE(pi2): 8.327e-05, MSE(pi3): 9.386e-04\n",
      "Epoch 3300, Train loss: 1.240e+03, Test loss: 1.852e+03, MSE(e): 1.089e-04, MSE(pi1): 5.552e-03, MSE(pi2): 8.437e-05, MSE(pi3): 9.536e-04\n",
      "Epoch 3400, Train loss: 1.273e+03, Test loss: 1.748e+03, MSE(e): 1.124e-04, MSE(pi1): 5.300e-03, MSE(pi2): 8.478e-05, MSE(pi3): 9.583e-04\n",
      "Epoch 3500, Train loss: 1.276e+03, Test loss: 1.690e+03, MSE(e): 1.127e-04, MSE(pi1): 5.256e-03, MSE(pi2): 8.395e-05, MSE(pi3): 9.617e-04\n",
      "Epoch 3600, Train loss: 1.265e+03, Test loss: 1.647e+03, MSE(e): 1.120e-04, MSE(pi1): 4.931e-03, MSE(pi2): 8.270e-05, MSE(pi3): 9.557e-04\n"
     ]
    }
   ],
   "source": [
    "# Se carga el modelo y el optimizador\n",
    "pgnniv_model = HiddenStatePGNNIV(input_size=nn_input_shape, latent_space_output=latent_space_dim, explanatory_output_size=nn_output_shape, decoder_model=decoder, device=DEVICE)\n",
    "optimizer = torch.optim.Adam(pgnniv_model.parameters(), lr=1e-4)\n",
    "\n",
    "# Parametros de entrenamiento\n",
    "start_epoch = 900\n",
    "n_epochs = 10000\n",
    "\n",
    "batch_size = 64\n",
    "n_checkpoints = 1000\n",
    "\n",
    "train_loop(pgnniv_model, optimizer, n_checkpoints,\n",
    "           X_train_NN, y_train_NN, X_test_NN, y_test_NN, f_train_NN, f_test_NN,\n",
    "           D=D, start_epoch=start_epoch, n_epochs=n_epochs, batch_size=batch_size, \n",
    "           model_results_path=MODEL_RESULTS_PGNNIV_PATH, device=DEVICE,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SciML_test_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
