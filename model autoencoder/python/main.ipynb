{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from collections import Counter\n",
    "\n",
    "# Imports de la libreria propia\n",
    "from vecopsciml.kernels.derivative import DerivativeKernels\n",
    "from vecopsciml.utils import TensOps\n",
    "\n",
    "# Imports de las funciones creadas para este programa\n",
    "from utils.folders import create_folder\n",
    "from utils.load_data import load_data\n",
    "from trainers.train import train_loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Folder already exists at: /home/rmunoz/Escritorio/rmunozTMELab/Physically-Guided-Machine-Learning/results/non_linear\n",
      "Folder already exists at: /home/rmunoz/Escritorio/rmunozTMELab/Physically-Guided-Machine-Learning/results/non_linear/model_autoencoder\n"
     ]
    }
   ],
   "source": [
    "# Creamos los paths para las distintas carpetas\n",
    "ROOT_PATH = r'/home/rmunoz/Escritorio/rmunozTMELab/Physically-Guided-Machine-Learning'\n",
    "DATA_PATH = os.path.join(ROOT_PATH, r'data/non_linear/non_linear_decomposition.pkl')\n",
    "RESULTS_FOLDER_PATH = os.path.join(ROOT_PATH, r'results/non_linear')\n",
    "MODEL_RESULTS_PATH = os.path.join(ROOT_PATH, r'results/non_linear/model_autoencoder')\n",
    "\n",
    "# Creamos las carpetas que sean necesarias (si ya están creadas se avisará de ello)\n",
    "create_folder(RESULTS_FOLDER_PATH)\n",
    "create_folder(MODEL_RESULTS_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully loaded from: /home/rmunoz/Escritorio/rmunozTMELab/Physically-Guided-Machine-Learning/data/non_linear/non_linear_decomposition.pkl\n"
     ]
    }
   ],
   "source": [
    "# Load dataset\n",
    "dataset = load_data(DATA_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convolutional filters to derivate\n",
    "dx = dataset['x_step_size']\n",
    "dy = dataset['y_step_size']\n",
    "D = DerivativeKernels(dx, dy, 0).grad_kernels_two_dimensions()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## División de los datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset length: 8000\n",
      "Validation dataset length: 2000\n"
     ]
    }
   ],
   "source": [
    "X_train = torch.Tensor(dataset['X_train']).unsqueeze(1)\n",
    "y_train = torch.Tensor(dataset['y_train']).unsqueeze(1)\n",
    "K_train = torch.tensor(dataset['k_train']).unsqueeze(1)\n",
    "f_train = torch.tensor(dataset['f_train']).unsqueeze(1)\n",
    "\n",
    "X_val = torch.Tensor(dataset['X_val']).unsqueeze(1)\n",
    "y_val = TensOps(torch.Tensor(dataset['y_val']).unsqueeze(1).requires_grad_(True), space_dimension=2, contravariance=0, covariance=0)\n",
    "K_val = TensOps(torch.tensor(dataset['k_val']).unsqueeze(1).requires_grad_(True), space_dimension=2, contravariance=0, covariance=0)\n",
    "f_val = TensOps(torch.tensor(dataset['f_val']).unsqueeze(1).requires_grad_(True), space_dimension=2, contravariance=0, covariance=0)\n",
    "\n",
    "print(\"Train dataset length:\", len(X_train))\n",
    "print(\"Validation dataset length:\", len(X_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset length for the autoencoder: 2000\n",
      "Dataset length for the PGNNIV: 6000\n"
     ]
    }
   ],
   "source": [
    "N_data_AE = len(X_train)//4\n",
    "N_data_NN = len(X_train) - len(X_train)//4\n",
    "prop_data_NN = 1 - N_data_AE/(N_data_NN + N_data_AE)\n",
    "\n",
    "print(\"Dataset length for the autoencoder:\", N_data_AE)\n",
    "print(\"Dataset length for the PGNNIV:\", N_data_NN)\n",
    "\n",
    "X_AE, X_NN, y_AE, y_NN, K_AE, K_NN, f_AE, f_NN = train_test_split(X_train, y_train, K_train, f_train, test_size=prop_data_NN, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Datos para el autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_AE, y_test_AE = train_test_split(y_AE, test_size=0.2, random_state=42)\n",
    "\n",
    "y_train = TensOps(y_train_AE.requires_grad_(True).to(device), space_dimension=2, contravariance=0, covariance=0)\n",
    "y_test = TensOps(y_test_AE.requires_grad_(True).to(device), space_dimension=2, contravariance=0, covariance=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Datos para la PGNNIV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_NN, X_test_NN, y_train_NN, y_test_NN, K_train_NN, K_test_NN, f_train_NN, f_test_NN = train_test_split(X_NN, y_NN, K_NN, f_NN, test_size=0.2, random_state=42)\n",
    "\n",
    "X_train = X_train_NN.to(device)\n",
    "X_test = X_test_NN.to(device)\n",
    "\n",
    "y_train = TensOps(y_train_NN.requires_grad_(True).to(device), space_dimension=2, contravariance=0, covariance=0)\n",
    "y_test = TensOps(y_test_NN.requires_grad_(True).to(device), space_dimension=2, contravariance=0, covariance=0)\n",
    "\n",
    "K_train = TensOps(K_train_NN.to(device), space_dimension=2, contravariance=0, covariance=0)\n",
    "K_test = TensOps(K_test_NN.to(device), space_dimension=2, contravariance=0, covariance=0)\n",
    "\n",
    "f_train = TensOps(f_train_NN.to(device), space_dimension=2, contravariance=0, covariance=0)\n",
    "f_test = TensOps(f_test_NN.to(device), space_dimension=2, contravariance=0, covariance=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models import Autoencoder\n",
    "from trainers.eval import loss_function_autoencoder\n",
    "from utils.checkpoints import load_checkpoint, save_checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "autoencoder_shape = y_train.values[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_autoencoder_epoch(model, optimizer, X_train, y_train):\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    y_pred = model(X_train)\n",
    "\n",
    "    loss = loss_function_autoencoder(y_train, y_pred)\n",
    "\n",
    "    optimizer.zero_grad() \n",
    "    loss.backward(retain_graph=True)\n",
    "    optimizer.step()\n",
    "\n",
    "    return loss\n",
    "\n",
    "\n",
    "def test_autoencoder_epoch(model, X_test, y_test):\n",
    "\n",
    "    y_pred = model(X_test)\n",
    "    loss = loss_function_autoencoder(y_test, y_pred)\n",
    "\n",
    "    return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "def autoencoder_train_loop(model, optimizer, y_train, y_test, start_epoch, n_epochs, batch_size, model_results_path, device, lr_updated=None):\n",
    "\n",
    "    print(\"Start training\")\n",
    "\n",
    "    if start_epoch > 0:\n",
    "        \n",
    "        print(f'Starting from a checkpoint. Epoch {start_epoch}.')\n",
    "        resume_epoch = start_epoch\n",
    "        model, optimizer, lists = load_checkpoint(model, optimizer, resume_epoch, model_results_path)\n",
    "\n",
    "        if lr_updated != None:\n",
    "            for param_group in optimizer.param_groups:\n",
    "                param_group['lr'] = lr_updated\n",
    "\n",
    "        train_total_loss_list = lists['train_total_loss_list']\n",
    "        train_e_loss_list = lists['train_e_loss_list']\n",
    "        train_pi1_loss_list = lists['train_pi1_loss_list']\n",
    "        train_pi2_loss_list = lists['train_pi2_loss_list']\n",
    "        train_pi3_loss_list = lists['train_pi3_loss_list']\n",
    "\n",
    "        test_total_loss_list = lists['test_total_loss_list']\n",
    "        test_e_loss_list = lists['test_e_loss_list']\n",
    "        test_pi1_loss_list = lists['test_pi1_loss_list']\n",
    "        test_pi2_loss_list = lists['test_pi2_loss_list']\n",
    "        test_pi3_loss_list = lists['test_pi3_loss_list']\n",
    "\n",
    "    else:\n",
    "\n",
    "        train_total_loss_list = []\n",
    "        train_total_MSE_list = []\n",
    "        train_e_loss_list = []\n",
    "        train_pi1_loss_list = []\n",
    "        train_pi2_loss_list = []\n",
    "        train_pi3_loss_list = []\n",
    "\n",
    "        test_total_loss_list = []\n",
    "        test_total_MSE_list = []\n",
    "        test_e_loss_list = []\n",
    "        test_pi1_loss_list = []\n",
    "        test_pi2_loss_list = []\n",
    "        test_pi3_loss_list = []\n",
    "    \n",
    "    X_train = y_train.values\n",
    "    X_test = y_test.values\n",
    "    \n",
    "    N_train = X_train.shape[0]\n",
    "    N_test = X_test.shape[0]\n",
    "\n",
    "    for epoch_i in range(start_epoch, n_epochs):\n",
    "        for batch_start in range(0, N_train, batch_size):\n",
    "\n",
    "            X_batch = X_train[batch_start:(batch_start+batch_size)].to(device)\n",
    "            y_batch = TensOps(y_train.values[batch_start:(batch_start+batch_size)].to(device), space_dimension=y_train.space_dim, contravariance=y_train.order[0], covariance=y_train.order[1])\n",
    "\n",
    "        loss = train_autoencoder_epoch(model, optimizer, X_batch, y_batch)\n",
    "\n",
    "        train_total_loss_list.append((loss.item()/batch_size))\n",
    "        \n",
    "        loss_test= test_autoencoder_epoch(model, X_test, y_test)\n",
    "\n",
    "        test_total_loss_list.append((loss_test.item()/N_test))\n",
    "\n",
    "        if epoch_i % (1 if n_epochs < 100 else (10 if n_epochs <= 1000 else 100)) == 0:\n",
    "            print(f'Epoch {epoch_i}, Train loss: {loss.item()/batch_size:.3e}, Test loss: {loss_test.item()/N_test:.3e}')\n",
    "\n",
    "        if epoch_i % (int(n_epochs/10)) == 0:\n",
    "            save_checkpoint(model, optimizer, epoch_i, model_results_path, \n",
    "                            train_total_loss_list=train_total_loss_list, train_e_loss_list=train_e_loss_list, \n",
    "                            train_pi1_loss_list=train_pi1_loss_list, train_pi2_loss_list=train_pi2_loss_list, train_pi3_loss_list=train_pi3_loss_list,\n",
    "                            test_total_loss_list=test_total_loss_list, test_e_loss_list=test_e_loss_list,\n",
    "                            test_pi1_loss_list=test_pi1_loss_list, test_pi2_loss_list=test_pi2_loss_list, test_pi3_loss_list=test_pi3_loss_list\n",
    "                            )\n",
    "\n",
    "    print(\"\\nProceso finalizado después de\", n_epochs, \"épocas\\n\")\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training\n",
      "Epoch 0, Train loss: 1.056e+02, Test loss: 1.266e+02\n",
      "Epoch 100, Train loss: 6.187e+01, Test loss: 7.721e+01\n",
      "Epoch 200, Train loss: 3.295e+01, Test loss: 4.318e+01\n",
      "Epoch 300, Train loss: 1.572e+01, Test loss: 2.166e+01\n",
      "Epoch 400, Train loss: 7.636e+00, Test loss: 1.057e+01\n",
      "Epoch 500, Train loss: 4.708e+00, Test loss: 6.028e+00\n",
      "Epoch 600, Train loss: 3.789e+00, Test loss: 4.396e+00\n",
      "Epoch 700, Train loss: 3.516e+00, Test loss: 3.832e+00\n",
      "Epoch 800, Train loss: 3.434e+00, Test loss: 3.633e+00\n",
      "Epoch 900, Train loss: 3.403e+00, Test loss: 3.556e+00\n",
      "Epoch 1000, Train loss: 3.386e+00, Test loss: 3.519e+00\n",
      "Epoch 1100, Train loss: 3.372e+00, Test loss: 3.493e+00\n",
      "Epoch 1200, Train loss: 3.360e+00, Test loss: 3.471e+00\n",
      "Epoch 1300, Train loss: 3.346e+00, Test loss: 3.450e+00\n",
      "Epoch 1400, Train loss: 3.330e+00, Test loss: 3.429e+00\n",
      "Epoch 1500, Train loss: 3.311e+00, Test loss: 3.405e+00\n",
      "Epoch 1600, Train loss: 3.287e+00, Test loss: 3.377e+00\n",
      "Epoch 1700, Train loss: 3.257e+00, Test loss: 3.342e+00\n",
      "Epoch 1800, Train loss: 3.219e+00, Test loss: 3.296e+00\n",
      "Epoch 1900, Train loss: 3.171e+00, Test loss: 3.238e+00\n",
      "Epoch 2000, Train loss: 3.113e+00, Test loss: 3.165e+00\n",
      "Epoch 2100, Train loss: 3.042e+00, Test loss: 3.076e+00\n",
      "Epoch 2200, Train loss: 2.961e+00, Test loss: 2.972e+00\n",
      "Epoch 2300, Train loss: 2.871e+00, Test loss: 2.856e+00\n",
      "Epoch 2400, Train loss: 2.775e+00, Test loss: 2.734e+00\n",
      "Epoch 2500, Train loss: 2.676e+00, Test loss: 2.611e+00\n",
      "Epoch 2600, Train loss: 2.580e+00, Test loss: 2.495e+00\n",
      "Epoch 2700, Train loss: 2.490e+00, Test loss: 2.390e+00\n",
      "Epoch 2800, Train loss: 2.408e+00, Test loss: 2.297e+00\n",
      "Epoch 2900, Train loss: 2.334e+00, Test loss: 2.218e+00\n",
      "Epoch 3000, Train loss: 2.268e+00, Test loss: 2.151e+00\n",
      "Epoch 3100, Train loss: 2.209e+00, Test loss: 2.093e+00\n",
      "Epoch 3200, Train loss: 2.155e+00, Test loss: 2.043e+00\n",
      "Epoch 3300, Train loss: 2.105e+00, Test loss: 1.999e+00\n",
      "Epoch 3400, Train loss: 2.058e+00, Test loss: 1.958e+00\n",
      "Epoch 3500, Train loss: 2.013e+00, Test loss: 1.920e+00\n",
      "Epoch 3600, Train loss: 1.968e+00, Test loss: 1.882e+00\n",
      "Epoch 3700, Train loss: 1.923e+00, Test loss: 1.844e+00\n",
      "Epoch 3800, Train loss: 1.878e+00, Test loss: 1.805e+00\n",
      "Epoch 3900, Train loss: 1.829e+00, Test loss: 1.762e+00\n",
      "Epoch 4000, Train loss: 1.777e+00, Test loss: 1.715e+00\n",
      "Epoch 4100, Train loss: 1.722e+00, Test loss: 1.664e+00\n",
      "Epoch 4200, Train loss: 1.664e+00, Test loss: 1.608e+00\n",
      "Epoch 4300, Train loss: 1.604e+00, Test loss: 1.549e+00\n",
      "Epoch 4400, Train loss: 1.541e+00, Test loss: 1.486e+00\n",
      "Epoch 4500, Train loss: 1.476e+00, Test loss: 1.418e+00\n",
      "Epoch 4600, Train loss: 1.409e+00, Test loss: 1.346e+00\n",
      "Epoch 4700, Train loss: 1.339e+00, Test loss: 1.271e+00\n",
      "Epoch 4800, Train loss: 1.269e+00, Test loss: 1.194e+00\n",
      "Epoch 4900, Train loss: 1.198e+00, Test loss: 1.116e+00\n",
      "Epoch 5000, Train loss: 1.127e+00, Test loss: 1.038e+00\n",
      "Epoch 5100, Train loss: 1.058e+00, Test loss: 9.627e-01\n",
      "Epoch 5200, Train loss: 9.910e-01, Test loss: 8.896e-01\n",
      "Epoch 5300, Train loss: 9.263e-01, Test loss: 8.195e-01\n",
      "Epoch 5400, Train loss: 8.659e-01, Test loss: 7.536e-01\n",
      "Epoch 5500, Train loss: 8.116e-01, Test loss: 6.940e-01\n",
      "Epoch 5600, Train loss: 7.645e-01, Test loss: 6.421e-01\n",
      "Epoch 5700, Train loss: 7.246e-01, Test loss: 5.981e-01\n",
      "Epoch 5800, Train loss: 6.915e-01, Test loss: 5.620e-01\n",
      "Epoch 5900, Train loss: 6.644e-01, Test loss: 5.332e-01\n",
      "Epoch 6000, Train loss: 6.423e-01, Test loss: 5.095e-01\n",
      "Epoch 6100, Train loss: 6.244e-01, Test loss: 4.909e-01\n",
      "Epoch 6200, Train loss: 6.098e-01, Test loss: 4.769e-01\n",
      "Epoch 6300, Train loss: 5.975e-01, Test loss: 4.650e-01\n",
      "Epoch 6400, Train loss: 5.870e-01, Test loss: 4.568e-01\n",
      "Epoch 6500, Train loss: 5.775e-01, Test loss: 4.494e-01\n",
      "Epoch 6600, Train loss: 5.687e-01, Test loss: 4.429e-01\n",
      "Epoch 6700, Train loss: 5.599e-01, Test loss: 4.367e-01\n",
      "Epoch 6800, Train loss: 5.505e-01, Test loss: 4.305e-01\n",
      "Epoch 6900, Train loss: 5.400e-01, Test loss: 4.230e-01\n",
      "Epoch 7000, Train loss: 5.273e-01, Test loss: 4.140e-01\n",
      "Epoch 7100, Train loss: 5.114e-01, Test loss: 4.026e-01\n",
      "Epoch 7200, Train loss: 4.907e-01, Test loss: 3.872e-01\n",
      "Epoch 7300, Train loss: 4.634e-01, Test loss: 3.666e-01\n",
      "Epoch 7400, Train loss: 4.282e-01, Test loss: 3.393e-01\n",
      "Epoch 7500, Train loss: 3.849e-01, Test loss: 3.054e-01\n",
      "Epoch 7600, Train loss: 3.374e-01, Test loss: 2.679e-01\n",
      "Epoch 7700, Train loss: 2.905e-01, Test loss: 2.307e-01\n",
      "Epoch 7800, Train loss: 2.480e-01, Test loss: 1.959e-01\n",
      "Epoch 7900, Train loss: 2.102e-01, Test loss: 1.668e-01\n",
      "Epoch 8000, Train loss: 1.755e-01, Test loss: 1.391e-01\n",
      "Epoch 8100, Train loss: 1.433e-01, Test loss: 1.133e-01\n",
      "Epoch 8200, Train loss: 1.143e-01, Test loss: 9.002e-02\n",
      "Epoch 8300, Train loss: 8.966e-02, Test loss: 7.024e-02\n",
      "Epoch 8400, Train loss: 7.053e-02, Test loss: 5.578e-02\n",
      "Epoch 8500, Train loss: 5.631e-02, Test loss: 4.515e-02\n",
      "Epoch 8600, Train loss: 4.599e-02, Test loss: 3.765e-02\n",
      "Epoch 8700, Train loss: 3.846e-02, Test loss: 3.235e-02\n",
      "Epoch 8800, Train loss: 3.290e-02, Test loss: 2.859e-02\n",
      "Epoch 8900, Train loss: 2.871e-02, Test loss: 2.560e-02\n",
      "Epoch 9000, Train loss: 2.548e-02, Test loss: 2.330e-02\n",
      "Epoch 9100, Train loss: 2.295e-02, Test loss: 2.147e-02\n",
      "Epoch 9200, Train loss: 2.089e-02, Test loss: 1.990e-02\n",
      "Epoch 9300, Train loss: 1.920e-02, Test loss: 1.861e-02\n",
      "Epoch 9400, Train loss: 1.778e-02, Test loss: 1.762e-02\n",
      "Epoch 9500, Train loss: 1.658e-02, Test loss: 1.672e-02\n",
      "Epoch 9600, Train loss: 1.554e-02, Test loss: 1.607e-02\n",
      "Epoch 9700, Train loss: 1.467e-02, Test loss: 1.526e-02\n",
      "Epoch 9800, Train loss: 1.385e-02, Test loss: 1.486e-02\n",
      "Epoch 9900, Train loss: 1.314e-02, Test loss: 1.430e-02\n",
      "Epoch 10000, Train loss: 1.250e-02, Test loss: 1.378e-02\n",
      "Epoch 10100, Train loss: 1.193e-02, Test loss: 1.325e-02\n",
      "Epoch 10200, Train loss: 1.145e-02, Test loss: 1.299e-02\n",
      "Epoch 10300, Train loss: 1.093e-02, Test loss: 1.225e-02\n",
      "Epoch 10400, Train loss: 1.049e-02, Test loss: 1.178e-02\n",
      "Epoch 10500, Train loss: 1.008e-02, Test loss: 1.133e-02\n",
      "Epoch 10600, Train loss: 9.698e-03, Test loss: 1.087e-02\n",
      "Epoch 10700, Train loss: 9.331e-03, Test loss: 1.047e-02\n",
      "Epoch 10800, Train loss: 8.982e-03, Test loss: 1.000e-02\n",
      "Epoch 10900, Train loss: 8.664e-03, Test loss: 9.683e-03\n",
      "Epoch 11000, Train loss: 8.323e-03, Test loss: 9.163e-03\n",
      "Epoch 11100, Train loss: 8.012e-03, Test loss: 8.754e-03\n",
      "Epoch 11200, Train loss: 7.718e-03, Test loss: 8.373e-03\n",
      "Epoch 11300, Train loss: 7.436e-03, Test loss: 8.004e-03\n",
      "Epoch 11400, Train loss: 7.167e-03, Test loss: 7.636e-03\n",
      "Epoch 11500, Train loss: 6.913e-03, Test loss: 7.318e-03\n",
      "Epoch 11600, Train loss: 6.720e-03, Test loss: 7.157e-03\n",
      "Epoch 11700, Train loss: 6.443e-03, Test loss: 6.702e-03\n",
      "Epoch 11800, Train loss: 6.225e-03, Test loss: 6.413e-03\n",
      "Epoch 11900, Train loss: 6.020e-03, Test loss: 6.148e-03\n",
      "Epoch 12000, Train loss: 5.825e-03, Test loss: 5.892e-03\n",
      "Epoch 12100, Train loss: 5.640e-03, Test loss: 5.650e-03\n",
      "Epoch 12200, Train loss: 5.465e-03, Test loss: 5.422e-03\n",
      "Epoch 12300, Train loss: 5.400e-03, Test loss: 5.217e-03\n",
      "Epoch 12400, Train loss: 5.141e-03, Test loss: 5.001e-03\n",
      "Epoch 12500, Train loss: 4.989e-03, Test loss: 4.805e-03\n",
      "Epoch 12600, Train loss: 4.847e-03, Test loss: 4.622e-03\n",
      "Epoch 12700, Train loss: 4.711e-03, Test loss: 4.447e-03\n",
      "Epoch 12800, Train loss: 4.582e-03, Test loss: 4.287e-03\n",
      "Epoch 12900, Train loss: 4.459e-03, Test loss: 4.127e-03\n",
      "Epoch 13000, Train loss: 4.343e-03, Test loss: 3.993e-03\n",
      "Epoch 13100, Train loss: 4.229e-03, Test loss: 3.840e-03\n",
      "Epoch 13200, Train loss: 4.154e-03, Test loss: 3.785e-03\n",
      "Epoch 13300, Train loss: 4.020e-03, Test loss: 3.580e-03\n",
      "Epoch 13400, Train loss: 3.930e-03, Test loss: 3.509e-03\n",
      "Epoch 13500, Train loss: 3.828e-03, Test loss: 3.347e-03\n",
      "Epoch 13600, Train loss: 3.738e-03, Test loss: 3.237e-03\n",
      "Epoch 13700, Train loss: 3.652e-03, Test loss: 3.128e-03\n",
      "Epoch 13800, Train loss: 3.569e-03, Test loss: 3.035e-03\n",
      "Epoch 13900, Train loss: 3.495e-03, Test loss: 2.942e-03\n",
      "Epoch 14000, Train loss: 3.413e-03, Test loss: 2.854e-03\n",
      "Epoch 14100, Train loss: 3.338e-03, Test loss: 2.765e-03\n",
      "Epoch 14200, Train loss: 3.268e-03, Test loss: 2.684e-03\n",
      "Epoch 14300, Train loss: 3.199e-03, Test loss: 2.611e-03\n",
      "Epoch 14400, Train loss: 3.134e-03, Test loss: 2.554e-03\n",
      "Epoch 14500, Train loss: 3.069e-03, Test loss: 2.468e-03\n",
      "Epoch 14600, Train loss: 3.006e-03, Test loss: 2.390e-03\n",
      "Epoch 14700, Train loss: 2.947e-03, Test loss: 2.340e-03\n",
      "Epoch 14800, Train loss: 2.888e-03, Test loss: 2.279e-03\n",
      "Epoch 14900, Train loss: 2.832e-03, Test loss: 2.232e-03\n",
      "Epoch 15000, Train loss: 2.777e-03, Test loss: 2.168e-03\n",
      "Epoch 15100, Train loss: 2.731e-03, Test loss: 2.350e-03\n",
      "Epoch 15200, Train loss: 2.671e-03, Test loss: 2.067e-03\n",
      "Epoch 15300, Train loss: 2.620e-03, Test loss: 2.019e-03\n",
      "Epoch 15400, Train loss: 2.570e-03, Test loss: 1.978e-03\n",
      "Epoch 15500, Train loss: 2.521e-03, Test loss: 1.931e-03\n",
      "Epoch 15600, Train loss: 2.527e-03, Test loss: 1.835e-03\n",
      "Epoch 15700, Train loss: 2.426e-03, Test loss: 1.851e-03\n",
      "Epoch 15800, Train loss: 2.379e-03, Test loss: 1.813e-03\n",
      "Epoch 15900, Train loss: 2.337e-03, Test loss: 1.749e-03\n",
      "Epoch 16000, Train loss: 2.289e-03, Test loss: 1.742e-03\n",
      "Epoch 16100, Train loss: 2.244e-03, Test loss: 1.709e-03\n",
      "Epoch 16200, Train loss: 2.201e-03, Test loss: 1.673e-03\n",
      "Epoch 16300, Train loss: 2.158e-03, Test loss: 1.645e-03\n",
      "Epoch 16400, Train loss: 2.244e-03, Test loss: 2.072e-03\n",
      "Epoch 16500, Train loss: 2.073e-03, Test loss: 1.585e-03\n",
      "Epoch 16600, Train loss: 2.031e-03, Test loss: 1.557e-03\n",
      "Epoch 16700, Train loss: 1.993e-03, Test loss: 1.556e-03\n",
      "Epoch 16800, Train loss: 1.948e-03, Test loss: 1.504e-03\n",
      "Epoch 16900, Train loss: 1.907e-03, Test loss: 1.457e-03\n",
      "Epoch 17000, Train loss: 1.868e-03, Test loss: 1.456e-03\n",
      "Epoch 17100, Train loss: 1.828e-03, Test loss: 1.429e-03\n",
      "Epoch 17200, Train loss: 1.866e-03, Test loss: 1.770e-03\n",
      "Epoch 17300, Train loss: 1.750e-03, Test loss: 1.382e-03\n",
      "Epoch 17400, Train loss: 1.712e-03, Test loss: 1.360e-03\n",
      "Epoch 17500, Train loss: 1.690e-03, Test loss: 1.333e-03\n",
      "Epoch 17600, Train loss: 1.637e-03, Test loss: 1.317e-03\n",
      "Epoch 17700, Train loss: 1.599e-03, Test loss: 1.304e-03\n",
      "Epoch 17800, Train loss: 1.563e-03, Test loss: 1.274e-03\n",
      "Epoch 17900, Train loss: 1.528e-03, Test loss: 1.255e-03\n",
      "Epoch 18000, Train loss: 1.492e-03, Test loss: 1.260e-03\n",
      "Epoch 18100, Train loss: 1.458e-03, Test loss: 1.217e-03\n",
      "Epoch 18200, Train loss: 1.428e-03, Test loss: 1.159e-03\n",
      "Epoch 18300, Train loss: 1.391e-03, Test loss: 1.180e-03\n",
      "Epoch 18400, Train loss: 1.359e-03, Test loss: 1.163e-03\n",
      "Epoch 18500, Train loss: 1.328e-03, Test loss: 1.142e-03\n",
      "Epoch 18600, Train loss: 1.297e-03, Test loss: 1.129e-03\n",
      "Epoch 18700, Train loss: 1.267e-03, Test loss: 1.113e-03\n",
      "Epoch 18800, Train loss: 1.238e-03, Test loss: 1.095e-03\n",
      "Epoch 18900, Train loss: 1.210e-03, Test loss: 1.082e-03\n",
      "Epoch 19000, Train loss: 1.374e-03, Test loss: 9.816e-04\n",
      "Epoch 19100, Train loss: 1.155e-03, Test loss: 1.052e-03\n",
      "Epoch 19200, Train loss: 1.128e-03, Test loss: 1.037e-03\n",
      "Epoch 19300, Train loss: 1.105e-03, Test loss: 1.006e-03\n",
      "Epoch 19400, Train loss: 1.080e-03, Test loss: 1.011e-03\n",
      "Epoch 19500, Train loss: 1.056e-03, Test loss: 9.978e-04\n",
      "Epoch 19600, Train loss: 1.034e-03, Test loss: 9.058e-04\n",
      "Epoch 19700, Train loss: 1.010e-03, Test loss: 9.709e-04\n",
      "Epoch 19800, Train loss: 9.884e-04, Test loss: 9.478e-04\n",
      "Epoch 19900, Train loss: 9.677e-04, Test loss: 9.447e-04\n",
      "Epoch 20000, Train loss: 9.474e-04, Test loss: 9.370e-04\n",
      "Epoch 20100, Train loss: 9.374e-04, Test loss: 8.825e-04\n",
      "Epoch 20200, Train loss: 9.087e-04, Test loss: 9.151e-04\n",
      "Epoch 20300, Train loss: 9.592e-04, Test loss: 1.295e-03\n",
      "Epoch 20400, Train loss: 8.726e-04, Test loss: 8.946e-04\n",
      "Epoch 20500, Train loss: 8.552e-04, Test loss: 8.846e-04\n",
      "Epoch 20600, Train loss: 8.392e-04, Test loss: 8.639e-04\n",
      "Epoch 20700, Train loss: 8.229e-04, Test loss: 8.651e-04\n",
      "Epoch 20800, Train loss: 8.203e-04, Test loss: 7.908e-04\n",
      "Epoch 20900, Train loss: 7.926e-04, Test loss: 8.470e-04\n",
      "Epoch 21000, Train loss: 7.847e-04, Test loss: 7.865e-04\n",
      "Epoch 21100, Train loss: 7.642e-04, Test loss: 8.270e-04\n",
      "Epoch 21200, Train loss: 7.505e-04, Test loss: 8.218e-04\n",
      "Epoch 21300, Train loss: 7.381e-04, Test loss: 8.226e-04\n",
      "Epoch 21400, Train loss: 7.251e-04, Test loss: 8.058e-04\n",
      "Epoch 21500, Train loss: 7.345e-04, Test loss: 7.223e-04\n",
      "Epoch 21600, Train loss: 7.013e-04, Test loss: 7.900e-04\n",
      "Epoch 21700, Train loss: 6.898e-04, Test loss: 7.833e-04\n",
      "Epoch 21800, Train loss: 6.795e-04, Test loss: 7.535e-04\n",
      "Epoch 21900, Train loss: 6.689e-04, Test loss: 7.700e-04\n",
      "Epoch 22000, Train loss: 6.589e-04, Test loss: 7.632e-04\n",
      "Epoch 22100, Train loss: 6.487e-04, Test loss: 7.588e-04\n",
      "Epoch 22200, Train loss: 6.392e-04, Test loss: 7.532e-04\n",
      "Epoch 22300, Train loss: 6.297e-04, Test loss: 7.434e-04\n",
      "Epoch 22400, Train loss: 6.307e-04, Test loss: 7.641e-04\n",
      "Epoch 22500, Train loss: 6.119e-04, Test loss: 7.313e-04\n",
      "Epoch 22600, Train loss: 6.032e-04, Test loss: 7.251e-04\n",
      "Epoch 22700, Train loss: 5.950e-04, Test loss: 7.073e-04\n",
      "Epoch 22800, Train loss: 5.871e-04, Test loss: 7.137e-04\n",
      "Epoch 22900, Train loss: 5.791e-04, Test loss: 7.074e-04\n",
      "Epoch 23000, Train loss: 5.719e-04, Test loss: 6.915e-04\n",
      "Epoch 23100, Train loss: 5.643e-04, Test loss: 6.967e-04\n",
      "Epoch 23200, Train loss: 5.684e-04, Test loss: 7.971e-04\n",
      "Epoch 23300, Train loss: 5.502e-04, Test loss: 6.892e-04\n",
      "Epoch 23400, Train loss: 5.434e-04, Test loss: 6.808e-04\n",
      "Epoch 23500, Train loss: 5.366e-04, Test loss: 7.161e-04\n",
      "Epoch 23600, Train loss: 5.303e-04, Test loss: 6.712e-04\n",
      "Epoch 23700, Train loss: 5.241e-04, Test loss: 6.529e-04\n",
      "Epoch 23800, Train loss: 5.180e-04, Test loss: 6.649e-04\n",
      "Epoch 23900, Train loss: 5.120e-04, Test loss: 6.559e-04\n",
      "Epoch 24000, Train loss: 5.527e-04, Test loss: 1.071e-03\n",
      "Epoch 24100, Train loss: 5.006e-04, Test loss: 6.483e-04\n",
      "Epoch 24200, Train loss: 4.951e-04, Test loss: 6.420e-04\n",
      "Epoch 24300, Train loss: 5.203e-04, Test loss: 5.943e-04\n",
      "Epoch 24400, Train loss: 4.844e-04, Test loss: 6.323e-04\n",
      "Epoch 24500, Train loss: 4.792e-04, Test loss: 6.287e-04\n",
      "Epoch 24600, Train loss: 4.744e-04, Test loss: 6.354e-04\n",
      "Epoch 24700, Train loss: 4.694e-04, Test loss: 6.200e-04\n",
      "Epoch 24800, Train loss: 4.816e-04, Test loss: 7.506e-04\n",
      "Epoch 24900, Train loss: 4.599e-04, Test loss: 6.094e-04\n",
      "Epoch 25000, Train loss: 4.554e-04, Test loss: 6.074e-04\n",
      "Epoch 25100, Train loss: 7.480e-04, Test loss: 1.529e-03\n",
      "Epoch 25200, Train loss: 4.467e-04, Test loss: 6.020e-04\n",
      "Epoch 25300, Train loss: 4.425e-04, Test loss: 5.956e-04\n",
      "Epoch 25400, Train loss: 4.382e-04, Test loss: 5.913e-04\n",
      "Epoch 25500, Train loss: 4.342e-04, Test loss: 5.855e-04\n",
      "Epoch 25600, Train loss: 4.302e-04, Test loss: 5.841e-04\n",
      "Epoch 25700, Train loss: 4.681e-04, Test loss: 5.132e-04\n",
      "Epoch 25800, Train loss: 4.224e-04, Test loss: 5.762e-04\n",
      "Epoch 25900, Train loss: 4.186e-04, Test loss: 5.730e-04\n",
      "Epoch 26000, Train loss: 4.335e-04, Test loss: 6.132e-04\n",
      "Epoch 26100, Train loss: 4.113e-04, Test loss: 5.663e-04\n",
      "Epoch 26200, Train loss: 4.076e-04, Test loss: 5.625e-04\n",
      "Epoch 26300, Train loss: 4.045e-04, Test loss: 5.538e-04\n",
      "Epoch 26400, Train loss: 4.008e-04, Test loss: 5.553e-04\n",
      "Epoch 26500, Train loss: 4.975e-04, Test loss: 9.821e-04\n",
      "Epoch 26600, Train loss: 3.941e-04, Test loss: 5.498e-04\n",
      "Epoch 26700, Train loss: 3.909e-04, Test loss: 5.454e-04\n",
      "Epoch 26800, Train loss: 7.782e-04, Test loss: 7.551e-04\n",
      "Epoch 26900, Train loss: 3.846e-04, Test loss: 5.401e-04\n",
      "Epoch 27000, Train loss: 3.816e-04, Test loss: 5.359e-04\n",
      "Epoch 27100, Train loss: 3.793e-04, Test loss: 5.146e-04\n",
      "Epoch 27200, Train loss: 3.756e-04, Test loss: 5.318e-04\n",
      "Epoch 27300, Train loss: 3.727e-04, Test loss: 5.267e-04\n",
      "Epoch 27400, Train loss: 3.725e-04, Test loss: 5.461e-04\n",
      "Epoch 27500, Train loss: 3.671e-04, Test loss: 5.212e-04\n",
      "Epoch 27600, Train loss: 3.642e-04, Test loss: 5.180e-04\n",
      "Epoch 27700, Train loss: 3.620e-04, Test loss: 5.229e-04\n",
      "Epoch 27800, Train loss: 3.588e-04, Test loss: 5.119e-04\n",
      "Epoch 27900, Train loss: 3.576e-04, Test loss: 5.549e-04\n",
      "Epoch 28000, Train loss: 3.536e-04, Test loss: 5.069e-04\n",
      "Epoch 28100, Train loss: 3.510e-04, Test loss: 5.033e-04\n",
      "Epoch 28200, Train loss: 3.500e-04, Test loss: 4.848e-04\n",
      "Epoch 28300, Train loss: 3.461e-04, Test loss: 4.980e-04\n",
      "Epoch 28400, Train loss: 3.436e-04, Test loss: 4.953e-04\n",
      "Epoch 28500, Train loss: 3.414e-04, Test loss: 4.827e-04\n",
      "Epoch 28600, Train loss: 3.391e-04, Test loss: 4.903e-04\n",
      "Epoch 28700, Train loss: 3.368e-04, Test loss: 4.876e-04\n",
      "Epoch 28800, Train loss: 3.348e-04, Test loss: 4.996e-04\n",
      "Epoch 28900, Train loss: 3.323e-04, Test loss: 4.791e-04\n",
      "Epoch 29000, Train loss: 3.301e-04, Test loss: 4.802e-04\n",
      "Epoch 29100, Train loss: 3.278e-04, Test loss: 4.798e-04\n",
      "Epoch 29200, Train loss: 3.258e-04, Test loss: 4.706e-04\n",
      "Epoch 29300, Train loss: 3.236e-04, Test loss: 4.728e-04\n",
      "Epoch 29400, Train loss: 3.456e-04, Test loss: 6.369e-04\n",
      "Epoch 29500, Train loss: 3.194e-04, Test loss: 4.683e-04\n",
      "Epoch 29600, Train loss: 3.173e-04, Test loss: 4.643e-04\n",
      "Epoch 29700, Train loss: 3.153e-04, Test loss: 4.607e-04\n",
      "Epoch 29800, Train loss: 3.133e-04, Test loss: 4.610e-04\n",
      "Epoch 29900, Train loss: 3.153e-04, Test loss: 5.166e-04\n",
      "Epoch 30000, Train loss: 3.095e-04, Test loss: 4.564e-04\n",
      "Epoch 30100, Train loss: 3.084e-04, Test loss: 4.761e-04\n",
      "Epoch 30200, Train loss: 3.057e-04, Test loss: 4.503e-04\n",
      "Epoch 30300, Train loss: 3.038e-04, Test loss: 4.502e-04\n",
      "Epoch 30400, Train loss: 3.023e-04, Test loss: 4.468e-04\n",
      "Epoch 30500, Train loss: 3.002e-04, Test loss: 4.459e-04\n",
      "Epoch 30600, Train loss: 3.196e-04, Test loss: 4.619e-04\n",
      "Epoch 30700, Train loss: 2.966e-04, Test loss: 4.416e-04\n",
      "Epoch 30800, Train loss: 2.949e-04, Test loss: 4.397e-04\n",
      "Epoch 30900, Train loss: 2.932e-04, Test loss: 4.302e-04\n",
      "Epoch 31000, Train loss: 2.915e-04, Test loss: 4.356e-04\n",
      "Epoch 31100, Train loss: 5.850e-04, Test loss: 1.227e-03\n",
      "Epoch 31200, Train loss: 2.882e-04, Test loss: 4.302e-04\n",
      "Epoch 31300, Train loss: 2.866e-04, Test loss: 4.298e-04\n",
      "Epoch 31400, Train loss: 2.894e-04, Test loss: 5.065e-04\n",
      "Epoch 31500, Train loss: 2.834e-04, Test loss: 4.262e-04\n",
      "Epoch 31600, Train loss: 2.818e-04, Test loss: 4.239e-04\n",
      "Epoch 31700, Train loss: 2.802e-04, Test loss: 4.169e-04\n",
      "Epoch 31800, Train loss: 2.787e-04, Test loss: 4.206e-04\n",
      "Epoch 31900, Train loss: 3.667e-04, Test loss: 7.875e-04\n",
      "Epoch 32000, Train loss: 2.758e-04, Test loss: 4.191e-04\n",
      "Epoch 32100, Train loss: 2.742e-04, Test loss: 4.151e-04\n",
      "Epoch 32200, Train loss: 2.740e-04, Test loss: 4.465e-04\n",
      "Epoch 32300, Train loss: 2.714e-04, Test loss: 4.117e-04\n",
      "Epoch 32400, Train loss: 5.134e-04, Test loss: 3.711e-04\n",
      "Epoch 32500, Train loss: 2.685e-04, Test loss: 4.077e-04\n",
      "Epoch 32600, Train loss: 2.671e-04, Test loss: 4.068e-04\n",
      "Epoch 32700, Train loss: 2.658e-04, Test loss: 4.158e-04\n",
      "Epoch 32800, Train loss: 2.644e-04, Test loss: 4.035e-04\n",
      "Epoch 32900, Train loss: 2.771e-04, Test loss: 3.495e-04\n",
      "Epoch 33000, Train loss: 2.617e-04, Test loss: 3.995e-04\n",
      "Epoch 33100, Train loss: 2.604e-04, Test loss: 3.983e-04\n",
      "Epoch 33200, Train loss: 2.591e-04, Test loss: 3.938e-04\n",
      "Epoch 33300, Train loss: 2.578e-04, Test loss: 3.954e-04\n",
      "Epoch 33400, Train loss: 2.581e-04, Test loss: 3.642e-04\n",
      "Epoch 33500, Train loss: 2.553e-04, Test loss: 3.926e-04\n",
      "Epoch 33600, Train loss: 5.972e-04, Test loss: 5.232e-04\n",
      "Epoch 33700, Train loss: 2.528e-04, Test loss: 3.880e-04\n",
      "Epoch 33800, Train loss: 2.516e-04, Test loss: 3.878e-04\n",
      "Epoch 33900, Train loss: 2.511e-04, Test loss: 3.970e-04\n",
      "Epoch 34000, Train loss: 2.492e-04, Test loss: 3.851e-04\n",
      "Epoch 34100, Train loss: 2.569e-04, Test loss: 4.229e-04\n",
      "Epoch 34200, Train loss: 2.468e-04, Test loss: 3.822e-04\n",
      "Epoch 34300, Train loss: 3.922e-04, Test loss: 6.652e-04\n",
      "Epoch 34400, Train loss: 2.445e-04, Test loss: 3.807e-04\n",
      "Epoch 34500, Train loss: 2.434e-04, Test loss: 3.775e-04\n",
      "Epoch 34600, Train loss: 2.425e-04, Test loss: 3.847e-04\n",
      "Epoch 34700, Train loss: 2.412e-04, Test loss: 3.755e-04\n",
      "Epoch 34800, Train loss: 2.422e-04, Test loss: 3.699e-04\n",
      "Epoch 34900, Train loss: 2.390e-04, Test loss: 3.726e-04\n",
      "Epoch 35000, Train loss: 2.394e-04, Test loss: 3.991e-04\n",
      "Epoch 35100, Train loss: 2.369e-04, Test loss: 3.704e-04\n",
      "Epoch 35200, Train loss: 2.358e-04, Test loss: 3.692e-04\n",
      "Epoch 35300, Train loss: 2.347e-04, Test loss: 3.535e-04\n",
      "Epoch 35400, Train loss: 2.337e-04, Test loss: 3.668e-04\n",
      "Epoch 35500, Train loss: 3.784e-04, Test loss: 6.370e-04\n",
      "Epoch 35600, Train loss: 2.317e-04, Test loss: 3.653e-04\n",
      "Epoch 35700, Train loss: 2.307e-04, Test loss: 3.643e-04\n",
      "Epoch 35800, Train loss: 2.298e-04, Test loss: 3.557e-04\n",
      "Epoch 35900, Train loss: 2.287e-04, Test loss: 3.607e-04\n",
      "Epoch 36000, Train loss: 2.297e-04, Test loss: 3.765e-04\n",
      "Epoch 36100, Train loss: 2.268e-04, Test loss: 3.583e-04\n",
      "Epoch 36200, Train loss: 2.626e-04, Test loss: 3.410e-04\n",
      "Epoch 36300, Train loss: 2.249e-04, Test loss: 3.566e-04\n",
      "Epoch 36400, Train loss: 2.242e-04, Test loss: 3.455e-04\n",
      "Epoch 36500, Train loss: 2.231e-04, Test loss: 3.574e-04\n",
      "Epoch 36600, Train loss: 2.221e-04, Test loss: 3.525e-04\n",
      "Epoch 36700, Train loss: 2.215e-04, Test loss: 3.531e-04\n",
      "Epoch 36800, Train loss: 2.203e-04, Test loss: 3.505e-04\n",
      "Epoch 36900, Train loss: 2.740e-04, Test loss: 3.424e-04\n",
      "Epoch 37000, Train loss: 2.185e-04, Test loss: 3.496e-04\n",
      "Epoch 37100, Train loss: 2.177e-04, Test loss: 3.474e-04\n",
      "Epoch 37200, Train loss: 2.189e-04, Test loss: 3.388e-04\n",
      "Epoch 37300, Train loss: 2.159e-04, Test loss: 3.452e-04\n",
      "Epoch 37400, Train loss: 2.737e-04, Test loss: 6.044e-04\n",
      "Epoch 37500, Train loss: 2.143e-04, Test loss: 3.438e-04\n",
      "Epoch 37600, Train loss: 2.134e-04, Test loss: 3.424e-04\n",
      "Epoch 37700, Train loss: 2.140e-04, Test loss: 3.663e-04\n",
      "Epoch 37800, Train loss: 2.118e-04, Test loss: 3.402e-04\n",
      "Epoch 37900, Train loss: 3.347e-04, Test loss: 3.237e-04\n",
      "Epoch 38000, Train loss: 2.102e-04, Test loss: 3.394e-04\n",
      "Epoch 38100, Train loss: 2.095e-04, Test loss: 3.451e-04\n",
      "Epoch 38200, Train loss: 2.086e-04, Test loss: 3.334e-04\n",
      "Epoch 38300, Train loss: 2.078e-04, Test loss: 3.355e-04\n",
      "Epoch 38400, Train loss: 2.071e-04, Test loss: 3.290e-04\n",
      "Epoch 38500, Train loss: 2.062e-04, Test loss: 3.336e-04\n",
      "Epoch 38600, Train loss: 2.841e-04, Test loss: 3.010e-04\n",
      "Epoch 38700, Train loss: 2.047e-04, Test loss: 3.314e-04\n",
      "Epoch 38800, Train loss: 2.040e-04, Test loss: 3.310e-04\n",
      "Epoch 38900, Train loss: 2.059e-04, Test loss: 3.660e-04\n",
      "Epoch 39000, Train loss: 2.025e-04, Test loss: 3.295e-04\n",
      "Epoch 39100, Train loss: 2.498e-04, Test loss: 5.506e-04\n",
      "Epoch 39200, Train loss: 2.011e-04, Test loss: 3.259e-04\n",
      "Epoch 39300, Train loss: 2.003e-04, Test loss: 3.265e-04\n",
      "Epoch 39400, Train loss: 2.002e-04, Test loss: 3.412e-04\n",
      "Epoch 39500, Train loss: 1.989e-04, Test loss: 3.252e-04\n",
      "Epoch 39600, Train loss: 1.984e-04, Test loss: 2.969e-04\n",
      "Epoch 39700, Train loss: 1.975e-04, Test loss: 3.232e-04\n",
      "Epoch 39800, Train loss: 1.973e-04, Test loss: 3.108e-04\n",
      "Epoch 39900, Train loss: 1.962e-04, Test loss: 3.186e-04\n",
      "Epoch 40000, Train loss: 1.955e-04, Test loss: 3.209e-04\n",
      "Epoch 40100, Train loss: 1.948e-04, Test loss: 3.084e-04\n",
      "Epoch 40200, Train loss: 1.941e-04, Test loss: 3.196e-04\n",
      "Epoch 40300, Train loss: 1.973e-04, Test loss: 2.793e-04\n",
      "Epoch 40400, Train loss: 1.928e-04, Test loss: 3.189e-04\n",
      "Epoch 40500, Train loss: 1.922e-04, Test loss: 3.179e-04\n",
      "Epoch 40600, Train loss: 1.916e-04, Test loss: 3.164e-04\n",
      "Epoch 40700, Train loss: 1.909e-04, Test loss: 3.156e-04\n",
      "Epoch 40800, Train loss: 1.997e-04, Test loss: 3.439e-04\n",
      "Epoch 40900, Train loss: 1.896e-04, Test loss: 3.140e-04\n",
      "Epoch 41000, Train loss: 1.962e-04, Test loss: 3.763e-04\n",
      "Epoch 41100, Train loss: 1.884e-04, Test loss: 3.107e-04\n",
      "Epoch 41200, Train loss: 1.878e-04, Test loss: 3.120e-04\n",
      "Epoch 41300, Train loss: 1.874e-04, Test loss: 3.184e-04\n",
      "Epoch 41400, Train loss: 1.866e-04, Test loss: 3.105e-04\n",
      "Epoch 41500, Train loss: 5.228e-04, Test loss: 1.012e-03\n",
      "Epoch 41600, Train loss: 1.854e-04, Test loss: 3.107e-04\n",
      "Epoch 41700, Train loss: 1.848e-04, Test loss: 3.085e-04\n",
      "Epoch 41800, Train loss: 1.852e-04, Test loss: 3.029e-04\n",
      "Epoch 41900, Train loss: 1.837e-04, Test loss: 3.072e-04\n",
      "Epoch 42000, Train loss: 1.840e-04, Test loss: 2.913e-04\n",
      "Epoch 42100, Train loss: 1.825e-04, Test loss: 3.058e-04\n",
      "Epoch 42200, Train loss: 1.819e-04, Test loss: 3.050e-04\n",
      "Epoch 42300, Train loss: 1.821e-04, Test loss: 3.202e-04\n",
      "Epoch 42400, Train loss: 1.808e-04, Test loss: 3.039e-04\n",
      "Epoch 42500, Train loss: 2.734e-04, Test loss: 5.050e-04\n",
      "Epoch 42600, Train loss: 1.797e-04, Test loss: 3.019e-04\n",
      "Epoch 42700, Train loss: 1.792e-04, Test loss: 3.020e-04\n",
      "Epoch 42800, Train loss: 1.788e-04, Test loss: 2.912e-04\n",
      "Epoch 42900, Train loss: 1.781e-04, Test loss: 3.006e-04\n",
      "Epoch 43000, Train loss: 1.875e-04, Test loss: 2.907e-04\n",
      "Epoch 43100, Train loss: 1.771e-04, Test loss: 2.992e-04\n",
      "Epoch 43200, Train loss: 1.766e-04, Test loss: 3.011e-04\n",
      "Epoch 43300, Train loss: 1.761e-04, Test loss: 3.028e-04\n",
      "Epoch 43400, Train loss: 1.755e-04, Test loss: 2.977e-04\n",
      "Epoch 43500, Train loss: 1.966e-04, Test loss: 2.626e-04\n",
      "Epoch 43600, Train loss: 1.745e-04, Test loss: 2.973e-04\n",
      "Epoch 43700, Train loss: 1.741e-04, Test loss: 2.912e-04\n",
      "Epoch 43800, Train loss: 1.735e-04, Test loss: 2.990e-04\n",
      "Epoch 43900, Train loss: 1.730e-04, Test loss: 2.947e-04\n",
      "Epoch 44000, Train loss: 1.744e-04, Test loss: 3.363e-04\n",
      "Epoch 44100, Train loss: 1.721e-04, Test loss: 2.937e-04\n",
      "Epoch 44200, Train loss: 1.716e-04, Test loss: 2.933e-04\n",
      "Epoch 44300, Train loss: 1.714e-04, Test loss: 2.900e-04\n",
      "Epoch 44400, Train loss: 1.706e-04, Test loss: 2.921e-04\n",
      "Epoch 44500, Train loss: 1.730e-04, Test loss: 2.672e-04\n",
      "Epoch 44600, Train loss: 1.697e-04, Test loss: 2.908e-04\n",
      "Epoch 44700, Train loss: 2.119e-04, Test loss: 2.616e-04\n",
      "Epoch 44800, Train loss: 1.687e-04, Test loss: 2.894e-04\n",
      "Epoch 44900, Train loss: 1.683e-04, Test loss: 2.885e-04\n",
      "Epoch 45000, Train loss: 1.680e-04, Test loss: 2.845e-04\n",
      "Epoch 45100, Train loss: 1.674e-04, Test loss: 2.883e-04\n",
      "Epoch 45200, Train loss: 2.091e-04, Test loss: 2.508e-04\n",
      "Epoch 45300, Train loss: 1.665e-04, Test loss: 2.872e-04\n",
      "Epoch 45400, Train loss: 1.660e-04, Test loss: 2.856e-04\n",
      "Epoch 45500, Train loss: 1.658e-04, Test loss: 2.810e-04\n",
      "Epoch 45600, Train loss: 1.652e-04, Test loss: 2.857e-04\n",
      "Epoch 45700, Train loss: 1.739e-04, Test loss: 2.527e-04\n",
      "Epoch 45800, Train loss: 1.643e-04, Test loss: 2.846e-04\n",
      "Epoch 45900, Train loss: 1.677e-04, Test loss: 2.581e-04\n",
      "Epoch 46000, Train loss: 1.635e-04, Test loss: 2.860e-04\n",
      "Epoch 46100, Train loss: 1.630e-04, Test loss: 2.834e-04\n",
      "Epoch 46200, Train loss: 1.635e-04, Test loss: 2.635e-04\n",
      "Epoch 46300, Train loss: 1.622e-04, Test loss: 2.825e-04\n",
      "Epoch 46400, Train loss: 2.070e-04, Test loss: 2.816e-04\n",
      "Epoch 46500, Train loss: 1.614e-04, Test loss: 2.803e-04\n",
      "Epoch 46600, Train loss: 1.610e-04, Test loss: 2.813e-04\n",
      "Epoch 46700, Train loss: 1.606e-04, Test loss: 2.811e-04\n",
      "Epoch 46800, Train loss: 1.602e-04, Test loss: 2.800e-04\n",
      "Epoch 46900, Train loss: 2.760e-04, Test loss: 2.432e-04\n",
      "Epoch 47000, Train loss: 1.594e-04, Test loss: 2.797e-04\n",
      "Epoch 47100, Train loss: 1.590e-04, Test loss: 2.787e-04\n",
      "Epoch 47200, Train loss: 1.605e-04, Test loss: 2.773e-04\n",
      "Epoch 47300, Train loss: 1.582e-04, Test loss: 2.777e-04\n",
      "Epoch 47400, Train loss: 1.578e-04, Test loss: 2.769e-04\n",
      "Epoch 47500, Train loss: 1.575e-04, Test loss: 2.727e-04\n",
      "Epoch 47600, Train loss: 1.571e-04, Test loss: 2.765e-04\n",
      "Epoch 47700, Train loss: 1.567e-04, Test loss: 2.438e-04\n",
      "Epoch 47800, Train loss: 1.563e-04, Test loss: 2.762e-04\n",
      "Epoch 47900, Train loss: 1.560e-04, Test loss: 2.753e-04\n",
      "Epoch 48000, Train loss: 1.560e-04, Test loss: 2.669e-04\n",
      "Epoch 48100, Train loss: 1.552e-04, Test loss: 2.745e-04\n",
      "Epoch 48200, Train loss: 1.549e-04, Test loss: 2.739e-04\n",
      "Epoch 48300, Train loss: 1.545e-04, Test loss: 2.805e-04\n",
      "Epoch 48400, Train loss: 1.542e-04, Test loss: 2.733e-04\n",
      "Epoch 48500, Train loss: 1.539e-04, Test loss: 2.677e-04\n",
      "Epoch 48600, Train loss: 1.535e-04, Test loss: 2.703e-04\n",
      "Epoch 48700, Train loss: 1.531e-04, Test loss: 2.720e-04\n",
      "Epoch 48800, Train loss: 1.528e-04, Test loss: 2.708e-04\n",
      "Epoch 48900, Train loss: 1.526e-04, Test loss: 2.766e-04\n",
      "Epoch 49000, Train loss: 1.521e-04, Test loss: 2.709e-04\n",
      "Epoch 49100, Train loss: 1.742e-04, Test loss: 2.365e-04\n",
      "Epoch 49200, Train loss: 1.514e-04, Test loss: 2.689e-04\n",
      "Epoch 49300, Train loss: 1.511e-04, Test loss: 2.696e-04\n",
      "Epoch 49400, Train loss: 1.522e-04, Test loss: 2.516e-04\n",
      "Epoch 49500, Train loss: 1.504e-04, Test loss: 2.697e-04\n",
      "Epoch 49600, Train loss: 1.501e-04, Test loss: 2.685e-04\n",
      "Epoch 49700, Train loss: 1.577e-04, Test loss: 3.323e-04\n",
      "Epoch 49800, Train loss: 1.494e-04, Test loss: 2.689e-04\n",
      "Epoch 49900, Train loss: 1.491e-04, Test loss: 2.674e-04\n",
      "\n",
      "Proceso finalizado después de 50000 épocas\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = Autoencoder(input_size=u_shape, encoding_dim=20, output_size=u_shape, device=device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "\n",
    "autoencoder_train_loop(model, optimizer, y_train, y_test, 0, 50000, 64, MODEL_RESULTS_PATH, device, lr_updated=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = model.encoder\n",
    "decoder = model.decoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Red neuronal general"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SciML_test_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
