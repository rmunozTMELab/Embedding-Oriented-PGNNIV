{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Imports de la libreria propia\n",
    "from vecopsciml.kernels.derivative import DerivativeKernels\n",
    "from vecopsciml.utils import TensOps\n",
    "\n",
    "# Imports de las funciones creadas para este programa\n",
    "from models.non_constant_diffusivity import NonConstantDiffusivityNeuralNetwork\n",
    "from utils.folders import create_folder\n",
    "from utils.load_data import load_data\n",
    "from trainers.train import train_loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Folder already exists at: C:\\Users\\usuario\\Desktop\\rmunozTMELab\\Physically-Guided-Machine-Learning\\results\\non_linear\n",
      "Folder already exists at: C:\\Users\\usuario\\Desktop\\rmunozTMELab\\Physically-Guided-Machine-Learning\\results\\non_linear\\model_fft\n"
     ]
    }
   ],
   "source": [
    "# Creamos los paths para las distintas carpetas\n",
    "ROOT_PATH = r'C:\\Users\\usuario\\Desktop\\rmunozTMELab\\Physically-Guided-Machine-Learning'\n",
    "DATA_PATH = os.path.join(ROOT_PATH, r'data\\non_linear\\non_linear_fft_tests.pkl')\n",
    "RESULTS_FOLDER_PATH = os.path.join(ROOT_PATH, r'results\\non_linear')\n",
    "MODEL_RESULTS_PATH = os.path.join(ROOT_PATH, r'results\\non_linear\\model_fft')\n",
    "\n",
    "# Creamos las carpetas que sean necesarias (si ya están creadas se avisará de ello)\n",
    "create_folder(RESULTS_FOLDER_PATH)\n",
    "create_folder(MODEL_RESULTS_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully loaded from: C:\\Users\\usuario\\Desktop\\rmunozTMELab\\Physically-Guided-Machine-Learning\\data\\non_linear\\non_linear_fft_tests.pkl\n"
     ]
    }
   ],
   "source": [
    "# Load dataset\n",
    "dataset = load_data(DATA_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convolutional filters to derivate\n",
    "dx = dataset['x_step_size']\n",
    "dy = dataset['y_step_size']\n",
    "D = DerivativeKernels(dx, dy, 0).grad_kernels_two_dimensions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "u = torch.Tensor(dataset['y_train'])\n",
    "\n",
    "f_coef = torch.fft.fft2(u)\n",
    "F_ordered = f_coef.flatten(start_dim=1)\n",
    "\n",
    "mask = np.zeros_like(F_ordered)\n",
    "mask[:, 0:10] = 1\n",
    "\n",
    "F_full = f_coef*mask.reshape(f_coef.shape)\n",
    "u_reconstructed = torch.fft.ifft2(F_full).real\n",
    "\n",
    "# y_train = F_ordered[:, 0:10].reshape(u.shape[0], 10)\n",
    "\n",
    "real_part = F_ordered[:, 0:10].reshape(u.shape[0], 10).real\n",
    "imag_part = F_ordered[:, 0:10].reshape(u.shape[0], 10).imag\n",
    "\n",
    "y_train = torch.stack([\n",
    "    real_part,\n",
    "    imag_part,\n",
    "], dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = torch.Tensor(dataset['X_train']).unsqueeze(1)\n",
    "K_train = torch.tensor(dataset['k_train'], dtype=torch.float32, requires_grad=True)\n",
    "f_train = torch.tensor(dataset['f_train'], dtype=torch.float32, requires_grad=True)\n",
    "\n",
    "input_size = X_train[0].shape\n",
    "predictive_output_size = y_train[0].shape\n",
    "explanatory_output_size = K_train[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from vecopsciml.utils import TensOps\n",
    "from vecopsciml.operators.zero_order import Mx, My\n",
    "\n",
    "class FFTNeuralNetwork(nn.Module):\n",
    "\n",
    "    def __init__(self, input_size, predictive_output_size, explanatory_output_size, **kwargs):\n",
    "        super(FFTNeuralNetwork, self).__init__()\n",
    "\n",
    "        self.input = input_size\n",
    "        self.output_pred = predictive_output_size\n",
    "        self.output_expl = explanatory_output_size\n",
    "\n",
    "        self.hidden_units_pred = 10\n",
    "        self.hidden_units_exp = 15\n",
    "        self.filters_exp = 10\n",
    "\n",
    "        # Predictive network\n",
    "        self.flatten_layer_pred = nn.Flatten(start_dim=1, end_dim=-1)\n",
    "        self.hidden1_layer_pred = nn.Linear(torch.prod(torch.tensor(self.input)), self.hidden_units_pred)  \n",
    "        self.hidden2_layer_pred = nn.Linear(self.hidden_units_pred, self.hidden_units_pred)  \n",
    "        self.output_layer_pred = nn.Linear(self.hidden_units_pred, self.output_pred[0]*self.output_pred[1])\n",
    "\n",
    "        # Explanatory network\n",
    "        self.conv1_exp = nn.Conv2d(in_channels=1, out_channels=self.filters_exp, kernel_size=1)\n",
    "        self.flatten_layer_exp = nn.Flatten()\n",
    "        self.hidden1_layer_exp = nn.LazyLinear(self.hidden_units_exp)\n",
    "        self.hidden2_layer_exp = nn.Linear(self.hidden_units_exp, self.hidden_units_exp)\n",
    "        self.output_layer_exp = nn.Linear(self.hidden_units_exp, self.filters_exp * (self.output_expl[0] - 1) * (self.output_expl[1] - 1))\n",
    "        self.conv2_exp = nn.Conv2d(in_channels=self.filters_exp, out_channels=1, kernel_size=1)\n",
    "\n",
    "\n",
    "    def forward(self, X):\n",
    "\n",
    "        # Predictive network\n",
    "        X = self.flatten_layer_pred(X)\n",
    "        X = torch.sigmoid(self.hidden1_layer_pred(X))\n",
    "        X = torch.sigmoid(self.hidden2_layer_pred(X))\n",
    "        output_predictive_net = self.output_layer_pred(X)\n",
    "\n",
    "        # Obtain real and imaginary part of output and transform it in a complex number\n",
    "        output_predictive = output_predictive_net.view(output_predictive_net.size(0), self.output_pred[0], self.output_pred[1])\n",
    "        real = output_predictive[..., 0]\n",
    "        imag = output_predictive[..., 1]\n",
    "        \n",
    "        # Reconstruction of u(x, y) with iFFT\n",
    "        f_coef = torch.zeros(output_predictive_net.size(0), 1, self.output_expl[0], self.output_expl[1], dtype=torch.complex64)\n",
    "        f_coef[:, 0, 0:10, 0] = torch.complex(real, imag)\n",
    "\n",
    "        u_pred = torch.fft.ifft2(f_coef).real\n",
    "        um_pred = Mx(My(TensOps(u_pred, space_dimension=2, contravariance=0, covariance=0))).values\n",
    "\n",
    "        # Explanatory network\n",
    "        x = torch.sigmoid(self.conv1_exp(um_pred))\n",
    "        x = self.flatten_layer_exp(x)\n",
    "        x = torch.sigmoid(self.hidden1_layer_exp(x))\n",
    "        x = torch.sigmoid(self.hidden2_layer_exp(x))\n",
    "        x = self.output_layer_exp(x)\n",
    "        x = x.view(x.size(0), self.filters_exp, self.output_expl[0] - 1, self.output_expl[1] - 1)\n",
    "        K_pred = self.conv2_exp(x)\n",
    "\n",
    "        return output_predictive, u_pred, K_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fourier_transform_split(dataset):\n",
    "\n",
    "    u = torch.Tensor(dataset)\n",
    "    \n",
    "    f_coef = torch.fft.fft2(u)\n",
    "    \n",
    "    F_ordered = f_coef.flatten(start_dim=1)\n",
    "    \n",
    "    mask = np.zeros_like(F_ordered)\n",
    "    mask[:, 0:10] = 1\n",
    "    \n",
    "    F_full = f_coef * mask.reshape(f_coef.shape)\n",
    "    \n",
    "    u_reconstructed = torch.fft.ifft2(F_full).real\n",
    "    \n",
    "    real_part = F_ordered[:, 0:10].reshape(u.shape[0], 10).real\n",
    "    imag_part = F_ordered[:, 0:10].reshape(u.shape[0], 10).imag\n",
    "    \n",
    "    y_transformed = torch.stack([real_part, imag_part], dim=-1)\n",
    "    \n",
    "    return y_transformed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tratamiento de los datos para dividirlos en train y test\n",
    "X_train = torch.Tensor(dataset['X_train']).unsqueeze(1)\n",
    "y_train = TensOps(fourier_transform_split(dataset['y_train']).requires_grad_(True), space_dimension=2, contravariance=0, covariance=0)\n",
    "K_train = TensOps(torch.tensor(dataset['k_train'], dtype=torch.float32, requires_grad=True).unsqueeze(1), space_dimension=2, contravariance=0, covariance=0)\n",
    "f_train = TensOps(torch.tensor(dataset['f_train'], dtype=torch.float32, requires_grad=True).unsqueeze(1), space_dimension=2, contravariance=0, covariance=0)\n",
    "\n",
    "X_val = torch.Tensor(dataset['X_val']).unsqueeze(1)\n",
    "y_val = TensOps(fourier_transform_split(dataset['y_val']).requires_grad_(True), space_dimension=2, contravariance=0, covariance=0)\n",
    "K_val = TensOps(torch.tensor(dataset['k_val'], dtype=torch.float32, requires_grad=True).unsqueeze(1), space_dimension=2, contravariance=0, covariance=0)\n",
    "f_val = TensOps(torch.tensor(dataset['f_val'], dtype=torch.float32, requires_grad=True).unsqueeze(1), space_dimension=2, contravariance=0, covariance=0)\n",
    "\n",
    "X_np = X_train\n",
    "y_np = y_train.values\n",
    "K_np = K_train.values\n",
    "f_np = f_train.values\n",
    "\n",
    "X_train_np, X_test_np, y_train_np, y_test_np, K_train_np, K_test_np, f_train_np, f_test_np = train_test_split(X_np, y_np, K_np, f_np, test_size=0.2, random_state=42)\n",
    "\n",
    "X_train = X_train_np.to(device)\n",
    "X_test = X_test_np.to(device)\n",
    "\n",
    "y_train = TensOps(y_train_np.to(device), space_dimension=y_train.space_dim, contravariance=0, covariance=0)\n",
    "y_test = TensOps(y_test_np.to(device), space_dimension=y_train.space_dim, contravariance=0, covariance=0)\n",
    "\n",
    "K_train = TensOps(K_train_np.to(device), space_dimension=K_train.space_dim, contravariance=0, covariance=0)\n",
    "K_test = TensOps(K_test_np.to(device), space_dimension=K_train.space_dim, contravariance=0, covariance=0)\n",
    "\n",
    "f_train = TensOps(f_train_np.to(device), space_dimension=K_train.space_dim, contravariance=0, covariance=0)\n",
    "f_test = TensOps(f_test_np.to(device), space_dimension=K_train.space_dim, contravariance=0, covariance=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training\n",
      "Epoch 0, Train loss: 1.039e+12, Test loss: 1.002e+12, MSE(e): 1.039e+04, MSE(pi1): 4.494e+00, MSE(pi2): 4.174e+01, MSE(pi3): 2.754e-01\n",
      "Epoch 100, Train loss: 9.614e+11, Test loss: 9.275e+11, MSE(e): 9.614e+03, MSE(pi1): 6.128e-01, MSE(pi2): 3.926e+01, MSE(pi3): 2.231e-01\n",
      "Epoch 200, Train loss: 8.863e+11, Test loss: 8.548e+11, MSE(e): 8.863e+03, MSE(pi1): 6.045e-01, MSE(pi2): 3.609e+01, MSE(pi3): 2.161e-01\n",
      "Epoch 300, Train loss: 8.232e+11, Test loss: 7.936e+11, MSE(e): 8.232e+03, MSE(pi1): 6.039e-01, MSE(pi2): 3.353e+01, MSE(pi3): 2.162e-01\n",
      "Epoch 400, Train loss: 7.658e+11, Test loss: 7.379e+11, MSE(e): 7.658e+03, MSE(pi1): 6.040e-01, MSE(pi2): 3.124e+01, MSE(pi3): 2.163e-01\n",
      "Epoch 500, Train loss: 7.116e+11, Test loss: 6.853e+11, MSE(e): 7.116e+03, MSE(pi1): 6.040e-01, MSE(pi2): 2.909e+01, MSE(pi3): 2.163e-01\n",
      "Epoch 600, Train loss: 6.600e+11, Test loss: 6.353e+11, MSE(e): 6.600e+03, MSE(pi1): 6.041e-01, MSE(pi2): 2.703e+01, MSE(pi3): 2.163e-01\n",
      "Epoch 700, Train loss: 6.105e+11, Test loss: 5.875e+11, MSE(e): 6.105e+03, MSE(pi1): 6.041e-01, MSE(pi2): 2.506e+01, MSE(pi3): 2.163e-01\n",
      "Epoch 800, Train loss: 5.632e+11, Test loss: 5.418e+11, MSE(e): 5.632e+03, MSE(pi1): 6.041e-01, MSE(pi2): 2.318e+01, MSE(pi3): 2.163e-01\n",
      "Epoch 900, Train loss: 5.181e+11, Test loss: 4.982e+11, MSE(e): 5.181e+03, MSE(pi1): 6.041e-01, MSE(pi2): 2.138e+01, MSE(pi3): 2.163e-01\n",
      "Epoch 1000, Train loss: 4.750e+11, Test loss: 4.568e+11, MSE(e): 4.750e+03, MSE(pi1): 6.041e-01, MSE(pi2): 1.967e+01, MSE(pi3): 2.163e-01\n",
      "Epoch 1100, Train loss: 4.340e+11, Test loss: 4.174e+11, MSE(e): 4.340e+03, MSE(pi1): 6.041e-01, MSE(pi2): 1.804e+01, MSE(pi3): 2.163e-01\n",
      "Epoch 1200, Train loss: 3.951e+11, Test loss: 3.801e+11, MSE(e): 3.951e+03, MSE(pi1): 6.041e-01, MSE(pi2): 1.649e+01, MSE(pi3): 2.163e-01\n",
      "Epoch 1300, Train loss: 3.582e+11, Test loss: 3.448e+11, MSE(e): 3.582e+03, MSE(pi1): 6.041e-01, MSE(pi2): 1.502e+01, MSE(pi3): 2.163e-01\n",
      "Epoch 1400, Train loss: 3.234e+11, Test loss: 3.116e+11, MSE(e): 3.234e+03, MSE(pi1): 6.041e-01, MSE(pi2): 1.364e+01, MSE(pi3): 2.163e-01\n",
      "Epoch 1500, Train loss: 2.907e+11, Test loss: 2.804e+11, MSE(e): 2.907e+03, MSE(pi1): 6.041e-01, MSE(pi2): 1.234e+01, MSE(pi3): 2.163e-01\n",
      "Epoch 1600, Train loss: 2.599e+11, Test loss: 2.513e+11, MSE(e): 2.599e+03, MSE(pi1): 6.041e-01, MSE(pi2): 1.112e+01, MSE(pi3): 2.163e-01\n",
      "Epoch 1700, Train loss: 2.312e+11, Test loss: 2.242e+11, MSE(e): 2.312e+03, MSE(pi1): 6.041e-01, MSE(pi2): 9.982e+00, MSE(pi3): 2.163e-01\n",
      "Epoch 1800, Train loss: 2.046e+11, Test loss: 1.991e+11, MSE(e): 2.046e+03, MSE(pi1): 6.041e-01, MSE(pi2): 8.924e+00, MSE(pi3): 2.163e-01\n",
      "Epoch 1900, Train loss: 1.799e+11, Test loss: 1.760e+11, MSE(e): 1.799e+03, MSE(pi1): 6.041e-01, MSE(pi2): 7.947e+00, MSE(pi3): 2.163e-01\n",
      "Epoch 2000, Train loss: 1.573e+11, Test loss: 1.548e+11, MSE(e): 1.573e+03, MSE(pi1): 6.041e-01, MSE(pi2): 7.049e+00, MSE(pi3): 2.163e-01\n",
      "Epoch 2100, Train loss: 1.366e+11, Test loss: 1.357e+11, MSE(e): 1.366e+03, MSE(pi1): 6.041e-01, MSE(pi2): 6.231e+00, MSE(pi3): 2.163e-01\n",
      "Epoch 2200, Train loss: 1.179e+11, Test loss: 1.185e+11, MSE(e): 1.179e+03, MSE(pi1): 6.041e-01, MSE(pi2): 5.491e+00, MSE(pi3): 2.163e-01\n",
      "Epoch 2300, Train loss: 1.011e+11, Test loss: 1.032e+11, MSE(e): 1.011e+03, MSE(pi1): 6.041e-01, MSE(pi2): 4.828e+00, MSE(pi3): 2.163e-01\n",
      "Epoch 2400, Train loss: 8.622e+10, Test loss: 8.985e+10, MSE(e): 8.622e+02, MSE(pi1): 6.041e-01, MSE(pi2): 4.243e+00, MSE(pi3): 2.163e-01\n",
      "Epoch 2500, Train loss: 7.325e+10, Test loss: 7.834e+10, MSE(e): 7.325e+02, MSE(pi1): 6.041e-01, MSE(pi2): 3.732e+00, MSE(pi3): 2.163e-01\n",
      "Epoch 2600, Train loss: 6.213e+10, Test loss: 6.770e+10, MSE(e): 6.213e+02, MSE(pi1): 6.088e-01, MSE(pi2): 3.291e+00, MSE(pi3): 2.162e-01\n",
      "Epoch 2700, Train loss: 5.122e+10, Test loss: 5.647e+10, MSE(e): 5.122e+02, MSE(pi1): 6.182e-01, MSE(pi2): 2.848e+00, MSE(pi3): 2.156e-01\n",
      "Epoch 2800, Train loss: 4.182e+10, Test loss: 4.715e+10, MSE(e): 4.182e+02, MSE(pi1): 6.128e-01, MSE(pi2): 2.474e+00, MSE(pi3): 2.127e-01\n",
      "Epoch 2900, Train loss: 3.389e+10, Test loss: 3.914e+10, MSE(e): 3.389e+02, MSE(pi1): 6.030e-01, MSE(pi2): 2.161e+00, MSE(pi3): 2.067e-01\n",
      "Epoch 3000, Train loss: 2.740e+10, Test loss: 3.239e+10, MSE(e): 2.740e+02, MSE(pi1): 5.869e-01, MSE(pi2): 1.905e+00, MSE(pi3): 2.053e-01\n",
      "Epoch 3100, Train loss: 2.217e+10, Test loss: 2.682e+10, MSE(e): 2.217e+02, MSE(pi1): 5.686e-01, MSE(pi2): 1.700e+00, MSE(pi3): 2.036e-01\n",
      "Epoch 3200, Train loss: 1.770e+10, Test loss: 2.212e+10, MSE(e): 1.770e+02, MSE(pi1): 5.671e-01, MSE(pi2): 1.525e+00, MSE(pi3): 2.001e-01\n",
      "Epoch 3300, Train loss: 1.392e+10, Test loss: 1.808e+10, MSE(e): 1.392e+02, MSE(pi1): 5.724e-01, MSE(pi2): 1.377e+00, MSE(pi3): 1.979e-01\n",
      "Epoch 3400, Train loss: 1.095e+10, Test loss: 1.476e+10, MSE(e): 1.095e+02, MSE(pi1): 5.702e-01, MSE(pi2): 1.259e+00, MSE(pi3): 1.973e-01\n",
      "Epoch 3500, Train loss: 8.739e+09, Test loss: 1.209e+10, MSE(e): 8.739e+01, MSE(pi1): 5.638e-01, MSE(pi2): 1.172e+00, MSE(pi3): 1.971e-01\n",
      "Epoch 3600, Train loss: 7.121e+09, Test loss: 9.993e+09, MSE(e): 7.121e+01, MSE(pi1): 5.572e-01, MSE(pi2): 1.109e+00, MSE(pi3): 1.968e-01\n",
      "Epoch 3700, Train loss: 5.942e+09, Test loss: 8.394e+09, MSE(e): 5.942e+01, MSE(pi1): 5.507e-01, MSE(pi2): 1.064e+00, MSE(pi3): 1.964e-01\n",
      "Epoch 3800, Train loss: 5.051e+09, Test loss: 7.133e+09, MSE(e): 5.051e+01, MSE(pi1): 5.421e-01, MSE(pi2): 1.029e+00, MSE(pi3): 1.956e-01\n",
      "Epoch 3900, Train loss: 4.396e+09, Test loss: 6.117e+09, MSE(e): 4.396e+01, MSE(pi1): 5.365e-01, MSE(pi2): 1.003e+00, MSE(pi3): 1.951e-01\n",
      "Epoch 4000, Train loss: 3.916e+09, Test loss: 5.299e+09, MSE(e): 3.916e+01, MSE(pi1): 5.363e-01, MSE(pi2): 9.841e-01, MSE(pi3): 1.949e-01\n",
      "Epoch 4100, Train loss: 3.595e+09, Test loss: 4.664e+09, MSE(e): 3.595e+01, MSE(pi1): 5.367e-01, MSE(pi2): 9.713e-01, MSE(pi3): 1.947e-01\n",
      "Epoch 4200, Train loss: 3.397e+09, Test loss: 4.184e+09, MSE(e): 3.397e+01, MSE(pi1): 5.370e-01, MSE(pi2): 9.634e-01, MSE(pi3): 1.940e-01\n",
      "Epoch 4300, Train loss: 3.261e+09, Test loss: 3.821e+09, MSE(e): 3.261e+01, MSE(pi1): 5.411e-01, MSE(pi2): 9.579e-01, MSE(pi3): 1.932e-01\n",
      "Epoch 4400, Train loss: 3.160e+09, Test loss: 3.549e+09, MSE(e): 3.160e+01, MSE(pi1): 5.461e-01, MSE(pi2): 9.539e-01, MSE(pi3): 1.927e-01\n",
      "Epoch 4500, Train loss: 3.079e+09, Test loss: 3.337e+09, MSE(e): 3.079e+01, MSE(pi1): 5.491e-01, MSE(pi2): 9.507e-01, MSE(pi3): 1.923e-01\n",
      "Epoch 4600, Train loss: 3.007e+09, Test loss: 3.164e+09, MSE(e): 3.007e+01, MSE(pi1): 5.480e-01, MSE(pi2): 9.480e-01, MSE(pi3): 1.922e-01\n",
      "Epoch 4700, Train loss: 2.940e+09, Test loss: 3.017e+09, MSE(e): 2.940e+01, MSE(pi1): 5.464e-01, MSE(pi2): 9.456e-01, MSE(pi3): 1.923e-01\n",
      "Epoch 4800, Train loss: 2.874e+09, Test loss: 2.886e+09, MSE(e): 2.874e+01, MSE(pi1): 5.449e-01, MSE(pi2): 9.436e-01, MSE(pi3): 1.926e-01\n",
      "Epoch 4900, Train loss: 2.806e+09, Test loss: 2.762e+09, MSE(e): 2.806e+01, MSE(pi1): 5.425e-01, MSE(pi2): 9.420e-01, MSE(pi3): 1.931e-01\n",
      "Epoch 5000, Train loss: 2.739e+09, Test loss: 2.647e+09, MSE(e): 2.739e+01, MSE(pi1): 5.391e-01, MSE(pi2): 9.412e-01, MSE(pi3): 1.938e-01\n",
      "Epoch 5100, Train loss: 2.669e+09, Test loss: 2.537e+09, MSE(e): 2.669e+01, MSE(pi1): 5.309e-01, MSE(pi2): 9.410e-01, MSE(pi3): 1.944e-01\n",
      "Epoch 5200, Train loss: 2.575e+09, Test loss: 2.345e+09, MSE(e): 2.575e+01, MSE(pi1): 5.197e-01, MSE(pi2): 9.414e-01, MSE(pi3): 1.949e-01\n",
      "Epoch 5300, Train loss: 2.369e+09, Test loss: 2.107e+09, MSE(e): 2.369e+01, MSE(pi1): 5.114e-01, MSE(pi2): 9.483e-01, MSE(pi3): 1.972e-01\n",
      "Epoch 5400, Train loss: 1.791e+09, Test loss: 1.704e+09, MSE(e): 1.791e+01, MSE(pi1): 6.372e-01, MSE(pi2): 9.815e-01, MSE(pi3): 2.041e-01\n",
      "Epoch 5500, Train loss: 1.233e+09, Test loss: 1.285e+09, MSE(e): 1.233e+01, MSE(pi1): 8.260e-01, MSE(pi2): 1.039e+00, MSE(pi3): 2.121e-01\n",
      "Epoch 5600, Train loss: 8.240e+08, Test loss: 9.480e+08, MSE(e): 8.240e+00, MSE(pi1): 8.746e-01, MSE(pi2): 1.108e+00, MSE(pi3): 2.180e-01\n",
      "Epoch 5700, Train loss: 5.776e+08, Test loss: 7.148e+08, MSE(e): 5.776e+00, MSE(pi1): 7.788e-01, MSE(pi2): 1.155e+00, MSE(pi3): 2.215e-01\n",
      "Epoch 5800, Train loss: 4.125e+08, Test loss: 5.358e+08, MSE(e): 4.125e+00, MSE(pi1): 6.674e-01, MSE(pi2): 1.179e+00, MSE(pi3): 2.242e-01\n",
      "Epoch 5900, Train loss: 2.961e+08, Test loss: 3.894e+08, MSE(e): 2.960e+00, MSE(pi1): 5.996e-01, MSE(pi2): 1.192e+00, MSE(pi3): 2.249e-01\n",
      "Epoch 6000, Train loss: 2.135e+08, Test loss: 2.781e+08, MSE(e): 2.135e+00, MSE(pi1): 5.639e-01, MSE(pi2): 1.201e+00, MSE(pi3): 2.244e-01\n",
      "Epoch 6100, Train loss: 1.571e+08, Test loss: 2.178e+08, MSE(e): 1.570e+00, MSE(pi1): 5.292e-01, MSE(pi2): 1.206e+00, MSE(pi3): 2.230e-01\n",
      "Epoch 6200, Train loss: 1.193e+08, Test loss: 1.796e+08, MSE(e): 1.193e+00, MSE(pi1): 5.125e-01, MSE(pi2): 1.209e+00, MSE(pi3): 2.211e-01\n",
      "Epoch 6300, Train loss: 9.324e+07, Test loss: 1.533e+08, MSE(e): 9.321e-01, MSE(pi1): 5.109e-01, MSE(pi2): 1.211e+00, MSE(pi3): 2.190e-01\n",
      "Epoch 6400, Train loss: 7.432e+07, Test loss: 1.273e+08, MSE(e): 7.429e-01, MSE(pi1): 5.219e-01, MSE(pi2): 1.213e+00, MSE(pi3): 2.169e-01\n",
      "Epoch 6500, Train loss: 6.014e+07, Test loss: 1.050e+08, MSE(e): 6.012e-01, MSE(pi1): 5.136e-01, MSE(pi2): 1.215e+00, MSE(pi3): 2.182e-01\n",
      "Epoch 6600, Train loss: 5.013e+07, Test loss: 8.955e+07, MSE(e): 5.010e-01, MSE(pi1): 5.213e-01, MSE(pi2): 1.217e+00, MSE(pi3): 2.174e-01\n",
      "Epoch 6700, Train loss: 4.330e+07, Test loss: 7.781e+07, MSE(e): 4.327e-01, MSE(pi1): 5.258e-01, MSE(pi2): 1.218e+00, MSE(pi3): 2.170e-01\n",
      "Epoch 6800, Train loss: 3.853e+07, Test loss: 6.885e+07, MSE(e): 3.850e-01, MSE(pi1): 5.279e-01, MSE(pi2): 1.219e+00, MSE(pi3): 2.168e-01\n",
      "Epoch 6900, Train loss: 3.494e+07, Test loss: 6.182e+07, MSE(e): 3.492e-01, MSE(pi1): 5.286e-01, MSE(pi2): 1.220e+00, MSE(pi3): 2.166e-01\n",
      "Epoch 7000, Train loss: 3.206e+07, Test loss: 5.616e+07, MSE(e): 3.203e-01, MSE(pi1): 5.281e-01, MSE(pi2): 1.220e+00, MSE(pi3): 2.164e-01\n",
      "Epoch 7100, Train loss: 2.964e+07, Test loss: 5.150e+07, MSE(e): 2.961e-01, MSE(pi1): 5.267e-01, MSE(pi2): 1.221e+00, MSE(pi3): 2.161e-01\n",
      "Epoch 7200, Train loss: 2.756e+07, Test loss: 4.757e+07, MSE(e): 2.753e-01, MSE(pi1): 5.250e-01, MSE(pi2): 1.221e+00, MSE(pi3): 2.159e-01\n",
      "Epoch 7300, Train loss: 2.573e+07, Test loss: 4.414e+07, MSE(e): 2.570e-01, MSE(pi1): 5.233e-01, MSE(pi2): 1.222e+00, MSE(pi3): 2.156e-01\n",
      "Epoch 7400, Train loss: 2.393e+07, Test loss: 4.089e+07, MSE(e): 2.390e-01, MSE(pi1): 5.207e-01, MSE(pi2): 1.222e+00, MSE(pi3): 2.153e-01\n",
      "Epoch 7500, Train loss: 2.185e+07, Test loss: 3.766e+07, MSE(e): 2.182e-01, MSE(pi1): 5.187e-01, MSE(pi2): 1.223e+00, MSE(pi3): 2.148e-01\n",
      "Epoch 7600, Train loss: 2.014e+07, Test loss: 3.498e+07, MSE(e): 2.011e-01, MSE(pi1): 5.155e-01, MSE(pi2): 1.223e+00, MSE(pi3): 2.144e-01\n",
      "Epoch 7700, Train loss: 1.866e+07, Test loss: 3.258e+07, MSE(e): 1.863e-01, MSE(pi1): 5.136e-01, MSE(pi2): 1.223e+00, MSE(pi3): 2.140e-01\n",
      "Epoch 7800, Train loss: 1.727e+07, Test loss: 3.043e+07, MSE(e): 1.724e-01, MSE(pi1): 5.089e-01, MSE(pi2): 1.223e+00, MSE(pi3): 2.139e-01\n",
      "Epoch 7900, Train loss: 1.599e+07, Test loss: 2.853e+07, MSE(e): 1.596e-01, MSE(pi1): 5.054e-01, MSE(pi2): 1.224e+00, MSE(pi3): 2.138e-01\n",
      "Epoch 8000, Train loss: 1.483e+07, Test loss: 2.686e+07, MSE(e): 1.480e-01, MSE(pi1): 5.036e-01, MSE(pi2): 1.224e+00, MSE(pi3): 2.135e-01\n",
      "Epoch 8100, Train loss: 1.378e+07, Test loss: 2.537e+07, MSE(e): 1.375e-01, MSE(pi1): 5.014e-01, MSE(pi2): 1.224e+00, MSE(pi3): 2.133e-01\n",
      "Epoch 8200, Train loss: 1.282e+07, Test loss: 2.405e+07, MSE(e): 1.280e-01, MSE(pi1): 4.995e-01, MSE(pi2): 1.225e+00, MSE(pi3): 2.132e-01\n",
      "Epoch 8300, Train loss: 1.196e+07, Test loss: 2.287e+07, MSE(e): 1.193e-01, MSE(pi1): 4.981e-01, MSE(pi2): 1.225e+00, MSE(pi3): 2.131e-01\n",
      "Epoch 8400, Train loss: 1.117e+07, Test loss: 2.180e+07, MSE(e): 1.114e-01, MSE(pi1): 4.970e-01, MSE(pi2): 1.225e+00, MSE(pi3): 2.131e-01\n",
      "Epoch 8500, Train loss: 1.044e+07, Test loss: 2.083e+07, MSE(e): 1.041e-01, MSE(pi1): 4.961e-01, MSE(pi2): 1.226e+00, MSE(pi3): 2.130e-01\n",
      "Epoch 8600, Train loss: 9.778e+06, Test loss: 1.995e+07, MSE(e): 9.750e-02, MSE(pi1): 4.953e-01, MSE(pi2): 1.226e+00, MSE(pi3): 2.130e-01\n",
      "Epoch 8700, Train loss: 9.168e+06, Test loss: 1.914e+07, MSE(e): 9.140e-02, MSE(pi1): 4.944e-01, MSE(pi2): 1.226e+00, MSE(pi3): 2.130e-01\n",
      "Epoch 8800, Train loss: 8.606e+06, Test loss: 1.839e+07, MSE(e): 8.579e-02, MSE(pi1): 4.932e-01, MSE(pi2): 1.227e+00, MSE(pi3): 2.130e-01\n",
      "Epoch 8900, Train loss: 8.088e+06, Test loss: 1.769e+07, MSE(e): 8.061e-02, MSE(pi1): 4.920e-01, MSE(pi2): 1.227e+00, MSE(pi3): 2.130e-01\n",
      "Epoch 9000, Train loss: 7.609e+06, Test loss: 1.703e+07, MSE(e): 7.582e-02, MSE(pi1): 4.909e-01, MSE(pi2): 1.227e+00, MSE(pi3): 2.129e-01\n",
      "Epoch 9100, Train loss: 7.166e+06, Test loss: 1.641e+07, MSE(e): 7.139e-02, MSE(pi1): 4.894e-01, MSE(pi2): 1.227e+00, MSE(pi3): 2.128e-01\n",
      "Epoch 9200, Train loss: 6.755e+06, Test loss: 1.582e+07, MSE(e): 6.728e-02, MSE(pi1): 4.854e-01, MSE(pi2): 1.228e+00, MSE(pi3): 2.129e-01\n",
      "Epoch 9300, Train loss: 6.374e+06, Test loss: 1.525e+07, MSE(e): 6.347e-02, MSE(pi1): 4.841e-01, MSE(pi2): 1.228e+00, MSE(pi3): 2.127e-01\n",
      "Epoch 9400, Train loss: 6.020e+06, Test loss: 1.470e+07, MSE(e): 5.992e-02, MSE(pi1): 4.795e-01, MSE(pi2): 1.228e+00, MSE(pi3): 2.128e-01\n",
      "Epoch 9500, Train loss: 5.690e+06, Test loss: 1.418e+07, MSE(e): 5.663e-02, MSE(pi1): 4.755e-01, MSE(pi2): 1.228e+00, MSE(pi3): 2.129e-01\n",
      "Epoch 9600, Train loss: 5.384e+06, Test loss: 1.369e+07, MSE(e): 5.357e-02, MSE(pi1): 4.719e-01, MSE(pi2): 1.229e+00, MSE(pi3): 2.129e-01\n",
      "Epoch 9700, Train loss: 5.101e+06, Test loss: 1.321e+07, MSE(e): 5.073e-02, MSE(pi1): 4.673e-01, MSE(pi2): 1.229e+00, MSE(pi3): 2.130e-01\n",
      "Epoch 9800, Train loss: 4.838e+06, Test loss: 1.276e+07, MSE(e): 4.811e-02, MSE(pi1): 4.625e-01, MSE(pi2): 1.229e+00, MSE(pi3): 2.131e-01\n",
      "Epoch 9900, Train loss: 4.598e+06, Test loss: 1.234e+07, MSE(e): 4.571e-02, MSE(pi1): 4.595e-01, MSE(pi2): 1.229e+00, MSE(pi3): 2.131e-01\n",
      "Epoch 10000, Train loss: 4.383e+06, Test loss: 1.193e+07, MSE(e): 4.356e-02, MSE(pi1): 4.512e-01, MSE(pi2): 1.229e+00, MSE(pi3): 2.137e-01\n",
      "Epoch 10100, Train loss: 4.221e+06, Test loss: 1.148e+07, MSE(e): 4.194e-02, MSE(pi1): 4.477e-01, MSE(pi2): 1.229e+00, MSE(pi3): 2.138e-01\n",
      "Epoch 10200, Train loss: 3.981e+06, Test loss: 1.105e+07, MSE(e): 3.954e-02, MSE(pi1): 4.440e-01, MSE(pi2): 1.230e+00, MSE(pi3): 2.139e-01\n",
      "Epoch 10300, Train loss: 3.768e+06, Test loss: 1.068e+07, MSE(e): 3.741e-02, MSE(pi1): 4.416e-01, MSE(pi2): 1.230e+00, MSE(pi3): 2.138e-01\n",
      "Epoch 10400, Train loss: 3.572e+06, Test loss: 1.034e+07, MSE(e): 3.545e-02, MSE(pi1): 4.373e-01, MSE(pi2): 1.230e+00, MSE(pi3): 2.139e-01\n",
      "Epoch 10500, Train loss: 3.395e+06, Test loss: 1.003e+07, MSE(e): 3.368e-02, MSE(pi1): 4.340e-01, MSE(pi2): 1.230e+00, MSE(pi3): 2.139e-01\n",
      "Epoch 10600, Train loss: 3.234e+06, Test loss: 9.741e+06, MSE(e): 3.207e-02, MSE(pi1): 4.306e-01, MSE(pi2): 1.230e+00, MSE(pi3): 2.138e-01\n",
      "Epoch 10700, Train loss: 3.087e+06, Test loss: 9.477e+06, MSE(e): 3.060e-02, MSE(pi1): 4.279e-01, MSE(pi2): 1.230e+00, MSE(pi3): 2.137e-01\n",
      "Epoch 10800, Train loss: 2.953e+06, Test loss: 9.233e+06, MSE(e): 2.926e-02, MSE(pi1): 4.238e-01, MSE(pi2): 1.230e+00, MSE(pi3): 2.137e-01\n",
      "Epoch 10900, Train loss: 2.830e+06, Test loss: 9.006e+06, MSE(e): 2.804e-02, MSE(pi1): 4.216e-01, MSE(pi2): 1.230e+00, MSE(pi3): 2.136e-01\n",
      "Epoch 11000, Train loss: 2.717e+06, Test loss: 8.794e+06, MSE(e): 2.691e-02, MSE(pi1): 4.194e-01, MSE(pi2): 1.230e+00, MSE(pi3): 2.134e-01\n",
      "Epoch 11100, Train loss: 2.613e+06, Test loss: 8.596e+06, MSE(e): 2.587e-02, MSE(pi1): 4.168e-01, MSE(pi2): 1.230e+00, MSE(pi3): 2.133e-01\n",
      "Epoch 11200, Train loss: 2.517e+06, Test loss: 8.409e+06, MSE(e): 2.491e-02, MSE(pi1): 4.148e-01, MSE(pi2): 1.230e+00, MSE(pi3): 2.132e-01\n",
      "Epoch 11300, Train loss: 2.428e+06, Test loss: 8.233e+06, MSE(e): 2.402e-02, MSE(pi1): 4.135e-01, MSE(pi2): 1.230e+00, MSE(pi3): 2.130e-01\n",
      "Epoch 11400, Train loss: 2.346e+06, Test loss: 8.066e+06, MSE(e): 2.320e-02, MSE(pi1): 4.107e-01, MSE(pi2): 1.230e+00, MSE(pi3): 2.129e-01\n",
      "Epoch 11500, Train loss: 2.270e+06, Test loss: 7.907e+06, MSE(e): 2.243e-02, MSE(pi1): 4.084e-01, MSE(pi2): 1.230e+00, MSE(pi3): 2.129e-01\n",
      "Epoch 11600, Train loss: 2.199e+06, Test loss: 7.756e+06, MSE(e): 2.172e-02, MSE(pi1): 4.073e-01, MSE(pi2): 1.230e+00, MSE(pi3): 2.127e-01\n",
      "Epoch 11700, Train loss: 2.132e+06, Test loss: 7.610e+06, MSE(e): 2.106e-02, MSE(pi1): 4.060e-01, MSE(pi2): 1.230e+00, MSE(pi3): 2.125e-01\n",
      "Epoch 11800, Train loss: 2.070e+06, Test loss: 7.471e+06, MSE(e): 2.044e-02, MSE(pi1): 4.046e-01, MSE(pi2): 1.230e+00, MSE(pi3): 2.124e-01\n",
      "Epoch 11900, Train loss: 2.012e+06, Test loss: 7.336e+06, MSE(e): 1.985e-02, MSE(pi1): 4.029e-01, MSE(pi2): 1.230e+00, MSE(pi3): 2.123e-01\n",
      "Epoch 12000, Train loss: 1.957e+06, Test loss: 7.207e+06, MSE(e): 1.931e-02, MSE(pi1): 4.017e-01, MSE(pi2): 1.230e+00, MSE(pi3): 2.121e-01\n",
      "Epoch 12100, Train loss: 1.906e+06, Test loss: 7.082e+06, MSE(e): 1.880e-02, MSE(pi1): 4.006e-01, MSE(pi2): 1.230e+00, MSE(pi3): 2.120e-01\n",
      "Epoch 12200, Train loss: 1.857e+06, Test loss: 6.960e+06, MSE(e): 1.831e-02, MSE(pi1): 3.995e-01, MSE(pi2): 1.230e+00, MSE(pi3): 2.118e-01\n",
      "Epoch 12300, Train loss: 1.812e+06, Test loss: 6.842e+06, MSE(e): 1.785e-02, MSE(pi1): 3.983e-01, MSE(pi2): 1.230e+00, MSE(pi3): 2.117e-01\n",
      "Epoch 12400, Train loss: 1.768e+06, Test loss: 6.716e+06, MSE(e): 1.742e-02, MSE(pi1): 3.974e-01, MSE(pi2): 1.230e+00, MSE(pi3): 2.116e-01\n",
      "Epoch 12500, Train loss: 1.728e+06, Test loss: 6.580e+06, MSE(e): 1.702e-02, MSE(pi1): 3.965e-01, MSE(pi2): 1.230e+00, MSE(pi3): 2.115e-01\n",
      "Epoch 12600, Train loss: 1.689e+06, Test loss: 6.452e+06, MSE(e): 1.662e-02, MSE(pi1): 3.956e-01, MSE(pi2): 1.230e+00, MSE(pi3): 2.114e-01\n",
      "Epoch 12700, Train loss: 1.648e+06, Test loss: 6.319e+06, MSE(e): 1.622e-02, MSE(pi1): 3.940e-01, MSE(pi2): 1.230e+00, MSE(pi3): 2.114e-01\n",
      "Epoch 12800, Train loss: 1.608e+06, Test loss: 6.180e+06, MSE(e): 1.582e-02, MSE(pi1): 3.937e-01, MSE(pi2): 1.230e+00, MSE(pi3): 2.112e-01\n",
      "Epoch 12900, Train loss: 1.568e+06, Test loss: 6.034e+06, MSE(e): 1.542e-02, MSE(pi1): 3.918e-01, MSE(pi2): 1.230e+00, MSE(pi3): 2.112e-01\n",
      "Epoch 13000, Train loss: 1.528e+06, Test loss: 5.891e+06, MSE(e): 1.502e-02, MSE(pi1): 3.912e-01, MSE(pi2): 1.230e+00, MSE(pi3): 2.111e-01\n",
      "Epoch 13100, Train loss: 1.489e+06, Test loss: 5.769e+06, MSE(e): 1.463e-02, MSE(pi1): 3.909e-01, MSE(pi2): 1.230e+00, MSE(pi3): 2.109e-01\n",
      "Epoch 13200, Train loss: 1.453e+06, Test loss: 5.700e+06, MSE(e): 1.426e-02, MSE(pi1): 3.903e-01, MSE(pi2): 1.230e+00, MSE(pi3): 2.108e-01\n",
      "Epoch 13300, Train loss: 1.420e+06, Test loss: 5.712e+06, MSE(e): 1.394e-02, MSE(pi1): 3.896e-01, MSE(pi2): 1.230e+00, MSE(pi3): 2.107e-01\n",
      "Epoch 13400, Train loss: 1.391e+06, Test loss: 5.784e+06, MSE(e): 1.365e-02, MSE(pi1): 3.887e-01, MSE(pi2): 1.230e+00, MSE(pi3): 2.107e-01\n",
      "Epoch 13500, Train loss: 1.361e+06, Test loss: 5.825e+06, MSE(e): 1.335e-02, MSE(pi1): 3.884e-01, MSE(pi2): 1.230e+00, MSE(pi3): 2.105e-01\n",
      "Epoch 13600, Train loss: 1.339e+06, Test loss: 5.849e+06, MSE(e): 1.313e-02, MSE(pi1): 3.888e-01, MSE(pi2): 1.230e+00, MSE(pi3): 2.103e-01\n",
      "Epoch 13700, Train loss: 1.313e+06, Test loss: 5.833e+06, MSE(e): 1.287e-02, MSE(pi1): 3.867e-01, MSE(pi2): 1.230e+00, MSE(pi3): 2.104e-01\n",
      "Epoch 13800, Train loss: 1.289e+06, Test loss: 5.807e+06, MSE(e): 1.263e-02, MSE(pi1): 3.874e-01, MSE(pi2): 1.230e+00, MSE(pi3): 2.102e-01\n",
      "Epoch 13900, Train loss: 1.267e+06, Test loss: 5.762e+06, MSE(e): 1.241e-02, MSE(pi1): 3.868e-01, MSE(pi2): 1.230e+00, MSE(pi3): 2.100e-01\n",
      "Epoch 14000, Train loss: 1.244e+06, Test loss: 5.711e+06, MSE(e): 1.218e-02, MSE(pi1): 3.853e-01, MSE(pi2): 1.230e+00, MSE(pi3): 2.102e-01\n",
      "Epoch 14100, Train loss: 1.223e+06, Test loss: 5.654e+06, MSE(e): 1.197e-02, MSE(pi1): 3.851e-01, MSE(pi2): 1.230e+00, MSE(pi3): 2.101e-01\n",
      "Epoch 14200, Train loss: 1.202e+06, Test loss: 5.593e+06, MSE(e): 1.176e-02, MSE(pi1): 3.849e-01, MSE(pi2): 1.230e+00, MSE(pi3): 2.100e-01\n",
      "Epoch 14300, Train loss: 1.182e+06, Test loss: 5.529e+06, MSE(e): 1.156e-02, MSE(pi1): 3.851e-01, MSE(pi2): 1.230e+00, MSE(pi3): 2.098e-01\n",
      "Epoch 14400, Train loss: 1.162e+06, Test loss: 5.462e+06, MSE(e): 1.136e-02, MSE(pi1): 3.837e-01, MSE(pi2): 1.230e+00, MSE(pi3): 2.098e-01\n",
      "Epoch 14500, Train loss: 1.143e+06, Test loss: 5.395e+06, MSE(e): 1.117e-02, MSE(pi1): 3.851e-01, MSE(pi2): 1.230e+00, MSE(pi3): 2.095e-01\n",
      "Epoch 14600, Train loss: 1.125e+06, Test loss: 5.326e+06, MSE(e): 1.099e-02, MSE(pi1): 3.838e-01, MSE(pi2): 1.230e+00, MSE(pi3): 2.096e-01\n",
      "Epoch 14700, Train loss: 1.107e+06, Test loss: 5.256e+06, MSE(e): 1.081e-02, MSE(pi1): 3.820e-01, MSE(pi2): 1.230e+00, MSE(pi3): 2.097e-01\n",
      "Epoch 14800, Train loss: 1.089e+06, Test loss: 5.185e+06, MSE(e): 1.063e-02, MSE(pi1): 3.819e-01, MSE(pi2): 1.230e+00, MSE(pi3): 2.096e-01\n",
      "Epoch 14900, Train loss: 1.072e+06, Test loss: 5.115e+06, MSE(e): 1.046e-02, MSE(pi1): 3.811e-01, MSE(pi2): 1.230e+00, MSE(pi3): 2.096e-01\n",
      "Epoch 15000, Train loss: 1.055e+06, Test loss: 5.044e+06, MSE(e): 1.029e-02, MSE(pi1): 3.805e-01, MSE(pi2): 1.230e+00, MSE(pi3): 2.096e-01\n",
      "Epoch 15100, Train loss: 1.038e+06, Test loss: 4.973e+06, MSE(e): 1.012e-02, MSE(pi1): 3.802e-01, MSE(pi2): 1.230e+00, MSE(pi3): 2.095e-01\n",
      "Epoch 15200, Train loss: 1.022e+06, Test loss: 4.903e+06, MSE(e): 9.962e-03, MSE(pi1): 3.808e-01, MSE(pi2): 1.230e+00, MSE(pi3): 2.093e-01\n",
      "Epoch 15300, Train loss: 1.006e+06, Test loss: 4.833e+06, MSE(e): 9.804e-03, MSE(pi1): 3.804e-01, MSE(pi2): 1.230e+00, MSE(pi3): 2.092e-01\n",
      "Epoch 15400, Train loss: 9.909e+05, Test loss: 4.764e+06, MSE(e): 9.649e-03, MSE(pi1): 3.785e-01, MSE(pi2): 1.230e+00, MSE(pi3): 2.094e-01\n",
      "Epoch 15500, Train loss: 9.755e+05, Test loss: 4.695e+06, MSE(e): 9.495e-03, MSE(pi1): 3.788e-01, MSE(pi2): 1.230e+00, MSE(pi3): 2.093e-01\n",
      "Epoch 15600, Train loss: 9.606e+05, Test loss: 4.628e+06, MSE(e): 9.347e-03, MSE(pi1): 3.778e-01, MSE(pi2): 1.230e+00, MSE(pi3): 2.094e-01\n",
      "Epoch 15700, Train loss: 9.459e+05, Test loss: 4.561e+06, MSE(e): 9.200e-03, MSE(pi1): 3.778e-01, MSE(pi2): 1.230e+00, MSE(pi3): 2.093e-01\n",
      "Epoch 15800, Train loss: 9.315e+05, Test loss: 4.495e+06, MSE(e): 9.055e-03, MSE(pi1): 3.788e-01, MSE(pi2): 1.230e+00, MSE(pi3): 2.092e-01\n",
      "Epoch 15900, Train loss: 9.173e+05, Test loss: 4.430e+06, MSE(e): 8.914e-03, MSE(pi1): 3.766e-01, MSE(pi2): 1.230e+00, MSE(pi3): 2.094e-01\n",
      "Epoch 16000, Train loss: 9.034e+05, Test loss: 4.365e+06, MSE(e): 8.774e-03, MSE(pi1): 3.767e-01, MSE(pi2): 1.230e+00, MSE(pi3): 2.094e-01\n",
      "Epoch 16100, Train loss: 8.897e+05, Test loss: 4.302e+06, MSE(e): 8.638e-03, MSE(pi1): 3.773e-01, MSE(pi2): 1.230e+00, MSE(pi3): 2.093e-01\n",
      "Epoch 16200, Train loss: 8.765e+05, Test loss: 4.240e+06, MSE(e): 8.505e-03, MSE(pi1): 3.765e-01, MSE(pi2): 1.230e+00, MSE(pi3): 2.093e-01\n",
      "Epoch 16300, Train loss: 8.635e+05, Test loss: 4.179e+06, MSE(e): 8.376e-03, MSE(pi1): 3.763e-01, MSE(pi2): 1.230e+00, MSE(pi3): 2.093e-01\n",
      "Epoch 16400, Train loss: 8.508e+05, Test loss: 4.118e+06, MSE(e): 8.249e-03, MSE(pi1): 3.771e-01, MSE(pi2): 1.230e+00, MSE(pi3): 2.092e-01\n",
      "Epoch 16500, Train loss: 8.387e+05, Test loss: 4.059e+06, MSE(e): 8.127e-03, MSE(pi1): 3.782e-01, MSE(pi2): 1.230e+00, MSE(pi3): 2.091e-01\n",
      "Epoch 16600, Train loss: 8.268e+05, Test loss: 4.000e+06, MSE(e): 8.009e-03, MSE(pi1): 3.771e-01, MSE(pi2): 1.230e+00, MSE(pi3): 2.091e-01\n",
      "Epoch 16700, Train loss: 8.154e+05, Test loss: 3.943e+06, MSE(e): 7.895e-03, MSE(pi1): 3.754e-01, MSE(pi2): 1.230e+00, MSE(pi3): 2.093e-01\n",
      "Epoch 16800, Train loss: 8.044e+05, Test loss: 3.886e+06, MSE(e): 7.785e-03, MSE(pi1): 3.760e-01, MSE(pi2): 1.230e+00, MSE(pi3): 2.093e-01\n",
      "Epoch 16900, Train loss: 7.939e+05, Test loss: 3.830e+06, MSE(e): 7.680e-03, MSE(pi1): 3.763e-01, MSE(pi2): 1.230e+00, MSE(pi3): 2.092e-01\n",
      "Epoch 17000, Train loss: 7.839e+05, Test loss: 3.775e+06, MSE(e): 7.580e-03, MSE(pi1): 3.756e-01, MSE(pi2): 1.230e+00, MSE(pi3): 2.093e-01\n",
      "Epoch 17100, Train loss: 7.745e+05, Test loss: 3.721e+06, MSE(e): 7.486e-03, MSE(pi1): 3.760e-01, MSE(pi2): 1.230e+00, MSE(pi3): 2.093e-01\n",
      "Epoch 17200, Train loss: 7.658e+05, Test loss: 3.668e+06, MSE(e): 7.399e-03, MSE(pi1): 3.760e-01, MSE(pi2): 1.230e+00, MSE(pi3): 2.092e-01\n",
      "Epoch 17300, Train loss: 7.590e+05, Test loss: 3.613e+06, MSE(e): 7.331e-03, MSE(pi1): 3.763e-01, MSE(pi2): 1.230e+00, MSE(pi3): 2.092e-01\n",
      "Epoch 17400, Train loss: 8.327e+05, Test loss: 3.602e+06, MSE(e): 8.068e-03, MSE(pi1): 3.770e-01, MSE(pi2): 1.230e+00, MSE(pi3): 2.091e-01\n",
      "Epoch 17500, Train loss: 7.987e+05, Test loss: 3.533e+06, MSE(e): 7.728e-03, MSE(pi1): 3.797e-01, MSE(pi2): 1.230e+00, MSE(pi3): 2.088e-01\n",
      "Epoch 17600, Train loss: 8.178e+05, Test loss: 3.500e+06, MSE(e): 7.919e-03, MSE(pi1): 3.795e-01, MSE(pi2): 1.230e+00, MSE(pi3): 2.088e-01\n",
      "Epoch 17700, Train loss: 7.750e+05, Test loss: 3.426e+06, MSE(e): 7.491e-03, MSE(pi1): 3.766e-01, MSE(pi2): 1.230e+00, MSE(pi3): 2.091e-01\n",
      "Epoch 17800, Train loss: 7.445e+05, Test loss: 3.373e+06, MSE(e): 7.187e-03, MSE(pi1): 3.794e-01, MSE(pi2): 1.230e+00, MSE(pi3): 2.087e-01\n",
      "Epoch 17900, Train loss: 7.880e+05, Test loss: 3.343e+06, MSE(e): 7.621e-03, MSE(pi1): 3.815e-01, MSE(pi2): 1.230e+00, MSE(pi3): 2.084e-01\n",
      "Epoch 18000, Train loss: 7.291e+05, Test loss: 3.284e+06, MSE(e): 7.032e-03, MSE(pi1): 3.783e-01, MSE(pi2): 1.230e+00, MSE(pi3): 2.088e-01\n",
      "Epoch 18100, Train loss: 7.298e+05, Test loss: 3.235e+06, MSE(e): 7.039e-03, MSE(pi1): 3.797e-01, MSE(pi2): 1.230e+00, MSE(pi3): 2.087e-01\n",
      "Epoch 18200, Train loss: 7.033e+05, Test loss: 3.218e+06, MSE(e): 6.774e-03, MSE(pi1): 3.797e-01, MSE(pi2): 1.230e+00, MSE(pi3): 2.086e-01\n",
      "Epoch 18300, Train loss: 7.102e+05, Test loss: 3.148e+06, MSE(e): 6.843e-03, MSE(pi1): 3.796e-01, MSE(pi2): 1.230e+00, MSE(pi3): 2.087e-01\n",
      "Epoch 18400, Train loss: 6.899e+05, Test loss: 3.153e+06, MSE(e): 6.640e-03, MSE(pi1): 3.803e-01, MSE(pi2): 1.230e+00, MSE(pi3): 2.086e-01\n",
      "Epoch 18500, Train loss: 6.887e+05, Test loss: 3.070e+06, MSE(e): 6.628e-03, MSE(pi1): 3.800e-01, MSE(pi2): 1.230e+00, MSE(pi3): 2.086e-01\n",
      "Epoch 18600, Train loss: 6.813e+05, Test loss: 3.029e+06, MSE(e): 6.554e-03, MSE(pi1): 3.800e-01, MSE(pi2): 1.230e+00, MSE(pi3): 2.086e-01\n",
      "Epoch 18700, Train loss: 6.696e+05, Test loss: 3.002e+06, MSE(e): 6.437e-03, MSE(pi1): 3.802e-01, MSE(pi2): 1.230e+00, MSE(pi3): 2.086e-01\n",
      "Epoch 18800, Train loss: 6.630e+05, Test loss: 2.954e+06, MSE(e): 6.371e-03, MSE(pi1): 3.803e-01, MSE(pi2): 1.230e+00, MSE(pi3): 2.086e-01\n",
      "Epoch 18900, Train loss: 6.620e+05, Test loss: 2.906e+06, MSE(e): 6.361e-03, MSE(pi1): 3.815e-01, MSE(pi2): 1.230e+00, MSE(pi3): 2.085e-01\n",
      "Epoch 19000, Train loss: 6.535e+05, Test loss: 2.873e+06, MSE(e): 6.276e-03, MSE(pi1): 3.813e-01, MSE(pi2): 1.230e+00, MSE(pi3): 2.085e-01\n",
      "Epoch 19100, Train loss: 6.474e+05, Test loss: 2.832e+06, MSE(e): 6.215e-03, MSE(pi1): 3.812e-01, MSE(pi2): 1.230e+00, MSE(pi3): 2.085e-01\n",
      "Epoch 19200, Train loss: 6.438e+05, Test loss: 2.790e+06, MSE(e): 6.179e-03, MSE(pi1): 3.802e-01, MSE(pi2): 1.230e+00, MSE(pi3): 2.086e-01\n",
      "Epoch 19300, Train loss: 6.370e+05, Test loss: 2.754e+06, MSE(e): 6.111e-03, MSE(pi1): 3.811e-01, MSE(pi2): 1.230e+00, MSE(pi3): 2.085e-01\n",
      "Epoch 19400, Train loss: 6.333e+05, Test loss: 2.709e+06, MSE(e): 6.074e-03, MSE(pi1): 3.811e-01, MSE(pi2): 1.230e+00, MSE(pi3): 2.085e-01\n",
      "Epoch 19500, Train loss: 6.274e+05, Test loss: 2.671e+06, MSE(e): 6.016e-03, MSE(pi1): 3.814e-01, MSE(pi2): 1.230e+00, MSE(pi3): 2.084e-01\n",
      "Epoch 19600, Train loss: 6.235e+05, Test loss: 2.626e+06, MSE(e): 5.976e-03, MSE(pi1): 3.819e-01, MSE(pi2): 1.230e+00, MSE(pi3): 2.083e-01\n",
      "Epoch 19700, Train loss: 6.184e+05, Test loss: 2.584e+06, MSE(e): 5.925e-03, MSE(pi1): 3.815e-01, MSE(pi2): 1.230e+00, MSE(pi3): 2.084e-01\n",
      "Epoch 19800, Train loss: 6.144e+05, Test loss: 2.537e+06, MSE(e): 5.886e-03, MSE(pi1): 3.822e-01, MSE(pi2): 1.230e+00, MSE(pi3): 2.083e-01\n",
      "Epoch 19900, Train loss: 6.093e+05, Test loss: 2.493e+06, MSE(e): 5.834e-03, MSE(pi1): 3.829e-01, MSE(pi2): 1.230e+00, MSE(pi3): 2.082e-01\n",
      "\n",
      "Proceso finalizado después de 20000 épocas\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Se carga el modelo y el optimizador\n",
    "model = FFTNeuralNetwork(input_size, predictive_output_size, explanatory_output_size)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=3e-4)\n",
    "\n",
    "# Parametros de entrenamiento\n",
    "start_epoch = 0\n",
    "n_epochs = 20000\n",
    "\n",
    "batch_size = 64\n",
    "n_checkpoints = 100\n",
    "\n",
    "train_loop(model, optimizer, n_checkpoints,\n",
    "           X_train, y_train, X_test, y_test, f_train, f_test,\n",
    "           D=D, start_epoch=start_epoch, n_epochs=n_epochs, batch_size=batch_size, \n",
    "           model_results_path=MODEL_RESULTS_PATH, device=device,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SciML_test_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
