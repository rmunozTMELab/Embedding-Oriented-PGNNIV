{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from collections import Counter\n",
    "\n",
    "# Imports de la libreria propia\n",
    "from vecopsciml.kernels.derivative import DerivativeKernels\n",
    "from vecopsciml.utils import TensOps\n",
    "\n",
    "# Imports de las funciones creadas para este programa\n",
    "from utils.folders import create_folder\n",
    "from utils.load_data import load_data\n",
    "from trainers.train import train_loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "print(f\"Using device: {DEVICE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Folder already exists at: /home/rmunoz/Escritorio/rmunozTMELab/Physically-Guided-Machine-Learning/results/non_linear\n",
      "Folder already exists at: /home/rmunoz/Escritorio/rmunozTMELab/Physically-Guided-Machine-Learning/results/non_linear/model_autoencoder_AE\n",
      "Folder already exists at: /home/rmunoz/Escritorio/rmunozTMELab/Physically-Guided-Machine-Learning/results/non_linear/model_autoencoder_NN\n"
     ]
    }
   ],
   "source": [
    "# Creamos los paths para las distintas carpetas\n",
    "ROOT_PATH = r'/home/rmunoz/Escritorio/rmunozTMELab/Physically-Guided-Machine-Learning'\n",
    "DATA_PATH = os.path.join(ROOT_PATH, r'data/non_linear/non_linear_decomposition.pkl')\n",
    "RESULTS_FOLDER_PATH = os.path.join(ROOT_PATH, r'results/non_linear')\n",
    "MODEL_RESULTS_AE_PATH = os.path.join(ROOT_PATH, r'results/non_linear/model_autoencoder_AE')\n",
    "MODEL_RESULTS_PGNNIV_PATH = os.path.join(ROOT_PATH, r'results/non_linear/model_autoencoder_NN')\n",
    "\n",
    "\n",
    "# Creamos las carpetas que sean necesarias (si ya están creadas se avisará de ello)\n",
    "create_folder(RESULTS_FOLDER_PATH)\n",
    "create_folder(MODEL_RESULTS_AE_PATH)\n",
    "create_folder(MODEL_RESULTS_PGNNIV_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully loaded from: /home/rmunoz/Escritorio/rmunozTMELab/Physically-Guided-Machine-Learning/data/non_linear/non_linear_decomposition.pkl\n"
     ]
    }
   ],
   "source": [
    "# Load dataset\n",
    "dataset = load_data(DATA_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convolutional filters to derivate\n",
    "dx = dataset['x_step_size']\n",
    "dy = dataset['y_step_size']\n",
    "D = DerivativeKernels(dx, dy, 0).grad_kernels_two_dimensions()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## División de los datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset length: 8000\n",
      "Validation dataset length: 2000\n"
     ]
    }
   ],
   "source": [
    "X_train = torch.Tensor(dataset['X_train']).unsqueeze(1)\n",
    "y_train = torch.Tensor(dataset['y_train']).unsqueeze(1)\n",
    "K_train = torch.tensor(dataset['k_train']).unsqueeze(1)\n",
    "f_train = torch.tensor(dataset['f_train']).unsqueeze(1).to(torch.float32)\n",
    "\n",
    "X_val = torch.Tensor(dataset['X_val']).unsqueeze(1)\n",
    "y_val = TensOps(torch.Tensor(dataset['y_val']).unsqueeze(1).requires_grad_(True), space_dimension=2, contravariance=0, covariance=0)\n",
    "K_val = TensOps(torch.tensor(dataset['k_val']).unsqueeze(1).requires_grad_(True), space_dimension=2, contravariance=0, covariance=0)\n",
    "f_val = TensOps(torch.tensor(dataset['f_val']).to(torch.float32).unsqueeze(1).requires_grad_(True), space_dimension=2, contravariance=0, covariance=0)\n",
    "\n",
    "print(\"Train dataset length:\", len(X_train))\n",
    "print(\"Validation dataset length:\", len(X_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset length for the autoencoder: 2000\n",
      "Dataset length for the PGNNIV: 6000\n"
     ]
    }
   ],
   "source": [
    "N_data_AE = len(X_train)//4\n",
    "N_data_NN = len(X_train) - len(X_train)//4\n",
    "prop_data_NN = 1 - N_data_AE/(N_data_NN + N_data_AE)\n",
    "\n",
    "print(\"Dataset length for the autoencoder:\", N_data_AE)\n",
    "print(\"Dataset length for the PGNNIV:\", N_data_NN)\n",
    "\n",
    "X_AE, X_NN, y_AE, y_NN, K_AE, K_NN, f_AE, f_NN = train_test_split(X_train, y_train, K_train, f_train, test_size=prop_data_NN, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Datos para el autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_AE, y_test_AE = train_test_split(y_AE, test_size=0.2, random_state=42)\n",
    "\n",
    "y_train_AE = TensOps(y_train_AE.requires_grad_(True).to(DEVICE), space_dimension=2, contravariance=0, covariance=0)\n",
    "y_test_AE = TensOps(y_test_AE.requires_grad_(True).to(DEVICE), space_dimension=2, contravariance=0, covariance=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Datos para la PGNNIV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_NN, X_test_NN, y_train_NN, y_test_NN, K_train_NN, K_test_NN, f_train_NN, f_test_NN = train_test_split(X_NN, y_NN, K_NN, f_NN, test_size=0.2, random_state=42)\n",
    "\n",
    "X_train_NN = X_train_NN.to(DEVICE)\n",
    "X_test_NN = X_test_NN.to(DEVICE)\n",
    "\n",
    "y_train_NN = TensOps(y_train_NN.requires_grad_(True).to(DEVICE), space_dimension=2, contravariance=0, covariance=0)\n",
    "y_test_NN = TensOps(y_test_NN.requires_grad_(True).to(DEVICE), space_dimension=2, contravariance=0, covariance=0)\n",
    "\n",
    "K_train_NN = TensOps(K_train_NN.to(DEVICE), space_dimension=2, contravariance=0, covariance=0)\n",
    "K_test_NN = TensOps(K_test_NN.to(DEVICE), space_dimension=2, contravariance=0, covariance=0)\n",
    "\n",
    "f_train_NN = TensOps(f_train_NN.to(DEVICE), space_dimension=2, contravariance=0, covariance=0)\n",
    "f_test_NN = TensOps(f_test_NN.to(DEVICE), space_dimension=2, contravariance=0, covariance=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models import Autoencoder\n",
    "from trainers.eval import loss_function_autoencoder\n",
    "from utils.checkpoints import load_checkpoint, save_checkpoint\n",
    "from utils.checkpoints import load_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_autoencoder_epoch(model, optimizer, X_train, y_train):\n",
    "    model.train()\n",
    "    y_pred = model(X_train)\n",
    "    loss = loss_function_autoencoder(y_train, y_pred)\n",
    "\n",
    "    optimizer.zero_grad() \n",
    "    loss.backward(retain_graph=True)\n",
    "    optimizer.step()\n",
    "    \n",
    "    return loss\n",
    "\n",
    "\n",
    "def test_autoencoder_epoch(model, X_test, y_test):\n",
    "    y_pred = model(X_test)\n",
    "    loss = loss_function_autoencoder(y_test, y_pred)\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def autoencoder_train_loop(model, optimizer, X_train, y_train, X_test, y_test, start_epoch, n_epochs, batch_size, i_checkpoint, model_results_path, device, lr_updated=None):\n",
    "\n",
    "    if start_epoch > 0:\n",
    "        print(f'Starting training from a checkpoint. Epoch {start_epoch}.')\n",
    "\n",
    "        resume_epoch = start_epoch\n",
    "        model, optimizer, lists = load_checkpoint(model, optimizer, resume_epoch, model_results_path)\n",
    "        train_total_loss_list = lists['train_total_loss_list']\n",
    "        test_total_loss_list = lists['test_total_loss_list']\n",
    "\n",
    "        if lr_updated != None:\n",
    "            for param_group in optimizer.param_groups:\n",
    "                param_group['lr'] = lr_updated\n",
    "\n",
    "    else:\n",
    "        print(\"Starting training from the beginning\")\n",
    "\n",
    "        train_total_loss_list = []\n",
    "        test_total_loss_list = []\n",
    "\n",
    "    N_train = X_train.shape[0]\n",
    "    N_test = X_test.shape[0]\n",
    "\n",
    "    for epoch_i in range(start_epoch, n_epochs):\n",
    "\n",
    "        for batch_start in range(0, N_train, batch_size):\n",
    "            X_batch = X_train[batch_start:(batch_start+batch_size)].to(device)\n",
    "            y_batch = TensOps(y_train.values[batch_start:(batch_start+batch_size)].to(device), space_dimension=y_train.space_dim, contravariance=y_train.order[0], covariance=y_train.order[1])\n",
    "\n",
    "            loss_train = train_autoencoder_epoch(model, optimizer, X_batch, y_batch).item()\n",
    "            loss_test = test_autoencoder_epoch(model, X_test, y_test).item()\n",
    "\n",
    "        train_total_loss_list.append(loss_train/batch_size)\n",
    "        test_total_loss_list.append(loss_test/N_test)\n",
    "\n",
    "        if epoch_i % (1 if n_epochs < 100 else (10 if n_epochs <= 1000 else 1000)) == 0:\n",
    "            print(f'Epoch {epoch_i}, Train loss: {loss_train/batch_size:.3e}, Test loss: {loss_test/N_test:.3e}')\n",
    "\n",
    "        if epoch_i % (i_checkpoint) == 0:\n",
    "            save_checkpoint(model, optimizer, epoch_i, model_results_path, train_total_loss_list=train_total_loss_list, test_total_loss_list=test_total_loss_list)\n",
    "\n",
    "    save_checkpoint(model, optimizer, epoch_i, model_results_path, end_flag=True, train_total_loss_list=train_total_loss_list, test_total_loss_list=test_total_loss_list)   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "autoencoder_input_shape = y_train_AE.values[0].shape\n",
    "latent_space_dim = 20\n",
    "autoencoder_output_shape = y_train_AE.values[0].shape\n",
    "\n",
    "start_epoch = 0\n",
    "n_epochs = 100000\n",
    "batch_size = 64\n",
    "i_checkpoint = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = y_train_AE.values\n",
    "y_train = y_train_AE\n",
    "\n",
    "X_test = y_test_AE.values\n",
    "y_test = y_test_AE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = Autoencoder(input_size=autoencoder_input_shape, encoding_dim=latent_space_dim, output_size=autoencoder_output_shape, device=DEVICE)\n",
    "# optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "\n",
    "# autoencoder_train_loop(model, optimizer, X_train, y_train, X_test, y_test,  \n",
    "#                        start_epoch, n_epochs, batch_size, i_checkpoint, MODEL_RESULTS_AE_PATH, DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start_epoch = 90000\n",
    "# n_epochs = 150000\n",
    "# batch_size = 64\n",
    "# i_checkpoint = 10000\n",
    "# new_lr = 5e-5\n",
    "\n",
    "# autoencoder_train_loop(model, optimizer, X_train, y_train, X_test, y_test,  \n",
    "#                        start_epoch, n_epochs, batch_size, i_checkpoint, MODEL_RESULTS_AE_PATH, DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "autoencoder = Autoencoder(input_size=autoencoder_input_shape, encoding_dim=latent_space_dim, output_size=autoencoder_output_shape, device=DEVICE)\n",
    "optimizer = torch.optim.Adam(autoencoder.parameters(), lr=1e-4)\n",
    "\n",
    "autoencoder, optimizer, lists = load_results(autoencoder, optimizer, MODEL_RESULTS_AE_PATH, map_location=torch.device('cpu'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = autoencoder.encoder\n",
    "decoder = autoencoder.decoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Red neuronal general"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_input_shape = X_train_NN[0].shape\n",
    "latent_space_dim = 20\n",
    "nn_output_shape = y_train_NN.values[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from vecopsciml.utils import TensOps\n",
    "from vecopsciml.operators.zero_order import Mx, My\n",
    "\n",
    "class HiddenStatePGNNIV(nn.Module):\n",
    "    def __init__(self, input_size, latent_space_output, explanatory_output_size, decoder_model, device, **kwargs):\n",
    "        super(HiddenStatePGNNIV, self).__init__()\n",
    "\n",
    "        self.input = input_size\n",
    "        self.latent_space_output = latent_space_output\n",
    "        self.output_expl = explanatory_output_size\n",
    "\n",
    "        self.hidden_units_pred = 10\n",
    "        self.hidden_units_exp = 15\n",
    "        self.filters_exp = 10\n",
    "\n",
    "        self.decoder = decoder_model\n",
    "\n",
    "        self.device = device\n",
    "\n",
    "        # Predictive network\n",
    "        self.flatten_layer_pred = nn.Flatten(start_dim=1, end_dim=-1)\n",
    "        self.hidden1_layer_pred = nn.Linear(torch.prod(torch.tensor(self.input, device=self.device)), self.hidden_units_pred).to(self.device)\n",
    "        self.hidden2_layer_pred = nn.Linear(self.hidden_units_pred, self.hidden_units_pred).to(self.device)\n",
    "        self.output_layer_pred = nn.Linear(self.hidden_units_pred, self.latent_space_output).to(self.device)\n",
    "\n",
    "        # Explanatory network (commented out since they are not used in forward method)\n",
    "        self.conv1_exp = nn.Conv2d(in_channels=1, out_channels=self.filters_exp, kernel_size=1).to(self.device)\n",
    "        self.flatten_layer_exp = nn.Flatten().to(self.device)\n",
    "        self.hidden1_layer_exp = nn.LazyLinear(self.hidden_units_exp).to(self.device)\n",
    "        self.hidden2_layer_exp = nn.Linear(self.hidden_units_exp, self.hidden_units_exp).to(self.device)\n",
    "        self.output_layer_exp = nn.Linear(self.hidden_units_exp, self.filters_exp * (self.output_expl[1] - 1) * (self.output_expl[2] - 1)).to(self.device)\n",
    "        self.conv2_exp = nn.Conv2d(in_channels=self.filters_exp, out_channels=1, kernel_size=1).to(self.device)\n",
    "\n",
    "    def forward(self, X):\n",
    "\n",
    "        X = X.to(self.device)\n",
    "\n",
    "        # Predictive network\n",
    "        X = self.flatten_layer_pred(X)\n",
    "        X = torch.sigmoid(self.hidden1_layer_pred(X))\n",
    "        X = torch.sigmoid(self.hidden2_layer_pred(X))\n",
    "        output_predictive_net = self.output_layer_pred(X)\n",
    "\n",
    "        u_pred = decoder(output_predictive_net)\n",
    "        um_pred = My(Mx(TensOps(u_pred, space_dimension=2, contravariance=0, covariance=0))).values\n",
    "\n",
    "        x = torch.sigmoid(self.conv1_exp(um_pred))\n",
    "        x = self.flatten_layer_exp(x)\n",
    "        x = torch.sigmoid(self.hidden1_layer_exp(x))\n",
    "        x = torch.sigmoid(self.hidden2_layer_exp(x))\n",
    "        x = self.output_layer_exp(x)\n",
    "        x = x.view(x.size(0), self.filters_exp, self.output_expl[1] - 1, self.output_expl[2] - 1)\n",
    "        K_pred = self.conv2_exp(x)\n",
    "\n",
    "        return u_pred, K_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in decoder.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training\n",
      "Epoch 0, Train loss: 1.716e+08, Test loss: 1.833e+08, MSE(e): 1.709e+01, MSE(pi1): 5.241e+01, MSE(pi2): 6.145e+00, MSE(pi3): 1.407e+00\n",
      "Epoch 100, Train loss: 5.368e+06, Test loss: 5.645e+06, MSE(e): 5.345e-01, MSE(pi1): 7.555e-01, MSE(pi2): 3.288e-01, MSE(pi3): 1.538e-01\n",
      "Epoch 200, Train loss: 3.715e+05, Test loss: 3.950e+05, MSE(e): 3.599e-02, MSE(pi1): 6.540e-01, MSE(pi2): 2.085e-02, MSE(pi3): 5.065e-02\n",
      "Epoch 300, Train loss: 9.459e+04, Test loss: 1.041e+05, MSE(e): 9.148e-03, MSE(pi1): 1.713e-01, MSE(pi2): 6.210e-03, MSE(pi3): 1.399e-02\n",
      "Epoch 400, Train loss: 4.203e+04, Test loss: 5.198e+04, MSE(e): 4.056e-03, MSE(pi1): 9.168e-02, MSE(pi2): 2.863e-03, MSE(pi3): 5.533e-03\n",
      "Epoch 500, Train loss: 2.342e+04, Test loss: 3.104e+04, MSE(e): 2.242e-03, MSE(pi1): 6.162e-02, MSE(pi2): 1.529e-03, MSE(pi3): 3.797e-03\n",
      "Epoch 600, Train loss: 1.787e+04, Test loss: 2.344e+04, MSE(e): 1.713e-03, MSE(pi1): 4.362e-02, MSE(pi2): 1.163e-03, MSE(pi3): 3.048e-03\n",
      "Epoch 700, Train loss: 1.576e+04, Test loss: 2.038e+04, MSE(e): 1.513e-03, MSE(pi1): 3.627e-02, MSE(pi2): 1.032e-03, MSE(pi3): 2.721e-03\n",
      "Epoch 800, Train loss: 1.443e+04, Test loss: 1.863e+04, MSE(e): 1.386e-03, MSE(pi1): 3.210e-02, MSE(pi2): 9.566e-04, MSE(pi3): 2.495e-03\n",
      "Epoch 900, Train loss: 1.347e+04, Test loss: 1.743e+04, MSE(e): 1.295e-03, MSE(pi1): 2.908e-02, MSE(pi2): 9.057e-04, MSE(pi3): 2.310e-03\n",
      "Epoch 1000, Train loss: 1.271e+04, Test loss: 1.653e+04, MSE(e): 1.222e-03, MSE(pi1): 2.660e-02, MSE(pi2): 8.664e-04, MSE(pi3): 2.154e-03\n",
      "Epoch 1100, Train loss: 1.201e+04, Test loss: 1.587e+04, MSE(e): 1.156e-03, MSE(pi1): 2.455e-02, MSE(pi2): 8.308e-04, MSE(pi3): 2.016e-03\n",
      "Epoch 1200, Train loss: 1.141e+04, Test loss: 1.535e+04, MSE(e): 1.099e-03, MSE(pi1): 2.300e-02, MSE(pi2): 7.992e-04, MSE(pi3): 1.888e-03\n",
      "Epoch 1300, Train loss: 1.097e+04, Test loss: 1.479e+04, MSE(e): 1.057e-03, MSE(pi1): 2.161e-02, MSE(pi2): 7.747e-04, MSE(pi3): 1.782e-03\n",
      "Epoch 1400, Train loss: 1.061e+04, Test loss: 1.424e+04, MSE(e): 1.023e-03, MSE(pi1): 2.029e-02, MSE(pi2): 7.542e-04, MSE(pi3): 1.694e-03\n",
      "Epoch 1500, Train loss: 1.029e+04, Test loss: 1.370e+04, MSE(e): 9.937e-04, MSE(pi1): 1.911e-02, MSE(pi2): 7.362e-04, MSE(pi3): 1.621e-03\n",
      "Epoch 1600, Train loss: 1.001e+04, Test loss: 1.318e+04, MSE(e): 9.668e-04, MSE(pi1): 1.812e-02, MSE(pi2): 7.200e-04, MSE(pi3): 1.562e-03\n",
      "Epoch 1700, Train loss: 9.602e+03, Test loss: 1.271e+04, MSE(e): 9.279e-04, MSE(pi1): 1.688e-02, MSE(pi2): 7.029e-04, MSE(pi3): 1.538e-03\n",
      "Epoch 1800, Train loss: 9.477e+03, Test loss: 1.223e+04, MSE(e): 9.167e-04, MSE(pi1): 1.608e-02, MSE(pi2): 6.901e-04, MSE(pi3): 1.484e-03\n",
      "Epoch 1900, Train loss: 9.112e+03, Test loss: 1.188e+04, MSE(e): 8.813e-04, MSE(pi1): 1.525e-02, MSE(pi2): 6.710e-04, MSE(pi3): 1.448e-03\n",
      "Epoch 2000, Train loss: 8.811e+03, Test loss: 1.157e+04, MSE(e): 8.523e-04, MSE(pi1): 1.459e-02, MSE(pi2): 6.544e-04, MSE(pi3): 1.416e-03\n",
      "Epoch 2100, Train loss: 8.555e+03, Test loss: 1.129e+04, MSE(e): 8.275e-04, MSE(pi1): 1.404e-02, MSE(pi2): 6.397e-04, MSE(pi3): 1.387e-03\n",
      "Epoch 2200, Train loss: 8.331e+03, Test loss: 1.102e+04, MSE(e): 8.058e-04, MSE(pi1): 1.359e-02, MSE(pi2): 6.263e-04, MSE(pi3): 1.363e-03\n",
      "Epoch 2300, Train loss: 8.132e+03, Test loss: 1.077e+04, MSE(e): 7.865e-04, MSE(pi1): 1.320e-02, MSE(pi2): 6.139e-04, MSE(pi3): 1.342e-03\n",
      "Epoch 2400, Train loss: 7.951e+03, Test loss: 1.054e+04, MSE(e): 7.689e-04, MSE(pi1): 1.284e-02, MSE(pi2): 6.022e-04, MSE(pi3): 1.324e-03\n",
      "Epoch 2500, Train loss: 7.784e+03, Test loss: 1.032e+04, MSE(e): 7.528e-04, MSE(pi1): 1.254e-02, MSE(pi2): 5.911e-04, MSE(pi3): 1.308e-03\n",
      "Epoch 2600, Train loss: 7.630e+03, Test loss: 1.011e+04, MSE(e): 7.377e-04, MSE(pi1): 1.229e-02, MSE(pi2): 5.804e-04, MSE(pi3): 1.293e-03\n",
      "Epoch 2700, Train loss: 7.484e+03, Test loss: 9.913e+03, MSE(e): 7.235e-04, MSE(pi1): 1.206e-02, MSE(pi2): 5.701e-04, MSE(pi3): 1.280e-03\n",
      "Epoch 2800, Train loss: 7.346e+03, Test loss: 9.722e+03, MSE(e): 7.100e-04, MSE(pi1): 1.185e-02, MSE(pi2): 5.600e-04, MSE(pi3): 1.268e-03\n",
      "Epoch 2900, Train loss: 7.214e+03, Test loss: 9.539e+03, MSE(e): 6.971e-04, MSE(pi1): 1.166e-02, MSE(pi2): 5.501e-04, MSE(pi3): 1.257e-03\n",
      "Epoch 3000, Train loss: 7.087e+03, Test loss: 9.362e+03, MSE(e): 6.847e-04, MSE(pi1): 1.148e-02, MSE(pi2): 5.404e-04, MSE(pi3): 1.247e-03\n",
      "Epoch 3100, Train loss: 6.965e+03, Test loss: 9.191e+03, MSE(e): 6.727e-04, MSE(pi1): 1.131e-02, MSE(pi2): 5.309e-04, MSE(pi3): 1.238e-03\n",
      "Epoch 3200, Train loss: 6.846e+03, Test loss: 9.025e+03, MSE(e): 6.611e-04, MSE(pi1): 1.115e-02, MSE(pi2): 5.215e-04, MSE(pi3): 1.230e-03\n",
      "Epoch 3300, Train loss: 6.730e+03, Test loss: 8.864e+03, MSE(e): 6.497e-04, MSE(pi1): 1.100e-02, MSE(pi2): 5.122e-04, MSE(pi3): 1.223e-03\n",
      "Epoch 3400, Train loss: 6.617e+03, Test loss: 8.707e+03, MSE(e): 6.386e-04, MSE(pi1): 1.085e-02, MSE(pi2): 5.030e-04, MSE(pi3): 1.216e-03\n",
      "Epoch 3500, Train loss: 6.506e+03, Test loss: 8.555e+03, MSE(e): 6.278e-04, MSE(pi1): 1.070e-02, MSE(pi2): 4.939e-04, MSE(pi3): 1.211e-03\n",
      "Epoch 3600, Train loss: 6.398e+03, Test loss: 8.406e+03, MSE(e): 6.171e-04, MSE(pi1): 1.055e-02, MSE(pi2): 4.849e-04, MSE(pi3): 1.206e-03\n",
      "Epoch 3700, Train loss: 6.291e+03, Test loss: 8.260e+03, MSE(e): 6.066e-04, MSE(pi1): 1.041e-02, MSE(pi2): 4.761e-04, MSE(pi3): 1.202e-03\n",
      "Epoch 3800, Train loss: 6.186e+03, Test loss: 8.118e+03, MSE(e): 5.963e-04, MSE(pi1): 1.027e-02, MSE(pi2): 4.673e-04, MSE(pi3): 1.198e-03\n",
      "Epoch 3900, Train loss: 6.082e+03, Test loss: 7.978e+03, MSE(e): 5.861e-04, MSE(pi1): 1.013e-02, MSE(pi2): 4.587e-04, MSE(pi3): 1.195e-03\n",
      "Epoch 4000, Train loss: 5.980e+03, Test loss: 7.841e+03, MSE(e): 5.760e-04, MSE(pi1): 9.993e-03, MSE(pi2): 4.502e-04, MSE(pi3): 1.193e-03\n",
      "Epoch 4100, Train loss: 5.879e+03, Test loss: 7.706e+03, MSE(e): 5.661e-04, MSE(pi1): 9.865e-03, MSE(pi2): 4.418e-04, MSE(pi3): 1.190e-03\n",
      "Epoch 4200, Train loss: 5.780e+03, Test loss: 7.574e+03, MSE(e): 5.563e-04, MSE(pi1): 9.745e-03, MSE(pi2): 4.336e-04, MSE(pi3): 1.188e-03\n",
      "Epoch 4300, Train loss: 5.682e+03, Test loss: 7.443e+03, MSE(e): 5.467e-04, MSE(pi1): 9.634e-03, MSE(pi2): 4.255e-04, MSE(pi3): 1.186e-03\n",
      "Epoch 4400, Train loss: 5.585e+03, Test loss: 7.314e+03, MSE(e): 5.371e-04, MSE(pi1): 9.533e-03, MSE(pi2): 4.175e-04, MSE(pi3): 1.184e-03\n",
      "Epoch 4500, Train loss: 5.489e+03, Test loss: 7.187e+03, MSE(e): 5.276e-04, MSE(pi1): 9.441e-03, MSE(pi2): 4.097e-04, MSE(pi3): 1.182e-03\n",
      "Epoch 4600, Train loss: 5.394e+03, Test loss: 7.062e+03, MSE(e): 5.182e-04, MSE(pi1): 9.356e-03, MSE(pi2): 4.019e-04, MSE(pi3): 1.180e-03\n",
      "Epoch 4700, Train loss: 5.300e+03, Test loss: 6.937e+03, MSE(e): 5.089e-04, MSE(pi1): 9.278e-03, MSE(pi2): 3.943e-04, MSE(pi3): 1.178e-03\n",
      "Epoch 4800, Train loss: 5.207e+03, Test loss: 6.814e+03, MSE(e): 4.997e-04, MSE(pi1): 9.207e-03, MSE(pi2): 3.868e-04, MSE(pi3): 1.175e-03\n",
      "Epoch 4900, Train loss: 5.114e+03, Test loss: 6.692e+03, MSE(e): 4.905e-04, MSE(pi1): 9.141e-03, MSE(pi2): 3.794e-04, MSE(pi3): 1.173e-03\n",
      "Epoch 5000, Train loss: 5.021e+03, Test loss: 6.571e+03, MSE(e): 4.813e-04, MSE(pi1): 9.082e-03, MSE(pi2): 3.720e-04, MSE(pi3): 1.170e-03\n",
      "Epoch 5100, Train loss: 4.928e+03, Test loss: 6.450e+03, MSE(e): 4.721e-04, MSE(pi1): 9.029e-03, MSE(pi2): 3.647e-04, MSE(pi3): 1.167e-03\n",
      "Epoch 5200, Train loss: 4.834e+03, Test loss: 6.330e+03, MSE(e): 4.628e-04, MSE(pi1): 8.982e-03, MSE(pi2): 3.573e-04, MSE(pi3): 1.163e-03\n",
      "Epoch 5300, Train loss: 4.733e+03, Test loss: 6.214e+03, MSE(e): 4.528e-04, MSE(pi1): 8.952e-03, MSE(pi2): 3.496e-04, MSE(pi3): 1.158e-03\n",
      "Epoch 5400, Train loss: 4.598e+03, Test loss: 6.094e+03, MSE(e): 4.393e-04, MSE(pi1): 8.994e-03, MSE(pi2): 3.388e-04, MSE(pi3): 1.146e-03\n",
      "Epoch 5500, Train loss: 4.491e+03, Test loss: 5.968e+03, MSE(e): 4.287e-04, MSE(pi1): 8.964e-03, MSE(pi2): 3.304e-04, MSE(pi3): 1.142e-03\n",
      "Epoch 5600, Train loss: 4.383e+03, Test loss: 5.838e+03, MSE(e): 4.180e-04, MSE(pi1): 8.940e-03, MSE(pi2): 3.220e-04, MSE(pi3): 1.137e-03\n",
      "Epoch 5700, Train loss: 4.274e+03, Test loss: 5.707e+03, MSE(e): 4.071e-04, MSE(pi1): 8.916e-03, MSE(pi2): 3.136e-04, MSE(pi3): 1.132e-03\n",
      "Epoch 5800, Train loss: 4.164e+03, Test loss: 5.574e+03, MSE(e): 3.962e-04, MSE(pi1): 8.892e-03, MSE(pi2): 3.052e-04, MSE(pi3): 1.128e-03\n",
      "Epoch 5900, Train loss: 4.054e+03, Test loss: 5.442e+03, MSE(e): 3.852e-04, MSE(pi1): 8.866e-03, MSE(pi2): 2.968e-04, MSE(pi3): 1.125e-03\n",
      "Epoch 6000, Train loss: 3.944e+03, Test loss: 5.311e+03, MSE(e): 3.743e-04, MSE(pi1): 8.839e-03, MSE(pi2): 2.886e-04, MSE(pi3): 1.123e-03\n",
      "Epoch 6100, Train loss: 3.836e+03, Test loss: 5.183e+03, MSE(e): 3.636e-04, MSE(pi1): 8.809e-03, MSE(pi2): 2.805e-04, MSE(pi3): 1.121e-03\n",
      "Epoch 6200, Train loss: 3.733e+03, Test loss: 5.059e+03, MSE(e): 3.533e-04, MSE(pi1): 8.777e-03, MSE(pi2): 2.727e-04, MSE(pi3): 1.120e-03\n",
      "Epoch 6300, Train loss: 3.637e+03, Test loss: 4.939e+03, MSE(e): 3.437e-04, MSE(pi1): 8.740e-03, MSE(pi2): 2.653e-04, MSE(pi3): 1.120e-03\n",
      "Epoch 6400, Train loss: 3.556e+03, Test loss: 4.823e+03, MSE(e): 3.357e-04, MSE(pi1): 8.695e-03, MSE(pi2): 2.588e-04, MSE(pi3): 1.121e-03\n",
      "Epoch 6500, Train loss: 3.537e+03, Test loss: 4.696e+03, MSE(e): 3.337e-04, MSE(pi1): 8.623e-03, MSE(pi2): 2.549e-04, MSE(pi3): 1.126e-03\n",
      "Epoch 6600, Train loss: 3.530e+03, Test loss: 4.565e+03, MSE(e): 3.331e-04, MSE(pi1): 8.571e-03, MSE(pi2): 2.514e-04, MSE(pi3): 1.130e-03\n",
      "Epoch 6700, Train loss: 3.512e+03, Test loss: 4.450e+03, MSE(e): 3.313e-04, MSE(pi1): 8.531e-03, MSE(pi2): 2.474e-04, MSE(pi3): 1.132e-03\n",
      "Epoch 6800, Train loss: 3.195e+03, Test loss: 4.621e+03, MSE(e): 2.997e-04, MSE(pi1): 8.749e-03, MSE(pi2): 2.305e-04, MSE(pi3): 1.104e-03\n",
      "Epoch 6900, Train loss: 3.088e+03, Test loss: 4.503e+03, MSE(e): 2.890e-04, MSE(pi1): 8.800e-03, MSE(pi2): 2.232e-04, MSE(pi3): 1.096e-03\n",
      "Epoch 7000, Train loss: 3.004e+03, Test loss: 4.381e+03, MSE(e): 2.806e-04, MSE(pi1): 8.818e-03, MSE(pi2): 2.171e-04, MSE(pi3): 1.092e-03\n",
      "Epoch 7100, Train loss: 2.930e+03, Test loss: 4.268e+03, MSE(e): 2.733e-04, MSE(pi1): 8.816e-03, MSE(pi2): 2.116e-04, MSE(pi3): 1.090e-03\n",
      "Epoch 7200, Train loss: 2.863e+03, Test loss: 4.165e+03, MSE(e): 2.666e-04, MSE(pi1): 8.800e-03, MSE(pi2): 2.065e-04, MSE(pi3): 1.088e-03\n",
      "Epoch 7300, Train loss: 2.801e+03, Test loss: 4.070e+03, MSE(e): 2.605e-04, MSE(pi1): 8.774e-03, MSE(pi2): 2.017e-04, MSE(pi3): 1.087e-03\n",
      "Epoch 7400, Train loss: 2.744e+03, Test loss: 3.982e+03, MSE(e): 2.548e-04, MSE(pi1): 8.739e-03, MSE(pi2): 1.972e-04, MSE(pi3): 1.086e-03\n",
      "Epoch 7500, Train loss: 2.690e+03, Test loss: 3.900e+03, MSE(e): 2.494e-04, MSE(pi1): 8.697e-03, MSE(pi2): 1.930e-04, MSE(pi3): 1.086e-03\n",
      "Epoch 7600, Train loss: 2.639e+03, Test loss: 3.824e+03, MSE(e): 2.444e-04, MSE(pi1): 8.650e-03, MSE(pi2): 1.889e-04, MSE(pi3): 1.086e-03\n",
      "Epoch 7700, Train loss: 2.591e+03, Test loss: 3.754e+03, MSE(e): 2.396e-04, MSE(pi1): 8.600e-03, MSE(pi2): 1.851e-04, MSE(pi3): 1.086e-03\n",
      "Epoch 7800, Train loss: 2.545e+03, Test loss: 3.687e+03, MSE(e): 2.350e-04, MSE(pi1): 8.548e-03, MSE(pi2): 1.814e-04, MSE(pi3): 1.086e-03\n",
      "Epoch 7900, Train loss: 2.501e+03, Test loss: 3.625e+03, MSE(e): 2.307e-04, MSE(pi1): 8.495e-03, MSE(pi2): 1.778e-04, MSE(pi3): 1.086e-03\n",
      "Epoch 8000, Train loss: 2.459e+03, Test loss: 3.566e+03, MSE(e): 2.265e-04, MSE(pi1): 8.442e-03, MSE(pi2): 1.744e-04, MSE(pi3): 1.086e-03\n",
      "Epoch 8100, Train loss: 2.418e+03, Test loss: 3.510e+03, MSE(e): 2.226e-04, MSE(pi1): 8.389e-03, MSE(pi2): 1.711e-04, MSE(pi3): 1.086e-03\n",
      "Epoch 8200, Train loss: 2.379e+03, Test loss: 3.457e+03, MSE(e): 2.187e-04, MSE(pi1): 8.337e-03, MSE(pi2): 1.679e-04, MSE(pi3): 1.085e-03\n",
      "Epoch 8300, Train loss: 2.341e+03, Test loss: 3.407e+03, MSE(e): 2.150e-04, MSE(pi1): 8.287e-03, MSE(pi2): 1.648e-04, MSE(pi3): 1.085e-03\n",
      "Epoch 8400, Train loss: 2.305e+03, Test loss: 3.359e+03, MSE(e): 2.114e-04, MSE(pi1): 8.238e-03, MSE(pi2): 1.618e-04, MSE(pi3): 1.084e-03\n",
      "Epoch 8500, Train loss: 2.270e+03, Test loss: 3.313e+03, MSE(e): 2.080e-04, MSE(pi1): 8.191e-03, MSE(pi2): 1.589e-04, MSE(pi3): 1.083e-03\n",
      "Epoch 8600, Train loss: 2.236e+03, Test loss: 3.269e+03, MSE(e): 2.046e-04, MSE(pi1): 8.145e-03, MSE(pi2): 1.561e-04, MSE(pi3): 1.082e-03\n",
      "Epoch 8700, Train loss: 2.203e+03, Test loss: 3.227e+03, MSE(e): 2.014e-04, MSE(pi1): 8.099e-03, MSE(pi2): 1.534e-04, MSE(pi3): 1.081e-03\n",
      "Epoch 8800, Train loss: 2.171e+03, Test loss: 3.187e+03, MSE(e): 1.983e-04, MSE(pi1): 8.055e-03, MSE(pi2): 1.508e-04, MSE(pi3): 1.080e-03\n",
      "Epoch 8900, Train loss: 2.140e+03, Test loss: 3.149e+03, MSE(e): 1.952e-04, MSE(pi1): 8.012e-03, MSE(pi2): 1.483e-04, MSE(pi3): 1.079e-03\n",
      "Epoch 9000, Train loss: 2.111e+03, Test loss: 3.112e+03, MSE(e): 1.923e-04, MSE(pi1): 7.970e-03, MSE(pi2): 1.458e-04, MSE(pi3): 1.078e-03\n",
      "Epoch 9100, Train loss: 2.082e+03, Test loss: 3.076e+03, MSE(e): 1.895e-04, MSE(pi1): 7.928e-03, MSE(pi2): 1.435e-04, MSE(pi3): 1.076e-03\n",
      "Epoch 9200, Train loss: 2.054e+03, Test loss: 3.042e+03, MSE(e): 1.867e-04, MSE(pi1): 7.887e-03, MSE(pi2): 1.412e-04, MSE(pi3): 1.075e-03\n",
      "Epoch 9300, Train loss: 2.027e+03, Test loss: 3.009e+03, MSE(e): 1.841e-04, MSE(pi1): 7.847e-03, MSE(pi2): 1.390e-04, MSE(pi3): 1.074e-03\n",
      "Epoch 9400, Train loss: 2.001e+03, Test loss: 2.977e+03, MSE(e): 1.816e-04, MSE(pi1): 7.807e-03, MSE(pi2): 1.370e-04, MSE(pi3): 1.072e-03\n",
      "Epoch 9500, Train loss: 1.976e+03, Test loss: 2.946e+03, MSE(e): 1.791e-04, MSE(pi1): 7.768e-03, MSE(pi2): 1.349e-04, MSE(pi3): 1.071e-03\n",
      "Epoch 9600, Train loss: 1.952e+03, Test loss: 2.916e+03, MSE(e): 1.767e-04, MSE(pi1): 7.729e-03, MSE(pi2): 1.330e-04, MSE(pi3): 1.070e-03\n",
      "Epoch 9700, Train loss: 1.928e+03, Test loss: 2.888e+03, MSE(e): 1.744e-04, MSE(pi1): 7.692e-03, MSE(pi2): 1.311e-04, MSE(pi3): 1.069e-03\n",
      "Epoch 9800, Train loss: 1.906e+03, Test loss: 2.860e+03, MSE(e): 1.722e-04, MSE(pi1): 7.655e-03, MSE(pi2): 1.293e-04, MSE(pi3): 1.067e-03\n",
      "Epoch 9900, Train loss: 1.884e+03, Test loss: 2.833e+03, MSE(e): 1.701e-04, MSE(pi1): 7.620e-03, MSE(pi2): 1.276e-04, MSE(pi3): 1.066e-03\n",
      "\n",
      "Proceso finalizado después de 10000 épocas\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Se carga el modelo y el optimizador\n",
    "pgnniv_model = HiddenStatePGNNIV(input_size=nn_input_shape, latent_space_output=latent_space_dim, explanatory_output_size=nn_output_shape, decoder_model=decoder, device=DEVICE)\n",
    "optimizer = torch.optim.Adam(pgnniv_model.parameters(), lr=1e-4)\n",
    "\n",
    "# Parametros de entrenamiento\n",
    "start_epoch = 0\n",
    "n_epochs = 10000\n",
    "\n",
    "batch_size = 64\n",
    "n_checkpoints = 1000\n",
    "\n",
    "train_loop(pgnniv_model, optimizer, n_checkpoints,\n",
    "           X_train_NN, y_train_NN, X_test_NN, y_test_NN, f_train_NN, f_test_NN,\n",
    "           D=D, start_epoch=start_epoch, n_epochs=n_epochs, batch_size=batch_size, \n",
    "           model_results_path=MODEL_RESULTS_PGNNIV_PATH, device=DEVICE,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SciML_test_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
