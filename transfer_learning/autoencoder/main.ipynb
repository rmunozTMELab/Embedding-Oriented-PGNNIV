{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import GPUtil\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Imports de la libreria propia\n",
    "from vecopsciml.kernels.derivative import DerivativeKernels\n",
    "from vecopsciml.utils import TensOps\n",
    "\n",
    "# Imports de las funciones creadas para este programa\n",
    "from utils.folders import create_folder\n",
    "from utils.load_data import load_data\n",
    "from trainers.train import train_loop\n",
    "\n",
    "from vecopsciml.operators.zero_order import Mx, My\n",
    "from utils.checkpoints import load_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Folder successfully created at: /home/rmunoz/Escritorio/rmunozTMELab/Physically-Guided-Machine-Learning/transfer_learning/results/sigmoid_no_training_decoder\n",
      "Folder already exists at: /home/rmunoz/Escritorio/rmunozTMELab/Physically-Guided-Machine-Learning/results/non_linear_1000_0/model_autoencoder_AE_10\n",
      "Folder already exists at: /home/rmunoz/Escritorio/rmunozTMELab/Physically-Guided-Machine-Learning/results/non_linear_1000_0/model_autoencoder_NN_10\n"
     ]
    }
   ],
   "source": [
    "# Creamos los paths para las distintas carpetas\n",
    "ROOT_PATH = r'/home/rmunoz/Escritorio/rmunozTMELab/Physically-Guided-Machine-Learning'\n",
    "DATA_PATH = os.path.join(ROOT_PATH, r'data/sigmoid_nonlinear/sigmoid_nonlinear.pkl')\n",
    "RESULTS_FOLDER_PATH = os.path.join(ROOT_PATH, r'transfer_learning/results/sigmoid_no_training_decoder')\n",
    "\n",
    "MODEL_RESULTS_AE_PATH = os.path.join(ROOT_PATH, r'results/non_linear_1000_0/model_autoencoder_AE_10')\n",
    "MODEL_RESULTS_PGNNIV_PATH = os.path.join(ROOT_PATH, r'results/non_linear_1000_0/model_autoencoder_NN_10')\n",
    "MODEL_RESULTS_TRANSFERLEARNING_PATH = os.path.join(ROOT_PATH, r'transfer_learning/results/sigmoid_no_training_decoder')\n",
    "\n",
    "# Creamos las carpetas que sean necesarias (si ya están creadas se avisará de ello)\n",
    "create_folder(RESULTS_FOLDER_PATH)\n",
    "create_folder(MODEL_RESULTS_AE_PATH)\n",
    "create_folder(MODEL_RESULTS_PGNNIV_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully loaded from: /home/rmunoz/Escritorio/rmunozTMELab/Physically-Guided-Machine-Learning/data/sigmoid_nonlinear/sigmoid_nonlinear.pkl\n"
     ]
    }
   ],
   "source": [
    "# Load dataset\n",
    "dataset = load_data(DATA_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convolutional filters to derivate\n",
    "dx = dataset['x_step_size']\n",
    "dy = dataset['y_step_size']\n",
    "D = DerivativeKernels(dx, dy, 0).grad_kernels_two_dimensions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "print(f\"Using device: {DEVICE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### División de los datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset length: 80\n",
      "Validation dataset length: 20\n"
     ]
    }
   ],
   "source": [
    "X_train = torch.Tensor(dataset['X_train']).unsqueeze(1)\n",
    "y_train = torch.Tensor(dataset['y_train']).unsqueeze(1)\n",
    "K_train = torch.tensor(dataset['k_train']).unsqueeze(1)\n",
    "f_train = torch.tensor(dataset['f_train']).unsqueeze(1).to(torch.float32)\n",
    "\n",
    "X_val = torch.Tensor(dataset['X_val']).unsqueeze(1)\n",
    "y_val = TensOps(torch.Tensor(dataset['y_val']).unsqueeze(1).requires_grad_(True), space_dimension=2, contravariance=0, covariance=0)\n",
    "K_val = TensOps(torch.tensor(dataset['k_val']).unsqueeze(1).requires_grad_(True), space_dimension=2, contravariance=0, covariance=0)\n",
    "f_val = TensOps(torch.tensor(dataset['f_val']).to(torch.float32).unsqueeze(1).requires_grad_(True), space_dimension=2, contravariance=0, covariance=0)\n",
    "\n",
    "print(\"Train dataset length:\", len(X_train))\n",
    "print(\"Validation dataset length:\", len(X_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test, K_train, K_test, f_train, f_test = train_test_split(X_train, y_train, K_train, f_train, test_size=0.2, random_state=42)\n",
    "\n",
    "X_train = X_train.to(DEVICE)\n",
    "X_test = X_test.to(DEVICE)\n",
    "\n",
    "y_train = TensOps(y_train.requires_grad_(True).to(DEVICE), space_dimension=2, contravariance=0, covariance=0)\n",
    "y_test = TensOps(y_test.requires_grad_(True).to(DEVICE), space_dimension=2, contravariance=0, covariance=0)\n",
    "\n",
    "K_train = TensOps(K_train.to(DEVICE), space_dimension=2, contravariance=0, covariance=0)\n",
    "K_test = TensOps(K_test.to(DEVICE), space_dimension=2, contravariance=0, covariance=0)\n",
    "\n",
    "f_train = TensOps(f_train.to(DEVICE), space_dimension=2, contravariance=0, covariance=0)\n",
    "f_test = TensOps(f_test.to(DEVICE), space_dimension=2, contravariance=0, covariance=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model import Autoencoder as PretrainedAutoencoder\n",
    "from model.ae_nonlinear_model import AutoencoderNonlinearModel as PretrainedPGNNNIV\n",
    "from model.transfer_learnign_ae import AutoencoderTransferLearning as TransferLearningAutoencoder\n",
    "from trainers.train import train_autoencoder_loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vecopsciml.operators.zero_order import Mx, My\n",
    "from model.ae_nonlinear_model import AutoencoderNonlinearModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Other parameters\n",
    "n_filters_explanatory = 5\n",
    "n_modes = 10\n",
    "\n",
    "# Predictive network architecture\n",
    "input_shape = X_train[0].shape\n",
    "predictive_layers = [20, 10, n_modes, 10, 20]\n",
    "predictive_output = y_train.values[0].shape\n",
    "\n",
    "# Explanatory network architecture\n",
    "explanatory_input = Mx(My(y_train)).values[0].shape\n",
    "explanatory_layers = [10]\n",
    "explanatory_output = Mx(My(f_train)).values[0].shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load autoencoder\n",
    "autoencoder_input_shape = y_train.values[0].shape\n",
    "latent_space_dim = [20, 10, n_modes, 10, 20]\n",
    "autoencoder_output_shape = y_train.values[0].shape\n",
    "\n",
    "pretrained_autoencoder = PretrainedAutoencoder(autoencoder_input_shape, latent_space_dim, autoencoder_output_shape).to(DEVICE)\n",
    "optimizer = torch.optim.Adam(pretrained_autoencoder.parameters(), lr=1e-4)\n",
    "pretrained_autoencoder, optimizer, lists = load_results(pretrained_autoencoder, optimizer, MODEL_RESULTS_AE_PATH, map_location=torch.device('cpu'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pretrained PGNNIV\n",
    "pretrained_encoder = pretrained_autoencoder.encoder\n",
    "pretrained_decoder = pretrained_autoencoder.decoder\n",
    "\n",
    "pretrained_pgnniv = AutoencoderNonlinearModel(input_shape, predictive_layers, pretrained_decoder, predictive_output, explanatory_input,\n",
    "                        explanatory_layers, explanatory_output, n_filters_explanatory).to(DEVICE)\n",
    "optimizer = torch.optim.Adam(pretrained_pgnniv.parameters(), lr=1e-4)\n",
    "pretrained_pgnniv, optimizer, lists = load_results(pretrained_pgnniv, optimizer, MODEL_RESULTS_PGNNIV_PATH, map_location=torch.device('cpu'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x7f06c572a270>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.scatter(Mx(My(TensOps(pretrained_pgnniv(X_train)[0], space_dimension=2, contravariance=0, covariance=0))).values.cpu().detach().numpy().flatten(), \n",
    "            pretrained_pgnniv(X_train)[1].cpu().detach().numpy().flatten())\n",
    "\n",
    "plt.scatter(y_train.values.cpu().detach().numpy().flatten(), \n",
    "           K_train.values.cpu().detach().numpy().flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hidden1_layer.weight: requires_grad=False\n",
      "hidden1_layer.bias: requires_grad=False\n",
      "hidden2_layer.weight: requires_grad=False\n",
      "hidden2_layer.bias: requires_grad=False\n",
      "latent_space_layer.weight: requires_grad=False\n",
      "latent_space_layer.bias: requires_grad=False\n"
     ]
    }
   ],
   "source": [
    "pgnniv_pretrained_encoder = pretrained_pgnniv.encoder\n",
    "\n",
    "for param in pgnniv_pretrained_encoder.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "for name, param in pgnniv_pretrained_encoder.named_parameters():\n",
    "    print(f\"{name}: requires_grad={param.requires_grad}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training from scratch.\n",
      "Epoch 0, Train loss: 2.798e+12, Test loss: 3.032e+12, MSE(e): 1.090e+02, MSE(pi1): 1.683e+05, MSE(pi2): 4.762e+01, MSE(pi3): 2.534e+02\n",
      "Epoch 100, Train loss: 4.611e+11, Test loss: 6.897e+11, MSE(e): 4.316e+01, MSE(pi1): 2.379e+03, MSE(pi2): 1.500e+01, MSE(pi3): 5.692e+01\n",
      "Epoch 200, Train loss: 1.877e+11, Test loss: 2.858e+11, MSE(e): 1.747e+01, MSE(pi1): 1.056e+03, MSE(pi2): 6.250e+00, MSE(pi3): 2.396e+01\n",
      "Epoch 300, Train loss: 1.269e+11, Test loss: 1.710e+11, MSE(e): 1.230e+01, MSE(pi1): 2.845e+02, MSE(pi2): 5.140e+00, MSE(pi3): 1.062e+01\n",
      "Epoch 400, Train loss: 1.156e+11, Test loss: 1.435e+11, MSE(e): 1.140e+01, MSE(pi1): 7.604e+01, MSE(pi2): 5.031e+00, MSE(pi3): 8.098e+00\n",
      "Epoch 500, Train loss: 1.137e+11, Test loss: 1.363e+11, MSE(e): 1.125e+01, MSE(pi1): 4.417e+01, MSE(pi2): 5.004e+00, MSE(pi3): 7.549e+00\n",
      "Epoch 600, Train loss: 1.131e+11, Test loss: 1.337e+11, MSE(e): 1.119e+01, MSE(pi1): 4.025e+01, MSE(pi2): 4.984e+00, MSE(pi3): 7.393e+00\n",
      "Epoch 700, Train loss: 1.125e+11, Test loss: 1.323e+11, MSE(e): 1.114e+01, MSE(pi1): 3.836e+01, MSE(pi2): 4.963e+00, MSE(pi3): 7.328e+00\n",
      "Epoch 800, Train loss: 1.120e+11, Test loss: 1.309e+11, MSE(e): 1.109e+01, MSE(pi1): 3.839e+01, MSE(pi2): 4.940e+00, MSE(pi3): 7.281e+00\n",
      "Epoch 900, Train loss: 1.114e+11, Test loss: 1.295e+11, MSE(e): 1.103e+01, MSE(pi1): 3.884e+01, MSE(pi2): 4.915e+00, MSE(pi3): 7.222e+00\n",
      "Epoch 1000, Train loss: 1.107e+11, Test loss: 1.280e+11, MSE(e): 1.096e+01, MSE(pi1): 3.982e+01, MSE(pi2): 4.886e+00, MSE(pi3): 7.163e+00\n",
      "Epoch 1100, Train loss: 1.099e+11, Test loss: 1.263e+11, MSE(e): 1.088e+01, MSE(pi1): 4.180e+01, MSE(pi2): 4.852e+00, MSE(pi3): 7.102e+00\n",
      "Epoch 1200, Train loss: 1.090e+11, Test loss: 1.243e+11, MSE(e): 1.078e+01, MSE(pi1): 4.486e+01, MSE(pi2): 4.811e+00, MSE(pi3): 7.035e+00\n",
      "Epoch 1300, Train loss: 1.079e+11, Test loss: 1.219e+11, MSE(e): 1.067e+01, MSE(pi1): 4.905e+01, MSE(pi2): 4.764e+00, MSE(pi3): 6.962e+00\n",
      "Epoch 1400, Train loss: 1.066e+11, Test loss: 1.193e+11, MSE(e): 1.054e+01, MSE(pi1): 5.441e+01, MSE(pi2): 4.711e+00, MSE(pi3): 6.883e+00\n",
      "Epoch 1500, Train loss: 1.052e+11, Test loss: 1.165e+11, MSE(e): 1.039e+01, MSE(pi1): 6.078e+01, MSE(pi2): 4.650e+00, MSE(pi3): 6.795e+00\n",
      "Epoch 1600, Train loss: 1.037e+11, Test loss: 1.133e+11, MSE(e): 1.024e+01, MSE(pi1): 6.777e+01, MSE(pi2): 4.585e+00, MSE(pi3): 6.697e+00\n",
      "Epoch 1700, Train loss: 1.021e+11, Test loss: 1.101e+11, MSE(e): 1.007e+01, MSE(pi1): 7.481e+01, MSE(pi2): 4.515e+00, MSE(pi3): 6.585e+00\n",
      "Epoch 1800, Train loss: 1.005e+11, Test loss: 1.067e+11, MSE(e): 9.902e+00, MSE(pi1): 8.128e+01, MSE(pi2): 4.442e+00, MSE(pi3): 6.455e+00\n",
      "Epoch 1900, Train loss: 9.877e+10, Test loss: 1.032e+11, MSE(e): 9.728e+00, MSE(pi1): 8.663e+01, MSE(pi2): 4.367e+00, MSE(pi3): 6.305e+00\n",
      "Epoch 2000, Train loss: 9.705e+10, Test loss: 9.976e+10, MSE(e): 9.553e+00, MSE(pi1): 9.044e+01, MSE(pi2): 4.292e+00, MSE(pi3): 6.133e+00\n",
      "Epoch 2100, Train loss: 9.532e+10, Test loss: 9.633e+10, MSE(e): 9.380e+00, MSE(pi1): 9.252e+01, MSE(pi2): 4.216e+00, MSE(pi3): 5.943e+00\n",
      "Epoch 2200, Train loss: 9.361e+10, Test loss: 9.300e+10, MSE(e): 9.211e+00, MSE(pi1): 9.291e+01, MSE(pi2): 4.142e+00, MSE(pi3): 5.739e+00\n",
      "Epoch 2300, Train loss: 9.196e+10, Test loss: 8.984e+10, MSE(e): 9.049e+00, MSE(pi1): 9.177e+01, MSE(pi2): 4.069e+00, MSE(pi3): 5.528e+00\n",
      "Epoch 2400, Train loss: 9.039e+10, Test loss: 8.693e+10, MSE(e): 8.897e+00, MSE(pi1): 8.936e+01, MSE(pi2): 4.000e+00, MSE(pi3): 5.319e+00\n",
      "Epoch 2500, Train loss: 8.893e+10, Test loss: 8.435e+10, MSE(e): 8.756e+00, MSE(pi1): 8.595e+01, MSE(pi2): 3.934e+00, MSE(pi3): 5.116e+00\n",
      "Epoch 2600, Train loss: 8.756e+10, Test loss: 8.214e+10, MSE(e): 8.625e+00, MSE(pi1): 8.180e+01, MSE(pi2): 3.871e+00, MSE(pi3): 4.926e+00\n",
      "Epoch 2700, Train loss: 8.628e+10, Test loss: 8.033e+10, MSE(e): 8.504e+00, MSE(pi1): 7.697e+01, MSE(pi2): 3.811e+00, MSE(pi3): 4.751e+00\n",
      "Epoch 2800, Train loss: 8.507e+10, Test loss: 7.889e+10, MSE(e): 8.390e+00, MSE(pi1): 7.132e+01, MSE(pi2): 3.753e+00, MSE(pi3): 4.584e+00\n",
      "Epoch 2900, Train loss: 8.394e+10, Test loss: 7.777e+10, MSE(e): 8.285e+00, MSE(pi1): 6.488e+01, MSE(pi2): 3.700e+00, MSE(pi3): 4.413e+00\n",
      "Epoch 3000, Train loss: 8.290e+10, Test loss: 7.695e+10, MSE(e): 8.189e+00, MSE(pi1): 5.816e+01, MSE(pi2): 3.653e+00, MSE(pi3): 4.229e+00\n",
      "Epoch 3100, Train loss: 8.195e+10, Test loss: 7.640e+10, MSE(e): 8.102e+00, MSE(pi1): 5.191e+01, MSE(pi2): 3.612e+00, MSE(pi3): 4.038e+00\n",
      "Epoch 3200, Train loss: 8.112e+10, Test loss: 7.611e+10, MSE(e): 8.027e+00, MSE(pi1): 4.633e+01, MSE(pi2): 3.576e+00, MSE(pi3): 3.850e+00\n",
      "Epoch 3300, Train loss: 8.039e+10, Test loss: 7.607e+10, MSE(e): 7.961e+00, MSE(pi1): 4.137e+01, MSE(pi2): 3.547e+00, MSE(pi3): 3.668e+00\n",
      "Epoch 3400, Train loss: 7.974e+10, Test loss: 7.621e+10, MSE(e): 7.902e+00, MSE(pi1): 3.717e+01, MSE(pi2): 3.521e+00, MSE(pi3): 3.497e+00\n",
      "Epoch 3500, Train loss: 7.911e+10, Test loss: 7.647e+10, MSE(e): 7.844e+00, MSE(pi1): 3.372e+01, MSE(pi2): 3.496e+00, MSE(pi3): 3.338e+00\n",
      "Epoch 3600, Train loss: 7.849e+10, Test loss: 7.679e+10, MSE(e): 7.786e+00, MSE(pi1): 3.096e+01, MSE(pi2): 3.470e+00, MSE(pi3): 3.193e+00\n",
      "Epoch 3700, Train loss: 7.784e+10, Test loss: 7.715e+10, MSE(e): 7.724e+00, MSE(pi1): 2.878e+01, MSE(pi2): 3.443e+00, MSE(pi3): 3.061e+00\n",
      "Epoch 3800, Train loss: 7.713e+10, Test loss: 7.753e+10, MSE(e): 7.657e+00, MSE(pi1): 2.709e+01, MSE(pi2): 3.413e+00, MSE(pi3): 2.938e+00\n",
      "Epoch 3900, Train loss: 7.634e+10, Test loss: 7.789e+10, MSE(e): 7.580e+00, MSE(pi1): 2.580e+01, MSE(pi2): 3.378e+00, MSE(pi3): 2.823e+00\n",
      "Epoch 4000, Train loss: 7.542e+10, Test loss: 7.817e+10, MSE(e): 7.490e+00, MSE(pi1): 2.485e+01, MSE(pi2): 3.337e+00, MSE(pi3): 2.715e+00\n",
      "Epoch 4100, Train loss: 7.430e+10, Test loss: 7.826e+10, MSE(e): 7.380e+00, MSE(pi1): 2.417e+01, MSE(pi2): 3.287e+00, MSE(pi3): 2.610e+00\n",
      "Epoch 4200, Train loss: 7.296e+10, Test loss: 7.818e+10, MSE(e): 7.247e+00, MSE(pi1): 2.366e+01, MSE(pi2): 3.226e+00, MSE(pi3): 2.509e+00\n",
      "Epoch 4300, Train loss: 7.139e+10, Test loss: 7.803e+10, MSE(e): 7.092e+00, MSE(pi1): 2.321e+01, MSE(pi2): 3.155e+00, MSE(pi3): 2.408e+00\n",
      "Epoch 4400, Train loss: 6.966e+10, Test loss: 7.794e+10, MSE(e): 6.920e+00, MSE(pi1): 2.271e+01, MSE(pi2): 3.075e+00, MSE(pi3): 2.308e+00\n",
      "Epoch 4500, Train loss: 6.782e+10, Test loss: 7.801e+10, MSE(e): 6.738e+00, MSE(pi1): 2.210e+01, MSE(pi2): 2.990e+00, MSE(pi3): 2.207e+00\n",
      "Epoch 4600, Train loss: 6.593e+10, Test loss: 7.834e+10, MSE(e): 6.550e+00, MSE(pi1): 2.133e+01, MSE(pi2): 2.902e+00, MSE(pi3): 2.107e+00\n",
      "Epoch 4700, Train loss: 6.397e+10, Test loss: 7.899e+10, MSE(e): 6.357e+00, MSE(pi1): 2.044e+01, MSE(pi2): 2.810e+00, MSE(pi3): 2.008e+00\n",
      "Epoch 4800, Train loss: 6.190e+10, Test loss: 7.998e+10, MSE(e): 6.151e+00, MSE(pi1): 1.954e+01, MSE(pi2): 2.712e+00, MSE(pi3): 1.911e+00\n",
      "Epoch 4900, Train loss: 5.966e+10, Test loss: 8.125e+10, MSE(e): 5.929e+00, MSE(pi1): 1.873e+01, MSE(pi2): 2.607e+00, MSE(pi3): 1.820e+00\n",
      "Epoch 5000, Train loss: 5.730e+10, Test loss: 8.269e+10, MSE(e): 5.694e+00, MSE(pi1): 1.817e+01, MSE(pi2): 2.496e+00, MSE(pi3): 1.740e+00\n",
      "Epoch 5100, Train loss: 5.494e+10, Test loss: 8.415e+10, MSE(e): 5.459e+00, MSE(pi1): 1.791e+01, MSE(pi2): 2.386e+00, MSE(pi3): 1.675e+00\n",
      "Epoch 5200, Train loss: 5.273e+10, Test loss: 8.547e+10, MSE(e): 5.239e+00, MSE(pi1): 1.790e+01, MSE(pi2): 2.283e+00, MSE(pi3): 1.628e+00\n",
      "Epoch 5300, Train loss: 5.076e+10, Test loss: 8.647e+10, MSE(e): 5.042e+00, MSE(pi1): 1.797e+01, MSE(pi2): 2.193e+00, MSE(pi3): 1.596e+00\n",
      "Epoch 5400, Train loss: 4.902e+10, Test loss: 8.711e+10, MSE(e): 4.869e+00, MSE(pi1): 1.804e+01, MSE(pi2): 2.115e+00, MSE(pi3): 1.576e+00\n",
      "Epoch 5500, Train loss: 4.750e+10, Test loss: 8.741e+10, MSE(e): 4.716e+00, MSE(pi1): 1.808e+01, MSE(pi2): 2.046e+00, MSE(pi3): 1.563e+00\n",
      "Epoch 5600, Train loss: 4.612e+10, Test loss: 8.751e+10, MSE(e): 4.579e+00, MSE(pi1): 1.809e+01, MSE(pi2): 1.985e+00, MSE(pi3): 1.552e+00\n",
      "Epoch 5700, Train loss: 4.485e+10, Test loss: 8.766e+10, MSE(e): 4.452e+00, MSE(pi1): 1.809e+01, MSE(pi2): 1.927e+00, MSE(pi3): 1.544e+00\n",
      "Epoch 5800, Train loss: 4.363e+10, Test loss: 8.819e+10, MSE(e): 4.330e+00, MSE(pi1): 1.808e+01, MSE(pi2): 1.873e+00, MSE(pi3): 1.536e+00\n",
      "Epoch 5900, Train loss: 4.244e+10, Test loss: 8.907e+10, MSE(e): 4.210e+00, MSE(pi1): 1.806e+01, MSE(pi2): 1.819e+00, MSE(pi3): 1.527e+00\n",
      "Epoch 6000, Train loss: 4.125e+10, Test loss: 8.994e+10, MSE(e): 4.092e+00, MSE(pi1): 1.804e+01, MSE(pi2): 1.768e+00, MSE(pi3): 1.518e+00\n",
      "Epoch 6100, Train loss: 4.009e+10, Test loss: 9.060e+10, MSE(e): 3.976e+00, MSE(pi1): 1.801e+01, MSE(pi2): 1.718e+00, MSE(pi3): 1.508e+00\n",
      "Epoch 6200, Train loss: 3.895e+10, Test loss: 9.099e+10, MSE(e): 3.862e+00, MSE(pi1): 1.798e+01, MSE(pi2): 1.671e+00, MSE(pi3): 1.498e+00\n",
      "Epoch 6300, Train loss: 3.778e+10, Test loss: 9.112e+10, MSE(e): 3.745e+00, MSE(pi1): 1.793e+01, MSE(pi2): 1.623e+00, MSE(pi3): 1.487e+00\n",
      "Epoch 6400, Train loss: 3.654e+10, Test loss: 9.140e+10, MSE(e): 3.621e+00, MSE(pi1): 1.786e+01, MSE(pi2): 1.572e+00, MSE(pi3): 1.475e+00\n",
      "Epoch 6500, Train loss: 3.529e+10, Test loss: 9.126e+10, MSE(e): 3.497e+00, MSE(pi1): 1.774e+01, MSE(pi2): 1.521e+00, MSE(pi3): 1.461e+00\n",
      "Epoch 6600, Train loss: 3.413e+10, Test loss: 9.013e+10, MSE(e): 3.381e+00, MSE(pi1): 1.758e+01, MSE(pi2): 1.476e+00, MSE(pi3): 1.445e+00\n",
      "Epoch 6700, Train loss: 3.309e+10, Test loss: 8.847e+10, MSE(e): 3.278e+00, MSE(pi1): 1.742e+01, MSE(pi2): 1.436e+00, MSE(pi3): 1.427e+00\n",
      "Epoch 6800, Train loss: 3.219e+10, Test loss: 8.641e+10, MSE(e): 3.188e+00, MSE(pi1): 1.729e+01, MSE(pi2): 1.403e+00, MSE(pi3): 1.404e+00\n",
      "Epoch 6900, Train loss: 3.140e+10, Test loss: 8.413e+10, MSE(e): 3.109e+00, MSE(pi1): 1.715e+01, MSE(pi2): 1.375e+00, MSE(pi3): 1.378e+00\n",
      "Epoch 7000, Train loss: 3.070e+10, Test loss: 8.177e+10, MSE(e): 3.040e+00, MSE(pi1): 1.701e+01, MSE(pi2): 1.350e+00, MSE(pi3): 1.351e+00\n",
      "Epoch 7100, Train loss: 3.007e+10, Test loss: 7.943e+10, MSE(e): 2.977e+00, MSE(pi1): 1.685e+01, MSE(pi2): 1.328e+00, MSE(pi3): 1.323e+00\n",
      "Epoch 7200, Train loss: 2.947e+10, Test loss: 7.726e+10, MSE(e): 2.917e+00, MSE(pi1): 1.668e+01, MSE(pi2): 1.307e+00, MSE(pi3): 1.293e+00\n",
      "Epoch 7300, Train loss: 2.889e+10, Test loss: 7.527e+10, MSE(e): 2.860e+00, MSE(pi1): 1.649e+01, MSE(pi2): 1.286e+00, MSE(pi3): 1.263e+00\n",
      "Epoch 7400, Train loss: 2.831e+10, Test loss: 7.354e+10, MSE(e): 2.802e+00, MSE(pi1): 1.630e+01, MSE(pi2): 1.265e+00, MSE(pi3): 1.232e+00\n",
      "Epoch 7500, Train loss: 2.772e+10, Test loss: 7.208e+10, MSE(e): 2.744e+00, MSE(pi1): 1.613e+01, MSE(pi2): 1.243e+00, MSE(pi3): 1.202e+00\n",
      "Epoch 7600, Train loss: 2.714e+10, Test loss: 7.094e+10, MSE(e): 2.686e+00, MSE(pi1): 1.596e+01, MSE(pi2): 1.221e+00, MSE(pi3): 1.171e+00\n",
      "Epoch 7700, Train loss: 2.658e+10, Test loss: 7.010e+10, MSE(e): 2.630e+00, MSE(pi1): 1.577e+01, MSE(pi2): 1.199e+00, MSE(pi3): 1.142e+00\n",
      "Epoch 7800, Train loss: 2.604e+10, Test loss: 6.947e+10, MSE(e): 2.578e+00, MSE(pi1): 1.556e+01, MSE(pi2): 1.179e+00, MSE(pi3): 1.113e+00\n",
      "Epoch 7900, Train loss: 2.555e+10, Test loss: 6.903e+10, MSE(e): 2.529e+00, MSE(pi1): 1.531e+01, MSE(pi2): 1.159e+00, MSE(pi3): 1.083e+00\n",
      "Epoch 8000, Train loss: 2.509e+10, Test loss: 6.870e+10, MSE(e): 2.484e+00, MSE(pi1): 1.503e+01, MSE(pi2): 1.142e+00, MSE(pi3): 1.053e+00\n",
      "Epoch 8100, Train loss: 2.468e+10, Test loss: 6.842e+10, MSE(e): 2.443e+00, MSE(pi1): 1.469e+01, MSE(pi2): 1.125e+00, MSE(pi3): 1.022e+00\n",
      "Epoch 8200, Train loss: 2.430e+10, Test loss: 6.821e+10, MSE(e): 2.406e+00, MSE(pi1): 1.429e+01, MSE(pi2): 1.111e+00, MSE(pi3): 9.888e-01\n",
      "Epoch 8300, Train loss: 2.395e+10, Test loss: 6.802e+10, MSE(e): 2.372e+00, MSE(pi1): 1.384e+01, MSE(pi2): 1.097e+00, MSE(pi3): 9.533e-01\n",
      "Epoch 8400, Train loss: 2.363e+10, Test loss: 6.785e+10, MSE(e): 2.340e+00, MSE(pi1): 1.332e+01, MSE(pi2): 1.084e+00, MSE(pi3): 9.155e-01\n",
      "Epoch 8500, Train loss: 2.333e+10, Test loss: 6.766e+10, MSE(e): 2.311e+00, MSE(pi1): 1.274e+01, MSE(pi2): 1.072e+00, MSE(pi3): 8.746e-01\n",
      "Epoch 8600, Train loss: 2.304e+10, Test loss: 6.742e+10, MSE(e): 2.283e+00, MSE(pi1): 1.211e+01, MSE(pi2): 1.060e+00, MSE(pi3): 8.317e-01\n",
      "Epoch 8700, Train loss: 2.276e+10, Test loss: 6.706e+10, MSE(e): 2.257e+00, MSE(pi1): 1.143e+01, MSE(pi2): 1.049e+00, MSE(pi3): 7.876e-01\n",
      "Epoch 8800, Train loss: 2.247e+10, Test loss: 6.636e+10, MSE(e): 2.229e+00, MSE(pi1): 1.070e+01, MSE(pi2): 1.037e+00, MSE(pi3): 7.404e-01\n",
      "Epoch 8900, Train loss: 2.209e+10, Test loss: 6.488e+10, MSE(e): 2.192e+00, MSE(pi1): 9.936e+00, MSE(pi2): 1.020e+00, MSE(pi3): 6.934e-01\n",
      "Epoch 9000, Train loss: 2.169e+10, Test loss: 6.286e+10, MSE(e): 2.153e+00, MSE(pi1): 9.226e+00, MSE(pi2): 1.003e+00, MSE(pi3): 6.469e-01\n",
      "Epoch 9100, Train loss: 2.139e+10, Test loss: 6.187e+10, MSE(e): 2.124e+00, MSE(pi1): 8.538e+00, MSE(pi2): 9.901e-01, MSE(pi3): 6.050e-01\n",
      "Epoch 9200, Train loss: 2.112e+10, Test loss: 6.126e+10, MSE(e): 2.099e+00, MSE(pi1): 7.872e+00, MSE(pi2): 9.792e-01, MSE(pi3): 5.641e-01\n",
      "Epoch 9300, Train loss: 2.088e+10, Test loss: 6.088e+10, MSE(e): 2.075e+00, MSE(pi1): 7.236e+00, MSE(pi2): 9.684e-01, MSE(pi3): 5.300e-01\n",
      "Epoch 9400, Train loss: 2.064e+10, Test loss: 6.058e+10, MSE(e): 2.053e+00, MSE(pi1): 6.662e+00, MSE(pi2): 9.583e-01, MSE(pi3): 4.981e-01\n",
      "Epoch 9500, Train loss: 2.043e+10, Test loss: 6.031e+10, MSE(e): 2.032e+00, MSE(pi1): 6.166e+00, MSE(pi2): 9.487e-01, MSE(pi3): 4.682e-01\n",
      "Epoch 9600, Train loss: 2.022e+10, Test loss: 6.004e+10, MSE(e): 2.012e+00, MSE(pi1): 5.743e+00, MSE(pi2): 9.395e-01, MSE(pi3): 4.426e-01\n",
      "Epoch 9700, Train loss: 2.003e+10, Test loss: 5.980e+10, MSE(e): 1.993e+00, MSE(pi1): 5.387e+00, MSE(pi2): 9.302e-01, MSE(pi3): 4.244e-01\n",
      "Epoch 9800, Train loss: 1.983e+10, Test loss: 5.945e+10, MSE(e): 1.974e+00, MSE(pi1): 5.117e+00, MSE(pi2): 9.217e-01, MSE(pi3): 4.001e-01\n",
      "Epoch 9900, Train loss: 1.965e+10, Test loss: 5.913e+10, MSE(e): 1.956e+00, MSE(pi1): 4.891e+00, MSE(pi2): 9.130e-01, MSE(pi3): 3.837e-01\n",
      "Epoch 10000, Train loss: 1.948e+10, Test loss: 5.879e+10, MSE(e): 1.940e+00, MSE(pi1): 4.714e+00, MSE(pi2): 9.051e-01, MSE(pi3): 3.678e-01\n",
      "Epoch 10100, Train loss: 1.931e+10, Test loss: 5.845e+10, MSE(e): 1.923e+00, MSE(pi1): 4.571e+00, MSE(pi2): 8.970e-01, MSE(pi3): 3.545e-01\n",
      "Epoch 10200, Train loss: 1.915e+10, Test loss: 5.809e+10, MSE(e): 1.907e+00, MSE(pi1): 4.454e+00, MSE(pi2): 8.891e-01, MSE(pi3): 3.422e-01\n",
      "Epoch 10300, Train loss: 1.900e+10, Test loss: 5.777e+10, MSE(e): 1.892e+00, MSE(pi1): 4.348e+00, MSE(pi2): 8.814e-01, MSE(pi3): 3.411e-01\n",
      "Epoch 10400, Train loss: 1.884e+10, Test loss: 5.737e+10, MSE(e): 1.876e+00, MSE(pi1): 4.307e+00, MSE(pi2): 8.738e-01, MSE(pi3): 3.235e-01\n",
      "Epoch 10500, Train loss: 1.869e+10, Test loss: 5.699e+10, MSE(e): 1.862e+00, MSE(pi1): 4.209e+00, MSE(pi2): 8.664e-01, MSE(pi3): 3.120e-01\n",
      "Epoch 10600, Train loss: 1.854e+10, Test loss: 5.660e+10, MSE(e): 1.847e+00, MSE(pi1): 4.111e+00, MSE(pi2): 8.591e-01, MSE(pi3): 3.067e-01\n",
      "Epoch 10700, Train loss: 1.840e+10, Test loss: 5.619e+10, MSE(e): 1.833e+00, MSE(pi1): 4.047e+00, MSE(pi2): 8.521e-01, MSE(pi3): 2.998e-01\n",
      "Epoch 10800, Train loss: 1.826e+10, Test loss: 5.576e+10, MSE(e): 1.819e+00, MSE(pi1): 4.024e+00, MSE(pi2): 8.453e-01, MSE(pi3): 2.938e-01\n",
      "Epoch 10900, Train loss: 1.812e+10, Test loss: 5.531e+10, MSE(e): 1.805e+00, MSE(pi1): 3.935e+00, MSE(pi2): 8.387e-01, MSE(pi3): 2.876e-01\n",
      "Epoch 11000, Train loss: 1.799e+10, Test loss: 5.484e+10, MSE(e): 1.792e+00, MSE(pi1): 3.886e+00, MSE(pi2): 8.324e-01, MSE(pi3): 2.817e-01\n",
      "Epoch 11100, Train loss: 1.786e+10, Test loss: 5.435e+10, MSE(e): 1.779e+00, MSE(pi1): 3.836e+00, MSE(pi2): 8.262e-01, MSE(pi3): 2.763e-01\n",
      "Epoch 11200, Train loss: 1.773e+10, Test loss: 5.382e+10, MSE(e): 1.767e+00, MSE(pi1): 3.802e+00, MSE(pi2): 8.202e-01, MSE(pi3): 2.705e-01\n",
      "Epoch 11300, Train loss: 1.761e+10, Test loss: 5.327e+10, MSE(e): 1.754e+00, MSE(pi1): 3.723e+00, MSE(pi2): 8.144e-01, MSE(pi3): 2.683e-01\n",
      "Epoch 11400, Train loss: 1.749e+10, Test loss: 5.268e+10, MSE(e): 1.742e+00, MSE(pi1): 3.663e+00, MSE(pi2): 8.089e-01, MSE(pi3): 2.660e-01\n",
      "Epoch 11500, Train loss: 1.737e+10, Test loss: 5.202e+10, MSE(e): 1.731e+00, MSE(pi1): 3.609e+00, MSE(pi2): 8.035e-01, MSE(pi3): 2.591e-01\n",
      "Epoch 11600, Train loss: 1.725e+10, Test loss: 5.132e+10, MSE(e): 1.719e+00, MSE(pi1): 3.551e+00, MSE(pi2): 7.983e-01, MSE(pi3): 2.551e-01\n",
      "Epoch 11700, Train loss: 1.714e+10, Test loss: 5.055e+10, MSE(e): 1.708e+00, MSE(pi1): 3.505e+00, MSE(pi2): 7.932e-01, MSE(pi3): 2.550e-01\n",
      "Epoch 11800, Train loss: 1.703e+10, Test loss: 4.972e+10, MSE(e): 1.697e+00, MSE(pi1): 3.442e+00, MSE(pi2): 7.882e-01, MSE(pi3): 2.472e-01\n",
      "Epoch 11900, Train loss: 1.691e+10, Test loss: 4.881e+10, MSE(e): 1.686e+00, MSE(pi1): 3.401e+00, MSE(pi2): 7.833e-01, MSE(pi3): 2.424e-01\n",
      "Epoch 12000, Train loss: 1.680e+10, Test loss: 4.785e+10, MSE(e): 1.674e+00, MSE(pi1): 3.333e+00, MSE(pi2): 7.783e-01, MSE(pi3): 2.405e-01\n",
      "Epoch 12100, Train loss: 1.669e+10, Test loss: 4.684e+10, MSE(e): 1.664e+00, MSE(pi1): 3.368e+00, MSE(pi2): 7.734e-01, MSE(pi3): 2.374e-01\n",
      "Epoch 12200, Train loss: 1.658e+10, Test loss: 4.577e+10, MSE(e): 1.652e+00, MSE(pi1): 3.253e+00, MSE(pi2): 7.685e-01, MSE(pi3): 2.329e-01\n",
      "Epoch 12300, Train loss: 1.647e+10, Test loss: 4.467e+10, MSE(e): 1.642e+00, MSE(pi1): 3.225e+00, MSE(pi2): 7.638e-01, MSE(pi3): 2.287e-01\n",
      "Epoch 12400, Train loss: 1.635e+10, Test loss: 4.358e+10, MSE(e): 1.630e+00, MSE(pi1): 3.189e+00, MSE(pi2): 7.583e-01, MSE(pi3): 2.286e-01\n",
      "Epoch 12500, Train loss: 1.623e+10, Test loss: 4.248e+10, MSE(e): 1.618e+00, MSE(pi1): 3.105e+00, MSE(pi2): 7.532e-01, MSE(pi3): 2.262e-01\n",
      "Epoch 12600, Train loss: 1.612e+10, Test loss: 4.143e+10, MSE(e): 1.606e+00, MSE(pi1): 3.073e+00, MSE(pi2): 7.479e-01, MSE(pi3): 2.242e-01\n",
      "Epoch 12700, Train loss: 1.600e+10, Test loss: 4.043e+10, MSE(e): 1.594e+00, MSE(pi1): 3.023e+00, MSE(pi2): 7.426e-01, MSE(pi3): 2.214e-01\n",
      "Epoch 12800, Train loss: 1.588e+10, Test loss: 3.950e+10, MSE(e): 1.582e+00, MSE(pi1): 2.985e+00, MSE(pi2): 7.371e-01, MSE(pi3): 2.194e-01\n",
      "Epoch 12900, Train loss: 1.575e+10, Test loss: 3.863e+10, MSE(e): 1.570e+00, MSE(pi1): 2.949e+00, MSE(pi2): 7.316e-01, MSE(pi3): 2.174e-01\n",
      "Epoch 13000, Train loss: 1.563e+10, Test loss: 3.787e+10, MSE(e): 1.558e+00, MSE(pi1): 2.914e+00, MSE(pi2): 7.261e-01, MSE(pi3): 2.154e-01\n",
      "Epoch 13100, Train loss: 1.550e+10, Test loss: 3.720e+10, MSE(e): 1.545e+00, MSE(pi1): 2.880e+00, MSE(pi2): 7.206e-01, MSE(pi3): 2.140e-01\n",
      "Epoch 13200, Train loss: 1.538e+10, Test loss: 3.658e+10, MSE(e): 1.533e+00, MSE(pi1): 2.850e+00, MSE(pi2): 7.148e-01, MSE(pi3): 2.125e-01\n",
      "Epoch 13300, Train loss: 1.525e+10, Test loss: 3.604e+10, MSE(e): 1.520e+00, MSE(pi1): 2.829e+00, MSE(pi2): 7.090e-01, MSE(pi3): 2.113e-01\n",
      "Epoch 13400, Train loss: 1.511e+10, Test loss: 3.556e+10, MSE(e): 1.506e+00, MSE(pi1): 2.802e+00, MSE(pi2): 7.031e-01, MSE(pi3): 2.085e-01\n",
      "Epoch 13500, Train loss: 1.498e+10, Test loss: 3.516e+10, MSE(e): 1.493e+00, MSE(pi1): 2.879e+00, MSE(pi2): 6.972e-01, MSE(pi3): 2.079e-01\n",
      "Epoch 13600, Train loss: 1.485e+10, Test loss: 3.480e+10, MSE(e): 1.480e+00, MSE(pi1): 2.737e+00, MSE(pi2): 6.912e-01, MSE(pi3): 2.061e-01\n",
      "Epoch 13700, Train loss: 1.471e+10, Test loss: 3.450e+10, MSE(e): 1.466e+00, MSE(pi1): 2.716e+00, MSE(pi2): 6.851e-01, MSE(pi3): 2.051e-01\n",
      "Epoch 13800, Train loss: 1.458e+10, Test loss: 3.424e+10, MSE(e): 1.453e+00, MSE(pi1): 2.697e+00, MSE(pi2): 6.793e-01, MSE(pi3): 2.060e-01\n",
      "Epoch 13900, Train loss: 1.444e+10, Test loss: 3.400e+10, MSE(e): 1.439e+00, MSE(pi1): 2.666e+00, MSE(pi2): 6.730e-01, MSE(pi3): 2.023e-01\n",
      "Epoch 14000, Train loss: 1.431e+10, Test loss: 3.380e+10, MSE(e): 1.426e+00, MSE(pi1): 2.648e+00, MSE(pi2): 6.670e-01, MSE(pi3): 2.019e-01\n",
      "Epoch 14100, Train loss: 1.417e+10, Test loss: 3.362e+10, MSE(e): 1.412e+00, MSE(pi1): 2.631e+00, MSE(pi2): 6.610e-01, MSE(pi3): 1.995e-01\n",
      "Epoch 14200, Train loss: 1.403e+10, Test loss: 3.346e+10, MSE(e): 1.399e+00, MSE(pi1): 2.619e+00, MSE(pi2): 6.549e-01, MSE(pi3): 1.997e-01\n",
      "Epoch 14300, Train loss: 1.390e+10, Test loss: 3.333e+10, MSE(e): 1.385e+00, MSE(pi1): 2.583e+00, MSE(pi2): 6.489e-01, MSE(pi3): 1.977e-01\n",
      "Epoch 14400, Train loss: 1.376e+10, Test loss: 3.321e+10, MSE(e): 1.372e+00, MSE(pi1): 2.564e+00, MSE(pi2): 6.429e-01, MSE(pi3): 1.967e-01\n",
      "Epoch 14500, Train loss: 1.363e+10, Test loss: 3.308e+10, MSE(e): 1.359e+00, MSE(pi1): 2.585e+00, MSE(pi2): 6.371e-01, MSE(pi3): 1.946e-01\n",
      "Epoch 14600, Train loss: 1.350e+10, Test loss: 3.301e+10, MSE(e): 1.346e+00, MSE(pi1): 2.523e+00, MSE(pi2): 6.312e-01, MSE(pi3): 1.949e-01\n",
      "Epoch 14700, Train loss: 1.338e+10, Test loss: 3.290e+10, MSE(e): 1.333e+00, MSE(pi1): 2.509e+00, MSE(pi2): 6.256e-01, MSE(pi3): 1.930e-01\n",
      "Epoch 14800, Train loss: 1.325e+10, Test loss: 3.285e+10, MSE(e): 1.320e+00, MSE(pi1): 2.505e+00, MSE(pi2): 6.198e-01, MSE(pi3): 1.936e-01\n",
      "Epoch 14900, Train loss: 1.312e+10, Test loss: 3.277e+10, MSE(e): 1.308e+00, MSE(pi1): 2.479e+00, MSE(pi2): 6.141e-01, MSE(pi3): 1.917e-01\n",
      "Epoch 15000, Train loss: 1.301e+10, Test loss: 3.267e+10, MSE(e): 1.297e+00, MSE(pi1): 2.545e+00, MSE(pi2): 6.090e-01, MSE(pi3): 1.888e-01\n",
      "Epoch 15100, Train loss: 1.287e+10, Test loss: 3.260e+10, MSE(e): 1.283e+00, MSE(pi1): 2.453e+00, MSE(pi2): 6.029e-01, MSE(pi3): 1.907e-01\n",
      "Epoch 15200, Train loss: 1.275e+10, Test loss: 3.251e+10, MSE(e): 1.271e+00, MSE(pi1): 2.464e+00, MSE(pi2): 5.975e-01, MSE(pi3): 1.896e-01\n",
      "Epoch 15300, Train loss: 1.265e+10, Test loss: 3.237e+10, MSE(e): 1.261e+00, MSE(pi1): 2.414e+00, MSE(pi2): 5.928e-01, MSE(pi3): 1.865e-01\n",
      "Epoch 15400, Train loss: 1.252e+10, Test loss: 3.229e+10, MSE(e): 1.248e+00, MSE(pi1): 2.381e+00, MSE(pi2): 5.869e-01, MSE(pi3): 1.873e-01\n",
      "Epoch 15500, Train loss: 1.241e+10, Test loss: 3.217e+10, MSE(e): 1.236e+00, MSE(pi1): 2.371e+00, MSE(pi2): 5.817e-01, MSE(pi3): 1.865e-01\n",
      "Epoch 15600, Train loss: 1.230e+10, Test loss: 3.203e+10, MSE(e): 1.225e+00, MSE(pi1): 2.420e+00, MSE(pi2): 5.766e-01, MSE(pi3): 1.853e-01\n",
      "Epoch 15700, Train loss: 1.219e+10, Test loss: 3.185e+10, MSE(e): 1.214e+00, MSE(pi1): 2.374e+00, MSE(pi2): 5.715e-01, MSE(pi3): 1.848e-01\n",
      "Epoch 15800, Train loss: 1.208e+10, Test loss: 3.165e+10, MSE(e): 1.203e+00, MSE(pi1): 2.316e+00, MSE(pi2): 5.664e-01, MSE(pi3): 1.838e-01\n",
      "Epoch 15900, Train loss: 1.197e+10, Test loss: 3.146e+10, MSE(e): 1.193e+00, MSE(pi1): 2.312e+00, MSE(pi2): 5.614e-01, MSE(pi3): 1.824e-01\n",
      "Epoch 16000, Train loss: 1.186e+10, Test loss: 3.125e+10, MSE(e): 1.182e+00, MSE(pi1): 2.286e+00, MSE(pi2): 5.565e-01, MSE(pi3): 1.814e-01\n",
      "Epoch 16100, Train loss: 1.176e+10, Test loss: 3.101e+10, MSE(e): 1.172e+00, MSE(pi1): 2.271e+00, MSE(pi2): 5.515e-01, MSE(pi3): 1.804e-01\n",
      "Epoch 16200, Train loss: 1.165e+10, Test loss: 3.077e+10, MSE(e): 1.161e+00, MSE(pi1): 2.343e+00, MSE(pi2): 5.467e-01, MSE(pi3): 1.796e-01\n",
      "Epoch 16300, Train loss: 1.155e+10, Test loss: 3.052e+10, MSE(e): 1.151e+00, MSE(pi1): 2.252e+00, MSE(pi2): 5.418e-01, MSE(pi3): 1.786e-01\n",
      "Epoch 16400, Train loss: 1.145e+10, Test loss: 3.022e+10, MSE(e): 1.140e+00, MSE(pi1): 2.325e+00, MSE(pi2): 5.370e-01, MSE(pi3): 1.768e-01\n",
      "Epoch 16500, Train loss: 1.134e+10, Test loss: 2.993e+10, MSE(e): 1.130e+00, MSE(pi1): 2.203e+00, MSE(pi2): 5.321e-01, MSE(pi3): 1.762e-01\n",
      "Epoch 16600, Train loss: 1.124e+10, Test loss: 2.960e+10, MSE(e): 1.120e+00, MSE(pi1): 2.191e+00, MSE(pi2): 5.272e-01, MSE(pi3): 1.743e-01\n",
      "Epoch 16700, Train loss: 1.113e+10, Test loss: 2.930e+10, MSE(e): 1.109e+00, MSE(pi1): 2.169e+00, MSE(pi2): 5.224e-01, MSE(pi3): 1.739e-01\n",
      "Epoch 16800, Train loss: 1.103e+10, Test loss: 2.898e+10, MSE(e): 1.099e+00, MSE(pi1): 2.155e+00, MSE(pi2): 5.175e-01, MSE(pi3): 1.723e-01\n",
      "Epoch 16900, Train loss: 1.092e+10, Test loss: 2.864e+10, MSE(e): 1.088e+00, MSE(pi1): 2.134e+00, MSE(pi2): 5.126e-01, MSE(pi3): 1.711e-01\n",
      "Epoch 17000, Train loss: 1.082e+10, Test loss: 2.830e+10, MSE(e): 1.078e+00, MSE(pi1): 2.116e+00, MSE(pi2): 5.077e-01, MSE(pi3): 1.700e-01\n",
      "Epoch 17100, Train loss: 1.071e+10, Test loss: 2.796e+10, MSE(e): 1.067e+00, MSE(pi1): 2.151e+00, MSE(pi2): 5.028e-01, MSE(pi3): 1.684e-01\n",
      "Epoch 17200, Train loss: 1.060e+10, Test loss: 2.758e+10, MSE(e): 1.057e+00, MSE(pi1): 2.085e+00, MSE(pi2): 4.978e-01, MSE(pi3): 1.664e-01\n",
      "Epoch 17300, Train loss: 1.049e+10, Test loss: 2.721e+10, MSE(e): 1.046e+00, MSE(pi1): 2.069e+00, MSE(pi2): 4.927e-01, MSE(pi3): 1.650e-01\n",
      "Epoch 17400, Train loss: 1.038e+10, Test loss: 2.685e+10, MSE(e): 1.034e+00, MSE(pi1): 2.053e+00, MSE(pi2): 4.876e-01, MSE(pi3): 1.645e-01\n",
      "Epoch 17500, Train loss: 1.027e+10, Test loss: 2.649e+10, MSE(e): 1.023e+00, MSE(pi1): 2.035e+00, MSE(pi2): 4.825e-01, MSE(pi3): 1.631e-01\n",
      "Epoch 17600, Train loss: 1.016e+10, Test loss: 2.611e+10, MSE(e): 1.012e+00, MSE(pi1): 1.999e+00, MSE(pi2): 4.773e-01, MSE(pi3): 1.618e-01\n",
      "Epoch 17700, Train loss: 1.004e+10, Test loss: 2.573e+10, MSE(e): 1.001e+00, MSE(pi1): 1.996e+00, MSE(pi2): 4.719e-01, MSE(pi3): 1.597e-01\n",
      "Epoch 17800, Train loss: 9.931e+09, Test loss: 2.537e+10, MSE(e): 9.896e-01, MSE(pi1): 1.959e+00, MSE(pi2): 4.667e-01, MSE(pi3): 1.580e-01\n",
      "Epoch 17900, Train loss: 9.882e+09, Test loss: 2.502e+10, MSE(e): 9.846e-01, MSE(pi1): 2.082e+00, MSE(pi2): 4.640e-01, MSE(pi3): 1.542e-01\n",
      "Epoch 18000, Train loss: 9.700e+09, Test loss: 2.462e+10, MSE(e): 9.665e-01, MSE(pi1): 1.917e+00, MSE(pi2): 4.559e-01, MSE(pi3): 1.548e-01\n",
      "Epoch 18100, Train loss: 9.585e+09, Test loss: 2.427e+10, MSE(e): 9.551e-01, MSE(pi1): 1.907e+00, MSE(pi2): 4.505e-01, MSE(pi3): 1.532e-01\n",
      "Epoch 18200, Train loss: 9.470e+09, Test loss: 2.392e+10, MSE(e): 9.436e-01, MSE(pi1): 1.873e+00, MSE(pi2): 4.451e-01, MSE(pi3): 1.515e-01\n",
      "Epoch 18300, Train loss: 9.358e+09, Test loss: 2.358e+10, MSE(e): 9.324e-01, MSE(pi1): 1.845e+00, MSE(pi2): 4.397e-01, MSE(pi3): 1.512e-01\n",
      "Epoch 18400, Train loss: 9.242e+09, Test loss: 2.326e+10, MSE(e): 9.209e-01, MSE(pi1): 1.832e+00, MSE(pi2): 4.342e-01, MSE(pi3): 1.481e-01\n",
      "Epoch 18500, Train loss: 9.125e+09, Test loss: 2.294e+10, MSE(e): 9.092e-01, MSE(pi1): 1.854e+00, MSE(pi2): 4.285e-01, MSE(pi3): 1.465e-01\n",
      "Epoch 18600, Train loss: 9.006e+09, Test loss: 2.263e+10, MSE(e): 8.974e-01, MSE(pi1): 1.790e+00, MSE(pi2): 4.228e-01, MSE(pi3): 1.445e-01\n",
      "Epoch 18700, Train loss: 8.891e+09, Test loss: 2.235e+10, MSE(e): 8.859e-01, MSE(pi1): 1.770e+00, MSE(pi2): 4.172e-01, MSE(pi3): 1.423e-01\n",
      "Epoch 18800, Train loss: 8.776e+09, Test loss: 2.207e+10, MSE(e): 8.743e-01, MSE(pi1): 1.858e+00, MSE(pi2): 4.116e-01, MSE(pi3): 1.408e-01\n",
      "Epoch 18900, Train loss: 8.661e+09, Test loss: 2.182e+10, MSE(e): 8.630e-01, MSE(pi1): 1.731e+00, MSE(pi2): 4.060e-01, MSE(pi3): 1.382e-01\n",
      "Epoch 19000, Train loss: 8.546e+09, Test loss: 2.157e+10, MSE(e): 8.514e-01, MSE(pi1): 1.894e+00, MSE(pi2): 4.002e-01, MSE(pi3): 1.368e-01\n",
      "Epoch 19100, Train loss: 8.430e+09, Test loss: 2.135e+10, MSE(e): 8.400e-01, MSE(pi1): 1.688e+00, MSE(pi2): 3.946e-01, MSE(pi3): 1.346e-01\n",
      "Epoch 19200, Train loss: 8.409e+09, Test loss: 2.134e+10, MSE(e): 8.378e-01, MSE(pi1): 1.765e+00, MSE(pi2): 3.926e-01, MSE(pi3): 1.313e-01\n",
      "Epoch 19300, Train loss: 8.207e+09, Test loss: 2.094e+10, MSE(e): 8.177e-01, MSE(pi1): 1.651e+00, MSE(pi2): 3.834e-01, MSE(pi3): 1.308e-01\n",
      "Epoch 19400, Train loss: 8.099e+09, Test loss: 2.077e+10, MSE(e): 8.070e-01, MSE(pi1): 1.640e+00, MSE(pi2): 3.780e-01, MSE(pi3): 1.287e-01\n",
      "Epoch 19500, Train loss: 7.993e+09, Test loss: 2.061e+10, MSE(e): 7.965e-01, MSE(pi1): 1.619e+00, MSE(pi2): 3.727e-01, MSE(pi3): 1.270e-01\n",
      "Epoch 19600, Train loss: 7.893e+09, Test loss: 2.046e+10, MSE(e): 7.864e-01, MSE(pi1): 1.603e+00, MSE(pi2): 3.677e-01, MSE(pi3): 1.252e-01\n",
      "Epoch 19700, Train loss: 7.794e+09, Test loss: 2.034e+10, MSE(e): 7.765e-01, MSE(pi1): 1.590e+00, MSE(pi2): 3.627e-01, MSE(pi3): 1.233e-01\n",
      "Epoch 19800, Train loss: 7.698e+09, Test loss: 2.023e+10, MSE(e): 7.670e-01, MSE(pi1): 1.574e+00, MSE(pi2): 3.579e-01, MSE(pi3): 1.216e-01\n",
      "Epoch 19900, Train loss: 7.606e+09, Test loss: 2.012e+10, MSE(e): 7.578e-01, MSE(pi1): 1.569e+00, MSE(pi2): 3.532e-01, MSE(pi3): 1.199e-01\n",
      "Epoch 20000, Train loss: 7.514e+09, Test loss: 2.002e+10, MSE(e): 7.487e-01, MSE(pi1): 1.550e+00, MSE(pi2): 3.487e-01, MSE(pi3): 1.181e-01\n",
      "Epoch 20100, Train loss: 7.428e+09, Test loss: 1.996e+10, MSE(e): 7.400e-01, MSE(pi1): 1.639e+00, MSE(pi2): 3.443e-01, MSE(pi3): 1.167e-01\n",
      "Epoch 20200, Train loss: 7.343e+09, Test loss: 1.988e+10, MSE(e): 7.316e-01, MSE(pi1): 1.527e+00, MSE(pi2): 3.401e-01, MSE(pi3): 1.150e-01\n",
      "Epoch 20300, Train loss: 7.261e+09, Test loss: 1.981e+10, MSE(e): 7.234e-01, MSE(pi1): 1.532e+00, MSE(pi2): 3.360e-01, MSE(pi3): 1.131e-01\n",
      "Epoch 20400, Train loss: 7.183e+09, Test loss: 1.982e+10, MSE(e): 7.157e-01, MSE(pi1): 1.510e+00, MSE(pi2): 3.322e-01, MSE(pi3): 1.116e-01\n",
      "Epoch 20500, Train loss: 7.110e+09, Test loss: 1.982e+10, MSE(e): 7.084e-01, MSE(pi1): 1.503e+00, MSE(pi2): 3.285e-01, MSE(pi3): 1.099e-01\n",
      "Epoch 20600, Train loss: 7.036e+09, Test loss: 1.981e+10, MSE(e): 7.010e-01, MSE(pi1): 1.493e+00, MSE(pi2): 3.249e-01, MSE(pi3): 1.088e-01\n",
      "Epoch 20700, Train loss: 6.968e+09, Test loss: 1.983e+10, MSE(e): 6.942e-01, MSE(pi1): 1.484e+00, MSE(pi2): 3.216e-01, MSE(pi3): 1.074e-01\n",
      "Epoch 20800, Train loss: 6.900e+09, Test loss: 1.986e+10, MSE(e): 6.875e-01, MSE(pi1): 1.480e+00, MSE(pi2): 3.183e-01, MSE(pi3): 1.063e-01\n",
      "Epoch 20900, Train loss: 6.836e+09, Test loss: 1.992e+10, MSE(e): 6.811e-01, MSE(pi1): 1.468e+00, MSE(pi2): 3.152e-01, MSE(pi3): 1.049e-01\n",
      "Epoch 21000, Train loss: 6.772e+09, Test loss: 1.999e+10, MSE(e): 6.747e-01, MSE(pi1): 1.463e+00, MSE(pi2): 3.122e-01, MSE(pi3): 1.039e-01\n",
      "Epoch 21100, Train loss: 6.711e+09, Test loss: 2.008e+10, MSE(e): 6.686e-01, MSE(pi1): 1.452e+00, MSE(pi2): 3.093e-01, MSE(pi3): 1.028e-01\n",
      "Epoch 21200, Train loss: 6.653e+09, Test loss: 2.016e+10, MSE(e): 6.628e-01, MSE(pi1): 1.454e+00, MSE(pi2): 3.065e-01, MSE(pi3): 1.015e-01\n",
      "Epoch 21300, Train loss: 6.593e+09, Test loss: 2.034e+10, MSE(e): 6.568e-01, MSE(pi1): 1.438e+00, MSE(pi2): 3.037e-01, MSE(pi3): 1.006e-01\n",
      "Epoch 21400, Train loss: 6.536e+09, Test loss: 2.051e+10, MSE(e): 6.512e-01, MSE(pi1): 1.440e+00, MSE(pi2): 3.010e-01, MSE(pi3): 9.983e-02\n",
      "Epoch 21500, Train loss: 6.481e+09, Test loss: 2.074e+10, MSE(e): 6.457e-01, MSE(pi1): 1.438e+00, MSE(pi2): 2.985e-01, MSE(pi3): 9.824e-02\n",
      "Epoch 21600, Train loss: 6.426e+09, Test loss: 2.089e+10, MSE(e): 6.402e-01, MSE(pi1): 1.432e+00, MSE(pi2): 2.959e-01, MSE(pi3): 9.762e-02\n",
      "Epoch 21700, Train loss: 6.373e+09, Test loss: 2.111e+10, MSE(e): 6.350e-01, MSE(pi1): 1.409e+00, MSE(pi2): 2.935e-01, MSE(pi3): 9.686e-02\n",
      "Epoch 21800, Train loss: 6.346e+09, Test loss: 2.137e+10, MSE(e): 6.322e-01, MSE(pi1): 1.433e+00, MSE(pi2): 2.921e-01, MSE(pi3): 9.459e-02\n",
      "Epoch 21900, Train loss: 6.272e+09, Test loss: 2.164e+10, MSE(e): 6.247e-01, MSE(pi1): 1.516e+00, MSE(pi2): 2.888e-01, MSE(pi3): 9.628e-02\n",
      "Epoch 22000, Train loss: 6.236e+09, Test loss: 2.207e+10, MSE(e): 6.212e-01, MSE(pi1): 1.514e+00, MSE(pi2): 2.871e-01, MSE(pi3): 9.296e-02\n",
      "Epoch 22100, Train loss: 6.174e+09, Test loss: 2.224e+10, MSE(e): 6.151e-01, MSE(pi1): 1.381e+00, MSE(pi2): 2.844e-01, MSE(pi3): 9.328e-02\n",
      "Epoch 22200, Train loss: 6.126e+09, Test loss: 2.257e+10, MSE(e): 6.103e-01, MSE(pi1): 1.382e+00, MSE(pi2): 2.823e-01, MSE(pi3): 9.265e-02\n",
      "Epoch 22300, Train loss: 6.080e+09, Test loss: 2.293e+10, MSE(e): 6.057e-01, MSE(pi1): 1.366e+00, MSE(pi2): 2.802e-01, MSE(pi3): 9.152e-02\n",
      "Epoch 22400, Train loss: 6.034e+09, Test loss: 2.329e+10, MSE(e): 6.011e-01, MSE(pi1): 1.357e+00, MSE(pi2): 2.781e-01, MSE(pi3): 9.084e-02\n",
      "Epoch 22500, Train loss: 5.989e+09, Test loss: 2.367e+10, MSE(e): 5.967e-01, MSE(pi1): 1.349e+00, MSE(pi2): 2.761e-01, MSE(pi3): 9.019e-02\n",
      "Epoch 22600, Train loss: 5.944e+09, Test loss: 2.409e+10, MSE(e): 5.922e-01, MSE(pi1): 1.341e+00, MSE(pi2): 2.741e-01, MSE(pi3): 8.917e-02\n",
      "Epoch 22700, Train loss: 5.901e+09, Test loss: 2.449e+10, MSE(e): 5.879e-01, MSE(pi1): 1.330e+00, MSE(pi2): 2.722e-01, MSE(pi3): 8.855e-02\n",
      "Epoch 22800, Train loss: 5.866e+09, Test loss: 2.501e+10, MSE(e): 5.844e-01, MSE(pi1): 1.329e+00, MSE(pi2): 2.707e-01, MSE(pi3): 8.714e-02\n",
      "Epoch 22900, Train loss: 5.816e+09, Test loss: 2.536e+10, MSE(e): 5.794e-01, MSE(pi1): 1.311e+00, MSE(pi2): 2.685e-01, MSE(pi3): 8.708e-02\n",
      "Epoch 23000, Train loss: 5.778e+09, Test loss: 2.571e+10, MSE(e): 5.756e-01, MSE(pi1): 1.299e+00, MSE(pi2): 2.668e-01, MSE(pi3): 8.697e-02\n",
      "Epoch 23100, Train loss: 5.737e+09, Test loss: 2.625e+10, MSE(e): 5.714e-01, MSE(pi1): 1.423e+00, MSE(pi2): 2.650e-01, MSE(pi3): 8.726e-02\n",
      "Epoch 23200, Train loss: 5.696e+09, Test loss: 2.672e+10, MSE(e): 5.674e-01, MSE(pi1): 1.280e+00, MSE(pi2): 2.633e-01, MSE(pi3): 8.493e-02\n",
      "Epoch 23300, Train loss: 5.657e+09, Test loss: 2.720e+10, MSE(e): 5.636e-01, MSE(pi1): 1.274e+00, MSE(pi2): 2.616e-01, MSE(pi3): 8.419e-02\n",
      "Epoch 23400, Train loss: 5.636e+09, Test loss: 2.783e+10, MSE(e): 5.615e-01, MSE(pi1): 1.275e+00, MSE(pi2): 2.607e-01, MSE(pi3): 8.247e-02\n",
      "Epoch 23500, Train loss: 5.581e+09, Test loss: 2.818e+10, MSE(e): 5.559e-01, MSE(pi1): 1.347e+00, MSE(pi2): 2.583e-01, MSE(pi3): 8.375e-02\n",
      "Epoch 23600, Train loss: 5.600e+09, Test loss: 2.879e+10, MSE(e): 5.579e-01, MSE(pi1): 1.280e+00, MSE(pi2): 2.590e-01, MSE(pi3): 8.068e-02\n",
      "Epoch 23700, Train loss: 5.505e+09, Test loss: 2.920e+10, MSE(e): 5.484e-01, MSE(pi1): 1.227e+00, MSE(pi2): 2.551e-01, MSE(pi3): 8.166e-02\n",
      "Epoch 23800, Train loss: 5.468e+09, Test loss: 2.970e+10, MSE(e): 5.448e-01, MSE(pi1): 1.215e+00, MSE(pi2): 2.535e-01, MSE(pi3): 8.098e-02\n",
      "Epoch 23900, Train loss: 5.432e+09, Test loss: 3.025e+10, MSE(e): 5.412e-01, MSE(pi1): 1.202e+00, MSE(pi2): 2.520e-01, MSE(pi3): 8.044e-02\n",
      "Epoch 24000, Train loss: 5.397e+09, Test loss: 3.079e+10, MSE(e): 5.377e-01, MSE(pi1): 1.193e+00, MSE(pi2): 2.504e-01, MSE(pi3): 7.983e-02\n",
      "Epoch 24100, Train loss: 5.523e+09, Test loss: 3.183e+10, MSE(e): 5.503e-01, MSE(pi1): 1.287e+00, MSE(pi2): 2.555e-01, MSE(pi3): 7.764e-02\n",
      "Epoch 24200, Train loss: 5.328e+09, Test loss: 3.188e+10, MSE(e): 5.308e-01, MSE(pi1): 1.170e+00, MSE(pi2): 2.475e-01, MSE(pi3): 7.858e-02\n",
      "Epoch 24300, Train loss: 5.305e+09, Test loss: 3.237e+10, MSE(e): 5.285e-01, MSE(pi1): 1.240e+00, MSE(pi2): 2.465e-01, MSE(pi3): 7.916e-02\n",
      "Epoch 24400, Train loss: 5.260e+09, Test loss: 3.301e+10, MSE(e): 5.241e-01, MSE(pi1): 1.148e+00, MSE(pi2): 2.446e-01, MSE(pi3): 7.750e-02\n",
      "Epoch 24500, Train loss: 5.230e+09, Test loss: 3.361e+10, MSE(e): 5.211e-01, MSE(pi1): 1.146e+00, MSE(pi2): 2.433e-01, MSE(pi3): 7.651e-02\n",
      "Epoch 24600, Train loss: 5.194e+09, Test loss: 3.416e+10, MSE(e): 5.175e-01, MSE(pi1): 1.126e+00, MSE(pi2): 2.418e-01, MSE(pi3): 7.645e-02\n",
      "Epoch 24700, Train loss: 5.162e+09, Test loss: 3.473e+10, MSE(e): 5.143e-01, MSE(pi1): 1.116e+00, MSE(pi2): 2.404e-01, MSE(pi3): 7.588e-02\n",
      "Epoch 24800, Train loss: 5.130e+09, Test loss: 3.533e+10, MSE(e): 5.111e-01, MSE(pi1): 1.126e+00, MSE(pi2): 2.391e-01, MSE(pi3): 7.561e-02\n",
      "Epoch 24900, Train loss: 5.099e+09, Test loss: 3.592e+10, MSE(e): 5.080e-01, MSE(pi1): 1.096e+00, MSE(pi2): 2.377e-01, MSE(pi3): 7.489e-02\n",
      "Epoch 25000, Train loss: 5.361e+09, Test loss: 3.690e+10, MSE(e): 5.341e-01, MSE(pi1): 1.188e+00, MSE(pi2): 2.484e-01, MSE(pi3): 7.375e-02\n",
      "Epoch 25100, Train loss: 5.037e+09, Test loss: 3.712e+10, MSE(e): 5.019e-01, MSE(pi1): 1.076e+00, MSE(pi2): 2.351e-01, MSE(pi3): 7.388e-02\n",
      "Epoch 25200, Train loss: 5.015e+09, Test loss: 3.782e+10, MSE(e): 4.997e-01, MSE(pi1): 1.094e+00, MSE(pi2): 2.341e-01, MSE(pi3): 7.284e-02\n",
      "Epoch 25300, Train loss: 4.977e+09, Test loss: 3.834e+10, MSE(e): 4.959e-01, MSE(pi1): 1.057e+00, MSE(pi2): 2.325e-01, MSE(pi3): 7.299e-02\n",
      "Epoch 25400, Train loss: 4.947e+09, Test loss: 3.892e+10, MSE(e): 4.929e-01, MSE(pi1): 1.049e+00, MSE(pi2): 2.313e-01, MSE(pi3): 7.267e-02\n",
      "Epoch 25500, Train loss: 4.918e+09, Test loss: 3.959e+10, MSE(e): 4.900e-01, MSE(pi1): 1.041e+00, MSE(pi2): 2.300e-01, MSE(pi3): 7.210e-02\n",
      "Epoch 25600, Train loss: 4.890e+09, Test loss: 4.024e+10, MSE(e): 4.872e-01, MSE(pi1): 1.033e+00, MSE(pi2): 2.288e-01, MSE(pi3): 7.148e-02\n",
      "Epoch 25700, Train loss: 4.862e+09, Test loss: 4.090e+10, MSE(e): 4.844e-01, MSE(pi1): 1.072e+00, MSE(pi2): 2.276e-01, MSE(pi3): 7.141e-02\n",
      "Epoch 25800, Train loss: 4.833e+09, Test loss: 4.149e+10, MSE(e): 4.816e-01, MSE(pi1): 1.015e+00, MSE(pi2): 2.264e-01, MSE(pi3): 7.078e-02\n",
      "Epoch 25900, Train loss: 4.809e+09, Test loss: 4.210e+10, MSE(e): 4.791e-01, MSE(pi1): 1.119e+00, MSE(pi2): 2.253e-01, MSE(pi3): 7.162e-02\n",
      "Epoch 26000, Train loss: 4.778e+09, Test loss: 4.277e+10, MSE(e): 4.761e-01, MSE(pi1): 9.994e-01, MSE(pi2): 2.240e-01, MSE(pi3): 6.996e-02\n",
      "Epoch 26100, Train loss: 4.939e+09, Test loss: 4.376e+10, MSE(e): 4.921e-01, MSE(pi1): 1.061e+00, MSE(pi2): 2.305e-01, MSE(pi3): 6.872e-02\n",
      "Epoch 26200, Train loss: 4.724e+09, Test loss: 4.406e+10, MSE(e): 4.708e-01, MSE(pi1): 9.839e-01, MSE(pi2): 2.217e-01, MSE(pi3): 6.910e-02\n",
      "Epoch 26300, Train loss: 4.698e+09, Test loss: 4.472e+10, MSE(e): 4.681e-01, MSE(pi1): 9.818e-01, MSE(pi2): 2.206e-01, MSE(pi3): 6.861e-02\n",
      "Epoch 26400, Train loss: 4.672e+09, Test loss: 4.537e+10, MSE(e): 4.656e-01, MSE(pi1): 9.709e-01, MSE(pi2): 2.195e-01, MSE(pi3): 6.821e-02\n",
      "Epoch 26500, Train loss: 4.646e+09, Test loss: 4.603e+10, MSE(e): 4.630e-01, MSE(pi1): 9.647e-01, MSE(pi2): 2.184e-01, MSE(pi3): 6.786e-02\n",
      "Epoch 26600, Train loss: 4.621e+09, Test loss: 4.669e+10, MSE(e): 4.605e-01, MSE(pi1): 9.586e-01, MSE(pi2): 2.173e-01, MSE(pi3): 6.740e-02\n",
      "Epoch 26700, Train loss: 4.597e+09, Test loss: 4.739e+10, MSE(e): 4.580e-01, MSE(pi1): 9.549e-01, MSE(pi2): 2.163e-01, MSE(pi3): 6.695e-02\n",
      "Epoch 26800, Train loss: 4.571e+09, Test loss: 4.802e+10, MSE(e): 4.555e-01, MSE(pi1): 9.453e-01, MSE(pi2): 2.152e-01, MSE(pi3): 6.675e-02\n",
      "Epoch 26900, Train loss: 4.667e+09, Test loss: 4.906e+10, MSE(e): 4.651e-01, MSE(pi1): 1.033e+00, MSE(pi2): 2.190e-01, MSE(pi3): 6.592e-02\n",
      "Epoch 27000, Train loss: 4.522e+09, Test loss: 4.934e+10, MSE(e): 4.506e-01, MSE(pi1): 9.331e-01, MSE(pi2): 2.131e-01, MSE(pi3): 6.609e-02\n",
      "Epoch 27100, Train loss: 4.499e+09, Test loss: 5.003e+10, MSE(e): 4.482e-01, MSE(pi1): 1.056e+00, MSE(pi2): 2.121e-01, MSE(pi3): 6.681e-02\n",
      "Epoch 27200, Train loss: 4.475e+09, Test loss: 5.070e+10, MSE(e): 4.459e-01, MSE(pi1): 9.231e-01, MSE(pi2): 2.111e-01, MSE(pi3): 6.534e-02\n",
      "Epoch 27300, Train loss: 4.452e+09, Test loss: 5.137e+10, MSE(e): 4.436e-01, MSE(pi1): 9.517e-01, MSE(pi2): 2.101e-01, MSE(pi3): 6.495e-02\n",
      "Epoch 27400, Train loss: 4.434e+09, Test loss: 5.202e+10, MSE(e): 4.419e-01, MSE(pi1): 9.084e-01, MSE(pi2): 2.093e-01, MSE(pi3): 6.524e-02\n",
      "Epoch 27500, Train loss: 4.406e+09, Test loss: 5.271e+10, MSE(e): 4.391e-01, MSE(pi1): 9.058e-01, MSE(pi2): 2.081e-01, MSE(pi3): 6.438e-02\n",
      "Epoch 27600, Train loss: 4.436e+09, Test loss: 5.364e+10, MSE(e): 4.421e-01, MSE(pi1): 9.484e-01, MSE(pi2): 2.093e-01, MSE(pi3): 6.312e-02\n",
      "Epoch 27700, Train loss: 4.361e+09, Test loss: 5.407e+10, MSE(e): 4.346e-01, MSE(pi1): 8.958e-01, MSE(pi2): 2.062e-01, MSE(pi3): 6.372e-02\n",
      "Epoch 27800, Train loss: 4.347e+09, Test loss: 5.484e+10, MSE(e): 4.331e-01, MSE(pi1): 9.181e-01, MSE(pi2): 2.056e-01, MSE(pi3): 6.293e-02\n",
      "Epoch 27900, Train loss: 4.318e+09, Test loss: 5.545e+10, MSE(e): 4.302e-01, MSE(pi1): 8.855e-01, MSE(pi2): 2.044e-01, MSE(pi3): 6.309e-02\n",
      "Epoch 28000, Train loss: 4.296e+09, Test loss: 5.614e+10, MSE(e): 4.281e-01, MSE(pi1): 8.797e-01, MSE(pi2): 2.035e-01, MSE(pi3): 6.309e-02\n",
      "Epoch 28100, Train loss: 4.275e+09, Test loss: 5.682e+10, MSE(e): 4.260e-01, MSE(pi1): 8.752e-01, MSE(pi2): 2.025e-01, MSE(pi3): 6.251e-02\n",
      "Epoch 28200, Train loss: 4.254e+09, Test loss: 5.751e+10, MSE(e): 4.239e-01, MSE(pi1): 9.013e-01, MSE(pi2): 2.017e-01, MSE(pi3): 6.220e-02\n",
      "Epoch 28300, Train loss: 4.232e+09, Test loss: 5.824e+10, MSE(e): 4.218e-01, MSE(pi1): 8.672e-01, MSE(pi2): 2.007e-01, MSE(pi3): 6.178e-02\n",
      "Epoch 28400, Train loss: 4.213e+09, Test loss: 5.891e+10, MSE(e): 4.198e-01, MSE(pi1): 8.586e-01, MSE(pi2): 1.999e-01, MSE(pi3): 6.184e-02\n",
      "Epoch 28500, Train loss: 4.195e+09, Test loss: 5.961e+10, MSE(e): 4.179e-01, MSE(pi1): 9.173e-01, MSE(pi2): 1.991e-01, MSE(pi3): 6.122e-02\n",
      "Epoch 28600, Train loss: 4.170e+09, Test loss: 6.033e+10, MSE(e): 4.156e-01, MSE(pi1): 8.515e-01, MSE(pi2): 1.981e-01, MSE(pi3): 6.098e-02\n",
      "Epoch 28700, Train loss: 4.152e+09, Test loss: 6.103e+10, MSE(e): 4.138e-01, MSE(pi1): 8.437e-01, MSE(pi2): 1.974e-01, MSE(pi3): 6.099e-02\n",
      "Epoch 28800, Train loss: 4.131e+09, Test loss: 6.175e+10, MSE(e): 4.116e-01, MSE(pi1): 9.065e-01, MSE(pi2): 1.964e-01, MSE(pi3): 6.072e-02\n",
      "Epoch 28900, Train loss: 4.111e+09, Test loss: 6.245e+10, MSE(e): 4.096e-01, MSE(pi1): 8.391e-01, MSE(pi2): 1.956e-01, MSE(pi3): 6.007e-02\n",
      "Epoch 29000, Train loss: 4.152e+09, Test loss: 6.337e+10, MSE(e): 4.138e-01, MSE(pi1): 8.719e-01, MSE(pi2): 1.972e-01, MSE(pi3): 5.914e-02\n",
      "Epoch 29100, Train loss: 4.072e+09, Test loss: 6.385e+10, MSE(e): 4.058e-01, MSE(pi1): 8.293e-01, MSE(pi2): 1.939e-01, MSE(pi3): 5.954e-02\n",
      "Epoch 29200, Train loss: 4.053e+09, Test loss: 6.454e+10, MSE(e): 4.039e-01, MSE(pi1): 8.360e-01, MSE(pi2): 1.931e-01, MSE(pi3): 5.928e-02\n",
      "Epoch 29300, Train loss: 4.035e+09, Test loss: 6.526e+10, MSE(e): 4.021e-01, MSE(pi1): 8.168e-01, MSE(pi2): 1.924e-01, MSE(pi3): 5.933e-02\n",
      "Epoch 29400, Train loss: 4.015e+09, Test loss: 6.597e+10, MSE(e): 4.001e-01, MSE(pi1): 8.156e-01, MSE(pi2): 1.915e-01, MSE(pi3): 5.886e-02\n",
      "Epoch 29500, Train loss: 4.076e+09, Test loss: 6.662e+10, MSE(e): 4.062e-01, MSE(pi1): 8.062e-01, MSE(pi2): 1.940e-01, MSE(pi3): 6.121e-02\n",
      "Epoch 29600, Train loss: 3.978e+09, Test loss: 6.739e+10, MSE(e): 3.964e-01, MSE(pi1): 8.061e-01, MSE(pi2): 1.899e-01, MSE(pi3): 5.827e-02\n",
      "Epoch 29700, Train loss: 3.961e+09, Test loss: 6.809e+10, MSE(e): 3.947e-01, MSE(pi1): 8.045e-01, MSE(pi2): 1.893e-01, MSE(pi3): 5.846e-02\n",
      "Epoch 29800, Train loss: 3.942e+09, Test loss: 6.878e+10, MSE(e): 3.928e-01, MSE(pi1): 8.254e-01, MSE(pi2): 1.884e-01, MSE(pi3): 5.789e-02\n",
      "Epoch 29900, Train loss: 3.948e+09, Test loss: 6.951e+10, MSE(e): 3.934e-01, MSE(pi1): 8.565e-01, MSE(pi2): 1.886e-01, MSE(pi3): 5.836e-02\n",
      "Epoch 30000, Train loss: 3.907e+09, Test loss: 7.018e+10, MSE(e): 3.893e-01, MSE(pi1): 7.895e-01, MSE(pi2): 1.869e-01, MSE(pi3): 5.727e-02\n",
      "Epoch 30100, Train loss: 4.213e+09, Test loss: 7.138e+10, MSE(e): 4.198e-01, MSE(pi1): 9.160e-01, MSE(pi2): 1.993e-01, MSE(pi3): 5.774e-02\n",
      "Epoch 30200, Train loss: 3.872e+09, Test loss: 7.160e+10, MSE(e): 3.858e-01, MSE(pi1): 7.815e-01, MSE(pi2): 1.855e-01, MSE(pi3): 5.675e-02\n",
      "Epoch 30300, Train loss: 3.866e+09, Test loss: 7.226e+10, MSE(e): 3.853e-01, MSE(pi1): 7.710e-01, MSE(pi2): 1.852e-01, MSE(pi3): 5.740e-02\n",
      "Epoch 30400, Train loss: 3.838e+09, Test loss: 7.301e+10, MSE(e): 3.825e-01, MSE(pi1): 7.753e-01, MSE(pi2): 1.840e-01, MSE(pi3): 5.633e-02\n",
      "Epoch 30500, Train loss: 3.881e+09, Test loss: 7.382e+10, MSE(e): 3.868e-01, MSE(pi1): 8.009e-01, MSE(pi2): 1.857e-01, MSE(pi3): 5.536e-02\n",
      "Epoch 30600, Train loss: 3.805e+09, Test loss: 7.440e+10, MSE(e): 3.791e-01, MSE(pi1): 7.647e-01, MSE(pi2): 1.826e-01, MSE(pi3): 5.585e-02\n",
      "Epoch 30700, Train loss: 4.012e+09, Test loss: 7.509e+10, MSE(e): 3.998e-01, MSE(pi1): 7.687e-01, MSE(pi2): 1.911e-01, MSE(pi3): 6.174e-02\n",
      "Epoch 30800, Train loss: 3.772e+09, Test loss: 7.581e+10, MSE(e): 3.759e-01, MSE(pi1): 7.573e-01, MSE(pi2): 1.812e-01, MSE(pi3): 5.538e-02\n",
      "Epoch 30900, Train loss: 3.795e+09, Test loss: 7.646e+10, MSE(e): 3.781e-01, MSE(pi1): 8.157e-01, MSE(pi2): 1.822e-01, MSE(pi3): 5.781e-02\n",
      "Epoch 31000, Train loss: 3.740e+09, Test loss: 7.723e+10, MSE(e): 3.727e-01, MSE(pi1): 7.493e-01, MSE(pi2): 1.799e-01, MSE(pi3): 5.497e-02\n",
      "Epoch 31100, Train loss: 3.727e+09, Test loss: 7.792e+10, MSE(e): 3.714e-01, MSE(pi1): 7.454e-01, MSE(pi2): 1.793e-01, MSE(pi3): 5.517e-02\n",
      "Epoch 31200, Train loss: 3.708e+09, Test loss: 7.862e+10, MSE(e): 3.695e-01, MSE(pi1): 7.418e-01, MSE(pi2): 1.786e-01, MSE(pi3): 5.455e-02\n",
      "Epoch 31300, Train loss: 3.697e+09, Test loss: 7.930e+10, MSE(e): 3.684e-01, MSE(pi1): 7.446e-01, MSE(pi2): 1.781e-01, MSE(pi3): 5.408e-02\n",
      "Epoch 31400, Train loss: 3.677e+09, Test loss: 8.002e+10, MSE(e): 3.665e-01, MSE(pi1): 7.345e-01, MSE(pi2): 1.772e-01, MSE(pi3): 5.412e-02\n",
      "Epoch 31500, Train loss: 3.825e+09, Test loss: 8.086e+10, MSE(e): 3.812e-01, MSE(pi1): 8.006e-01, MSE(pi2): 1.832e-01, MSE(pi3): 5.366e-02\n",
      "Epoch 31600, Train loss: 3.647e+09, Test loss: 8.141e+10, MSE(e): 3.635e-01, MSE(pi1): 7.273e-01, MSE(pi2): 1.760e-01, MSE(pi3): 5.371e-02\n",
      "Epoch 31700, Train loss: 3.632e+09, Test loss: 8.213e+10, MSE(e): 3.620e-01, MSE(pi1): 7.255e-01, MSE(pi2): 1.753e-01, MSE(pi3): 5.347e-02\n",
      "Epoch 31800, Train loss: 3.618e+09, Test loss: 8.280e+10, MSE(e): 3.605e-01, MSE(pi1): 7.191e-01, MSE(pi2): 1.747e-01, MSE(pi3): 5.338e-02\n",
      "Epoch 31900, Train loss: 3.603e+09, Test loss: 8.351e+10, MSE(e): 3.590e-01, MSE(pi1): 7.160e-01, MSE(pi2): 1.741e-01, MSE(pi3): 5.313e-02\n",
      "Epoch 32000, Train loss: 3.589e+09, Test loss: 8.421e+10, MSE(e): 3.577e-01, MSE(pi1): 7.153e-01, MSE(pi2): 1.735e-01, MSE(pi3): 5.277e-02\n",
      "Epoch 32100, Train loss: 3.574e+09, Test loss: 8.491e+10, MSE(e): 3.562e-01, MSE(pi1): 7.167e-01, MSE(pi2): 1.729e-01, MSE(pi3): 5.264e-02\n",
      "Epoch 32200, Train loss: 3.611e+09, Test loss: 8.556e+10, MSE(e): 3.599e-01, MSE(pi1): 7.046e-01, MSE(pi2): 1.744e-01, MSE(pi3): 5.470e-02\n",
      "Epoch 32300, Train loss: 3.546e+09, Test loss: 8.626e+10, MSE(e): 3.534e-01, MSE(pi1): 7.015e-01, MSE(pi2): 1.717e-01, MSE(pi3): 5.240e-02\n",
      "Epoch 32400, Train loss: 3.533e+09, Test loss: 8.695e+10, MSE(e): 3.520e-01, MSE(pi1): 7.771e-01, MSE(pi2): 1.711e-01, MSE(pi3): 5.278e-02\n",
      "Epoch 32500, Train loss: 3.590e+09, Test loss: 8.776e+10, MSE(e): 3.578e-01, MSE(pi1): 7.334e-01, MSE(pi2): 1.734e-01, MSE(pi3): 5.135e-02\n",
      "Epoch 32600, Train loss: 3.505e+09, Test loss: 8.835e+10, MSE(e): 3.493e-01, MSE(pi1): 6.911e-01, MSE(pi2): 1.700e-01, MSE(pi3): 5.184e-02\n",
      "Epoch 32700, Train loss: 3.497e+09, Test loss: 8.905e+10, MSE(e): 3.485e-01, MSE(pi1): 6.843e-01, MSE(pi2): 1.696e-01, MSE(pi3): 5.221e-02\n",
      "Epoch 32800, Train loss: 3.477e+09, Test loss: 8.975e+10, MSE(e): 3.465e-01, MSE(pi1): 6.834e-01, MSE(pi2): 1.688e-01, MSE(pi3): 5.161e-02\n",
      "Epoch 32900, Train loss: 3.465e+09, Test loss: 9.045e+10, MSE(e): 3.453e-01, MSE(pi1): 6.835e-01, MSE(pi2): 1.683e-01, MSE(pi3): 5.117e-02\n",
      "Epoch 33000, Train loss: 3.452e+09, Test loss: 9.115e+10, MSE(e): 3.439e-01, MSE(pi1): 8.071e-01, MSE(pi2): 1.677e-01, MSE(pi3): 5.150e-02\n",
      "Epoch 33100, Train loss: 3.438e+09, Test loss: 9.185e+10, MSE(e): 3.426e-01, MSE(pi1): 6.747e-01, MSE(pi2): 1.671e-01, MSE(pi3): 5.098e-02\n",
      "Epoch 33200, Train loss: 3.425e+09, Test loss: 9.253e+10, MSE(e): 3.413e-01, MSE(pi1): 6.718e-01, MSE(pi2): 1.666e-01, MSE(pi3): 5.079e-02\n",
      "Epoch 33300, Train loss: 3.416e+09, Test loss: 9.321e+10, MSE(e): 3.404e-01, MSE(pi1): 6.651e-01, MSE(pi2): 1.662e-01, MSE(pi3): 5.100e-02\n",
      "Epoch 33400, Train loss: 3.400e+09, Test loss: 9.393e+10, MSE(e): 3.387e-01, MSE(pi1): 7.609e-01, MSE(pi2): 1.655e-01, MSE(pi3): 5.029e-02\n",
      "Epoch 33500, Train loss: 3.387e+09, Test loss: 9.462e+10, MSE(e): 3.375e-01, MSE(pi1): 6.603e-01, MSE(pi2): 1.650e-01, MSE(pi3): 5.046e-02\n",
      "Epoch 33600, Train loss: 3.400e+09, Test loss: 9.545e+10, MSE(e): 3.388e-01, MSE(pi1): 6.850e-01, MSE(pi2): 1.655e-01, MSE(pi3): 4.950e-02\n",
      "Epoch 33700, Train loss: 3.361e+09, Test loss: 9.602e+10, MSE(e): 3.349e-01, MSE(pi1): 6.587e-01, MSE(pi2): 1.639e-01, MSE(pi3): 4.996e-02\n",
      "Epoch 33800, Train loss: 3.354e+09, Test loss: 9.669e+10, MSE(e): 3.342e-01, MSE(pi1): 6.540e-01, MSE(pi2): 1.636e-01, MSE(pi3): 5.017e-02\n",
      "Epoch 33900, Train loss: 3.336e+09, Test loss: 9.740e+10, MSE(e): 3.325e-01, MSE(pi1): 6.501e-01, MSE(pi2): 1.628e-01, MSE(pi3): 4.966e-02\n",
      "Epoch 34000, Train loss: 3.325e+09, Test loss: 9.808e+10, MSE(e): 3.314e-01, MSE(pi1): 6.466e-01, MSE(pi2): 1.624e-01, MSE(pi3): 4.974e-02\n",
      "Epoch 34100, Train loss: 3.312e+09, Test loss: 9.878e+10, MSE(e): 3.301e-01, MSE(pi1): 6.446e-01, MSE(pi2): 1.618e-01, MSE(pi3): 4.936e-02\n",
      "Epoch 34200, Train loss: 3.303e+09, Test loss: 9.946e+10, MSE(e): 3.291e-01, MSE(pi1): 6.400e-01, MSE(pi2): 1.614e-01, MSE(pi3): 4.945e-02\n",
      "Epoch 34300, Train loss: 3.289e+09, Test loss: 1.001e+11, MSE(e): 3.277e-01, MSE(pi1): 6.387e-01, MSE(pi2): 1.608e-01, MSE(pi3): 4.906e-02\n",
      "Epoch 34400, Train loss: 3.294e+09, Test loss: 1.009e+11, MSE(e): 3.283e-01, MSE(pi1): 6.719e-01, MSE(pi2): 1.610e-01, MSE(pi3): 4.865e-02\n",
      "Epoch 34500, Train loss: 3.265e+09, Test loss: 1.015e+11, MSE(e): 3.254e-01, MSE(pi1): 6.327e-01, MSE(pi2): 1.598e-01, MSE(pi3): 4.880e-02\n",
      "Epoch 34600, Train loss: 3.257e+09, Test loss: 1.022e+11, MSE(e): 3.246e-01, MSE(pi1): 6.440e-01, MSE(pi2): 1.595e-01, MSE(pi3): 4.928e-02\n",
      "Epoch 34700, Train loss: 3.242e+09, Test loss: 1.029e+11, MSE(e): 3.231e-01, MSE(pi1): 6.271e-01, MSE(pi2): 1.589e-01, MSE(pi3): 4.851e-02\n",
      "Epoch 34800, Train loss: 3.231e+09, Test loss: 1.036e+11, MSE(e): 3.220e-01, MSE(pi1): 6.276e-01, MSE(pi2): 1.584e-01, MSE(pi3): 4.822e-02\n",
      "Epoch 34900, Train loss: 3.221e+09, Test loss: 1.043e+11, MSE(e): 3.210e-01, MSE(pi1): 6.200e-01, MSE(pi2): 1.580e-01, MSE(pi3): 4.841e-02\n",
      "Epoch 35000, Train loss: 3.209e+09, Test loss: 1.049e+11, MSE(e): 3.198e-01, MSE(pi1): 6.201e-01, MSE(pi2): 1.575e-01, MSE(pi3): 4.804e-02\n",
      "Epoch 35100, Train loss: 3.198e+09, Test loss: 1.056e+11, MSE(e): 3.187e-01, MSE(pi1): 6.170e-01, MSE(pi2): 1.570e-01, MSE(pi3): 4.794e-02\n",
      "Epoch 35200, Train loss: 3.191e+09, Test loss: 1.063e+11, MSE(e): 3.180e-01, MSE(pi1): 6.104e-01, MSE(pi2): 1.567e-01, MSE(pi3): 4.816e-02\n",
      "Epoch 35300, Train loss: 3.176e+09, Test loss: 1.070e+11, MSE(e): 3.166e-01, MSE(pi1): 6.138e-01, MSE(pi2): 1.561e-01, MSE(pi3): 4.766e-02\n",
      "Epoch 35400, Train loss: 3.166e+09, Test loss: 1.076e+11, MSE(e): 3.155e-01, MSE(pi1): 6.090e-01, MSE(pi2): 1.556e-01, MSE(pi3): 4.755e-02\n",
      "Epoch 35500, Train loss: 3.156e+09, Test loss: 1.083e+11, MSE(e): 3.145e-01, MSE(pi1): 6.463e-01, MSE(pi2): 1.552e-01, MSE(pi3): 4.777e-02\n",
      "Epoch 35600, Train loss: 3.164e+09, Test loss: 1.091e+11, MSE(e): 3.154e-01, MSE(pi1): 6.239e-01, MSE(pi2): 1.555e-01, MSE(pi3): 4.676e-02\n",
      "Epoch 35700, Train loss: 3.135e+09, Test loss: 1.097e+11, MSE(e): 3.124e-01, MSE(pi1): 6.012e-01, MSE(pi2): 1.543e-01, MSE(pi3): 4.718e-02\n",
      "Epoch 35800, Train loss: 3.283e+09, Test loss: 1.104e+11, MSE(e): 3.271e-01, MSE(pi1): 6.770e-01, MSE(pi2): 1.603e-01, MSE(pi3): 5.058e-02\n",
      "Epoch 35900, Train loss: 3.114e+09, Test loss: 1.110e+11, MSE(e): 3.104e-01, MSE(pi1): 5.967e-01, MSE(pi2): 1.535e-01, MSE(pi3): 4.691e-02\n",
      "Epoch 36000, Train loss: 3.626e+09, Test loss: 1.118e+11, MSE(e): 3.613e-01, MSE(pi1): 6.845e-01, MSE(pi2): 1.743e-01, MSE(pi3): 5.664e-02\n",
      "Epoch 36100, Train loss: 3.094e+09, Test loss: 1.123e+11, MSE(e): 3.084e-01, MSE(pi1): 5.912e-01, MSE(pi2): 1.526e-01, MSE(pi3): 4.671e-02\n",
      "Epoch 36200, Train loss: 3.216e+09, Test loss: 1.130e+11, MSE(e): 3.204e-01, MSE(pi1): 7.103e-01, MSE(pi2): 1.576e-01, MSE(pi3): 5.136e-02\n",
      "Epoch 36300, Train loss: 3.075e+09, Test loss: 1.137e+11, MSE(e): 3.064e-01, MSE(pi1): 5.869e-01, MSE(pi2): 1.518e-01, MSE(pi3): 4.646e-02\n",
      "Epoch 36400, Train loss: 3.117e+09, Test loss: 1.145e+11, MSE(e): 3.106e-01, MSE(pi1): 6.240e-01, MSE(pi2): 1.535e-01, MSE(pi3): 4.580e-02\n",
      "Epoch 36500, Train loss: 3.056e+09, Test loss: 1.150e+11, MSE(e): 3.045e-01, MSE(pi1): 5.824e-01, MSE(pi2): 1.510e-01, MSE(pi3): 4.624e-02\n",
      "Epoch 36600, Train loss: 3.046e+09, Test loss: 1.157e+11, MSE(e): 3.036e-01, MSE(pi1): 5.800e-01, MSE(pi2): 1.506e-01, MSE(pi3): 4.613e-02\n",
      "Epoch 36700, Train loss: 3.037e+09, Test loss: 1.163e+11, MSE(e): 3.026e-01, MSE(pi1): 5.782e-01, MSE(pi2): 1.502e-01, MSE(pi3): 4.599e-02\n",
      "Epoch 36800, Train loss: 3.027e+09, Test loss: 1.170e+11, MSE(e): 3.017e-01, MSE(pi1): 5.719e-01, MSE(pi2): 1.498e-01, MSE(pi3): 4.635e-02\n",
      "Epoch 36900, Train loss: 3.028e+09, Test loss: 1.176e+11, MSE(e): 3.018e-01, MSE(pi1): 5.731e-01, MSE(pi2): 1.498e-01, MSE(pi3): 4.639e-02\n",
      "Epoch 37000, Train loss: 3.009e+09, Test loss: 1.183e+11, MSE(e): 2.999e-01, MSE(pi1): 5.709e-01, MSE(pi2): 1.490e-01, MSE(pi3): 4.569e-02\n",
      "Epoch 37100, Train loss: 3.016e+09, Test loss: 1.189e+11, MSE(e): 3.006e-01, MSE(pi1): 5.647e-01, MSE(pi2): 1.493e-01, MSE(pi3): 4.631e-02\n",
      "Epoch 37200, Train loss: 2.991e+09, Test loss: 1.196e+11, MSE(e): 2.981e-01, MSE(pi1): 5.665e-01, MSE(pi2): 1.482e-01, MSE(pi3): 4.548e-02\n",
      "Epoch 37300, Train loss: 3.325e+09, Test loss: 1.208e+11, MSE(e): 3.313e-01, MSE(pi1): 7.000e-01, MSE(pi2): 1.617e-01, MSE(pi3): 4.704e-02\n",
      "Epoch 37400, Train loss: 2.974e+09, Test loss: 1.209e+11, MSE(e): 2.963e-01, MSE(pi1): 5.605e-01, MSE(pi2): 1.475e-01, MSE(pi3): 4.538e-02\n",
      "Epoch 37500, Train loss: 2.965e+09, Test loss: 1.215e+11, MSE(e): 2.955e-01, MSE(pi1): 5.571e-01, MSE(pi2): 1.471e-01, MSE(pi3): 4.534e-02\n",
      "Epoch 37600, Train loss: 2.956e+09, Test loss: 1.222e+11, MSE(e): 2.946e-01, MSE(pi1): 5.569e-01, MSE(pi2): 1.468e-01, MSE(pi3): 4.512e-02\n",
      "Epoch 37700, Train loss: 2.956e+09, Test loss: 1.229e+11, MSE(e): 2.946e-01, MSE(pi1): 5.762e-01, MSE(pi2): 1.467e-01, MSE(pi3): 4.479e-02\n",
      "Epoch 37800, Train loss: 2.939e+09, Test loss: 1.235e+11, MSE(e): 2.929e-01, MSE(pi1): 5.517e-01, MSE(pi2): 1.460e-01, MSE(pi3): 4.499e-02\n",
      "Epoch 37900, Train loss: 3.134e+09, Test loss: 1.245e+11, MSE(e): 3.122e-01, MSE(pi1): 7.314e-01, MSE(pi2): 1.539e-01, MSE(pi3): 4.525e-02\n",
      "Epoch 38000, Train loss: 2.922e+09, Test loss: 1.248e+11, MSE(e): 2.912e-01, MSE(pi1): 5.472e-01, MSE(pi2): 1.453e-01, MSE(pi3): 4.484e-02\n",
      "Epoch 38100, Train loss: 2.914e+09, Test loss: 1.254e+11, MSE(e): 2.904e-01, MSE(pi1): 5.455e-01, MSE(pi2): 1.450e-01, MSE(pi3): 4.472e-02\n",
      "Epoch 38200, Train loss: 2.906e+09, Test loss: 1.261e+11, MSE(e): 2.896e-01, MSE(pi1): 5.439e-01, MSE(pi2): 1.446e-01, MSE(pi3): 4.462e-02\n",
      "Epoch 38300, Train loss: 2.899e+09, Test loss: 1.267e+11, MSE(e): 2.889e-01, MSE(pi1): 5.395e-01, MSE(pi2): 1.443e-01, MSE(pi3): 4.471e-02\n",
      "Epoch 38400, Train loss: 2.890e+09, Test loss: 1.273e+11, MSE(e): 2.880e-01, MSE(pi1): 5.464e-01, MSE(pi2): 1.439e-01, MSE(pi3): 4.458e-02\n",
      "Epoch 38500, Train loss: 2.889e+09, Test loss: 1.280e+11, MSE(e): 2.879e-01, MSE(pi1): 5.334e-01, MSE(pi2): 1.439e-01, MSE(pi3): 4.481e-02\n",
      "Epoch 38600, Train loss: 2.874e+09, Test loss: 1.286e+11, MSE(e): 2.864e-01, MSE(pi1): 5.359e-01, MSE(pi2): 1.433e-01, MSE(pi3): 4.427e-02\n",
      "Epoch 38700, Train loss: 2.915e+09, Test loss: 1.292e+11, MSE(e): 2.905e-01, MSE(pi1): 5.290e-01, MSE(pi2): 1.449e-01, MSE(pi3): 4.582e-02\n",
      "Epoch 38800, Train loss: 2.859e+09, Test loss: 1.298e+11, MSE(e): 2.849e-01, MSE(pi1): 5.317e-01, MSE(pi2): 1.426e-01, MSE(pi3): 4.413e-02\n",
      "Epoch 38900, Train loss: 2.853e+09, Test loss: 1.305e+11, MSE(e): 2.843e-01, MSE(pi1): 5.432e-01, MSE(pi2): 1.424e-01, MSE(pi3): 4.437e-02\n",
      "Epoch 39000, Train loss: 2.844e+09, Test loss: 1.311e+11, MSE(e): 2.834e-01, MSE(pi1): 5.277e-01, MSE(pi2): 1.420e-01, MSE(pi3): 4.398e-02\n",
      "Epoch 39100, Train loss: 2.842e+09, Test loss: 1.318e+11, MSE(e): 2.833e-01, MSE(pi1): 5.348e-01, MSE(pi2): 1.419e-01, MSE(pi3): 4.359e-02\n",
      "Epoch 39200, Train loss: 2.829e+09, Test loss: 1.323e+11, MSE(e): 2.819e-01, MSE(pi1): 5.241e-01, MSE(pi2): 1.413e-01, MSE(pi3): 4.382e-02\n",
      "Epoch 39300, Train loss: 2.825e+09, Test loss: 1.329e+11, MSE(e): 2.816e-01, MSE(pi1): 5.234e-01, MSE(pi2): 1.412e-01, MSE(pi3): 4.394e-02\n",
      "Epoch 39400, Train loss: 2.814e+09, Test loss: 1.336e+11, MSE(e): 2.805e-01, MSE(pi1): 5.209e-01, MSE(pi2): 1.407e-01, MSE(pi3): 4.365e-02\n",
      "Epoch 39500, Train loss: 2.812e+09, Test loss: 1.342e+11, MSE(e): 2.803e-01, MSE(pi1): 5.165e-01, MSE(pi2): 1.406e-01, MSE(pi3): 4.394e-02\n",
      "Epoch 39600, Train loss: 2.800e+09, Test loss: 1.348e+11, MSE(e): 2.791e-01, MSE(pi1): 5.179e-01, MSE(pi2): 1.401e-01, MSE(pi3): 4.346e-02\n",
      "Epoch 39700, Train loss: 2.943e+09, Test loss: 1.355e+11, MSE(e): 2.933e-01, MSE(pi1): 5.220e-01, MSE(pi2): 1.459e-01, MSE(pi3): 4.699e-02\n",
      "Epoch 39800, Train loss: 2.786e+09, Test loss: 1.360e+11, MSE(e): 2.777e-01, MSE(pi1): 5.141e-01, MSE(pi2): 1.395e-01, MSE(pi3): 4.332e-02\n",
      "Epoch 39900, Train loss: 2.823e+09, Test loss: 1.367e+11, MSE(e): 2.813e-01, MSE(pi1): 5.430e-01, MSE(pi2): 1.410e-01, MSE(pi3): 4.286e-02\n",
      "Epoch 40000, Train loss: 2.772e+09, Test loss: 1.372e+11, MSE(e): 2.763e-01, MSE(pi1): 5.109e-01, MSE(pi2): 1.389e-01, MSE(pi3): 4.317e-02\n",
      "Epoch 40100, Train loss: 2.784e+09, Test loss: 1.378e+11, MSE(e): 2.774e-01, MSE(pi1): 5.033e-01, MSE(pi2): 1.394e-01, MSE(pi3): 4.394e-02\n",
      "Epoch 40200, Train loss: 2.759e+09, Test loss: 1.384e+11, MSE(e): 2.750e-01, MSE(pi1): 5.076e-01, MSE(pi2): 1.383e-01, MSE(pi3): 4.302e-02\n",
      "Epoch 40300, Train loss: 2.752e+09, Test loss: 1.390e+11, MSE(e): 2.743e-01, MSE(pi1): 5.076e-01, MSE(pi2): 1.381e-01, MSE(pi3): 4.294e-02\n",
      "Epoch 40400, Train loss: 2.746e+09, Test loss: 1.396e+11, MSE(e): 2.737e-01, MSE(pi1): 5.047e-01, MSE(pi2): 1.378e-01, MSE(pi3): 4.289e-02\n",
      "Epoch 40500, Train loss: 2.950e+09, Test loss: 1.404e+11, MSE(e): 2.940e-01, MSE(pi1): 5.834e-01, MSE(pi2): 1.460e-01, MSE(pi3): 4.342e-02\n",
      "Epoch 40600, Train loss: 2.733e+09, Test loss: 1.408e+11, MSE(e): 2.724e-01, MSE(pi1): 5.014e-01, MSE(pi2): 1.372e-01, MSE(pi3): 4.276e-02\n",
      "Epoch 40700, Train loss: 2.754e+09, Test loss: 1.414e+11, MSE(e): 2.745e-01, MSE(pi1): 4.941e-01, MSE(pi2): 1.381e-01, MSE(pi3): 4.366e-02\n",
      "Epoch 40800, Train loss: 2.721e+09, Test loss: 1.420e+11, MSE(e): 2.711e-01, MSE(pi1): 4.984e-01, MSE(pi2): 1.367e-01, MSE(pi3): 4.262e-02\n",
      "Epoch 40900, Train loss: 2.744e+09, Test loss: 1.426e+11, MSE(e): 2.735e-01, MSE(pi1): 5.038e-01, MSE(pi2): 1.377e-01, MSE(pi3): 4.359e-02\n",
      "Epoch 41000, Train loss: 2.709e+09, Test loss: 1.431e+11, MSE(e): 2.699e-01, MSE(pi1): 4.957e-01, MSE(pi2): 1.362e-01, MSE(pi3): 4.247e-02\n",
      "Epoch 41100, Train loss: 2.702e+09, Test loss: 1.437e+11, MSE(e): 2.693e-01, MSE(pi1): 4.940e-01, MSE(pi2): 1.359e-01, MSE(pi3): 4.242e-02\n",
      "Epoch 41200, Train loss: 2.697e+09, Test loss: 1.443e+11, MSE(e): 2.688e-01, MSE(pi1): 4.925e-01, MSE(pi2): 1.357e-01, MSE(pi3): 4.240e-02\n",
      "Epoch 41300, Train loss: 2.691e+09, Test loss: 1.449e+11, MSE(e): 2.681e-01, MSE(pi1): 4.911e-01, MSE(pi2): 1.354e-01, MSE(pi3): 4.229e-02\n",
      "Epoch 41400, Train loss: 2.685e+09, Test loss: 1.454e+11, MSE(e): 2.676e-01, MSE(pi1): 4.936e-01, MSE(pi2): 1.352e-01, MSE(pi3): 4.230e-02\n",
      "Epoch 41500, Train loss: 2.679e+09, Test loss: 1.460e+11, MSE(e): 2.670e-01, MSE(pi1): 4.882e-01, MSE(pi2): 1.349e-01, MSE(pi3): 4.217e-02\n",
      "Epoch 41600, Train loss: 2.678e+09, Test loss: 1.466e+11, MSE(e): 2.669e-01, MSE(pi1): 4.977e-01, MSE(pi2): 1.349e-01, MSE(pi3): 4.183e-02\n",
      "Epoch 41700, Train loss: 2.667e+09, Test loss: 1.471e+11, MSE(e): 2.658e-01, MSE(pi1): 4.855e-01, MSE(pi2): 1.344e-01, MSE(pi3): 4.205e-02\n",
      "Epoch 41800, Train loss: 2.664e+09, Test loss: 1.477e+11, MSE(e): 2.655e-01, MSE(pi1): 4.885e-01, MSE(pi2): 1.343e-01, MSE(pi3): 4.183e-02\n",
      "Epoch 41900, Train loss: 2.656e+09, Test loss: 1.483e+11, MSE(e): 2.647e-01, MSE(pi1): 4.829e-01, MSE(pi2): 1.339e-01, MSE(pi3): 4.192e-02\n",
      "Epoch 42000, Train loss: 2.655e+09, Test loss: 1.488e+11, MSE(e): 2.646e-01, MSE(pi1): 4.890e-01, MSE(pi2): 1.339e-01, MSE(pi3): 4.160e-02\n",
      "Epoch 42100, Train loss: 2.645e+09, Test loss: 1.494e+11, MSE(e): 2.636e-01, MSE(pi1): 4.814e-01, MSE(pi2): 1.335e-01, MSE(pi3): 4.177e-02\n",
      "Epoch 42200, Train loss: 2.640e+09, Test loss: 1.499e+11, MSE(e): 2.631e-01, MSE(pi1): 4.781e-01, MSE(pi2): 1.332e-01, MSE(pi3): 4.181e-02\n",
      "Epoch 42300, Train loss: 2.635e+09, Test loss: 1.505e+11, MSE(e): 2.626e-01, MSE(pi1): 4.796e-01, MSE(pi2): 1.330e-01, MSE(pi3): 4.159e-02\n",
      "Epoch 42400, Train loss: 2.629e+09, Test loss: 1.510e+11, MSE(e): 2.621e-01, MSE(pi1): 4.767e-01, MSE(pi2): 1.328e-01, MSE(pi3): 4.161e-02\n",
      "Epoch 42500, Train loss: 2.624e+09, Test loss: 1.516e+11, MSE(e): 2.615e-01, MSE(pi1): 4.822e-01, MSE(pi2): 1.325e-01, MSE(pi3): 4.164e-02\n",
      "Epoch 42600, Train loss: 2.619e+09, Test loss: 1.521e+11, MSE(e): 2.611e-01, MSE(pi1): 4.724e-01, MSE(pi2): 1.324e-01, MSE(pi3): 4.164e-02\n",
      "Epoch 42700, Train loss: 2.614e+09, Test loss: 1.526e+11, MSE(e): 2.605e-01, MSE(pi1): 4.726e-01, MSE(pi2): 1.321e-01, MSE(pi3): 4.147e-02\n",
      "Epoch 42800, Train loss: 2.611e+09, Test loss: 1.532e+11, MSE(e): 2.602e-01, MSE(pi1): 4.686e-01, MSE(pi2): 1.320e-01, MSE(pi3): 4.165e-02\n",
      "Epoch 42900, Train loss: 2.604e+09, Test loss: 1.537e+11, MSE(e): 2.595e-01, MSE(pi1): 4.709e-01, MSE(pi2): 1.317e-01, MSE(pi3): 4.132e-02\n",
      "Epoch 43000, Train loss: 2.599e+09, Test loss: 1.542e+11, MSE(e): 2.590e-01, MSE(pi1): 4.695e-01, MSE(pi2): 1.315e-01, MSE(pi3): 4.128e-02\n",
      "Epoch 43100, Train loss: 2.594e+09, Test loss: 1.548e+11, MSE(e): 2.585e-01, MSE(pi1): 4.670e-01, MSE(pi2): 1.313e-01, MSE(pi3): 4.131e-02\n",
      "Epoch 43200, Train loss: 2.590e+09, Test loss: 1.553e+11, MSE(e): 2.580e-01, MSE(pi1): 5.120e-01, MSE(pi2): 1.310e-01, MSE(pi3): 4.185e-02\n",
      "Epoch 43300, Train loss: 2.584e+09, Test loss: 1.558e+11, MSE(e): 2.576e-01, MSE(pi1): 4.648e-01, MSE(pi2): 1.308e-01, MSE(pi3): 4.119e-02\n",
      "Epoch 43400, Train loss: 2.579e+09, Test loss: 1.564e+11, MSE(e): 2.571e-01, MSE(pi1): 4.636e-01, MSE(pi2): 1.306e-01, MSE(pi3): 4.114e-02\n",
      "Epoch 43500, Train loss: 2.575e+09, Test loss: 1.569e+11, MSE(e): 2.566e-01, MSE(pi1): 4.620e-01, MSE(pi2): 1.304e-01, MSE(pi3): 4.114e-02\n",
      "Epoch 43600, Train loss: 2.570e+09, Test loss: 1.574e+11, MSE(e): 2.561e-01, MSE(pi1): 4.620e-01, MSE(pi2): 1.302e-01, MSE(pi3): 4.099e-02\n",
      "Epoch 43700, Train loss: 2.568e+09, Test loss: 1.579e+11, MSE(e): 2.560e-01, MSE(pi1): 4.598e-01, MSE(pi2): 1.301e-01, MSE(pi3): 4.114e-02\n",
      "Epoch 43800, Train loss: 2.561e+09, Test loss: 1.584e+11, MSE(e): 2.552e-01, MSE(pi1): 4.597e-01, MSE(pi2): 1.298e-01, MSE(pi3): 4.091e-02\n",
      "Epoch 43900, Train loss: 2.557e+09, Test loss: 1.589e+11, MSE(e): 2.548e-01, MSE(pi1): 5.067e-01, MSE(pi2): 1.296e-01, MSE(pi3): 4.081e-02\n",
      "Epoch 44000, Train loss: 2.552e+09, Test loss: 1.595e+11, MSE(e): 2.543e-01, MSE(pi1): 4.568e-01, MSE(pi2): 1.294e-01, MSE(pi3): 4.085e-02\n",
      "Epoch 44100, Train loss: 2.552e+09, Test loss: 1.600e+11, MSE(e): 2.543e-01, MSE(pi1): 5.414e-01, MSE(pi2): 1.294e-01, MSE(pi3): 4.166e-02\n",
      "Epoch 44200, Train loss: 2.543e+09, Test loss: 1.605e+11, MSE(e): 2.535e-01, MSE(pi1): 4.556e-01, MSE(pi2): 1.290e-01, MSE(pi3): 4.070e-02\n",
      "Epoch 44300, Train loss: 2.540e+09, Test loss: 1.610e+11, MSE(e): 2.530e-01, MSE(pi1): 5.425e-01, MSE(pi2): 1.288e-01, MSE(pi3): 4.087e-02\n",
      "Epoch 44400, Train loss: 2.535e+09, Test loss: 1.615e+11, MSE(e): 2.526e-01, MSE(pi1): 4.534e-01, MSE(pi2): 1.287e-01, MSE(pi3): 4.063e-02\n",
      "Epoch 44500, Train loss: 2.530e+09, Test loss: 1.620e+11, MSE(e): 2.522e-01, MSE(pi1): 4.525e-01, MSE(pi2): 1.285e-01, MSE(pi3): 4.057e-02\n",
      "Epoch 44600, Train loss: 2.526e+09, Test loss: 1.625e+11, MSE(e): 2.518e-01, MSE(pi1): 4.545e-01, MSE(pi2): 1.283e-01, MSE(pi3): 4.051e-02\n",
      "Epoch 44700, Train loss: 2.522e+09, Test loss: 1.630e+11, MSE(e): 2.513e-01, MSE(pi1): 4.503e-01, MSE(pi2): 1.281e-01, MSE(pi3): 4.048e-02\n",
      "Epoch 44800, Train loss: 2.520e+09, Test loss: 1.635e+11, MSE(e): 2.511e-01, MSE(pi1): 4.676e-01, MSE(pi2): 1.280e-01, MSE(pi3): 4.062e-02\n",
      "Epoch 44900, Train loss: 2.514e+09, Test loss: 1.640e+11, MSE(e): 2.505e-01, MSE(pi1): 4.483e-01, MSE(pi2): 1.277e-01, MSE(pi3): 4.041e-02\n",
      "Epoch 45000, Train loss: 2.966e+09, Test loss: 1.647e+11, MSE(e): 2.956e-01, MSE(pi1): 4.994e-01, MSE(pi2): 1.461e-01, MSE(pi3): 4.933e-02\n",
      "Epoch 45100, Train loss: 2.506e+09, Test loss: 1.650e+11, MSE(e): 2.497e-01, MSE(pi1): 4.469e-01, MSE(pi2): 1.274e-01, MSE(pi3): 4.028e-02\n",
      "Epoch 45200, Train loss: 2.536e+09, Test loss: 1.655e+11, MSE(e): 2.527e-01, MSE(pi1): 5.057e-01, MSE(pi2): 1.286e-01, MSE(pi3): 4.120e-02\n",
      "Epoch 45300, Train loss: 2.498e+09, Test loss: 1.660e+11, MSE(e): 2.489e-01, MSE(pi1): 4.446e-01, MSE(pi2): 1.270e-01, MSE(pi3): 4.022e-02\n",
      "Epoch 45400, Train loss: 2.494e+09, Test loss: 1.664e+11, MSE(e): 2.486e-01, MSE(pi1): 4.424e-01, MSE(pi2): 1.269e-01, MSE(pi3): 4.028e-02\n",
      "Epoch 45500, Train loss: 2.490e+09, Test loss: 1.669e+11, MSE(e): 2.481e-01, MSE(pi1): 4.429e-01, MSE(pi2): 1.267e-01, MSE(pi3): 4.014e-02\n",
      "Epoch 45600, Train loss: 2.514e+09, Test loss: 1.674e+11, MSE(e): 2.506e-01, MSE(pi1): 4.607e-01, MSE(pi2): 1.277e-01, MSE(pi3): 3.968e-02\n",
      "Epoch 45700, Train loss: 2.482e+09, Test loss: 1.679e+11, MSE(e): 2.474e-01, MSE(pi1): 4.411e-01, MSE(pi2): 1.264e-01, MSE(pi3): 4.007e-02\n",
      "Epoch 45800, Train loss: 2.581e+09, Test loss: 1.684e+11, MSE(e): 2.571e-01, MSE(pi1): 5.728e-01, MSE(pi2): 1.303e-01, MSE(pi3): 4.005e-02\n",
      "Epoch 45900, Train loss: 2.475e+09, Test loss: 1.688e+11, MSE(e): 2.466e-01, MSE(pi1): 4.397e-01, MSE(pi2): 1.260e-01, MSE(pi3): 3.996e-02\n",
      "Epoch 46000, Train loss: 2.471e+09, Test loss: 1.693e+11, MSE(e): 2.463e-01, MSE(pi1): 4.384e-01, MSE(pi2): 1.259e-01, MSE(pi3): 3.994e-02\n",
      "Epoch 46100, Train loss: 2.467e+09, Test loss: 1.698e+11, MSE(e): 2.459e-01, MSE(pi1): 4.412e-01, MSE(pi2): 1.257e-01, MSE(pi3): 3.993e-02\n",
      "Epoch 46200, Train loss: 2.464e+09, Test loss: 1.702e+11, MSE(e): 2.456e-01, MSE(pi1): 4.353e-01, MSE(pi2): 1.256e-01, MSE(pi3): 3.996e-02\n",
      "Epoch 46300, Train loss: 2.460e+09, Test loss: 1.707e+11, MSE(e): 2.452e-01, MSE(pi1): 4.357e-01, MSE(pi2): 1.254e-01, MSE(pi3): 3.982e-02\n",
      "Epoch 46400, Train loss: 2.458e+09, Test loss: 1.712e+11, MSE(e): 2.450e-01, MSE(pi1): 4.391e-01, MSE(pi2): 1.253e-01, MSE(pi3): 3.966e-02\n",
      "Epoch 46500, Train loss: 2.453e+09, Test loss: 1.716e+11, MSE(e): 2.445e-01, MSE(pi1): 4.341e-01, MSE(pi2): 1.251e-01, MSE(pi3): 3.975e-02\n",
      "Epoch 46600, Train loss: 2.451e+09, Test loss: 1.721e+11, MSE(e): 2.442e-01, MSE(pi1): 4.368e-01, MSE(pi2): 1.250e-01, MSE(pi3): 3.957e-02\n",
      "Epoch 46700, Train loss: 2.446e+09, Test loss: 1.726e+11, MSE(e): 2.438e-01, MSE(pi1): 4.329e-01, MSE(pi2): 1.247e-01, MSE(pi3): 3.966e-02\n",
      "Epoch 46800, Train loss: 2.443e+09, Test loss: 1.730e+11, MSE(e): 2.435e-01, MSE(pi1): 4.315e-01, MSE(pi2): 1.246e-01, MSE(pi3): 3.965e-02\n",
      "Epoch 46900, Train loss: 2.439e+09, Test loss: 1.735e+11, MSE(e): 2.431e-01, MSE(pi1): 4.310e-01, MSE(pi2): 1.244e-01, MSE(pi3): 3.960e-02\n",
      "Epoch 47000, Train loss: 2.452e+09, Test loss: 1.739e+11, MSE(e): 2.444e-01, MSE(pi1): 4.349e-01, MSE(pi2): 1.249e-01, MSE(pi3): 4.012e-02\n",
      "Epoch 47100, Train loss: 2.433e+09, Test loss: 1.744e+11, MSE(e): 2.425e-01, MSE(pi1): 4.295e-01, MSE(pi2): 1.242e-01, MSE(pi3): 3.951e-02\n",
      "Epoch 47200, Train loss: 2.455e+09, Test loss: 1.749e+11, MSE(e): 2.446e-01, MSE(pi1): 4.419e-01, MSE(pi2): 1.250e-01, MSE(pi3): 4.053e-02\n",
      "Epoch 47300, Train loss: 2.426e+09, Test loss: 1.753e+11, MSE(e): 2.418e-01, MSE(pi1): 4.276e-01, MSE(pi2): 1.239e-01, MSE(pi3): 3.946e-02\n",
      "Epoch 47400, Train loss: 2.423e+09, Test loss: 1.757e+11, MSE(e): 2.415e-01, MSE(pi1): 4.269e-01, MSE(pi2): 1.237e-01, MSE(pi3): 3.941e-02\n",
      "Epoch 47500, Train loss: 2.420e+09, Test loss: 1.762e+11, MSE(e): 2.412e-01, MSE(pi1): 4.252e-01, MSE(pi2): 1.236e-01, MSE(pi3): 3.945e-02\n",
      "Epoch 47600, Train loss: 2.417e+09, Test loss: 1.766e+11, MSE(e): 2.408e-01, MSE(pi1): 4.254e-01, MSE(pi2): 1.234e-01, MSE(pi3): 3.934e-02\n",
      "Epoch 47700, Train loss: 2.414e+09, Test loss: 1.770e+11, MSE(e): 2.406e-01, MSE(pi1): 4.244e-01, MSE(pi2): 1.233e-01, MSE(pi3): 3.942e-02\n",
      "Epoch 47800, Train loss: 2.410e+09, Test loss: 1.775e+11, MSE(e): 2.402e-01, MSE(pi1): 4.233e-01, MSE(pi2): 1.231e-01, MSE(pi3): 3.931e-02\n",
      "Epoch 47900, Train loss: 2.546e+09, Test loss: 1.781e+11, MSE(e): 2.537e-01, MSE(pi1): 4.819e-01, MSE(pi2): 1.286e-01, MSE(pi3): 3.932e-02\n",
      "Epoch 48000, Train loss: 2.404e+09, Test loss: 1.784e+11, MSE(e): 2.396e-01, MSE(pi1): 4.229e-01, MSE(pi2): 1.229e-01, MSE(pi3): 3.918e-02\n",
      "Epoch 48100, Train loss: 2.401e+09, Test loss: 1.788e+11, MSE(e): 2.393e-01, MSE(pi1): 4.246e-01, MSE(pi2): 1.227e-01, MSE(pi3): 3.912e-02\n",
      "Epoch 48200, Train loss: 2.896e+09, Test loss: 1.795e+11, MSE(e): 2.886e-01, MSE(pi1): 5.198e-01, MSE(pi2): 1.428e-01, MSE(pi3): 4.948e-02\n",
      "Epoch 48300, Train loss: 2.395e+09, Test loss: 1.797e+11, MSE(e): 2.387e-01, MSE(pi1): 4.199e-01, MSE(pi2): 1.224e-01, MSE(pi3): 3.912e-02\n",
      "Epoch 48400, Train loss: 2.403e+09, Test loss: 1.801e+11, MSE(e): 2.395e-01, MSE(pi1): 4.296e-01, MSE(pi2): 1.227e-01, MSE(pi3): 3.874e-02\n",
      "Epoch 48500, Train loss: 2.389e+09, Test loss: 1.805e+11, MSE(e): 2.381e-01, MSE(pi1): 4.188e-01, MSE(pi2): 1.222e-01, MSE(pi3): 3.903e-02\n",
      "Epoch 48600, Train loss: 2.396e+09, Test loss: 1.809e+11, MSE(e): 2.388e-01, MSE(pi1): 4.175e-01, MSE(pi2): 1.224e-01, MSE(pi3): 3.945e-02\n",
      "Epoch 48700, Train loss: 2.383e+09, Test loss: 1.814e+11, MSE(e): 2.375e-01, MSE(pi1): 4.172e-01, MSE(pi2): 1.219e-01, MSE(pi3): 3.900e-02\n",
      "Epoch 48800, Train loss: 2.381e+09, Test loss: 1.818e+11, MSE(e): 2.373e-01, MSE(pi1): 4.189e-01, MSE(pi2): 1.218e-01, MSE(pi3): 3.882e-02\n",
      "Epoch 48900, Train loss: 2.378e+09, Test loss: 1.822e+11, MSE(e): 2.370e-01, MSE(pi1): 4.181e-01, MSE(pi2): 1.217e-01, MSE(pi3): 3.879e-02\n",
      "Epoch 49000, Train loss: 2.375e+09, Test loss: 1.826e+11, MSE(e): 2.367e-01, MSE(pi1): 4.148e-01, MSE(pi2): 1.215e-01, MSE(pi3): 3.891e-02\n",
      "Epoch 49100, Train loss: 2.405e+09, Test loss: 1.831e+11, MSE(e): 2.397e-01, MSE(pi1): 4.182e-01, MSE(pi2): 1.227e-01, MSE(pi3): 3.997e-02\n",
      "Epoch 49200, Train loss: 2.369e+09, Test loss: 1.835e+11, MSE(e): 2.361e-01, MSE(pi1): 4.145e-01, MSE(pi2): 1.212e-01, MSE(pi3): 3.878e-02\n",
      "Epoch 49300, Train loss: 2.371e+09, Test loss: 1.839e+11, MSE(e): 2.363e-01, MSE(pi1): 4.166e-01, MSE(pi2): 1.213e-01, MSE(pi3): 3.917e-02\n",
      "Epoch 49400, Train loss: 2.363e+09, Test loss: 1.843e+11, MSE(e): 2.355e-01, MSE(pi1): 4.130e-01, MSE(pi2): 1.210e-01, MSE(pi3): 3.871e-02\n",
      "Epoch 49500, Train loss: 2.361e+09, Test loss: 1.847e+11, MSE(e): 2.353e-01, MSE(pi1): 4.137e-01, MSE(pi2): 1.209e-01, MSE(pi3): 3.863e-02\n",
      "Epoch 49600, Train loss: 2.358e+09, Test loss: 1.851e+11, MSE(e): 2.350e-01, MSE(pi1): 4.112e-01, MSE(pi2): 1.207e-01, MSE(pi3): 3.869e-02\n",
      "Epoch 49700, Train loss: 2.355e+09, Test loss: 1.855e+11, MSE(e): 2.347e-01, MSE(pi1): 4.108e-01, MSE(pi2): 1.206e-01, MSE(pi3): 3.864e-02\n",
      "Epoch 49800, Train loss: 2.353e+09, Test loss: 1.859e+11, MSE(e): 2.345e-01, MSE(pi1): 4.122e-01, MSE(pi2): 1.205e-01, MSE(pi3): 3.849e-02\n",
      "Epoch 49900, Train loss: 2.350e+09, Test loss: 1.863e+11, MSE(e): 2.342e-01, MSE(pi1): 4.095e-01, MSE(pi2): 1.204e-01, MSE(pi3): 3.858e-02\n",
      "\n",
      "Training process finished after 50000 epochs\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# explanatory_layers = [100]\n",
    "\n",
    "model = TransferLearningAutoencoder(input_shape, predictive_layers, pgnniv_pretrained_encoder, predictive_output, explanatory_input,\n",
    "                                   explanatory_layers, explanatory_output, n_filters_explanatory).to(DEVICE)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "# Parametros de entrenamiento\n",
    "start_epoch = 0\n",
    "n_epochs = 50000\n",
    "\n",
    "batch_size = 64\n",
    "n_checkpoints = 10\n",
    "\n",
    "train_loop(model, optimizer, X_train, y_train, f_train, X_test, y_test, f_test,\n",
    "           D, n_checkpoints, start_epoch=start_epoch, n_epochs=n_epochs, batch_size=batch_size, \n",
    "           model_results_path=MODEL_RESULTS_TRANSFERLEARNING_PATH, device=DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'plt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m plt\u001b[38;5;241m.\u001b[39mscatter(Mx(My(TensOps(model(X_train)[\u001b[38;5;241m0\u001b[39m], space_dimension\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m, contravariance\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, covariance\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)))\u001b[38;5;241m.\u001b[39mvalues\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mnumpy()\u001b[38;5;241m.\u001b[39mflatten(), \n\u001b[1;32m      2\u001b[0m             model(X_train)[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mnumpy()\u001b[38;5;241m.\u001b[39mflatten())\n\u001b[1;32m      4\u001b[0m plt\u001b[38;5;241m.\u001b[39mscatter(y_train\u001b[38;5;241m.\u001b[39mvalues\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mnumpy()\u001b[38;5;241m.\u001b[39mflatten(), \n\u001b[1;32m      5\u001b[0m            K_train\u001b[38;5;241m.\u001b[39mvalues\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mnumpy()\u001b[38;5;241m.\u001b[39mflatten())\n",
      "\u001b[0;31mNameError\u001b[0m: name 'plt' is not defined"
     ]
    }
   ],
   "source": [
    "plt.scatter(Mx(My(TensOps(model(X_train)[0], space_dimension=2, contravariance=0, covariance=0))).values.cpu().detach().numpy().flatten(), \n",
    "            model(X_train)[1].cpu().detach().numpy().flatten())\n",
    "\n",
    "plt.scatter(y_train.values.cpu().detach().numpy().flatten(), \n",
    "           K_train.values.cpu().detach().numpy().flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Parametros de entrenamiento\n",
    "# start_epoch = 9000\n",
    "# n_epochs = 100000\n",
    "\n",
    "# batch_size = 64 \n",
    "# n_checkpoints = 100\n",
    "\n",
    "# second_lr = 1e-4\n",
    "\n",
    "# train_loop(model, optimizer, X_train_NN, y_train_NN, f_train_NN, X_test_NN, y_test_NN, f_test_NN,\n",
    "#            D, n_checkpoints, start_epoch=start_epoch, n_epochs=n_epochs, batch_size=batch_size, \n",
    "#            model_results_path=MODEL_RESULTS_TRANSFERLEARNING_PATH, device=DEVICE, new_lr=second_lr)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SciML_test_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
