{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import GPUtil\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Imports de la libreria propia\n",
    "from vecopsciml.kernels.derivative import DerivativeKernels\n",
    "from vecopsciml.utils import TensOps\n",
    "\n",
    "# Imports de las funciones creadas para este programa\n",
    "from utils.folders import create_folder\n",
    "from utils.load_data import load_data\n",
    "from trainers.train import train_loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Folder already exists at: /home/rmunoz/Escritorio/rmunozTMELab/Physically-Guided-Machine-Learning/transfer_learning/results/non_linear\n",
      "Folder already exists at: /home/rmunoz/Escritorio/rmunozTMELab/Physically-Guided-Machine-Learning/transfer_learning/results/sigmoid_nonlinear/model_autoencoder_AE\n",
      "Folder already exists at: /home/rmunoz/Escritorio/rmunozTMELab/Physically-Guided-Machine-Learning/transfer_learning/results/sigmoid_nonlinear/model_autoencoder_NN\n"
     ]
    }
   ],
   "source": [
    "# Creamos los paths para las distintas carpetas\n",
    "ROOT_PATH = r'/home/rmunoz/Escritorio/rmunozTMELab/Physically-Guided-Machine-Learning'\n",
    "DATA_PATH = os.path.join(ROOT_PATH, r'data/sigmoid_nonlinear/sigmoid_nonlinear.pkl')\n",
    "RESULTS_FOLDER_PATH = os.path.join(ROOT_PATH, r'transfer_learning/results/sigmoid_nonlinear')\n",
    "MODEL_RESULTS_AE_PATH = os.path.join(ROOT_PATH, r'transfer_learning/results/sigmoid_nonlinear/model_autoencoder_AE')\n",
    "MODEL_RESULTS_PGNNIV_PATH = os.path.join(ROOT_PATH, r'transfer_learning/results/sigmoid_nonlinear/model_autoencoder_NN')\n",
    "\n",
    "\n",
    "# Creamos las carpetas que sean necesarias (si ya están creadas se avisará de ello)\n",
    "create_folder(RESULTS_FOLDER_PATH)\n",
    "create_folder(MODEL_RESULTS_AE_PATH)\n",
    "create_folder(MODEL_RESULTS_PGNNIV_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully loaded from: /home/rmunoz/Escritorio/rmunozTMELab/Physically-Guided-Machine-Learning/data/sigmoid_nonlinear/sigmoid_nonlinear.pkl\n"
     ]
    }
   ],
   "source": [
    "# Load dataset\n",
    "dataset = load_data(DATA_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convolutional filters to derivate\n",
    "dx = dataset['x_step_size']\n",
    "dy = dataset['y_step_size']\n",
    "D = DerivativeKernels(dx, dy, 0).grad_kernels_two_dimensions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "print(f\"Using device: {DEVICE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### División de los datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset length: 80\n",
      "Validation dataset length: 20\n"
     ]
    }
   ],
   "source": [
    "X_train = torch.Tensor(dataset['X_train']).unsqueeze(1)\n",
    "y_train = torch.Tensor(dataset['y_train']).unsqueeze(1)\n",
    "K_train = torch.tensor(dataset['k_train']).unsqueeze(1)\n",
    "f_train = torch.tensor(dataset['f_train']).unsqueeze(1).to(torch.float32)\n",
    "\n",
    "X_val = torch.Tensor(dataset['X_val']).unsqueeze(1)\n",
    "y_val = TensOps(torch.Tensor(dataset['y_val']).unsqueeze(1).requires_grad_(True), space_dimension=2, contravariance=0, covariance=0)\n",
    "K_val = TensOps(torch.tensor(dataset['k_val']).unsqueeze(1).requires_grad_(True), space_dimension=2, contravariance=0, covariance=0)\n",
    "f_val = TensOps(torch.tensor(dataset['f_val']).to(torch.float32).unsqueeze(1).requires_grad_(True), space_dimension=2, contravariance=0, covariance=0)\n",
    "\n",
    "print(\"Train dataset length:\", len(X_train))\n",
    "print(\"Validation dataset length:\", len(X_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset length for the autoencoder: 40\n",
      "Dataset length for the PGNNIV: 40\n"
     ]
    }
   ],
   "source": [
    "N_data_AE = len(X_train)//2\n",
    "N_data_NN = len(X_train) - len(X_train)//2\n",
    "prop_data_NN = 1 - N_data_AE/(N_data_NN + N_data_AE)\n",
    "\n",
    "print(\"Dataset length for the autoencoder:\", N_data_AE)\n",
    "print(\"Dataset length for the PGNNIV:\", N_data_NN)\n",
    "\n",
    "X_AE, X_NN, y_AE, y_NN, K_AE, K_NN, f_AE, f_NN = train_test_split(X_train, y_train, K_train, f_train, test_size=prop_data_NN, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Datos para el autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_AE, y_test_AE = train_test_split(y_AE, test_size=0.2, random_state=42)\n",
    "\n",
    "y_train_AE = TensOps(y_train_AE.requires_grad_(True).to(DEVICE), space_dimension=2, contravariance=0, covariance=0)\n",
    "y_test_AE = TensOps(y_test_AE.requires_grad_(True).to(DEVICE), space_dimension=2, contravariance=0, covariance=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Datos para la PGNNIV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_NN, X_test_NN, y_train_NN, y_test_NN, K_train_NN, K_test_NN, f_train_NN, f_test_NN = train_test_split(X_NN, y_NN, K_NN, f_NN, test_size=0.2, random_state=42)\n",
    "\n",
    "X_train_NN = X_train_NN.to(DEVICE)\n",
    "X_test_NN = X_test_NN.to(DEVICE)\n",
    "\n",
    "y_train_NN = TensOps(y_train_NN.requires_grad_(True).to(DEVICE), space_dimension=2, contravariance=0, covariance=0)\n",
    "y_test_NN = TensOps(y_test_NN.requires_grad_(True).to(DEVICE), space_dimension=2, contravariance=0, covariance=0)\n",
    "\n",
    "K_train_NN = TensOps(K_train_NN.to(DEVICE), space_dimension=2, contravariance=0, covariance=0)\n",
    "K_test_NN = TensOps(K_test_NN.to(DEVICE), space_dimension=2, contravariance=0, covariance=0)\n",
    "\n",
    "f_train_NN = TensOps(f_train_NN.to(DEVICE), space_dimension=2, contravariance=0, covariance=0)\n",
    "f_test_NN = TensOps(f_test_NN.to(DEVICE), space_dimension=2, contravariance=0, covariance=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model import Autoencoder\n",
    "from trainers.train import train_autoencoder_loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_modes = 10\n",
    "\n",
    "autoencoder_input_shape = y_train_AE.values[0].shape\n",
    "latent_space_dim = [20, 10, n_modes, 10, 20]\n",
    "autoencoder_output_shape = y_train_AE.values[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = y_train_AE.values\n",
    "y_train = y_train_AE\n",
    "\n",
    "X_test = y_test_AE.values\n",
    "y_test = y_test_AE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training from scratch.\n",
      "Epoch 0, Train loss: 6.113e+01, Test loss: 1.006e+02\n",
      "Epoch 100, Train loss: 8.462e+00, Test loss: 1.261e+01\n",
      "Epoch 200, Train loss: 6.199e+00, Test loss: 1.087e+01\n",
      "Epoch 300, Train loss: 5.323e+00, Test loss: 1.106e+01\n",
      "Epoch 400, Train loss: 3.396e+00, Test loss: 1.083e+01\n",
      "Epoch 500, Train loss: 2.946e+00, Test loss: 1.000e+01\n",
      "Epoch 600, Train loss: 2.425e+00, Test loss: 7.302e+00\n",
      "Epoch 700, Train loss: 1.892e+00, Test loss: 4.614e+00\n",
      "Epoch 800, Train loss: 1.421e+00, Test loss: 3.764e+00\n",
      "Epoch 900, Train loss: 1.298e+00, Test loss: 3.564e+00\n",
      "Epoch 1000, Train loss: 1.205e+00, Test loss: 3.353e+00\n",
      "Epoch 1100, Train loss: 1.129e+00, Test loss: 3.139e+00\n",
      "Epoch 1200, Train loss: 1.058e+00, Test loss: 3.005e+00\n",
      "Epoch 1300, Train loss: 1.002e+00, Test loss: 2.885e+00\n",
      "Epoch 1400, Train loss: 9.556e-01, Test loss: 2.767e+00\n",
      "Epoch 1500, Train loss: 9.161e-01, Test loss: 2.651e+00\n",
      "Epoch 1600, Train loss: 8.852e-01, Test loss: 2.536e+00\n",
      "Epoch 1700, Train loss: 8.539e-01, Test loss: 2.460e+00\n",
      "Epoch 1800, Train loss: 8.296e-01, Test loss: 2.387e+00\n",
      "Epoch 1900, Train loss: 8.091e-01, Test loss: 2.327e+00\n",
      "Epoch 2000, Train loss: 7.915e-01, Test loss: 2.199e+00\n",
      "Epoch 2100, Train loss: 7.766e-01, Test loss: 2.191e+00\n",
      "Epoch 2200, Train loss: 7.644e-01, Test loss: 2.141e+00\n",
      "Epoch 2300, Train loss: 7.539e-01, Test loss: 2.089e+00\n",
      "Epoch 2400, Train loss: 7.452e-01, Test loss: 2.046e+00\n",
      "Epoch 2500, Train loss: 7.378e-01, Test loss: 2.007e+00\n",
      "Epoch 2600, Train loss: 7.319e-01, Test loss: 1.951e+00\n",
      "Epoch 2700, Train loss: 7.265e-01, Test loss: 1.941e+00\n",
      "Epoch 2800, Train loss: 7.251e-01, Test loss: 1.908e+00\n",
      "Epoch 2900, Train loss: 7.185e-01, Test loss: 1.888e+00\n",
      "Epoch 3000, Train loss: 7.153e-01, Test loss: 1.867e+00\n",
      "Epoch 3100, Train loss: 7.126e-01, Test loss: 1.839e+00\n",
      "Epoch 3200, Train loss: 7.100e-01, Test loss: 1.820e+00\n",
      "Epoch 3300, Train loss: 7.077e-01, Test loss: 1.805e+00\n",
      "Epoch 3400, Train loss: 7.053e-01, Test loss: 1.775e+00\n",
      "Epoch 3500, Train loss: 7.034e-01, Test loss: 1.719e+00\n",
      "Epoch 3600, Train loss: 7.017e-01, Test loss: 1.742e+00\n",
      "Epoch 3700, Train loss: 7.003e-01, Test loss: 1.729e+00\n",
      "Epoch 3800, Train loss: 6.993e-01, Test loss: 1.705e+00\n",
      "Epoch 3900, Train loss: 6.979e-01, Test loss: 1.704e+00\n",
      "Epoch 4000, Train loss: 6.978e-01, Test loss: 1.646e+00\n",
      "Epoch 4100, Train loss: 6.959e-01, Test loss: 1.682e+00\n",
      "Epoch 4200, Train loss: 6.950e-01, Test loss: 1.673e+00\n",
      "Epoch 4300, Train loss: 6.947e-01, Test loss: 1.693e+00\n",
      "Epoch 4400, Train loss: 6.934e-01, Test loss: 1.654e+00\n",
      "Epoch 4500, Train loss: 6.926e-01, Test loss: 1.646e+00\n",
      "Epoch 4600, Train loss: 6.921e-01, Test loss: 1.638e+00\n",
      "Epoch 4700, Train loss: 6.912e-01, Test loss: 1.629e+00\n",
      "Epoch 4800, Train loss: 6.906e-01, Test loss: 1.621e+00\n",
      "Epoch 4900, Train loss: 6.900e-01, Test loss: 1.616e+00\n",
      "Epoch 5000, Train loss: 6.894e-01, Test loss: 1.606e+00\n",
      "Epoch 5100, Train loss: 6.888e-01, Test loss: 1.597e+00\n",
      "Epoch 5200, Train loss: 6.882e-01, Test loss: 1.588e+00\n",
      "Epoch 5300, Train loss: 6.877e-01, Test loss: 1.583e+00\n",
      "Epoch 5400, Train loss: 6.955e-01, Test loss: 1.665e+00\n",
      "Epoch 5500, Train loss: 6.865e-01, Test loss: 1.567e+00\n",
      "Epoch 5600, Train loss: 6.859e-01, Test loss: 1.560e+00\n",
      "Epoch 5700, Train loss: 6.880e-01, Test loss: 1.597e+00\n",
      "Epoch 5800, Train loss: 6.847e-01, Test loss: 1.544e+00\n",
      "Epoch 5900, Train loss: 6.841e-01, Test loss: 1.537e+00\n",
      "Epoch 6000, Train loss: 6.841e-01, Test loss: 1.523e+00\n",
      "Epoch 6100, Train loss: 6.828e-01, Test loss: 1.521e+00\n",
      "Epoch 6200, Train loss: 6.820e-01, Test loss: 1.513e+00\n",
      "Epoch 6300, Train loss: 6.820e-01, Test loss: 1.495e+00\n",
      "Epoch 6400, Train loss: 6.803e-01, Test loss: 1.496e+00\n",
      "Epoch 6500, Train loss: 6.793e-01, Test loss: 1.489e+00\n",
      "Epoch 6600, Train loss: 6.954e-01, Test loss: 1.599e+00\n",
      "Epoch 6700, Train loss: 6.767e-01, Test loss: 1.472e+00\n",
      "Epoch 6800, Train loss: 6.748e-01, Test loss: 1.464e+00\n",
      "Epoch 6900, Train loss: 6.729e-01, Test loss: 1.442e+00\n",
      "Epoch 7000, Train loss: 6.684e-01, Test loss: 1.445e+00\n",
      "Epoch 7100, Train loss: 6.621e-01, Test loss: 1.433e+00\n",
      "Epoch 7200, Train loss: 6.495e-01, Test loss: 1.393e+00\n",
      "Epoch 7300, Train loss: 6.220e-01, Test loss: 1.365e+00\n",
      "Epoch 7400, Train loss: 5.470e-01, Test loss: 1.227e+00\n",
      "Epoch 7500, Train loss: 4.715e-01, Test loss: 9.961e-01\n",
      "Epoch 7600, Train loss: 4.520e-01, Test loss: 9.299e-01\n",
      "Epoch 7700, Train loss: 4.388e-01, Test loss: 9.062e-01\n",
      "Epoch 7800, Train loss: 4.240e-01, Test loss: 9.276e-01\n",
      "Epoch 7900, Train loss: 4.036e-01, Test loss: 9.538e-01\n",
      "Epoch 8000, Train loss: 3.742e-01, Test loss: 9.778e-01\n",
      "Epoch 8100, Train loss: 3.371e-01, Test loss: 1.028e+00\n",
      "Epoch 8200, Train loss: 3.010e-01, Test loss: 1.064e+00\n",
      "Epoch 8300, Train loss: 2.716e-01, Test loss: 1.069e+00\n",
      "Epoch 8400, Train loss: 2.547e-01, Test loss: 1.063e+00\n",
      "Epoch 8500, Train loss: 2.451e-01, Test loss: 1.039e+00\n",
      "Epoch 8600, Train loss: 2.399e-01, Test loss: 1.013e+00\n",
      "Epoch 8700, Train loss: 2.367e-01, Test loss: 9.888e-01\n",
      "Epoch 8800, Train loss: 2.417e-01, Test loss: 9.889e-01\n",
      "Epoch 8900, Train loss: 2.338e-01, Test loss: 9.522e-01\n",
      "Epoch 9000, Train loss: 2.330e-01, Test loss: 9.383e-01\n",
      "Epoch 9100, Train loss: 2.325e-01, Test loss: 9.290e-01\n",
      "Epoch 9200, Train loss: 2.319e-01, Test loss: 9.193e-01\n",
      "Epoch 9300, Train loss: 2.317e-01, Test loss: 9.137e-01\n",
      "Epoch 9400, Train loss: 2.311e-01, Test loss: 9.064e-01\n",
      "Epoch 9500, Train loss: 2.307e-01, Test loss: 9.010e-01\n",
      "Epoch 9600, Train loss: 2.383e-01, Test loss: 9.081e-01\n",
      "Epoch 9700, Train loss: 2.299e-01, Test loss: 8.934e-01\n",
      "Epoch 9800, Train loss: 2.296e-01, Test loss: 8.903e-01\n",
      "Epoch 9900, Train loss: 2.299e-01, Test loss: 8.872e-01\n"
     ]
    }
   ],
   "source": [
    "autoencoder = Autoencoder(autoencoder_input_shape, latent_space_dim, autoencoder_output_shape).to(DEVICE)\n",
    "optimizer = torch.optim.Adam(autoencoder.parameters(), lr=1e-3)\n",
    "\n",
    "start_epoch = 0\n",
    "n_epochs = 10000\n",
    "batch_size = 64\n",
    "n_checkpoint = 3\n",
    "new_lr = None\n",
    "\n",
    "train_autoencoder_loop(autoencoder, optimizer, X_train, y_train, X_test, y_test,  \n",
    "                       n_checkpoint, start_epoch, n_epochs, batch_size, MODEL_RESULTS_AE_PATH, DEVICE, new_lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training from a checkpoint. Epoch 9999.\n",
      "Epoch 10000, Train loss: 2.289e-01, Test loss: 8.856e-01\n",
      "Epoch 10100, Train loss: 2.288e-01, Test loss: 8.854e-01\n",
      "Epoch 10200, Train loss: 2.288e-01, Test loss: 8.851e-01\n",
      "Epoch 10300, Train loss: 2.287e-01, Test loss: 8.849e-01\n",
      "Epoch 10400, Train loss: 2.287e-01, Test loss: 8.846e-01\n",
      "Epoch 10500, Train loss: 2.286e-01, Test loss: 8.844e-01\n",
      "Epoch 10600, Train loss: 2.286e-01, Test loss: 8.841e-01\n",
      "Epoch 10700, Train loss: 2.285e-01, Test loss: 8.838e-01\n",
      "Epoch 10800, Train loss: 2.285e-01, Test loss: 8.836e-01\n",
      "Epoch 10900, Train loss: 2.284e-01, Test loss: 8.833e-01\n",
      "Epoch 11000, Train loss: 2.283e-01, Test loss: 8.829e-01\n",
      "Epoch 11100, Train loss: 2.283e-01, Test loss: 8.826e-01\n",
      "Epoch 11200, Train loss: 2.282e-01, Test loss: 8.823e-01\n",
      "Epoch 11300, Train loss: 2.281e-01, Test loss: 8.820e-01\n",
      "Epoch 11400, Train loss: 2.280e-01, Test loss: 8.816e-01\n",
      "Epoch 11500, Train loss: 2.279e-01, Test loss: 8.812e-01\n",
      "Epoch 11600, Train loss: 2.278e-01, Test loss: 8.809e-01\n",
      "Epoch 11700, Train loss: 2.277e-01, Test loss: 8.805e-01\n",
      "Epoch 11800, Train loss: 2.276e-01, Test loss: 8.800e-01\n",
      "Epoch 11900, Train loss: 2.275e-01, Test loss: 8.796e-01\n",
      "Epoch 12000, Train loss: 2.273e-01, Test loss: 8.791e-01\n",
      "Epoch 12100, Train loss: 2.272e-01, Test loss: 8.787e-01\n",
      "Epoch 12200, Train loss: 2.270e-01, Test loss: 8.782e-01\n",
      "Epoch 12300, Train loss: 2.269e-01, Test loss: 8.776e-01\n",
      "Epoch 12400, Train loss: 2.267e-01, Test loss: 8.771e-01\n",
      "Epoch 12500, Train loss: 2.265e-01, Test loss: 8.765e-01\n",
      "Epoch 12600, Train loss: 2.263e-01, Test loss: 8.758e-01\n",
      "Epoch 12700, Train loss: 2.260e-01, Test loss: 8.751e-01\n",
      "Epoch 12800, Train loss: 2.257e-01, Test loss: 8.743e-01\n",
      "Epoch 12900, Train loss: 2.254e-01, Test loss: 8.735e-01\n",
      "Epoch 13000, Train loss: 2.251e-01, Test loss: 8.726e-01\n",
      "Epoch 13100, Train loss: 2.247e-01, Test loss: 8.716e-01\n",
      "Epoch 13200, Train loss: 2.242e-01, Test loss: 8.704e-01\n",
      "Epoch 13300, Train loss: 2.237e-01, Test loss: 8.691e-01\n",
      "Epoch 13400, Train loss: 2.231e-01, Test loss: 8.676e-01\n",
      "Epoch 13500, Train loss: 2.224e-01, Test loss: 8.657e-01\n",
      "Epoch 13600, Train loss: 2.216e-01, Test loss: 8.636e-01\n",
      "Epoch 13700, Train loss: 2.205e-01, Test loss: 8.609e-01\n",
      "Epoch 13800, Train loss: 2.193e-01, Test loss: 8.575e-01\n",
      "Epoch 13900, Train loss: 2.177e-01, Test loss: 8.532e-01\n",
      "Epoch 14000, Train loss: 2.157e-01, Test loss: 8.474e-01\n",
      "Epoch 14100, Train loss: 2.131e-01, Test loss: 8.393e-01\n",
      "Epoch 14200, Train loss: 2.095e-01, Test loss: 8.276e-01\n",
      "Epoch 14300, Train loss: 2.043e-01, Test loss: 8.095e-01\n",
      "Epoch 14400, Train loss: 1.958e-01, Test loss: 7.781e-01\n",
      "Epoch 14500, Train loss: 1.788e-01, Test loss: 7.323e-01\n",
      "Epoch 14600, Train loss: 1.546e-01, Test loss: 6.595e-01\n",
      "Epoch 14700, Train loss: 1.266e-01, Test loss: 5.655e-01\n",
      "Epoch 14800, Train loss: 9.607e-02, Test loss: 4.548e-01\n",
      "Epoch 14900, Train loss: 6.793e-02, Test loss: 3.568e-01\n",
      "Epoch 15000, Train loss: 4.575e-02, Test loss: 2.743e-01\n",
      "Epoch 15100, Train loss: 3.196e-02, Test loss: 2.203e-01\n",
      "Epoch 15200, Train loss: 2.438e-02, Test loss: 1.876e-01\n",
      "Epoch 15300, Train loss: 2.033e-02, Test loss: 1.712e-01\n",
      "Epoch 15400, Train loss: 1.808e-02, Test loss: 1.564e-01\n",
      "Epoch 15500, Train loss: 1.615e-02, Test loss: 1.560e-01\n",
      "Epoch 15600, Train loss: 1.477e-02, Test loss: 1.523e-01\n",
      "Epoch 15700, Train loss: 1.364e-02, Test loss: 1.492e-01\n",
      "Epoch 15800, Train loss: 1.266e-02, Test loss: 1.469e-01\n",
      "Epoch 15900, Train loss: 1.181e-02, Test loss: 1.411e-01\n",
      "Epoch 16000, Train loss: 1.103e-02, Test loss: 1.427e-01\n",
      "Epoch 16100, Train loss: 1.033e-02, Test loss: 1.408e-01\n",
      "Epoch 16200, Train loss: 9.692e-03, Test loss: 1.385e-01\n",
      "Epoch 16300, Train loss: 9.113e-03, Test loss: 1.371e-01\n",
      "Epoch 16400, Train loss: 9.043e-03, Test loss: 1.505e-01\n",
      "Epoch 16500, Train loss: 8.097e-03, Test loss: 1.339e-01\n",
      "Epoch 16600, Train loss: 7.652e-03, Test loss: 1.323e-01\n",
      "Epoch 16700, Train loss: 7.224e-03, Test loss: 1.307e-01\n",
      "Epoch 16800, Train loss: 6.835e-03, Test loss: 1.295e-01\n",
      "Epoch 16900, Train loss: 6.472e-03, Test loss: 1.278e-01\n",
      "Epoch 17000, Train loss: 6.142e-03, Test loss: 1.251e-01\n",
      "Epoch 17100, Train loss: 5.827e-03, Test loss: 1.249e-01\n",
      "Epoch 17200, Train loss: 5.944e-03, Test loss: 1.139e-01\n",
      "Epoch 17300, Train loss: 5.272e-03, Test loss: 1.220e-01\n",
      "Epoch 17400, Train loss: 5.022e-03, Test loss: 1.207e-01\n",
      "Epoch 17500, Train loss: 4.794e-03, Test loss: 1.195e-01\n",
      "Epoch 17600, Train loss: 4.580e-03, Test loss: 1.181e-01\n",
      "Epoch 17700, Train loss: 4.384e-03, Test loss: 1.174e-01\n",
      "Epoch 17800, Train loss: 4.199e-03, Test loss: 1.159e-01\n",
      "Epoch 17900, Train loss: 4.031e-03, Test loss: 1.143e-01\n",
      "Epoch 18000, Train loss: 3.872e-03, Test loss: 1.138e-01\n",
      "Epoch 18100, Train loss: 3.748e-03, Test loss: 1.092e-01\n",
      "Epoch 18200, Train loss: 3.595e-03, Test loss: 1.120e-01\n",
      "Epoch 18300, Train loss: 3.470e-03, Test loss: 1.111e-01\n",
      "Epoch 18400, Train loss: 3.552e-03, Test loss: 1.180e-01\n",
      "Epoch 18500, Train loss: 3.240e-03, Test loss: 1.096e-01\n",
      "Epoch 18600, Train loss: 3.135e-03, Test loss: 1.091e-01\n",
      "Epoch 18700, Train loss: 3.038e-03, Test loss: 1.081e-01\n",
      "Epoch 18800, Train loss: 2.946e-03, Test loss: 1.076e-01\n",
      "Epoch 18900, Train loss: 2.860e-03, Test loss: 1.064e-01\n",
      "Epoch 19000, Train loss: 2.781e-03, Test loss: 1.064e-01\n",
      "Epoch 19100, Train loss: 2.703e-03, Test loss: 1.059e-01\n",
      "Epoch 19200, Train loss: 2.631e-03, Test loss: 1.055e-01\n",
      "Epoch 19300, Train loss: 2.562e-03, Test loss: 1.051e-01\n",
      "Epoch 19400, Train loss: 2.517e-03, Test loss: 1.071e-01\n",
      "Epoch 19500, Train loss: 2.437e-03, Test loss: 1.045e-01\n",
      "Epoch 19600, Train loss: 2.379e-03, Test loss: 1.036e-01\n",
      "Epoch 19700, Train loss: 2.324e-03, Test loss: 1.040e-01\n",
      "Epoch 19800, Train loss: 2.272e-03, Test loss: 1.039e-01\n",
      "Epoch 19900, Train loss: 2.223e-03, Test loss: 1.039e-01\n"
     ]
    }
   ],
   "source": [
    "start_epoch = 9999\n",
    "n_epochs = 20000\n",
    "batch_size = 64\n",
    "n_checkpoint = 3\n",
    "new_lr = 1e-4\n",
    "\n",
    "train_autoencoder_loop(autoencoder, optimizer, X_train, y_train, X_test, y_test,  \n",
    "                       n_checkpoint, start_epoch, n_epochs, batch_size, MODEL_RESULTS_AE_PATH, DEVICE, new_lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PGNNIV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vecopsciml.operators.zero_order import Mx, My\n",
    "from model.ae_nonlinear_model import AutoencoderNonlinearModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predictive network architecture\n",
    "input_shape = X_train_NN[0].shape\n",
    "predictive_layers = [20, 10, n_modes]\n",
    "predictive_output = y_train_NN.values[0].shape\n",
    "\n",
    "# Explanatory network architecture\n",
    "explanatory_input = Mx(My(y_train_NN)).values[0].shape\n",
    "explanatory_layers = [10, 10]\n",
    "explanatory_output = Mx(My(f_train_NN)).values[0].shape\n",
    "\n",
    "# Other parameters\n",
    "n_filters_explanatory = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hidden1_layer.weight: requires_grad=False\n",
      "hidden1_layer.bias: requires_grad=False\n",
      "hidden2_layer.weight: requires_grad=False\n",
      "hidden2_layer.bias: requires_grad=False\n",
      "output_layer.weight: requires_grad=False\n",
      "output_layer.bias: requires_grad=False\n"
     ]
    }
   ],
   "source": [
    "pretrained_decoder = autoencoder.decoder\n",
    "\n",
    "for param in pretrained_decoder.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "for name, param in pretrained_decoder.named_parameters():\n",
    "    print(f\"{name}: requires_grad={param.requires_grad}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training from scratch.\n",
      "Epoch 0, Train loss: 3.140e+11, Test loss: 4.405e+11, MSE(e): 3.138e+01, MSE(pi1): 7.508e+00, MSE(pi2): 1.347e+01, MSE(pi3): 1.068e+00\n",
      "Epoch 100, Train loss: 6.414e+10, Test loss: 5.539e+10, MSE(e): 6.400e+00, MSE(pi1): 7.944e+00, MSE(pi2): 2.822e+00, MSE(pi3): 6.284e-01\n",
      "Epoch 200, Train loss: 5.739e+10, Test loss: 5.002e+10, MSE(e): 5.726e+00, MSE(pi1): 7.661e+00, MSE(pi2): 2.534e+00, MSE(pi3): 5.946e-01\n",
      "Epoch 300, Train loss: 4.742e+10, Test loss: 4.276e+10, MSE(e): 4.730e+00, MSE(pi1): 6.797e+00, MSE(pi2): 2.113e+00, MSE(pi3): 5.170e-01\n",
      "Epoch 400, Train loss: 3.147e+10, Test loss: 3.079e+10, MSE(e): 3.139e+00, MSE(pi1): 4.246e+00, MSE(pi2): 1.471e+00, MSE(pi3): 3.270e-01\n",
      "Epoch 500, Train loss: 1.907e+10, Test loss: 2.341e+10, MSE(e): 1.903e+00, MSE(pi1): 2.588e+00, MSE(pi2): 9.877e-01, MSE(pi3): 2.091e-01\n",
      "Epoch 600, Train loss: 1.329e+10, Test loss: 1.930e+10, MSE(e): 1.325e+00, MSE(pi1): 2.397e+00, MSE(pi2): 7.247e-01, MSE(pi3): 1.707e-01\n",
      "Epoch 700, Train loss: 9.730e+09, Test loss: 1.508e+10, MSE(e): 9.692e-01, MSE(pi1): 2.378e+00, MSE(pi2): 5.331e-01, MSE(pi3): 1.357e-01\n",
      "Epoch 800, Train loss: 7.119e+09, Test loss: 1.292e+10, MSE(e): 7.084e-01, MSE(pi1): 2.367e+00, MSE(pi2): 3.916e-01, MSE(pi3): 1.119e-01\n",
      "Epoch 900, Train loss: 5.571e+09, Test loss: 1.179e+10, MSE(e): 5.537e-01, MSE(pi1): 2.342e+00, MSE(pi2): 3.082e-01, MSE(pi3): 1.012e-01\n",
      "Epoch 1000, Train loss: 4.642e+09, Test loss: 1.081e+10, MSE(e): 4.610e-01, MSE(pi1): 2.283e+00, MSE(pi2): 2.598e-01, MSE(pi3): 9.699e-02\n",
      "Epoch 1100, Train loss: 3.957e+09, Test loss: 9.393e+09, MSE(e): 3.926e-01, MSE(pi1): 2.190e+00, MSE(pi2): 2.240e-01, MSE(pi3): 9.259e-02\n",
      "Epoch 1200, Train loss: 3.356e+09, Test loss: 7.801e+09, MSE(e): 3.327e-01, MSE(pi1): 2.056e+00, MSE(pi2): 1.914e-01, MSE(pi3): 8.468e-02\n",
      "Epoch 1300, Train loss: 2.775e+09, Test loss: 6.345e+09, MSE(e): 2.749e-01, MSE(pi1): 1.868e+00, MSE(pi2): 1.582e-01, MSE(pi3): 7.217e-02\n",
      "Epoch 1400, Train loss: 2.198e+09, Test loss: 4.924e+09, MSE(e): 2.176e-01, MSE(pi1): 1.631e+00, MSE(pi2): 1.237e-01, MSE(pi3): 5.611e-02\n",
      "Epoch 1500, Train loss: 1.675e+09, Test loss: 3.629e+09, MSE(e): 1.656e-01, MSE(pi1): 1.405e+00, MSE(pi2): 9.158e-02, MSE(pi3): 4.151e-02\n",
      "Epoch 1600, Train loss: 1.258e+09, Test loss: 2.624e+09, MSE(e): 1.242e-01, MSE(pi1): 1.223e+00, MSE(pi2): 6.634e-02, MSE(pi3): 3.064e-02\n",
      "Epoch 1700, Train loss: 9.541e+08, Test loss: 1.846e+09, MSE(e): 9.406e-02, MSE(pi1): 1.104e+00, MSE(pi2): 4.884e-02, MSE(pi3): 2.425e-02\n",
      "Epoch 1800, Train loss: 7.404e+08, Test loss: 1.289e+09, MSE(e): 7.280e-02, MSE(pi1): 1.033e+00, MSE(pi2): 3.715e-02, MSE(pi3): 2.074e-02\n",
      "Epoch 1900, Train loss: 5.875e+08, Test loss: 9.384e+08, MSE(e): 5.758e-02, MSE(pi1): 9.876e-01, MSE(pi2): 2.912e-02, MSE(pi3): 1.856e-02\n",
      "Epoch 2000, Train loss: 4.750e+08, Test loss: 7.463e+08, MSE(e): 4.638e-02, MSE(pi1): 9.548e-01, MSE(pi2): 2.336e-02, MSE(pi3): 1.706e-02\n",
      "Epoch 2100, Train loss: 3.909e+08, Test loss: 6.466e+08, MSE(e): 3.800e-02, MSE(pi1): 9.292e-01, MSE(pi2): 1.912e-02, MSE(pi3): 1.599e-02\n",
      "Epoch 2200, Train loss: 3.269e+08, Test loss: 5.943e+08, MSE(e): 3.163e-02, MSE(pi1): 9.078e-01, MSE(pi2): 1.594e-02, MSE(pi3): 1.521e-02\n",
      "Epoch 2300, Train loss: 2.773e+08, Test loss: 5.675e+08, MSE(e): 2.670e-02, MSE(pi1): 8.889e-01, MSE(pi2): 1.350e-02, MSE(pi3): 1.461e-02\n",
      "Epoch 2400, Train loss: 2.382e+08, Test loss: 5.550e+08, MSE(e): 2.280e-02, MSE(pi1): 8.714e-01, MSE(pi2): 1.159e-02, MSE(pi3): 1.411e-02\n",
      "Epoch 2500, Train loss: 2.066e+08, Test loss: 5.508e+08, MSE(e): 1.967e-02, MSE(pi1): 8.549e-01, MSE(pi2): 1.005e-02, MSE(pi3): 1.368e-02\n",
      "Epoch 2600, Train loss: 1.808e+08, Test loss: 5.513e+08, MSE(e): 1.711e-02, MSE(pi1): 8.391e-01, MSE(pi2): 8.783e-03, MSE(pi3): 1.328e-02\n",
      "Epoch 2700, Train loss: 1.594e+08, Test loss: 5.545e+08, MSE(e): 1.498e-02, MSE(pi1): 8.241e-01, MSE(pi2): 7.730e-03, MSE(pi3): 1.291e-02\n",
      "Epoch 2800, Train loss: 1.414e+08, Test loss: 5.593e+08, MSE(e): 1.321e-02, MSE(pi1): 8.096e-01, MSE(pi2): 6.843e-03, MSE(pi3): 1.256e-02\n",
      "Epoch 2900, Train loss: 1.262e+08, Test loss: 5.650e+08, MSE(e): 1.171e-02, MSE(pi1): 7.956e-01, MSE(pi2): 6.089e-03, MSE(pi3): 1.224e-02\n",
      "Epoch 3000, Train loss: 1.133e+08, Test loss: 5.712e+08, MSE(e): 1.043e-02, MSE(pi1): 7.821e-01, MSE(pi2): 5.444e-03, MSE(pi3): 1.193e-02\n",
      "Epoch 3100, Train loss: 1.022e+08, Test loss: 5.777e+08, MSE(e): 9.332e-03, MSE(pi1): 7.689e-01, MSE(pi2): 4.889e-03, MSE(pi3): 1.163e-02\n",
      "Epoch 3200, Train loss: 9.258e+07, Test loss: 5.841e+08, MSE(e): 8.388e-03, MSE(pi1): 7.559e-01, MSE(pi2): 4.408e-03, MSE(pi3): 1.134e-02\n",
      "Epoch 3300, Train loss: 8.423e+07, Test loss: 5.901e+08, MSE(e): 7.569e-03, MSE(pi1): 7.432e-01, MSE(pi2): 3.989e-03, MSE(pi3): 1.107e-02\n",
      "Epoch 3400, Train loss: 7.692e+07, Test loss: 5.957e+08, MSE(e): 6.854e-03, MSE(pi1): 7.305e-01, MSE(pi2): 3.623e-03, MSE(pi3): 1.081e-02\n",
      "Epoch 3500, Train loss: 7.049e+07, Test loss: 6.007e+08, MSE(e): 6.226e-03, MSE(pi1): 7.179e-01, MSE(pi2): 3.301e-03, MSE(pi3): 1.056e-02\n",
      "Epoch 3600, Train loss: 6.480e+07, Test loss: 6.050e+08, MSE(e): 5.671e-03, MSE(pi1): 7.054e-01, MSE(pi2): 3.016e-03, MSE(pi3): 1.032e-02\n",
      "Epoch 3700, Train loss: 5.972e+07, Test loss: 6.086e+08, MSE(e): 5.179e-03, MSE(pi1): 6.928e-01, MSE(pi2): 2.763e-03, MSE(pi3): 1.008e-02\n",
      "Epoch 3800, Train loss: 5.519e+07, Test loss: 6.116e+08, MSE(e): 4.740e-03, MSE(pi1): 6.803e-01, MSE(pi2): 2.536e-03, MSE(pi3): 9.852e-03\n",
      "Epoch 3900, Train loss: 5.111e+07, Test loss: 6.140e+08, MSE(e): 4.347e-03, MSE(pi1): 6.677e-01, MSE(pi2): 2.334e-03, MSE(pi3): 9.631e-03\n",
      "Epoch 4000, Train loss: 4.744e+07, Test loss: 6.159e+08, MSE(e): 3.995e-03, MSE(pi1): 6.552e-01, MSE(pi2): 2.151e-03, MSE(pi3): 9.417e-03\n",
      "Epoch 4100, Train loss: 4.412e+07, Test loss: 6.173e+08, MSE(e): 3.678e-03, MSE(pi1): 6.427e-01, MSE(pi2): 1.988e-03, MSE(pi3): 9.210e-03\n",
      "Epoch 4200, Train loss: 4.112e+07, Test loss: 6.183e+08, MSE(e): 3.392e-03, MSE(pi1): 6.301e-01, MSE(pi2): 1.840e-03, MSE(pi3): 9.009e-03\n",
      "Epoch 4300, Train loss: 3.840e+07, Test loss: 6.188e+08, MSE(e): 3.134e-03, MSE(pi1): 6.177e-01, MSE(pi2): 1.707e-03, MSE(pi3): 8.814e-03\n",
      "Epoch 4400, Train loss: 3.593e+07, Test loss: 6.190e+08, MSE(e): 2.901e-03, MSE(pi1): 6.053e-01, MSE(pi2): 1.586e-03, MSE(pi3): 8.625e-03\n",
      "Epoch 4500, Train loss: 3.368e+07, Test loss: 6.186e+08, MSE(e): 2.691e-03, MSE(pi1): 5.930e-01, MSE(pi2): 1.478e-03, MSE(pi3): 8.443e-03\n",
      "Epoch 4600, Train loss: 3.215e+07, Test loss: 6.039e+08, MSE(e): 2.551e-03, MSE(pi1): 5.845e-01, MSE(pi2): 1.416e-03, MSE(pi3): 7.983e-03\n",
      "Epoch 4700, Train loss: 2.979e+07, Test loss: 6.166e+08, MSE(e): 2.330e-03, MSE(pi1): 5.688e-01, MSE(pi2): 1.291e-03, MSE(pi3): 8.096e-03\n",
      "Epoch 4800, Train loss: 2.811e+07, Test loss: 6.149e+08, MSE(e): 2.175e-03, MSE(pi1): 5.569e-01, MSE(pi2): 1.211e-03, MSE(pi3): 7.933e-03\n",
      "Epoch 4900, Train loss: 2.658e+07, Test loss: 6.115e+08, MSE(e): 2.035e-03, MSE(pi1): 5.455e-01, MSE(pi2): 1.140e-03, MSE(pi3): 7.757e-03\n",
      "Epoch 5000, Train loss: 2.518e+07, Test loss: 6.096e+08, MSE(e): 1.908e-03, MSE(pi1): 5.338e-01, MSE(pi2): 1.073e-03, MSE(pi3): 7.625e-03\n",
      "Epoch 5100, Train loss: 2.390e+07, Test loss: 6.074e+08, MSE(e): 1.793e-03, MSE(pi1): 5.225e-01, MSE(pi2): 1.013e-03, MSE(pi3): 7.492e-03\n",
      "Epoch 5200, Train loss: 2.273e+07, Test loss: 6.019e+08, MSE(e): 1.688e-03, MSE(pi1): 5.117e-01, MSE(pi2): 9.590e-04, MSE(pi3): 7.341e-03\n",
      "Epoch 5300, Train loss: 2.166e+07, Test loss: 6.001e+08, MSE(e): 1.593e-03, MSE(pi1): 5.007e-01, MSE(pi2): 9.073e-04, MSE(pi3): 7.241e-03\n",
      "Epoch 5400, Train loss: 2.067e+07, Test loss: 5.922e+08, MSE(e): 1.505e-03, MSE(pi1): 4.907e-01, MSE(pi2): 8.634e-04, MSE(pi3): 7.079e-03\n",
      "Epoch 5500, Train loss: 1.981e+07, Test loss: 5.814e+08, MSE(e): 1.430e-03, MSE(pi1): 4.818e-01, MSE(pi2): 8.289e-04, MSE(pi3): 6.870e-03\n",
      "Epoch 5600, Train loss: 1.892e+07, Test loss: 5.807e+08, MSE(e): 1.353e-03, MSE(pi1): 4.710e-01, MSE(pi2): 7.822e-04, MSE(pi3): 6.839e-03\n",
      "Epoch 5700, Train loss: 1.815e+07, Test loss: 5.767e+08, MSE(e): 1.286e-03, MSE(pi1): 4.611e-01, MSE(pi2): 7.438e-04, MSE(pi3): 6.761e-03\n",
      "Epoch 5800, Train loss: 1.742e+07, Test loss: 5.679e+08, MSE(e): 1.223e-03, MSE(pi1): 4.525e-01, MSE(pi2): 7.121e-04, MSE(pi3): 6.619e-03\n",
      "Epoch 5900, Train loss: 1.674e+07, Test loss: 5.601e+08, MSE(e): 1.165e-03, MSE(pi1): 4.438e-01, MSE(pi2): 6.811e-04, MSE(pi3): 6.504e-03\n",
      "Epoch 6000, Train loss: 1.611e+07, Test loss: 5.542e+08, MSE(e): 1.111e-03, MSE(pi1): 4.352e-01, MSE(pi2): 6.503e-04, MSE(pi3): 6.420e-03\n",
      "Epoch 6100, Train loss: 1.551e+07, Test loss: 5.468e+08, MSE(e): 1.061e-03, MSE(pi1): 4.270e-01, MSE(pi2): 6.222e-04, MSE(pi3): 6.324e-03\n",
      "Epoch 6200, Train loss: 1.496e+07, Test loss: 5.402e+08, MSE(e): 1.014e-03, MSE(pi1): 4.190e-01, MSE(pi2): 5.949e-04, MSE(pi3): 6.244e-03\n",
      "Epoch 6300, Train loss: 1.443e+07, Test loss: 5.326e+08, MSE(e): 9.700e-04, MSE(pi1): 4.114e-01, MSE(pi2): 5.702e-04, MSE(pi3): 6.151e-03\n",
      "Epoch 6400, Train loss: 1.393e+07, Test loss: 5.236e+08, MSE(e): 9.287e-04, MSE(pi1): 4.042e-01, MSE(pi2): 5.471e-04, MSE(pi3): 6.056e-03\n",
      "Epoch 6500, Train loss: 1.346e+07, Test loss: 5.179e+08, MSE(e): 8.896e-04, MSE(pi1): 3.968e-01, MSE(pi2): 5.234e-04, MSE(pi3): 5.989e-03\n",
      "Epoch 6600, Train loss: 1.305e+07, Test loss: 5.127e+08, MSE(e): 8.559e-04, MSE(pi1): 3.890e-01, MSE(pi2): 4.981e-04, MSE(pi3): 5.980e-03\n",
      "Epoch 6700, Train loss: 1.260e+07, Test loss: 5.030e+08, MSE(e): 8.182e-04, MSE(pi1): 3.829e-01, MSE(pi2): 4.811e-04, MSE(pi3): 5.839e-03\n",
      "Epoch 6800, Train loss: 1.222e+07, Test loss: 4.946e+08, MSE(e): 7.881e-04, MSE(pi1): 3.755e-01, MSE(pi2): 4.582e-04, MSE(pi3): 5.827e-03\n",
      "Epoch 6900, Train loss: 1.181e+07, Test loss: 4.883e+08, MSE(e): 7.548e-04, MSE(pi1): 3.696e-01, MSE(pi2): 4.432e-04, MSE(pi3): 5.696e-03\n",
      "Epoch 7000, Train loss: 1.145e+07, Test loss: 4.808e+08, MSE(e): 7.253e-04, MSE(pi1): 3.630e-01, MSE(pi2): 4.256e-04, MSE(pi3): 5.626e-03\n",
      "Epoch 7100, Train loss: 1.110e+07, Test loss: 4.741e+08, MSE(e): 6.979e-04, MSE(pi1): 3.566e-01, MSE(pi2): 4.091e-04, MSE(pi3): 5.558e-03\n",
      "Epoch 7200, Train loss: 1.077e+07, Test loss: 4.667e+08, MSE(e): 6.717e-04, MSE(pi1): 3.501e-01, MSE(pi2): 3.931e-04, MSE(pi3): 5.492e-03\n",
      "Epoch 7300, Train loss: 1.059e+07, Test loss: 4.612e+08, MSE(e): 6.611e-04, MSE(pi1): 3.420e-01, MSE(pi2): 3.735e-04, MSE(pi3): 5.578e-03\n",
      "Epoch 7400, Train loss: 1.014e+07, Test loss: 4.529e+08, MSE(e): 6.232e-04, MSE(pi1): 3.372e-01, MSE(pi2): 3.637e-04, MSE(pi3): 5.362e-03\n",
      "Epoch 7500, Train loss: 1.085e+07, Test loss: 4.551e+08, MSE(e): 7.012e-04, MSE(pi1): 3.266e-01, MSE(pi2): 3.643e-04, MSE(pi3): 5.698e-03\n",
      "Epoch 7600, Train loss: 9.560e+06, Test loss: 4.397e+08, MSE(e): 5.794e-04, MSE(pi1): 3.243e-01, MSE(pi2): 3.373e-04, MSE(pi3): 5.231e-03\n",
      "Epoch 7700, Train loss: 9.285e+06, Test loss: 4.321e+08, MSE(e): 5.590e-04, MSE(pi1): 3.179e-01, MSE(pi2): 3.259e-04, MSE(pi3): 5.152e-03\n",
      "Epoch 7800, Train loss: 9.020e+06, Test loss: 4.276e+08, MSE(e): 5.397e-04, MSE(pi1): 3.112e-01, MSE(pi2): 3.127e-04, MSE(pi3): 5.112e-03\n",
      "Epoch 7900, Train loss: 8.762e+06, Test loss: 4.210e+08, MSE(e): 5.211e-04, MSE(pi1): 3.047e-01, MSE(pi2): 3.019e-04, MSE(pi3): 5.042e-03\n",
      "Epoch 8000, Train loss: 8.517e+06, Test loss: 4.135e+08, MSE(e): 5.038e-04, MSE(pi1): 2.983e-01, MSE(pi2): 2.928e-04, MSE(pi3): 4.960e-03\n",
      "Epoch 8100, Train loss: 8.275e+06, Test loss: 4.098e+08, MSE(e): 4.868e-04, MSE(pi1): 2.915e-01, MSE(pi2): 2.813e-04, MSE(pi3): 4.919e-03\n",
      "Epoch 8200, Train loss: 8.040e+06, Test loss: 4.050e+08, MSE(e): 4.706e-04, MSE(pi1): 2.847e-01, MSE(pi2): 2.706e-04, MSE(pi3): 4.871e-03\n",
      "Epoch 8300, Train loss: 7.816e+06, Test loss: 3.987e+08, MSE(e): 4.553e-04, MSE(pi1): 2.784e-01, MSE(pi2): 2.631e-04, MSE(pi3): 4.788e-03\n",
      "Epoch 8400, Train loss: 7.596e+06, Test loss: 3.940e+08, MSE(e): 4.405e-04, MSE(pi1): 2.717e-01, MSE(pi2): 2.535e-04, MSE(pi3): 4.740e-03\n",
      "Epoch 8500, Train loss: 7.380e+06, Test loss: 4.001e+08, MSE(e): 4.261e-04, MSE(pi1): 2.651e-01, MSE(pi2): 2.451e-04, MSE(pi3): 4.677e-03\n",
      "Epoch 8600, Train loss: 7.174e+06, Test loss: 3.843e+08, MSE(e): 4.126e-04, MSE(pi1): 2.586e-01, MSE(pi2): 2.369e-04, MSE(pi3): 4.625e-03\n",
      "Epoch 8700, Train loss: 6.971e+06, Test loss: 3.795e+08, MSE(e): 3.993e-04, MSE(pi1): 2.521e-01, MSE(pi2): 2.290e-04, MSE(pi3): 4.569e-03\n",
      "Epoch 8800, Train loss: 6.778e+06, Test loss: 3.750e+08, MSE(e): 3.870e-04, MSE(pi1): 2.455e-01, MSE(pi2): 2.210e-04, MSE(pi3): 4.526e-03\n",
      "Epoch 8900, Train loss: 6.588e+06, Test loss: 3.709e+08, MSE(e): 3.748e-04, MSE(pi1): 2.393e-01, MSE(pi2): 2.145e-04, MSE(pi3): 4.464e-03\n",
      "Epoch 9000, Train loss: 8.325e+06, Test loss: 3.493e+08, MSE(e): 5.542e-04, MSE(pi1): 2.385e-01, MSE(pi2): 3.139e-04, MSE(pi3): 3.986e-03\n",
      "Epoch 9100, Train loss: 6.225e+06, Test loss: 3.627e+08, MSE(e): 3.520e-04, MSE(pi1): 2.269e-01, MSE(pi2): 2.009e-04, MSE(pi3): 4.366e-03\n",
      "Epoch 9200, Train loss: 6.052e+06, Test loss: 3.594e+08, MSE(e): 3.413e-04, MSE(pi1): 2.207e-01, MSE(pi2): 1.939e-04, MSE(pi3): 4.329e-03\n",
      "Epoch 9300, Train loss: 5.886e+06, Test loss: 3.547e+08, MSE(e): 3.309e-04, MSE(pi1): 2.150e-01, MSE(pi2): 1.886e-04, MSE(pi3): 4.270e-03\n",
      "Epoch 9400, Train loss: 5.724e+06, Test loss: 3.512e+08, MSE(e): 3.209e-04, MSE(pi1): 2.092e-01, MSE(pi2): 1.827e-04, MSE(pi3): 4.227e-03\n",
      "Epoch 9500, Train loss: 5.579e+06, Test loss: 3.500e+08, MSE(e): 3.124e-04, MSE(pi1): 2.033e-01, MSE(pi2): 1.758e-04, MSE(pi3): 4.216e-03\n",
      "Epoch 9600, Train loss: 5.420e+06, Test loss: 3.444e+08, MSE(e): 3.023e-04, MSE(pi1): 1.982e-01, MSE(pi2): 1.717e-04, MSE(pi3): 4.146e-03\n",
      "Epoch 9700, Train loss: 5.878e+06, Test loss: 3.493e+08, MSE(e): 3.538e-04, MSE(pi1): 1.903e-01, MSE(pi2): 1.788e-04, MSE(pi3): 4.360e-03\n",
      "Epoch 9800, Train loss: 5.139e+06, Test loss: 3.380e+08, MSE(e): 2.853e-04, MSE(pi1): 1.880e-01, MSE(pi2): 1.616e-04, MSE(pi3): 4.073e-03\n",
      "Epoch 9900, Train loss: 5.005e+06, Test loss: 3.347e+08, MSE(e): 2.771e-04, MSE(pi1): 1.831e-01, MSE(pi2): 1.568e-04, MSE(pi3): 4.038e-03\n",
      "Epoch 10000, Train loss: 4.884e+06, Test loss: 3.314e+08, MSE(e): 2.699e-04, MSE(pi1): 1.787e-01, MSE(pi2): 1.536e-04, MSE(pi3): 3.985e-03\n",
      "Epoch 10100, Train loss: 4.759e+06, Test loss: 3.290e+08, MSE(e): 2.621e-04, MSE(pi1): 1.741e-01, MSE(pi2): 1.480e-04, MSE(pi3): 3.976e-03\n",
      "Epoch 10200, Train loss: 6.460e+06, Test loss: 3.632e+08, MSE(e): 4.366e-04, MSE(pi1): 1.656e-01, MSE(pi2): 2.005e-04, MSE(pi3): 4.373e-03\n",
      "Epoch 10300, Train loss: 4.532e+06, Test loss: 3.234e+08, MSE(e): 2.482e-04, MSE(pi1): 1.658e-01, MSE(pi2): 1.399e-04, MSE(pi3): 3.919e-03\n",
      "Epoch 10400, Train loss: 4.426e+06, Test loss: 3.207e+08, MSE(e): 2.417e-04, MSE(pi1): 1.619e-01, MSE(pi2): 1.360e-04, MSE(pi3): 3.896e-03\n",
      "Epoch 10500, Train loss: 4.367e+06, Test loss: 3.149e+08, MSE(e): 2.397e-04, MSE(pi1): 1.590e-01, MSE(pi2): 1.365e-04, MSE(pi3): 3.812e-03\n",
      "Epoch 10600, Train loss: 4.229e+06, Test loss: 3.157e+08, MSE(e): 2.295e-04, MSE(pi1): 1.549e-01, MSE(pi2): 1.288e-04, MSE(pi3): 3.851e-03\n",
      "Epoch 10700, Train loss: 4.810e+06, Test loss: 2.968e+08, MSE(e): 2.906e-04, MSE(pi1): 1.543e-01, MSE(pi2): 1.619e-04, MSE(pi3): 3.610e-03\n",
      "Epoch 10800, Train loss: 4.052e+06, Test loss: 3.114e+08, MSE(e): 2.185e-04, MSE(pi1): 1.485e-01, MSE(pi2): 1.221e-04, MSE(pi3): 3.817e-03\n",
      "Epoch 10900, Train loss: 3.969e+06, Test loss: 3.087e+08, MSE(e): 2.133e-04, MSE(pi1): 1.457e-01, MSE(pi2): 1.192e-04, MSE(pi3): 3.795e-03\n",
      "Epoch 11000, Train loss: 5.615e+06, Test loss: 3.327e+08, MSE(e): 3.806e-04, MSE(pi1): 1.391e-01, MSE(pi2): 1.748e-04, MSE(pi3): 4.178e-03\n",
      "Epoch 11100, Train loss: 3.816e+06, Test loss: 3.042e+08, MSE(e): 2.036e-04, MSE(pi1): 1.405e-01, MSE(pi2): 1.135e-04, MSE(pi3): 3.761e-03\n",
      "Epoch 11200, Train loss: 3.745e+06, Test loss: 3.021e+08, MSE(e): 1.990e-04, MSE(pi1): 1.380e-01, MSE(pi2): 1.107e-04, MSE(pi3): 3.749e-03\n",
      "Epoch 11300, Train loss: 3.754e+06, Test loss: 3.044e+08, MSE(e): 2.023e-04, MSE(pi1): 1.349e-01, MSE(pi2): 1.090e-04, MSE(pi3): 3.813e-03\n",
      "Epoch 11400, Train loss: 3.615e+06, Test loss: 2.981e+08, MSE(e): 1.906e-04, MSE(pi1): 1.337e-01, MSE(pi2): 1.057e-04, MSE(pi3): 3.725e-03\n",
      "Epoch 11500, Train loss: 3.553e+06, Test loss: 2.959e+08, MSE(e): 1.865e-04, MSE(pi1): 1.317e-01, MSE(pi2): 1.033e-04, MSE(pi3): 3.712e-03\n",
      "Epoch 11600, Train loss: 3.502e+06, Test loss: 2.928e+08, MSE(e): 1.833e-04, MSE(pi1): 1.300e-01, MSE(pi2): 1.018e-04, MSE(pi3): 3.682e-03\n",
      "Epoch 11700, Train loss: 3.442e+06, Test loss: 2.922e+08, MSE(e): 1.792e-04, MSE(pi1): 1.281e-01, MSE(pi2): 9.891e-05, MSE(pi3): 3.693e-03\n",
      "Epoch 11800, Train loss: 4.712e+06, Test loss: 2.685e+08, MSE(e): 3.071e-04, MSE(pi1): 1.301e-01, MSE(pi2): 1.584e-04, MSE(pi3): 3.401e-03\n",
      "Epoch 11900, Train loss: 3.342e+06, Test loss: 2.888e+08, MSE(e): 1.725e-04, MSE(pi1): 1.249e-01, MSE(pi2): 9.479e-05, MSE(pi3): 3.680e-03\n",
      "Epoch 12000, Train loss: 3.295e+06, Test loss: 2.868e+08, MSE(e): 1.693e-04, MSE(pi1): 1.235e-01, MSE(pi2): 9.298e-05, MSE(pi3): 3.668e-03\n",
      "Epoch 12100, Train loss: 9.056e+06, Test loss: 2.664e+08, MSE(e): 7.440e-04, MSE(pi1): 1.302e-01, MSE(pi2): 3.430e-04, MSE(pi3): 3.139e-03\n",
      "Epoch 12200, Train loss: 3.208e+06, Test loss: 2.834e+08, MSE(e): 1.634e-04, MSE(pi1): 1.209e-01, MSE(pi2): 8.951e-05, MSE(pi3): 3.652e-03\n",
      "Epoch 12300, Train loss: 3.168e+06, Test loss: 2.819e+08, MSE(e): 1.607e-04, MSE(pi1): 1.197e-01, MSE(pi2): 8.780e-05, MSE(pi3): 3.649e-03\n",
      "Epoch 12400, Train loss: 4.619e+06, Test loss: 2.574e+08, MSE(e): 3.060e-04, MSE(pi1): 1.224e-01, MSE(pi2): 1.530e-04, MSE(pi3): 3.349e-03\n",
      "Epoch 12500, Train loss: 3.093e+06, Test loss: 2.790e+08, MSE(e): 1.555e-04, MSE(pi1): 1.174e-01, MSE(pi2): 8.459e-05, MSE(pi3): 3.642e-03\n",
      "Epoch 12600, Train loss: 3.058e+06, Test loss: 2.770e+08, MSE(e): 1.529e-04, MSE(pi1): 1.165e-01, MSE(pi2): 8.315e-05, MSE(pi3): 3.632e-03\n",
      "Epoch 12700, Train loss: 3.026e+06, Test loss: 2.744e+08, MSE(e): 1.508e-04, MSE(pi1): 1.155e-01, MSE(pi2): 8.169e-05, MSE(pi3): 3.637e-03\n",
      "Epoch 12800, Train loss: 2.993e+06, Test loss: 2.742e+08, MSE(e): 1.484e-04, MSE(pi1): 1.147e-01, MSE(pi2): 8.042e-05, MSE(pi3): 3.623e-03\n",
      "Epoch 12900, Train loss: 2.962e+06, Test loss: 2.726e+08, MSE(e): 1.462e-04, MSE(pi1): 1.139e-01, MSE(pi2): 7.909e-05, MSE(pi3): 3.618e-03\n",
      "Epoch 13000, Train loss: 2.940e+06, Test loss: 2.722e+08, MSE(e): 1.448e-04, MSE(pi1): 1.128e-01, MSE(pi2): 7.787e-05, MSE(pi3): 3.636e-03\n",
      "Epoch 13100, Train loss: 2.906e+06, Test loss: 2.699e+08, MSE(e): 1.422e-04, MSE(pi1): 1.123e-01, MSE(pi2): 7.664e-05, MSE(pi3): 3.611e-03\n",
      "Epoch 13200, Train loss: 2.956e+06, Test loss: 2.538e+08, MSE(e): 1.481e-04, MSE(pi1): 1.107e-01, MSE(pi2): 7.794e-05, MSE(pi3): 3.680e-03\n",
      "Epoch 13300, Train loss: 2.854e+06, Test loss: 2.672e+08, MSE(e): 1.384e-04, MSE(pi1): 1.110e-01, MSE(pi2): 7.438e-05, MSE(pi3): 3.603e-03\n",
      "Epoch 13400, Train loss: 2.830e+06, Test loss: 2.660e+08, MSE(e): 1.366e-04, MSE(pi1): 1.103e-01, MSE(pi2): 7.327e-05, MSE(pi3): 3.602e-03\n",
      "Epoch 13500, Train loss: 3.222e+06, Test loss: 2.786e+08, MSE(e): 1.766e-04, MSE(pi1): 1.079e-01, MSE(pi2): 8.803e-05, MSE(pi3): 3.771e-03\n",
      "Epoch 13600, Train loss: 2.783e+06, Test loss: 2.635e+08, MSE(e): 1.332e-04, MSE(pi1): 1.092e-01, MSE(pi2): 7.123e-05, MSE(pi3): 3.595e-03\n",
      "Epoch 13700, Train loss: 2.761e+06, Test loss: 2.621e+08, MSE(e): 1.316e-04, MSE(pi1): 1.086e-01, MSE(pi2): 7.022e-05, MSE(pi3): 3.592e-03\n",
      "Epoch 13800, Train loss: 2.743e+06, Test loss: 2.611e+08, MSE(e): 1.303e-04, MSE(pi1): 1.082e-01, MSE(pi2): 6.947e-05, MSE(pi3): 3.579e-03\n",
      "Epoch 13900, Train loss: 2.721e+06, Test loss: 2.600e+08, MSE(e): 1.286e-04, MSE(pi1): 1.076e-01, MSE(pi2): 6.842e-05, MSE(pi3): 3.588e-03\n",
      "Epoch 14000, Train loss: 2.701e+06, Test loss: 2.587e+08, MSE(e): 1.271e-04, MSE(pi1): 1.071e-01, MSE(pi2): 6.751e-05, MSE(pi3): 3.585e-03\n",
      "Epoch 14100, Train loss: 2.689e+06, Test loss: 2.570e+08, MSE(e): 1.263e-04, MSE(pi1): 1.069e-01, MSE(pi2): 6.701e-05, MSE(pi3): 3.564e-03\n",
      "Epoch 14200, Train loss: 2.665e+06, Test loss: 2.567e+08, MSE(e): 1.244e-04, MSE(pi1): 1.063e-01, MSE(pi2): 6.585e-05, MSE(pi3): 3.582e-03\n",
      "Epoch 14300, Train loss: 4.260e+06, Test loss: 2.927e+08, MSE(e): 2.841e-04, MSE(pi1): 1.025e-01, MSE(pi2): 1.302e-04, MSE(pi3): 3.937e-03\n",
      "Epoch 14400, Train loss: 2.631e+06, Test loss: 2.545e+08, MSE(e): 1.219e-04, MSE(pi1): 1.055e-01, MSE(pi2): 6.431e-05, MSE(pi3): 3.573e-03\n",
      "Epoch 14500, Train loss: 2.614e+06, Test loss: 2.536e+08, MSE(e): 1.206e-04, MSE(pi1): 1.051e-01, MSE(pi2): 6.352e-05, MSE(pi3): 3.576e-03\n",
      "Epoch 14600, Train loss: 1.091e+07, Test loss: 2.255e+08, MSE(e): 9.463e-04, MSE(pi1): 1.143e-01, MSE(pi2): 4.064e-04, MSE(pi3): 3.013e-03\n",
      "Epoch 14700, Train loss: 2.584e+06, Test loss: 2.518e+08, MSE(e): 1.183e-04, MSE(pi1): 1.044e-01, MSE(pi2): 6.212e-05, MSE(pi3): 3.570e-03\n",
      "Epoch 14800, Train loss: 2.569e+06, Test loss: 2.508e+08, MSE(e): 1.171e-04, MSE(pi1): 1.041e-01, MSE(pi2): 6.142e-05, MSE(pi3): 3.571e-03\n",
      "Epoch 14900, Train loss: 2.554e+06, Test loss: 2.500e+08, MSE(e): 1.160e-04, MSE(pi1): 1.037e-01, MSE(pi2): 6.072e-05, MSE(pi3): 3.573e-03\n",
      "Epoch 15000, Train loss: 2.541e+06, Test loss: 2.489e+08, MSE(e): 1.151e-04, MSE(pi1): 1.035e-01, MSE(pi2): 6.014e-05, MSE(pi3): 3.559e-03\n",
      "Epoch 15100, Train loss: 2.527e+06, Test loss: 2.480e+08, MSE(e): 1.139e-04, MSE(pi1): 1.031e-01, MSE(pi2): 5.945e-05, MSE(pi3): 3.566e-03\n",
      "Epoch 15200, Train loss: 2.515e+06, Test loss: 2.339e+08, MSE(e): 1.131e-04, MSE(pi1): 1.027e-01, MSE(pi2): 5.887e-05, MSE(pi3): 3.572e-03\n",
      "Epoch 15300, Train loss: 2.501e+06, Test loss: 2.463e+08, MSE(e): 1.119e-04, MSE(pi1): 1.026e-01, MSE(pi2): 5.824e-05, MSE(pi3): 3.562e-03\n",
      "Epoch 15400, Train loss: 2.488e+06, Test loss: 2.454e+08, MSE(e): 1.109e-04, MSE(pi1): 1.023e-01, MSE(pi2): 5.763e-05, MSE(pi3): 3.562e-03\n",
      "Epoch 15500, Train loss: 2.481e+06, Test loss: 2.421e+08, MSE(e): 1.104e-04, MSE(pi1): 1.022e-01, MSE(pi2): 5.726e-05, MSE(pi3): 3.543e-03\n",
      "Epoch 15600, Train loss: 2.464e+06, Test loss: 2.439e+08, MSE(e): 1.091e-04, MSE(pi1): 1.017e-01, MSE(pi2): 5.651e-05, MSE(pi3): 3.559e-03\n",
      "Epoch 15700, Train loss: 2.512e+06, Test loss: 2.368e+08, MSE(e): 1.140e-04, MSE(pi1): 1.022e-01, MSE(pi2): 5.830e-05, MSE(pi3): 3.498e-03\n",
      "Epoch 15800, Train loss: 2.442e+06, Test loss: 2.420e+08, MSE(e): 1.074e-04, MSE(pi1): 1.013e-01, MSE(pi2): 5.546e-05, MSE(pi3): 3.554e-03\n",
      "Epoch 15900, Train loss: 2.431e+06, Test loss: 2.416e+08, MSE(e): 1.065e-04, MSE(pi1): 1.010e-01, MSE(pi2): 5.494e-05, MSE(pi3): 3.555e-03\n",
      "Epoch 16000, Train loss: 2.420e+06, Test loss: 2.404e+08, MSE(e): 1.057e-04, MSE(pi1): 1.008e-01, MSE(pi2): 5.441e-05, MSE(pi3): 3.551e-03\n",
      "Epoch 16100, Train loss: 2.413e+06, Test loss: 2.409e+08, MSE(e): 1.052e-04, MSE(pi1): 1.004e-01, MSE(pi2): 5.408e-05, MSE(pi3): 3.566e-03\n",
      "Epoch 16200, Train loss: 2.400e+06, Test loss: 2.394e+08, MSE(e): 1.041e-04, MSE(pi1): 1.004e-01, MSE(pi2): 5.345e-05, MSE(pi3): 3.552e-03\n",
      "Epoch 16300, Train loss: 2.920e+06, Test loss: 2.230e+08, MSE(e): 1.559e-04, MSE(pi1): 1.023e-01, MSE(pi2): 7.431e-05, MSE(pi3): 3.376e-03\n",
      "Epoch 16400, Train loss: 2.381e+06, Test loss: 2.382e+08, MSE(e): 1.026e-04, MSE(pi1): 9.996e-02, MSE(pi2): 5.253e-05, MSE(pi3): 3.551e-03\n",
      "Epoch 16500, Train loss: 2.372e+06, Test loss: 2.380e+08, MSE(e): 1.019e-04, MSE(pi1): 9.969e-02, MSE(pi2): 5.211e-05, MSE(pi3): 3.555e-03\n",
      "Epoch 16600, Train loss: 2.363e+06, Test loss: 2.376e+08, MSE(e): 1.012e-04, MSE(pi1): 9.950e-02, MSE(pi2): 5.170e-05, MSE(pi3): 3.554e-03\n",
      "Epoch 16700, Train loss: 2.353e+06, Test loss: 2.361e+08, MSE(e): 1.004e-04, MSE(pi1): 9.939e-02, MSE(pi2): 5.120e-05, MSE(pi3): 3.546e-03\n",
      "Epoch 16800, Train loss: 3.176e+06, Test loss: 2.509e+08, MSE(e): 1.830e-04, MSE(pi1): 9.679e-02, MSE(pi2): 8.586e-05, MSE(pi3): 3.786e-03\n",
      "Epoch 16900, Train loss: 2.336e+06, Test loss: 2.349e+08, MSE(e): 9.910e-05, MSE(pi1): 9.905e-02, MSE(pi2): 5.038e-05, MSE(pi3): 3.543e-03\n",
      "Epoch 17000, Train loss: 2.327e+06, Test loss: 2.340e+08, MSE(e): 9.842e-05, MSE(pi1): 9.887e-02, MSE(pi2): 4.995e-05, MSE(pi3): 3.542e-03\n",
      "Epoch 17100, Train loss: 2.322e+06, Test loss: 2.344e+08, MSE(e): 9.813e-05, MSE(pi1): 9.854e-02, MSE(pi2): 4.977e-05, MSE(pi3): 3.556e-03\n",
      "Epoch 17200, Train loss: 2.311e+06, Test loss: 2.331e+08, MSE(e): 9.719e-05, MSE(pi1): 9.852e-02, MSE(pi2): 4.920e-05, MSE(pi3): 3.542e-03\n",
      "Epoch 17300, Train loss: 2.398e+06, Test loss: 2.246e+08, MSE(e): 1.059e-04, MSE(pi1): 9.924e-02, MSE(pi2): 5.237e-05, MSE(pi3): 3.466e-03\n",
      "Epoch 17400, Train loss: 2.297e+06, Test loss: 2.326e+08, MSE(e): 9.608e-05, MSE(pi1): 9.813e-02, MSE(pi2): 4.854e-05, MSE(pi3): 3.546e-03\n",
      "Epoch 17500, Train loss: 2.288e+06, Test loss: 2.315e+08, MSE(e): 9.541e-05, MSE(pi1): 9.805e-02, MSE(pi2): 4.811e-05, MSE(pi3): 3.539e-03\n",
      "Epoch 17600, Train loss: 6.379e+06, Test loss: 1.954e+08, MSE(e): 5.024e-04, MSE(pi1): 1.043e-01, MSE(pi2): 2.139e-04, MSE(pi3): 3.118e-03\n",
      "Epoch 17700, Train loss: 2.274e+06, Test loss: 2.304e+08, MSE(e): 9.428e-05, MSE(pi1): 9.774e-02, MSE(pi2): 4.742e-05, MSE(pi3): 3.538e-03\n",
      "Epoch 17800, Train loss: 2.267e+06, Test loss: 2.298e+08, MSE(e): 9.370e-05, MSE(pi1): 9.760e-02, MSE(pi2): 4.706e-05, MSE(pi3): 3.536e-03\n",
      "Epoch 17900, Train loss: 2.264e+06, Test loss: 2.271e+08, MSE(e): 9.356e-05, MSE(pi1): 9.763e-02, MSE(pi2): 4.683e-05, MSE(pi3): 3.520e-03\n",
      "Epoch 18000, Train loss: 2.253e+06, Test loss: 2.289e+08, MSE(e): 9.265e-05, MSE(pi1): 9.733e-02, MSE(pi2): 4.641e-05, MSE(pi3): 3.534e-03\n",
      "Epoch 18100, Train loss: 3.134e+06, Test loss: 2.557e+08, MSE(e): 1.808e-04, MSE(pi1): 9.472e-02, MSE(pi2): 8.387e-05, MSE(pi3): 3.783e-03\n",
      "Epoch 18200, Train loss: 2.241e+06, Test loss: 2.277e+08, MSE(e): 9.167e-05, MSE(pi1): 9.711e-02, MSE(pi2): 4.578e-05, MSE(pi3): 3.528e-03\n",
      "Epoch 18300, Train loss: 2.234e+06, Test loss: 2.275e+08, MSE(e): 9.112e-05, MSE(pi1): 9.691e-02, MSE(pi2): 4.546e-05, MSE(pi3): 3.532e-03\n",
      "Epoch 18400, Train loss: 2.270e+06, Test loss: 2.232e+08, MSE(e): 9.484e-05, MSE(pi1): 9.737e-02, MSE(pi2): 4.666e-05, MSE(pi3): 3.480e-03\n",
      "Epoch 18500, Train loss: 2.221e+06, Test loss: 2.267e+08, MSE(e): 9.017e-05, MSE(pi1): 9.666e-02, MSE(pi2): 4.489e-05, MSE(pi3): 3.531e-03\n",
      "Epoch 18600, Train loss: 2.215e+06, Test loss: 2.257e+08, MSE(e): 8.971e-05, MSE(pi1): 9.659e-02, MSE(pi2): 4.457e-05, MSE(pi3): 3.525e-03\n",
      "Epoch 18700, Train loss: 2.209e+06, Test loss: 2.263e+08, MSE(e): 8.925e-05, MSE(pi1): 9.642e-02, MSE(pi2): 4.431e-05, MSE(pi3): 3.528e-03\n",
      "Epoch 18800, Train loss: 2.204e+06, Test loss: 2.253e+08, MSE(e): 8.878e-05, MSE(pi1): 9.629e-02, MSE(pi2): 4.403e-05, MSE(pi3): 3.528e-03\n",
      "Epoch 18900, Train loss: 2.433e+06, Test loss: 2.206e+08, MSE(e): 1.116e-04, MSE(pi1): 9.756e-02, MSE(pi2): 5.270e-05, MSE(pi3): 3.412e-03\n",
      "Epoch 19000, Train loss: 2.192e+06, Test loss: 2.246e+08, MSE(e): 8.792e-05, MSE(pi1): 9.607e-02, MSE(pi2): 4.349e-05, MSE(pi3): 3.526e-03\n",
      "Epoch 19100, Train loss: 2.187e+06, Test loss: 2.241e+08, MSE(e): 8.747e-05, MSE(pi1): 9.594e-02, MSE(pi2): 4.322e-05, MSE(pi3): 3.527e-03\n",
      "Epoch 19200, Train loss: 2.182e+06, Test loss: 2.244e+08, MSE(e): 8.715e-05, MSE(pi1): 9.590e-02, MSE(pi2): 4.297e-05, MSE(pi3): 3.519e-03\n",
      "Epoch 19300, Train loss: 2.176e+06, Test loss: 2.234e+08, MSE(e): 8.666e-05, MSE(pi1): 9.572e-02, MSE(pi2): 4.272e-05, MSE(pi3): 3.525e-03\n",
      "Epoch 19400, Train loss: 8.955e+06, Test loss: 1.830e+08, MSE(e): 7.613e-04, MSE(pi1): 1.041e-01, MSE(pi2): 3.174e-04, MSE(pi3): 3.020e-03\n",
      "Epoch 19500, Train loss: 2.166e+06, Test loss: 2.227e+08, MSE(e): 8.587e-05, MSE(pi1): 9.552e-02, MSE(pi2): 4.222e-05, MSE(pi3): 3.522e-03\n",
      "Epoch 19600, Train loss: 2.161e+06, Test loss: 2.222e+08, MSE(e): 8.546e-05, MSE(pi1): 9.540e-02, MSE(pi2): 4.198e-05, MSE(pi3): 3.523e-03\n",
      "Epoch 19700, Train loss: 2.164e+06, Test loss: 2.181e+08, MSE(e): 8.582e-05, MSE(pi1): 9.553e-02, MSE(pi2): 4.194e-05, MSE(pi3): 3.501e-03\n",
      "Epoch 19800, Train loss: 2.151e+06, Test loss: 2.216e+08, MSE(e): 8.471e-05, MSE(pi1): 9.520e-02, MSE(pi2): 4.152e-05, MSE(pi3): 3.521e-03\n",
      "Epoch 19900, Train loss: 2.146e+06, Test loss: 2.212e+08, MSE(e): 8.432e-05, MSE(pi1): 9.509e-02, MSE(pi2): 4.127e-05, MSE(pi3): 3.521e-03\n",
      "\n",
      "Training process finished after 20000 epochs\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = AutoencoderNonlinearModel(input_shape, predictive_layers, pretrained_decoder, predictive_output, explanatory_input,\n",
    "                                   explanatory_layers, explanatory_output, n_filters_explanatory).to(DEVICE)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "# Parametros de entrenamiento\n",
    "start_epoch = 0\n",
    "n_epochs = 20000\n",
    "\n",
    "batch_size = 64\n",
    "n_checkpoints = 10\n",
    "\n",
    "train_loop(model, optimizer, X_train_NN, y_train_NN, f_train_NN, X_test_NN, y_test_NN, f_test_NN,\n",
    "           D, n_checkpoints, start_epoch=start_epoch, n_epochs=n_epochs, batch_size=batch_size, \n",
    "           model_results_path=MODEL_RESULTS_PGNNIV_PATH, device=DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training from a checkpoint. Epoch 18000.\n",
      "Epoch 18000, Train loss: 2.253e+06, Test loss: 2.289e+08, MSE(e): 9.265e-05, MSE(pi1): 9.732e-02, MSE(pi2): 4.641e-05, MSE(pi3): 3.535e-03\n",
      "Epoch 18100, Train loss: 2.252e+06, Test loss: 2.289e+08, MSE(e): 9.259e-05, MSE(pi1): 9.731e-02, MSE(pi2): 4.638e-05, MSE(pi3): 3.534e-03\n",
      "Epoch 18200, Train loss: 2.252e+06, Test loss: 2.288e+08, MSE(e): 9.253e-05, MSE(pi1): 9.729e-02, MSE(pi2): 4.634e-05, MSE(pi3): 3.534e-03\n",
      "Epoch 18300, Train loss: 2.251e+06, Test loss: 2.288e+08, MSE(e): 9.248e-05, MSE(pi1): 9.728e-02, MSE(pi2): 4.630e-05, MSE(pi3): 3.534e-03\n",
      "Epoch 18400, Train loss: 2.250e+06, Test loss: 2.287e+08, MSE(e): 9.241e-05, MSE(pi1): 9.726e-02, MSE(pi2): 4.626e-05, MSE(pi3): 3.534e-03\n",
      "Epoch 18500, Train loss: 2.249e+06, Test loss: 2.286e+08, MSE(e): 9.234e-05, MSE(pi1): 9.724e-02, MSE(pi2): 4.622e-05, MSE(pi3): 3.534e-03\n",
      "Epoch 18600, Train loss: 2.248e+06, Test loss: 2.285e+08, MSE(e): 9.227e-05, MSE(pi1): 9.722e-02, MSE(pi2): 4.618e-05, MSE(pi3): 3.534e-03\n",
      "Epoch 18700, Train loss: 2.247e+06, Test loss: 2.285e+08, MSE(e): 9.220e-05, MSE(pi1): 9.721e-02, MSE(pi2): 4.613e-05, MSE(pi3): 3.534e-03\n",
      "Epoch 18800, Train loss: 2.246e+06, Test loss: 2.284e+08, MSE(e): 9.212e-05, MSE(pi1): 9.719e-02, MSE(pi2): 4.608e-05, MSE(pi3): 3.534e-03\n",
      "Epoch 18900, Train loss: 2.245e+06, Test loss: 2.283e+08, MSE(e): 9.204e-05, MSE(pi1): 9.716e-02, MSE(pi2): 4.603e-05, MSE(pi3): 3.534e-03\n",
      "Epoch 19000, Train loss: 2.244e+06, Test loss: 2.282e+08, MSE(e): 9.196e-05, MSE(pi1): 9.714e-02, MSE(pi2): 4.598e-05, MSE(pi3): 3.533e-03\n",
      "Epoch 19100, Train loss: 2.243e+06, Test loss: 2.281e+08, MSE(e): 9.187e-05, MSE(pi1): 9.712e-02, MSE(pi2): 4.593e-05, MSE(pi3): 3.533e-03\n",
      "Epoch 19200, Train loss: 2.242e+06, Test loss: 2.280e+08, MSE(e): 9.178e-05, MSE(pi1): 9.709e-02, MSE(pi2): 4.587e-05, MSE(pi3): 3.533e-03\n",
      "Epoch 19300, Train loss: 2.241e+06, Test loss: 2.279e+08, MSE(e): 9.168e-05, MSE(pi1): 9.707e-02, MSE(pi2): 4.581e-05, MSE(pi3): 3.533e-03\n",
      "Epoch 19400, Train loss: 2.240e+06, Test loss: 2.277e+08, MSE(e): 9.158e-05, MSE(pi1): 9.704e-02, MSE(pi2): 4.575e-05, MSE(pi3): 3.533e-03\n",
      "Epoch 19500, Train loss: 2.238e+06, Test loss: 2.276e+08, MSE(e): 9.148e-05, MSE(pi1): 9.701e-02, MSE(pi2): 4.568e-05, MSE(pi3): 3.533e-03\n",
      "Epoch 19600, Train loss: 2.237e+06, Test loss: 2.275e+08, MSE(e): 9.137e-05, MSE(pi1): 9.699e-02, MSE(pi2): 4.561e-05, MSE(pi3): 3.533e-03\n",
      "Epoch 19700, Train loss: 2.235e+06, Test loss: 2.274e+08, MSE(e): 9.125e-05, MSE(pi1): 9.695e-02, MSE(pi2): 4.554e-05, MSE(pi3): 3.532e-03\n",
      "Epoch 19800, Train loss: 2.234e+06, Test loss: 2.272e+08, MSE(e): 9.113e-05, MSE(pi1): 9.692e-02, MSE(pi2): 4.547e-05, MSE(pi3): 3.532e-03\n",
      "Epoch 19900, Train loss: 2.232e+06, Test loss: 2.271e+08, MSE(e): 9.100e-05, MSE(pi1): 9.689e-02, MSE(pi2): 4.539e-05, MSE(pi3): 3.532e-03\n",
      "Epoch 20000, Train loss: 2.230e+06, Test loss: 2.269e+08, MSE(e): 9.087e-05, MSE(pi1): 9.685e-02, MSE(pi2): 4.531e-05, MSE(pi3): 3.532e-03\n",
      "Epoch 20100, Train loss: 2.229e+06, Test loss: 2.268e+08, MSE(e): 9.073e-05, MSE(pi1): 9.682e-02, MSE(pi2): 4.522e-05, MSE(pi3): 3.531e-03\n",
      "Epoch 20200, Train loss: 2.227e+06, Test loss: 2.266e+08, MSE(e): 9.059e-05, MSE(pi1): 9.678e-02, MSE(pi2): 4.513e-05, MSE(pi3): 3.531e-03\n",
      "Epoch 20300, Train loss: 2.225e+06, Test loss: 2.264e+08, MSE(e): 9.043e-05, MSE(pi1): 9.674e-02, MSE(pi2): 4.504e-05, MSE(pi3): 3.531e-03\n",
      "Epoch 20400, Train loss: 2.223e+06, Test loss: 2.263e+08, MSE(e): 9.028e-05, MSE(pi1): 9.670e-02, MSE(pi2): 4.494e-05, MSE(pi3): 3.531e-03\n",
      "Epoch 20500, Train loss: 2.221e+06, Test loss: 2.261e+08, MSE(e): 9.011e-05, MSE(pi1): 9.665e-02, MSE(pi2): 4.484e-05, MSE(pi3): 3.530e-03\n",
      "Epoch 20600, Train loss: 2.219e+06, Test loss: 2.259e+08, MSE(e): 8.994e-05, MSE(pi1): 9.661e-02, MSE(pi2): 4.473e-05, MSE(pi3): 3.530e-03\n",
      "Epoch 20700, Train loss: 2.216e+06, Test loss: 2.257e+08, MSE(e): 8.976e-05, MSE(pi1): 9.656e-02, MSE(pi2): 4.462e-05, MSE(pi3): 3.530e-03\n",
      "Epoch 20800, Train loss: 2.214e+06, Test loss: 2.255e+08, MSE(e): 8.957e-05, MSE(pi1): 9.651e-02, MSE(pi2): 4.450e-05, MSE(pi3): 3.530e-03\n",
      "Epoch 20900, Train loss: 2.211e+06, Test loss: 2.252e+08, MSE(e): 8.938e-05, MSE(pi1): 9.646e-02, MSE(pi2): 4.438e-05, MSE(pi3): 3.529e-03\n",
      "Epoch 21000, Train loss: 2.209e+06, Test loss: 2.250e+08, MSE(e): 8.917e-05, MSE(pi1): 9.641e-02, MSE(pi2): 4.426e-05, MSE(pi3): 3.529e-03\n",
      "Epoch 21100, Train loss: 2.206e+06, Test loss: 2.248e+08, MSE(e): 8.896e-05, MSE(pi1): 9.635e-02, MSE(pi2): 4.413e-05, MSE(pi3): 3.529e-03\n",
      "Epoch 21200, Train loss: 2.203e+06, Test loss: 2.245e+08, MSE(e): 8.874e-05, MSE(pi1): 9.629e-02, MSE(pi2): 4.399e-05, MSE(pi3): 3.528e-03\n",
      "Epoch 21300, Train loss: 2.200e+06, Test loss: 2.243e+08, MSE(e): 8.851e-05, MSE(pi1): 9.623e-02, MSE(pi2): 4.385e-05, MSE(pi3): 3.528e-03\n",
      "Epoch 21400, Train loss: 2.197e+06, Test loss: 2.240e+08, MSE(e): 8.827e-05, MSE(pi1): 9.617e-02, MSE(pi2): 4.370e-05, MSE(pi3): 3.527e-03\n",
      "Epoch 21500, Train loss: 2.194e+06, Test loss: 2.237e+08, MSE(e): 8.802e-05, MSE(pi1): 9.610e-02, MSE(pi2): 4.355e-05, MSE(pi3): 3.527e-03\n",
      "Epoch 21600, Train loss: 2.191e+06, Test loss: 2.234e+08, MSE(e): 8.776e-05, MSE(pi1): 9.603e-02, MSE(pi2): 4.338e-05, MSE(pi3): 3.527e-03\n",
      "Epoch 21700, Train loss: 2.187e+06, Test loss: 2.231e+08, MSE(e): 8.749e-05, MSE(pi1): 9.596e-02, MSE(pi2): 4.322e-05, MSE(pi3): 3.526e-03\n",
      "Epoch 21800, Train loss: 2.183e+06, Test loss: 2.228e+08, MSE(e): 8.721e-05, MSE(pi1): 9.588e-02, MSE(pi2): 4.305e-05, MSE(pi3): 3.526e-03\n",
      "Epoch 21900, Train loss: 2.180e+06, Test loss: 2.224e+08, MSE(e): 8.692e-05, MSE(pi1): 9.580e-02, MSE(pi2): 4.287e-05, MSE(pi3): 3.525e-03\n",
      "Epoch 22000, Train loss: 2.176e+06, Test loss: 2.221e+08, MSE(e): 8.662e-05, MSE(pi1): 9.572e-02, MSE(pi2): 4.268e-05, MSE(pi3): 3.525e-03\n",
      "Epoch 22100, Train loss: 2.172e+06, Test loss: 2.217e+08, MSE(e): 8.631e-05, MSE(pi1): 9.564e-02, MSE(pi2): 4.249e-05, MSE(pi3): 3.524e-03\n",
      "Epoch 22200, Train loss: 2.168e+06, Test loss: 2.214e+08, MSE(e): 8.599e-05, MSE(pi1): 9.555e-02, MSE(pi2): 4.229e-05, MSE(pi3): 3.523e-03\n",
      "Epoch 22300, Train loss: 2.163e+06, Test loss: 2.210e+08, MSE(e): 8.565e-05, MSE(pi1): 9.547e-02, MSE(pi2): 4.208e-05, MSE(pi3): 3.523e-03\n",
      "Epoch 22400, Train loss: 2.159e+06, Test loss: 2.206e+08, MSE(e): 8.530e-05, MSE(pi1): 9.537e-02, MSE(pi2): 4.187e-05, MSE(pi3): 3.522e-03\n",
      "Epoch 22500, Train loss: 2.154e+06, Test loss: 2.202e+08, MSE(e): 8.494e-05, MSE(pi1): 9.527e-02, MSE(pi2): 4.164e-05, MSE(pi3): 3.522e-03\n",
      "Epoch 22600, Train loss: 2.150e+06, Test loss: 2.197e+08, MSE(e): 8.457e-05, MSE(pi1): 9.517e-02, MSE(pi2): 4.142e-05, MSE(pi3): 3.521e-03\n",
      "Epoch 22700, Train loss: 2.146e+06, Test loss: 2.209e+08, MSE(e): 8.431e-05, MSE(pi1): 9.498e-02, MSE(pi2): 4.128e-05, MSE(pi3): 3.528e-03\n",
      "Epoch 22800, Train loss: 2.140e+06, Test loss: 2.189e+08, MSE(e): 8.384e-05, MSE(pi1): 9.497e-02, MSE(pi2): 4.096e-05, MSE(pi3): 3.520e-03\n",
      "Epoch 22900, Train loss: 2.141e+06, Test loss: 2.159e+08, MSE(e): 8.406e-05, MSE(pi1): 9.509e-02, MSE(pi2): 4.086e-05, MSE(pi3): 3.500e-03\n",
      "Epoch 23000, Train loss: 2.131e+06, Test loss: 2.181e+08, MSE(e): 8.313e-05, MSE(pi1): 9.477e-02, MSE(pi2): 4.052e-05, MSE(pi3): 3.519e-03\n",
      "Epoch 23100, Train loss: 2.140e+06, Test loss: 2.181e+08, MSE(e): 8.422e-05, MSE(pi1): 9.435e-02, MSE(pi2): 4.108e-05, MSE(pi3): 3.547e-03\n",
      "Epoch 23200, Train loss: 2.122e+06, Test loss: 2.173e+08, MSE(e): 8.244e-05, MSE(pi1): 9.458e-02, MSE(pi2): 4.010e-05, MSE(pi3): 3.517e-03\n",
      "Epoch 23300, Train loss: 2.118e+06, Test loss: 2.169e+08, MSE(e): 8.210e-05, MSE(pi1): 9.449e-02, MSE(pi2): 3.989e-05, MSE(pi3): 3.517e-03\n",
      "Epoch 23400, Train loss: 2.113e+06, Test loss: 2.165e+08, MSE(e): 8.177e-05, MSE(pi1): 9.440e-02, MSE(pi2): 3.968e-05, MSE(pi3): 3.516e-03\n",
      "Epoch 23500, Train loss: 2.109e+06, Test loss: 2.162e+08, MSE(e): 8.144e-05, MSE(pi1): 9.431e-02, MSE(pi2): 3.948e-05, MSE(pi3): 3.516e-03\n",
      "Epoch 23600, Train loss: 2.105e+06, Test loss: 2.159e+08, MSE(e): 8.113e-05, MSE(pi1): 9.422e-02, MSE(pi2): 3.929e-05, MSE(pi3): 3.516e-03\n",
      "Epoch 23700, Train loss: 2.101e+06, Test loss: 2.154e+08, MSE(e): 8.081e-05, MSE(pi1): 9.414e-02, MSE(pi2): 3.909e-05, MSE(pi3): 3.514e-03\n",
      "Epoch 23800, Train loss: 2.097e+06, Test loss: 2.149e+08, MSE(e): 8.050e-05, MSE(pi1): 9.406e-02, MSE(pi2): 3.889e-05, MSE(pi3): 3.513e-03\n",
      "Epoch 23900, Train loss: 2.093e+06, Test loss: 2.147e+08, MSE(e): 8.019e-05, MSE(pi1): 9.397e-02, MSE(pi2): 3.871e-05, MSE(pi3): 3.513e-03\n",
      "Epoch 24000, Train loss: 2.090e+06, Test loss: 2.148e+08, MSE(e): 7.995e-05, MSE(pi1): 9.382e-02, MSE(pi2): 3.859e-05, MSE(pi3): 3.518e-03\n",
      "Epoch 24100, Train loss: 2.085e+06, Test loss: 2.141e+08, MSE(e): 7.960e-05, MSE(pi1): 9.380e-02, MSE(pi2): 3.834e-05, MSE(pi3): 3.512e-03\n",
      "Epoch 24200, Train loss: 2.082e+06, Test loss: 2.150e+08, MSE(e): 7.942e-05, MSE(pi1): 9.363e-02, MSE(pi2): 3.826e-05, MSE(pi3): 3.520e-03\n",
      "Epoch 24300, Train loss: 2.078e+06, Test loss: 2.134e+08, MSE(e): 7.902e-05, MSE(pi1): 9.364e-02, MSE(pi2): 3.799e-05, MSE(pi3): 3.511e-03\n",
      "Epoch 24400, Train loss: 2.117e+06, Test loss: 2.089e+08, MSE(e): 8.292e-05, MSE(pi1): 9.414e-02, MSE(pi2): 3.919e-05, MSE(pi3): 3.461e-03\n",
      "Epoch 24500, Train loss: 2.070e+06, Test loss: 2.128e+08, MSE(e): 7.845e-05, MSE(pi1): 9.348e-02, MSE(pi2): 3.764e-05, MSE(pi3): 3.510e-03\n",
      "Epoch 24600, Train loss: 2.075e+06, Test loss: 2.138e+08, MSE(e): 7.901e-05, MSE(pi1): 9.316e-02, MSE(pi2): 3.796e-05, MSE(pi3): 3.531e-03\n",
      "Epoch 24700, Train loss: 2.063e+06, Test loss: 2.122e+08, MSE(e): 7.791e-05, MSE(pi1): 9.333e-02, MSE(pi2): 3.730e-05, MSE(pi3): 3.509e-03\n",
      "Epoch 24800, Train loss: 2.060e+06, Test loss: 2.119e+08, MSE(e): 7.764e-05, MSE(pi1): 9.325e-02, MSE(pi2): 3.713e-05, MSE(pi3): 3.509e-03\n",
      "Epoch 24900, Train loss: 2.056e+06, Test loss: 2.115e+08, MSE(e): 7.738e-05, MSE(pi1): 9.319e-02, MSE(pi2): 3.697e-05, MSE(pi3): 3.507e-03\n",
      "Epoch 25000, Train loss: 2.053e+06, Test loss: 2.113e+08, MSE(e): 7.713e-05, MSE(pi1): 9.311e-02, MSE(pi2): 3.682e-05, MSE(pi3): 3.507e-03\n",
      "Epoch 25100, Train loss: 2.051e+06, Test loss: 2.110e+08, MSE(e): 7.700e-05, MSE(pi1): 9.314e-02, MSE(pi2): 3.664e-05, MSE(pi3): 3.498e-03\n",
      "Epoch 25200, Train loss: 2.047e+06, Test loss: 2.107e+08, MSE(e): 7.662e-05, MSE(pi1): 9.297e-02, MSE(pi2): 3.650e-05, MSE(pi3): 3.506e-03\n",
      "Epoch 25300, Train loss: 2.047e+06, Test loss: 2.098e+08, MSE(e): 7.681e-05, MSE(pi1): 9.271e-02, MSE(pi2): 3.664e-05, MSE(pi3): 3.522e-03\n",
      "Epoch 25400, Train loss: 2.040e+06, Test loss: 2.102e+08, MSE(e): 7.613e-05, MSE(pi1): 9.282e-02, MSE(pi2): 3.620e-05, MSE(pi3): 3.506e-03\n",
      "Epoch 25500, Train loss: 2.037e+06, Test loss: 2.100e+08, MSE(e): 7.589e-05, MSE(pi1): 9.275e-02, MSE(pi2): 3.605e-05, MSE(pi3): 3.505e-03\n",
      "Epoch 25600, Train loss: 2.034e+06, Test loss: 2.097e+08, MSE(e): 7.566e-05, MSE(pi1): 9.267e-02, MSE(pi2): 3.591e-05, MSE(pi3): 3.505e-03\n",
      "Epoch 25700, Train loss: 2.031e+06, Test loss: 2.094e+08, MSE(e): 7.542e-05, MSE(pi1): 9.262e-02, MSE(pi2): 3.576e-05, MSE(pi3): 3.504e-03\n",
      "Epoch 25800, Train loss: 2.028e+06, Test loss: 2.095e+08, MSE(e): 7.522e-05, MSE(pi1): 9.250e-02, MSE(pi2): 3.566e-05, MSE(pi3): 3.508e-03\n",
      "Epoch 25900, Train loss: 2.025e+06, Test loss: 2.089e+08, MSE(e): 7.497e-05, MSE(pi1): 9.248e-02, MSE(pi2): 3.548e-05, MSE(pi3): 3.503e-03\n",
      "Epoch 26000, Train loss: 2.024e+06, Test loss: 2.075e+08, MSE(e): 7.494e-05, MSE(pi1): 9.254e-02, MSE(pi2): 3.534e-05, MSE(pi3): 3.492e-03\n",
      "Epoch 26100, Train loss: 2.019e+06, Test loss: 2.084e+08, MSE(e): 7.452e-05, MSE(pi1): 9.236e-02, MSE(pi2): 3.520e-05, MSE(pi3): 3.502e-03\n",
      "Epoch 26200, Train loss: 2.038e+06, Test loss: 2.045e+08, MSE(e): 7.643e-05, MSE(pi1): 9.270e-02, MSE(pi2): 3.569e-05, MSE(pi3): 3.467e-03\n",
      "Epoch 26300, Train loss: 2.013e+06, Test loss: 2.080e+08, MSE(e): 7.409e-05, MSE(pi1): 9.222e-02, MSE(pi2): 3.494e-05, MSE(pi3): 3.502e-03\n",
      "Epoch 26400, Train loss: 2.011e+06, Test loss: 2.078e+08, MSE(e): 7.388e-05, MSE(pi1): 9.216e-02, MSE(pi2): 3.481e-05, MSE(pi3): 3.501e-03\n",
      "Epoch 26500, Train loss: 2.008e+06, Test loss: 2.074e+08, MSE(e): 7.367e-05, MSE(pi1): 9.211e-02, MSE(pi2): 3.467e-05, MSE(pi3): 3.500e-03\n",
      "Epoch 26600, Train loss: 2.005e+06, Test loss: 2.076e+08, MSE(e): 7.348e-05, MSE(pi1): 9.201e-02, MSE(pi2): 3.458e-05, MSE(pi3): 3.503e-03\n",
      "Epoch 26700, Train loss: 2.002e+06, Test loss: 2.071e+08, MSE(e): 7.326e-05, MSE(pi1): 9.198e-02, MSE(pi2): 3.442e-05, MSE(pi3): 3.500e-03\n",
      "Epoch 26800, Train loss: 2.000e+06, Test loss: 2.068e+08, MSE(e): 7.306e-05, MSE(pi1): 9.191e-02, MSE(pi2): 3.430e-05, MSE(pi3): 3.500e-03\n",
      "Epoch 26900, Train loss: 1.997e+06, Test loss: 2.066e+08, MSE(e): 7.287e-05, MSE(pi1): 9.184e-02, MSE(pi2): 3.419e-05, MSE(pi3): 3.501e-03\n",
      "Epoch 27000, Train loss: 1.995e+06, Test loss: 2.064e+08, MSE(e): 7.267e-05, MSE(pi1): 9.180e-02, MSE(pi2): 3.406e-05, MSE(pi3): 3.499e-03\n",
      "Epoch 27100, Train loss: 1.995e+06, Test loss: 2.076e+08, MSE(e): 7.283e-05, MSE(pi1): 9.158e-02, MSE(pi2): 3.419e-05, MSE(pi3): 3.512e-03\n",
      "Epoch 27200, Train loss: 1.989e+06, Test loss: 2.060e+08, MSE(e): 7.229e-05, MSE(pi1): 9.168e-02, MSE(pi2): 3.382e-05, MSE(pi3): 3.498e-03\n",
      "Epoch 27300, Train loss: 1.987e+06, Test loss: 2.062e+08, MSE(e): 7.213e-05, MSE(pi1): 9.158e-02, MSE(pi2): 3.375e-05, MSE(pi3): 3.501e-03\n",
      "Epoch 27400, Train loss: 1.985e+06, Test loss: 2.056e+08, MSE(e): 7.192e-05, MSE(pi1): 9.156e-02, MSE(pi2): 3.360e-05, MSE(pi3): 3.498e-03\n",
      "Epoch 27500, Train loss: 1.982e+06, Test loss: 2.054e+08, MSE(e): 7.173e-05, MSE(pi1): 9.151e-02, MSE(pi2): 3.348e-05, MSE(pi3): 3.497e-03\n",
      "Epoch 27600, Train loss: 1.980e+06, Test loss: 2.049e+08, MSE(e): 7.158e-05, MSE(pi1): 9.150e-02, MSE(pi2): 3.335e-05, MSE(pi3): 3.492e-03\n",
      "Epoch 27700, Train loss: 1.977e+06, Test loss: 2.050e+08, MSE(e): 7.138e-05, MSE(pi1): 9.140e-02, MSE(pi2): 3.326e-05, MSE(pi3): 3.496e-03\n",
      "Epoch 27800, Train loss: 1.975e+06, Test loss: 2.061e+08, MSE(e): 7.122e-05, MSE(pi1): 9.131e-02, MSE(pi2): 3.318e-05, MSE(pi3): 3.498e-03\n",
      "Epoch 27900, Train loss: 1.973e+06, Test loss: 2.046e+08, MSE(e): 7.103e-05, MSE(pi1): 9.129e-02, MSE(pi2): 3.305e-05, MSE(pi3): 3.495e-03\n",
      "Epoch 28000, Train loss: 1.970e+06, Test loss: 2.044e+08, MSE(e): 7.086e-05, MSE(pi1): 9.124e-02, MSE(pi2): 3.294e-05, MSE(pi3): 3.495e-03\n",
      "Epoch 28100, Train loss: 1.968e+06, Test loss: 2.038e+08, MSE(e): 7.070e-05, MSE(pi1): 9.122e-02, MSE(pi2): 3.282e-05, MSE(pi3): 3.491e-03\n",
      "Epoch 28200, Train loss: 1.966e+06, Test loss: 2.041e+08, MSE(e): 7.053e-05, MSE(pi1): 9.113e-02, MSE(pi2): 3.273e-05, MSE(pi3): 3.494e-03\n",
      "Epoch 28300, Train loss: 1.967e+06, Test loss: 2.027e+08, MSE(e): 7.068e-05, MSE(pi1): 9.124e-02, MSE(pi2): 3.266e-05, MSE(pi3): 3.480e-03\n",
      "Epoch 28400, Train loss: 1.962e+06, Test loss: 2.037e+08, MSE(e): 7.020e-05, MSE(pi1): 9.103e-02, MSE(pi2): 3.253e-05, MSE(pi3): 3.493e-03\n",
      "Epoch 28500, Train loss: 1.959e+06, Test loss: 2.036e+08, MSE(e): 7.004e-05, MSE(pi1): 9.097e-02, MSE(pi2): 3.244e-05, MSE(pi3): 3.493e-03\n",
      "Epoch 28600, Train loss: 1.957e+06, Test loss: 2.034e+08, MSE(e): 6.988e-05, MSE(pi1): 9.091e-02, MSE(pi2): 3.234e-05, MSE(pi3): 3.494e-03\n",
      "Epoch 28700, Train loss: 1.955e+06, Test loss: 2.032e+08, MSE(e): 6.972e-05, MSE(pi1): 9.087e-02, MSE(pi2): 3.224e-05, MSE(pi3): 3.492e-03\n",
      "Epoch 28800, Train loss: 1.953e+06, Test loss: 2.032e+08, MSE(e): 6.957e-05, MSE(pi1): 9.081e-02, MSE(pi2): 3.215e-05, MSE(pi3): 3.493e-03\n",
      "Epoch 28900, Train loss: 1.951e+06, Test loss: 2.028e+08, MSE(e): 6.942e-05, MSE(pi1): 9.078e-02, MSE(pi2): 3.205e-05, MSE(pi3): 3.491e-03\n",
      "Epoch 29000, Train loss: 1.949e+06, Test loss: 2.026e+08, MSE(e): 6.927e-05, MSE(pi1): 9.074e-02, MSE(pi2): 3.194e-05, MSE(pi3): 3.490e-03\n",
      "Epoch 29100, Train loss: 1.947e+06, Test loss: 2.026e+08, MSE(e): 6.912e-05, MSE(pi1): 9.066e-02, MSE(pi2): 3.187e-05, MSE(pi3): 3.492e-03\n",
      "Epoch 29200, Train loss: 1.945e+06, Test loss: 2.024e+08, MSE(e): 6.897e-05, MSE(pi1): 9.062e-02, MSE(pi2): 3.177e-05, MSE(pi3): 3.491e-03\n",
      "Epoch 29300, Train loss: 1.943e+06, Test loss: 2.021e+08, MSE(e): 6.882e-05, MSE(pi1): 9.059e-02, MSE(pi2): 3.167e-05, MSE(pi3): 3.489e-03\n",
      "Epoch 29400, Train loss: 1.941e+06, Test loss: 2.021e+08, MSE(e): 6.868e-05, MSE(pi1): 9.052e-02, MSE(pi2): 3.160e-05, MSE(pi3): 3.491e-03\n",
      "Epoch 29500, Train loss: 1.939e+06, Test loss: 2.019e+08, MSE(e): 6.854e-05, MSE(pi1): 9.048e-02, MSE(pi2): 3.151e-05, MSE(pi3): 3.490e-03\n",
      "Epoch 29600, Train loss: 1.937e+06, Test loss: 2.014e+08, MSE(e): 6.840e-05, MSE(pi1): 9.046e-02, MSE(pi2): 3.140e-05, MSE(pi3): 3.487e-03\n",
      "Epoch 29700, Train loss: 1.935e+06, Test loss: 2.016e+08, MSE(e): 6.826e-05, MSE(pi1): 9.039e-02, MSE(pi2): 3.133e-05, MSE(pi3): 3.489e-03\n",
      "Epoch 29800, Train loss: 1.933e+06, Test loss: 2.017e+08, MSE(e): 6.812e-05, MSE(pi1): 9.036e-02, MSE(pi2): 3.124e-05, MSE(pi3): 3.487e-03\n",
      "Epoch 29900, Train loss: 1.932e+06, Test loss: 2.013e+08, MSE(e): 6.798e-05, MSE(pi1): 9.030e-02, MSE(pi2): 3.116e-05, MSE(pi3): 3.488e-03\n",
      "Epoch 30000, Train loss: 1.930e+06, Test loss: 2.012e+08, MSE(e): 6.787e-05, MSE(pi1): 9.030e-02, MSE(pi2): 3.106e-05, MSE(pi3): 3.484e-03\n",
      "Epoch 30100, Train loss: 1.928e+06, Test loss: 2.010e+08, MSE(e): 6.772e-05, MSE(pi1): 9.021e-02, MSE(pi2): 3.100e-05, MSE(pi3): 3.488e-03\n",
      "Epoch 30200, Train loss: 1.930e+06, Test loss: 2.001e+08, MSE(e): 6.791e-05, MSE(pi1): 9.032e-02, MSE(pi2): 3.095e-05, MSE(pi3): 3.473e-03\n",
      "Epoch 30300, Train loss: 1.924e+06, Test loss: 2.007e+08, MSE(e): 6.746e-05, MSE(pi1): 9.012e-02, MSE(pi2): 3.084e-05, MSE(pi3): 3.487e-03\n",
      "Epoch 30400, Train loss: 1.928e+06, Test loss: 2.007e+08, MSE(e): 6.790e-05, MSE(pi1): 8.987e-02, MSE(pi2): 3.113e-05, MSE(pi3): 3.504e-03\n",
      "Epoch 30500, Train loss: 1.921e+06, Test loss: 2.005e+08, MSE(e): 6.721e-05, MSE(pi1): 9.002e-02, MSE(pi2): 3.068e-05, MSE(pi3): 3.487e-03\n",
      "Epoch 30600, Train loss: 1.920e+06, Test loss: 2.013e+08, MSE(e): 6.717e-05, MSE(pi1): 8.991e-02, MSE(pi2): 3.069e-05, MSE(pi3): 3.493e-03\n",
      "Epoch 30700, Train loss: 1.918e+06, Test loss: 2.002e+08, MSE(e): 6.696e-05, MSE(pi1): 8.994e-02, MSE(pi2): 3.053e-05, MSE(pi3): 3.486e-03\n",
      "Epoch 30800, Train loss: 1.919e+06, Test loss: 2.010e+08, MSE(e): 6.713e-05, MSE(pi1): 8.975e-02, MSE(pi2): 3.067e-05, MSE(pi3): 3.498e-03\n",
      "Epoch 30900, Train loss: 1.914e+06, Test loss: 1.999e+08, MSE(e): 6.671e-05, MSE(pi1): 8.986e-02, MSE(pi2): 3.038e-05, MSE(pi3): 3.485e-03\n",
      "Epoch 31000, Train loss: 1.916e+06, Test loss: 1.984e+08, MSE(e): 6.693e-05, MSE(pi1): 8.998e-02, MSE(pi2): 3.034e-05, MSE(pi3): 3.471e-03\n",
      "Epoch 31100, Train loss: 1.911e+06, Test loss: 1.996e+08, MSE(e): 6.648e-05, MSE(pi1): 8.977e-02, MSE(pi2): 3.024e-05, MSE(pi3): 3.485e-03\n",
      "Epoch 31200, Train loss: 1.911e+06, Test loss: 1.984e+08, MSE(e): 6.654e-05, MSE(pi1): 8.985e-02, MSE(pi2): 3.016e-05, MSE(pi3): 3.474e-03\n",
      "Epoch 31300, Train loss: 1.908e+06, Test loss: 1.993e+08, MSE(e): 6.625e-05, MSE(pi1): 8.969e-02, MSE(pi2): 3.009e-05, MSE(pi3): 3.484e-03\n",
      "Epoch 31400, Train loss: 1.921e+06, Test loss: 1.962e+08, MSE(e): 6.758e-05, MSE(pi1): 8.998e-02, MSE(pi2): 3.040e-05, MSE(pi3): 3.455e-03\n",
      "Epoch 31500, Train loss: 1.905e+06, Test loss: 1.992e+08, MSE(e): 6.602e-05, MSE(pi1): 8.960e-02, MSE(pi2): 2.996e-05, MSE(pi3): 3.484e-03\n",
      "Epoch 31600, Train loss: 1.903e+06, Test loss: 1.990e+08, MSE(e): 6.591e-05, MSE(pi1): 8.956e-02, MSE(pi2): 2.988e-05, MSE(pi3): 3.483e-03\n",
      "Epoch 31700, Train loss: 1.902e+06, Test loss: 1.987e+08, MSE(e): 6.580e-05, MSE(pi1): 8.955e-02, MSE(pi2): 2.981e-05, MSE(pi3): 3.481e-03\n",
      "Epoch 31800, Train loss: 1.900e+06, Test loss: 1.988e+08, MSE(e): 6.569e-05, MSE(pi1): 8.948e-02, MSE(pi2): 2.975e-05, MSE(pi3): 3.483e-03\n",
      "Epoch 31900, Train loss: 1.899e+06, Test loss: 1.988e+08, MSE(e): 6.560e-05, MSE(pi1): 8.941e-02, MSE(pi2): 2.971e-05, MSE(pi3): 3.485e-03\n",
      "Epoch 32000, Train loss: 1.897e+06, Test loss: 1.986e+08, MSE(e): 6.548e-05, MSE(pi1): 8.940e-02, MSE(pi2): 2.962e-05, MSE(pi3): 3.482e-03\n",
      "Epoch 32100, Train loss: 1.896e+06, Test loss: 1.986e+08, MSE(e): 6.538e-05, MSE(pi1): 8.935e-02, MSE(pi2): 2.956e-05, MSE(pi3): 3.483e-03\n",
      "Epoch 32200, Train loss: 1.894e+06, Test loss: 1.979e+08, MSE(e): 6.529e-05, MSE(pi1): 8.937e-02, MSE(pi2): 2.947e-05, MSE(pi3): 3.477e-03\n",
      "Epoch 32300, Train loss: 1.893e+06, Test loss: 1.982e+08, MSE(e): 6.517e-05, MSE(pi1): 8.928e-02, MSE(pi2): 2.943e-05, MSE(pi3): 3.482e-03\n",
      "Epoch 32400, Train loss: 1.891e+06, Test loss: 1.998e+08, MSE(e): 6.510e-05, MSE(pi1): 8.920e-02, MSE(pi2): 2.941e-05, MSE(pi3): 3.485e-03\n",
      "Epoch 32500, Train loss: 1.890e+06, Test loss: 1.979e+08, MSE(e): 6.496e-05, MSE(pi1): 8.921e-02, MSE(pi2): 2.930e-05, MSE(pi3): 3.480e-03\n",
      "Epoch 32600, Train loss: 1.903e+06, Test loss: 1.994e+08, MSE(e): 6.637e-05, MSE(pi1): 8.884e-02, MSE(pi2): 3.008e-05, MSE(pi3): 3.509e-03\n",
      "Epoch 32700, Train loss: 1.887e+06, Test loss: 1.977e+08, MSE(e): 6.477e-05, MSE(pi1): 8.913e-02, MSE(pi2): 2.918e-05, MSE(pi3): 3.480e-03\n",
      "Epoch 32800, Train loss: 1.886e+06, Test loss: 1.968e+08, MSE(e): 6.467e-05, MSE(pi1): 8.912e-02, MSE(pi2): 2.911e-05, MSE(pi3): 3.478e-03\n",
      "Epoch 32900, Train loss: 1.884e+06, Test loss: 1.975e+08, MSE(e): 6.457e-05, MSE(pi1): 8.906e-02, MSE(pi2): 2.906e-05, MSE(pi3): 3.480e-03\n",
      "Epoch 33000, Train loss: 1.884e+06, Test loss: 1.981e+08, MSE(e): 6.459e-05, MSE(pi1): 8.912e-02, MSE(pi2): 2.899e-05, MSE(pi3): 3.471e-03\n",
      "Epoch 33100, Train loss: 1.882e+06, Test loss: 1.973e+08, MSE(e): 6.439e-05, MSE(pi1): 8.898e-02, MSE(pi2): 2.895e-05, MSE(pi3): 3.479e-03\n",
      "Epoch 33200, Train loss: 1.880e+06, Test loss: 1.972e+08, MSE(e): 6.429e-05, MSE(pi1): 8.895e-02, MSE(pi2): 2.889e-05, MSE(pi3): 3.479e-03\n",
      "Epoch 33300, Train loss: 1.879e+06, Test loss: 1.969e+08, MSE(e): 6.421e-05, MSE(pi1): 8.894e-02, MSE(pi2): 2.882e-05, MSE(pi3): 3.476e-03\n",
      "Epoch 33400, Train loss: 1.878e+06, Test loss: 1.970e+08, MSE(e): 6.411e-05, MSE(pi1): 8.887e-02, MSE(pi2): 2.878e-05, MSE(pi3): 3.478e-03\n",
      "Epoch 33500, Train loss: 1.876e+06, Test loss: 1.970e+08, MSE(e): 6.403e-05, MSE(pi1): 8.882e-02, MSE(pi2): 2.874e-05, MSE(pi3): 3.480e-03\n",
      "Epoch 33600, Train loss: 1.875e+06, Test loss: 1.968e+08, MSE(e): 6.393e-05, MSE(pi1): 8.880e-02, MSE(pi2): 2.867e-05, MSE(pi3): 3.478e-03\n",
      "Epoch 33700, Train loss: 1.876e+06, Test loss: 1.975e+08, MSE(e): 6.408e-05, MSE(pi1): 8.864e-02, MSE(pi2): 2.880e-05, MSE(pi3): 3.489e-03\n",
      "Epoch 33800, Train loss: 1.873e+06, Test loss: 1.966e+08, MSE(e): 6.376e-05, MSE(pi1): 8.873e-02, MSE(pi2): 2.856e-05, MSE(pi3): 3.477e-03\n",
      "Epoch 33900, Train loss: 1.871e+06, Test loss: 1.968e+08, MSE(e): 6.367e-05, MSE(pi1): 8.869e-02, MSE(pi2): 2.852e-05, MSE(pi3): 3.478e-03\n",
      "Epoch 34000, Train loss: 1.870e+06, Test loss: 1.964e+08, MSE(e): 6.359e-05, MSE(pi1): 8.866e-02, MSE(pi2): 2.846e-05, MSE(pi3): 3.477e-03\n",
      "Epoch 34100, Train loss: 1.869e+06, Test loss: 1.962e+08, MSE(e): 6.353e-05, MSE(pi1): 8.867e-02, MSE(pi2): 2.839e-05, MSE(pi3): 3.472e-03\n",
      "Epoch 34200, Train loss: 1.868e+06, Test loss: 1.962e+08, MSE(e): 6.342e-05, MSE(pi1): 8.859e-02, MSE(pi2): 2.836e-05, MSE(pi3): 3.476e-03\n",
      "Epoch 34300, Train loss: 1.867e+06, Test loss: 1.962e+08, MSE(e): 6.334e-05, MSE(pi1): 8.857e-02, MSE(pi2): 2.830e-05, MSE(pi3): 3.475e-03\n",
      "Epoch 34400, Train loss: 1.865e+06, Test loss: 1.961e+08, MSE(e): 6.325e-05, MSE(pi1): 8.852e-02, MSE(pi2): 2.826e-05, MSE(pi3): 3.476e-03\n",
      "Epoch 34500, Train loss: 1.864e+06, Test loss: 1.959e+08, MSE(e): 6.318e-05, MSE(pi1): 8.847e-02, MSE(pi2): 2.822e-05, MSE(pi3): 3.477e-03\n",
      "Epoch 34600, Train loss: 1.863e+06, Test loss: 1.959e+08, MSE(e): 6.309e-05, MSE(pi1): 8.845e-02, MSE(pi2): 2.816e-05, MSE(pi3): 3.475e-03\n",
      "Epoch 34700, Train loss: 1.862e+06, Test loss: 1.959e+08, MSE(e): 6.301e-05, MSE(pi1): 8.842e-02, MSE(pi2): 2.810e-05, MSE(pi3): 3.474e-03\n",
      "Epoch 34800, Train loss: 1.861e+06, Test loss: 1.957e+08, MSE(e): 6.294e-05, MSE(pi1): 8.838e-02, MSE(pi2): 2.806e-05, MSE(pi3): 3.475e-03\n",
      "Epoch 34900, Train loss: 1.862e+06, Test loss: 1.946e+08, MSE(e): 6.308e-05, MSE(pi1): 8.849e-02, MSE(pi2): 2.802e-05, MSE(pi3): 3.463e-03\n",
      "Epoch 35000, Train loss: 1.858e+06, Test loss: 1.955e+08, MSE(e): 6.278e-05, MSE(pi1): 8.832e-02, MSE(pi2): 2.797e-05, MSE(pi3): 3.474e-03\n",
      "Epoch 35100, Train loss: 1.897e+06, Test loss: 1.910e+08, MSE(e): 6.656e-05, MSE(pi1): 8.883e-02, MSE(pi2): 2.917e-05, MSE(pi3): 3.428e-03\n",
      "Epoch 35200, Train loss: 1.856e+06, Test loss: 1.953e+08, MSE(e): 6.263e-05, MSE(pi1): 8.826e-02, MSE(pi2): 2.787e-05, MSE(pi3): 3.473e-03\n",
      "Epoch 35300, Train loss: 1.856e+06, Test loss: 1.944e+08, MSE(e): 6.268e-05, MSE(pi1): 8.832e-02, MSE(pi2): 2.782e-05, MSE(pi3): 3.465e-03\n",
      "Epoch 35400, Train loss: 1.854e+06, Test loss: 1.952e+08, MSE(e): 6.249e-05, MSE(pi1): 8.818e-02, MSE(pi2): 2.779e-05, MSE(pi3): 3.474e-03\n",
      "Epoch 35500, Train loss: 1.873e+06, Test loss: 1.922e+08, MSE(e): 6.431e-05, MSE(pi1): 8.854e-02, MSE(pi2): 2.829e-05, MSE(pi3): 3.440e-03\n",
      "Epoch 35600, Train loss: 1.852e+06, Test loss: 1.950e+08, MSE(e): 6.234e-05, MSE(pi1): 8.812e-02, MSE(pi2): 2.770e-05, MSE(pi3): 3.473e-03\n",
      "Epoch 35700, Train loss: 1.873e+06, Test loss: 1.973e+08, MSE(e): 6.453e-05, MSE(pi1): 8.768e-02, MSE(pi2): 2.884e-05, MSE(pi3): 3.508e-03\n",
      "Epoch 35800, Train loss: 1.850e+06, Test loss: 1.949e+08, MSE(e): 6.220e-05, MSE(pi1): 8.805e-02, MSE(pi2): 2.761e-05, MSE(pi3): 3.472e-03\n",
      "Epoch 35900, Train loss: 1.872e+06, Test loss: 1.975e+08, MSE(e): 6.446e-05, MSE(pi1): 8.761e-02, MSE(pi2): 2.879e-05, MSE(pi3): 3.508e-03\n",
      "Epoch 36000, Train loss: 1.848e+06, Test loss: 1.947e+08, MSE(e): 6.206e-05, MSE(pi1): 8.799e-02, MSE(pi2): 2.753e-05, MSE(pi3): 3.472e-03\n",
      "Epoch 36100, Train loss: 1.849e+06, Test loss: 1.945e+08, MSE(e): 6.224e-05, MSE(pi1): 8.783e-02, MSE(pi2): 2.767e-05, MSE(pi3): 3.483e-03\n",
      "Epoch 36200, Train loss: 1.846e+06, Test loss: 1.945e+08, MSE(e): 6.192e-05, MSE(pi1): 8.792e-02, MSE(pi2): 2.745e-05, MSE(pi3): 3.471e-03\n",
      "Epoch 36300, Train loss: 1.846e+06, Test loss: 1.953e+08, MSE(e): 6.197e-05, MSE(pi1): 8.781e-02, MSE(pi2): 2.751e-05, MSE(pi3): 3.479e-03\n",
      "Epoch 36400, Train loss: 1.844e+06, Test loss: 1.944e+08, MSE(e): 6.179e-05, MSE(pi1): 8.786e-02, MSE(pi2): 2.736e-05, MSE(pi3): 3.471e-03\n",
      "Epoch 36500, Train loss: 1.843e+06, Test loss: 1.950e+08, MSE(e): 6.181e-05, MSE(pi1): 8.776e-02, MSE(pi2): 2.741e-05, MSE(pi3): 3.477e-03\n",
      "Epoch 36600, Train loss: 1.842e+06, Test loss: 1.942e+08, MSE(e): 6.166e-05, MSE(pi1): 8.780e-02, MSE(pi2): 2.729e-05, MSE(pi3): 3.470e-03\n",
      "Epoch 36700, Train loss: 1.841e+06, Test loss: 1.941e+08, MSE(e): 6.160e-05, MSE(pi1): 8.778e-02, MSE(pi2): 2.724e-05, MSE(pi3): 3.470e-03\n",
      "Epoch 36800, Train loss: 1.840e+06, Test loss: 1.940e+08, MSE(e): 6.153e-05, MSE(pi1): 8.775e-02, MSE(pi2): 2.720e-05, MSE(pi3): 3.469e-03\n",
      "Epoch 36900, Train loss: 1.839e+06, Test loss: 1.940e+08, MSE(e): 6.147e-05, MSE(pi1): 8.771e-02, MSE(pi2): 2.717e-05, MSE(pi3): 3.470e-03\n",
      "Epoch 37000, Train loss: 1.838e+06, Test loss: 1.941e+08, MSE(e): 6.141e-05, MSE(pi1): 8.767e-02, MSE(pi2): 2.715e-05, MSE(pi3): 3.471e-03\n",
      "Epoch 37100, Train loss: 1.837e+06, Test loss: 1.939e+08, MSE(e): 6.135e-05, MSE(pi1): 8.765e-02, MSE(pi2): 2.710e-05, MSE(pi3): 3.469e-03\n",
      "Epoch 37200, Train loss: 1.836e+06, Test loss: 1.938e+08, MSE(e): 6.129e-05, MSE(pi1): 8.764e-02, MSE(pi2): 2.705e-05, MSE(pi3): 3.467e-03\n",
      "Epoch 37300, Train loss: 1.835e+06, Test loss: 1.937e+08, MSE(e): 6.123e-05, MSE(pi1): 8.759e-02, MSE(pi2): 2.702e-05, MSE(pi3): 3.469e-03\n",
      "Epoch 37400, Train loss: 1.843e+06, Test loss: 1.947e+08, MSE(e): 6.206e-05, MSE(pi1): 8.731e-02, MSE(pi2): 2.751e-05, MSE(pi3): 3.491e-03\n",
      "Epoch 37500, Train loss: 1.833e+06, Test loss: 1.936e+08, MSE(e): 6.111e-05, MSE(pi1): 8.753e-02, MSE(pi2): 2.695e-05, MSE(pi3): 3.469e-03\n",
      "Epoch 37600, Train loss: 1.874e+06, Test loss: 1.907e+08, MSE(e): 6.516e-05, MSE(pi1): 8.806e-02, MSE(pi2): 2.828e-05, MSE(pi3): 3.421e-03\n",
      "Epoch 37700, Train loss: 1.831e+06, Test loss: 1.934e+08, MSE(e): 6.099e-05, MSE(pi1): 8.747e-02, MSE(pi2): 2.688e-05, MSE(pi3): 3.468e-03\n",
      "Epoch 37800, Train loss: 1.839e+06, Test loss: 1.911e+08, MSE(e): 6.179e-05, MSE(pi1): 8.770e-02, MSE(pi2): 2.704e-05, MSE(pi3): 3.446e-03\n",
      "Epoch 37900, Train loss: 1.830e+06, Test loss: 1.934e+08, MSE(e): 6.088e-05, MSE(pi1): 8.740e-02, MSE(pi2): 2.682e-05, MSE(pi3): 3.468e-03\n",
      "Epoch 38000, Train loss: 1.829e+06, Test loss: 1.933e+08, MSE(e): 6.082e-05, MSE(pi1): 8.738e-02, MSE(pi2): 2.678e-05, MSE(pi3): 3.467e-03\n",
      "Epoch 38100, Train loss: 1.828e+06, Test loss: 1.930e+08, MSE(e): 6.078e-05, MSE(pi1): 8.739e-02, MSE(pi2): 2.673e-05, MSE(pi3): 3.464e-03\n",
      "Epoch 38200, Train loss: 1.827e+06, Test loss: 1.931e+08, MSE(e): 6.071e-05, MSE(pi1): 8.732e-02, MSE(pi2): 2.671e-05, MSE(pi3): 3.467e-03\n",
      "Epoch 38300, Train loss: 1.829e+06, Test loss: 1.945e+08, MSE(e): 6.096e-05, MSE(pi1): 8.715e-02, MSE(pi2): 2.689e-05, MSE(pi3): 3.479e-03\n",
      "Epoch 38400, Train loss: 1.825e+06, Test loss: 1.930e+08, MSE(e): 6.060e-05, MSE(pi1): 8.727e-02, MSE(pi2): 2.665e-05, MSE(pi3): 3.466e-03\n",
      "Epoch 38500, Train loss: 1.824e+06, Test loss: 1.928e+08, MSE(e): 6.055e-05, MSE(pi1): 8.725e-02, MSE(pi2): 2.660e-05, MSE(pi3): 3.465e-03\n",
      "Epoch 38600, Train loss: 1.824e+06, Test loss: 1.929e+08, MSE(e): 6.050e-05, MSE(pi1): 8.719e-02, MSE(pi2): 2.659e-05, MSE(pi3): 3.467e-03\n",
      "Epoch 38700, Train loss: 1.823e+06, Test loss: 1.928e+08, MSE(e): 6.044e-05, MSE(pi1): 8.718e-02, MSE(pi2): 2.655e-05, MSE(pi3): 3.466e-03\n",
      "Epoch 38800, Train loss: 1.822e+06, Test loss: 1.928e+08, MSE(e): 6.041e-05, MSE(pi1): 8.712e-02, MSE(pi2): 2.655e-05, MSE(pi3): 3.468e-03\n",
      "Epoch 38900, Train loss: 1.821e+06, Test loss: 1.927e+08, MSE(e): 6.034e-05, MSE(pi1): 8.712e-02, MSE(pi2): 2.649e-05, MSE(pi3): 3.466e-03\n",
      "Epoch 39000, Train loss: 1.830e+06, Test loss: 1.926e+08, MSE(e): 6.126e-05, MSE(pi1): 8.683e-02, MSE(pi2): 2.701e-05, MSE(pi3): 3.488e-03\n",
      "Epoch 39100, Train loss: 1.820e+06, Test loss: 1.926e+08, MSE(e): 6.024e-05, MSE(pi1): 8.706e-02, MSE(pi2): 2.643e-05, MSE(pi3): 3.465e-03\n",
      "Epoch 39200, Train loss: 1.819e+06, Test loss: 1.925e+08, MSE(e): 6.019e-05, MSE(pi1): 8.704e-02, MSE(pi2): 2.640e-05, MSE(pi3): 3.465e-03\n",
      "Epoch 39300, Train loss: 1.818e+06, Test loss: 1.923e+08, MSE(e): 6.014e-05, MSE(pi1): 8.700e-02, MSE(pi2): 2.638e-05, MSE(pi3): 3.465e-03\n",
      "Epoch 39400, Train loss: 1.817e+06, Test loss: 1.924e+08, MSE(e): 6.009e-05, MSE(pi1): 8.698e-02, MSE(pi2): 2.634e-05, MSE(pi3): 3.464e-03\n",
      "Epoch 39500, Train loss: 1.817e+06, Test loss: 1.927e+08, MSE(e): 6.009e-05, MSE(pi1): 8.690e-02, MSE(pi2): 2.636e-05, MSE(pi3): 3.469e-03\n",
      "Epoch 39600, Train loss: 1.816e+06, Test loss: 1.923e+08, MSE(e): 5.999e-05, MSE(pi1): 8.692e-02, MSE(pi2): 2.628e-05, MSE(pi3): 3.464e-03\n",
      "Epoch 39700, Train loss: 1.815e+06, Test loss: 1.924e+08, MSE(e): 5.996e-05, MSE(pi1): 8.688e-02, MSE(pi2): 2.627e-05, MSE(pi3): 3.466e-03\n",
      "Epoch 39800, Train loss: 1.814e+06, Test loss: 1.921e+08, MSE(e): 5.990e-05, MSE(pi1): 8.687e-02, MSE(pi2): 2.622e-05, MSE(pi3): 3.464e-03\n",
      "Epoch 39900, Train loss: 1.814e+06, Test loss: 1.921e+08, MSE(e): 5.990e-05, MSE(pi1): 8.679e-02, MSE(pi2): 2.625e-05, MSE(pi3): 3.468e-03\n",
      "Epoch 40000, Train loss: 1.813e+06, Test loss: 1.920e+08, MSE(e): 5.981e-05, MSE(pi1): 8.682e-02, MSE(pi2): 2.617e-05, MSE(pi3): 3.463e-03\n",
      "Epoch 40100, Train loss: 1.833e+06, Test loss: 1.907e+08, MSE(e): 6.185e-05, MSE(pi1): 8.719e-02, MSE(pi2): 2.678e-05, MSE(pi3): 3.429e-03\n",
      "Epoch 40200, Train loss: 1.811e+06, Test loss: 1.919e+08, MSE(e): 5.971e-05, MSE(pi1): 8.676e-02, MSE(pi2): 2.612e-05, MSE(pi3): 3.463e-03\n",
      "Epoch 40300, Train loss: 1.810e+06, Test loss: 1.919e+08, MSE(e): 5.967e-05, MSE(pi1): 8.673e-02, MSE(pi2): 2.609e-05, MSE(pi3): 3.463e-03\n",
      "Epoch 40400, Train loss: 1.810e+06, Test loss: 1.919e+08, MSE(e): 5.963e-05, MSE(pi1): 8.672e-02, MSE(pi2): 2.606e-05, MSE(pi3): 3.461e-03\n",
      "Epoch 40500, Train loss: 1.809e+06, Test loss: 1.918e+08, MSE(e): 5.958e-05, MSE(pi1): 8.668e-02, MSE(pi2): 2.604e-05, MSE(pi3): 3.462e-03\n",
      "Epoch 40600, Train loss: 1.808e+06, Test loss: 1.918e+08, MSE(e): 5.958e-05, MSE(pi1): 8.660e-02, MSE(pi2): 2.606e-05, MSE(pi3): 3.466e-03\n",
      "Epoch 40700, Train loss: 1.807e+06, Test loss: 1.917e+08, MSE(e): 5.949e-05, MSE(pi1): 8.663e-02, MSE(pi2): 2.598e-05, MSE(pi3): 3.462e-03\n",
      "Epoch 40800, Train loss: 1.834e+06, Test loss: 1.908e+08, MSE(e): 6.207e-05, MSE(pi1): 8.705e-02, MSE(pi2): 2.678e-05, MSE(pi3): 3.424e-03\n",
      "Epoch 40900, Train loss: 1.806e+06, Test loss: 1.916e+08, MSE(e): 5.941e-05, MSE(pi1): 8.657e-02, MSE(pi2): 2.594e-05, MSE(pi3): 3.462e-03\n",
      "Epoch 41000, Train loss: 1.805e+06, Test loss: 1.915e+08, MSE(e): 5.937e-05, MSE(pi1): 8.655e-02, MSE(pi2): 2.591e-05, MSE(pi3): 3.461e-03\n",
      "Epoch 41100, Train loss: 1.805e+06, Test loss: 1.915e+08, MSE(e): 5.933e-05, MSE(pi1): 8.652e-02, MSE(pi2): 2.589e-05, MSE(pi3): 3.461e-03\n",
      "Epoch 41200, Train loss: 1.804e+06, Test loss: 1.914e+08, MSE(e): 5.929e-05, MSE(pi1): 8.650e-02, MSE(pi2): 2.586e-05, MSE(pi3): 3.460e-03\n",
      "Epoch 41300, Train loss: 1.803e+06, Test loss: 1.914e+08, MSE(e): 5.925e-05, MSE(pi1): 8.647e-02, MSE(pi2): 2.584e-05, MSE(pi3): 3.461e-03\n",
      "Epoch 41400, Train loss: 1.803e+06, Test loss: 1.912e+08, MSE(e): 5.921e-05, MSE(pi1): 8.646e-02, MSE(pi2): 2.581e-05, MSE(pi3): 3.459e-03\n",
      "Epoch 41500, Train loss: 1.802e+06, Test loss: 1.913e+08, MSE(e): 5.917e-05, MSE(pi1): 8.641e-02, MSE(pi2): 2.579e-05, MSE(pi3): 3.461e-03\n",
      "Epoch 41600, Train loss: 1.802e+06, Test loss: 1.918e+08, MSE(e): 5.919e-05, MSE(pi1): 8.633e-02, MSE(pi2): 2.583e-05, MSE(pi3): 3.466e-03\n",
      "Epoch 41700, Train loss: 1.801e+06, Test loss: 1.912e+08, MSE(e): 5.909e-05, MSE(pi1): 8.636e-02, MSE(pi2): 2.575e-05, MSE(pi3): 3.460e-03\n",
      "Epoch 41800, Train loss: 1.802e+06, Test loss: 1.901e+08, MSE(e): 5.920e-05, MSE(pi1): 8.645e-02, MSE(pi2): 2.572e-05, MSE(pi3): 3.450e-03\n",
      "Epoch 41900, Train loss: 1.799e+06, Test loss: 1.910e+08, MSE(e): 5.901e-05, MSE(pi1): 8.631e-02, MSE(pi2): 2.570e-05, MSE(pi3): 3.460e-03\n",
      "Epoch 42000, Train loss: 1.798e+06, Test loss: 1.910e+08, MSE(e): 5.897e-05, MSE(pi1): 8.628e-02, MSE(pi2): 2.568e-05, MSE(pi3): 3.460e-03\n",
      "Epoch 42100, Train loss: 1.798e+06, Test loss: 1.909e+08, MSE(e): 5.893e-05, MSE(pi1): 8.627e-02, MSE(pi2): 2.565e-05, MSE(pi3): 3.458e-03\n",
      "Epoch 42200, Train loss: 1.797e+06, Test loss: 1.909e+08, MSE(e): 5.889e-05, MSE(pi1): 8.623e-02, MSE(pi2): 2.563e-05, MSE(pi3): 3.459e-03\n",
      "Epoch 42300, Train loss: 1.797e+06, Test loss: 1.908e+08, MSE(e): 5.886e-05, MSE(pi1): 8.623e-02, MSE(pi2): 2.560e-05, MSE(pi3): 3.457e-03\n",
      "Epoch 42400, Train loss: 1.796e+06, Test loss: 1.907e+08, MSE(e): 5.882e-05, MSE(pi1): 8.620e-02, MSE(pi2): 2.558e-05, MSE(pi3): 3.457e-03\n",
      "Epoch 42500, Train loss: 1.795e+06, Test loss: 1.908e+08, MSE(e): 5.878e-05, MSE(pi1): 8.615e-02, MSE(pi2): 2.557e-05, MSE(pi3): 3.459e-03\n",
      "Epoch 42600, Train loss: 1.795e+06, Test loss: 1.910e+08, MSE(e): 5.876e-05, MSE(pi1): 8.611e-02, MSE(pi2): 2.557e-05, MSE(pi3): 3.460e-03\n",
      "Epoch 42700, Train loss: 1.794e+06, Test loss: 1.906e+08, MSE(e): 5.871e-05, MSE(pi1): 8.611e-02, MSE(pi2): 2.552e-05, MSE(pi3): 3.457e-03\n",
      "Epoch 42800, Train loss: 1.794e+06, Test loss: 1.902e+08, MSE(e): 5.869e-05, MSE(pi1): 8.613e-02, MSE(pi2): 2.548e-05, MSE(pi3): 3.454e-03\n",
      "Epoch 42900, Train loss: 1.793e+06, Test loss: 1.906e+08, MSE(e): 5.864e-05, MSE(pi1): 8.605e-02, MSE(pi2): 2.549e-05, MSE(pi3): 3.458e-03\n",
      "Epoch 43000, Train loss: 1.792e+06, Test loss: 1.905e+08, MSE(e): 5.860e-05, MSE(pi1): 8.604e-02, MSE(pi2): 2.546e-05, MSE(pi3): 3.457e-03\n",
      "Epoch 43100, Train loss: 1.791e+06, Test loss: 1.904e+08, MSE(e): 5.857e-05, MSE(pi1): 8.602e-02, MSE(pi2): 2.543e-05, MSE(pi3): 3.456e-03\n",
      "Epoch 43200, Train loss: 1.791e+06, Test loss: 1.903e+08, MSE(e): 5.853e-05, MSE(pi1): 8.600e-02, MSE(pi2): 2.541e-05, MSE(pi3): 3.456e-03\n",
      "Epoch 43300, Train loss: 1.790e+06, Test loss: 1.904e+08, MSE(e): 5.850e-05, MSE(pi1): 8.597e-02, MSE(pi2): 2.539e-05, MSE(pi3): 3.456e-03\n",
      "Epoch 43400, Train loss: 1.790e+06, Test loss: 1.896e+08, MSE(e): 5.854e-05, MSE(pi1): 8.601e-02, MSE(pi2): 2.537e-05, MSE(pi3): 3.450e-03\n",
      "Epoch 43500, Train loss: 1.789e+06, Test loss: 1.903e+08, MSE(e): 5.843e-05, MSE(pi1): 8.591e-02, MSE(pi2): 2.536e-05, MSE(pi3): 3.456e-03\n",
      "Epoch 43600, Train loss: 1.788e+06, Test loss: 1.903e+08, MSE(e): 5.840e-05, MSE(pi1): 8.588e-02, MSE(pi2): 2.534e-05, MSE(pi3): 3.456e-03\n",
      "Epoch 43700, Train loss: 1.788e+06, Test loss: 1.901e+08, MSE(e): 5.837e-05, MSE(pi1): 8.588e-02, MSE(pi2): 2.531e-05, MSE(pi3): 3.454e-03\n",
      "Epoch 43800, Train loss: 1.788e+06, Test loss: 1.897e+08, MSE(e): 5.837e-05, MSE(pi1): 8.590e-02, MSE(pi2): 2.529e-05, MSE(pi3): 3.450e-03\n",
      "Epoch 43900, Train loss: 1.787e+06, Test loss: 1.902e+08, MSE(e): 5.830e-05, MSE(pi1): 8.581e-02, MSE(pi2): 2.529e-05, MSE(pi3): 3.456e-03\n",
      "Epoch 44000, Train loss: 1.837e+06, Test loss: 1.855e+08, MSE(e): 6.322e-05, MSE(pi1): 8.640e-02, MSE(pi2): 2.698e-05, MSE(pi3): 3.404e-03\n",
      "Epoch 44100, Train loss: 1.786e+06, Test loss: 1.900e+08, MSE(e): 5.823e-05, MSE(pi1): 8.577e-02, MSE(pi2): 2.524e-05, MSE(pi3): 3.455e-03\n",
      "Epoch 44200, Train loss: 1.786e+06, Test loss: 1.908e+08, MSE(e): 5.832e-05, MSE(pi1): 8.565e-02, MSE(pi2): 2.532e-05, MSE(pi3): 3.462e-03\n",
      "Epoch 44300, Train loss: 1.784e+06, Test loss: 1.900e+08, MSE(e): 5.817e-05, MSE(pi1): 8.571e-02, MSE(pi2): 2.521e-05, MSE(pi3): 3.455e-03\n",
      "Epoch 44400, Train loss: 1.788e+06, Test loss: 1.885e+08, MSE(e): 5.851e-05, MSE(pi1): 8.586e-02, MSE(pi2): 2.525e-05, MSE(pi3): 3.440e-03\n",
      "Epoch 44500, Train loss: 1.783e+06, Test loss: 1.899e+08, MSE(e): 5.811e-05, MSE(pi1): 8.566e-02, MSE(pi2): 2.518e-05, MSE(pi3): 3.455e-03\n",
      "Epoch 44600, Train loss: 1.791e+06, Test loss: 1.911e+08, MSE(e): 5.899e-05, MSE(pi1): 8.539e-02, MSE(pi2): 2.567e-05, MSE(pi3): 3.476e-03\n",
      "Epoch 44700, Train loss: 1.782e+06, Test loss: 1.898e+08, MSE(e): 5.805e-05, MSE(pi1): 8.562e-02, MSE(pi2): 2.514e-05, MSE(pi3): 3.454e-03\n",
      "Epoch 44800, Train loss: 1.782e+06, Test loss: 1.910e+08, MSE(e): 5.810e-05, MSE(pi1): 8.552e-02, MSE(pi2): 2.519e-05, MSE(pi3): 3.460e-03\n",
      "Epoch 44900, Train loss: 1.781e+06, Test loss: 1.897e+08, MSE(e): 5.799e-05, MSE(pi1): 8.557e-02, MSE(pi2): 2.510e-05, MSE(pi3): 3.454e-03\n",
      "Epoch 45000, Train loss: 1.781e+06, Test loss: 1.887e+08, MSE(e): 5.803e-05, MSE(pi1): 8.548e-02, MSE(pi2): 2.515e-05, MSE(pi3): 3.459e-03\n",
      "Epoch 45100, Train loss: 1.780e+06, Test loss: 1.897e+08, MSE(e): 5.793e-05, MSE(pi1): 8.552e-02, MSE(pi2): 2.507e-05, MSE(pi3): 3.453e-03\n",
      "Epoch 45200, Train loss: 1.800e+06, Test loss: 1.881e+08, MSE(e): 5.991e-05, MSE(pi1): 8.589e-02, MSE(pi2): 2.568e-05, MSE(pi3): 3.420e-03\n",
      "Epoch 45300, Train loss: 1.779e+06, Test loss: 1.896e+08, MSE(e): 5.787e-05, MSE(pi1): 8.548e-02, MSE(pi2): 2.503e-05, MSE(pi3): 3.453e-03\n",
      "Epoch 45400, Train loss: 1.778e+06, Test loss: 1.889e+08, MSE(e): 5.786e-05, MSE(pi1): 8.549e-02, MSE(pi2): 2.501e-05, MSE(pi3): 3.449e-03\n",
      "Epoch 45500, Train loss: 1.778e+06, Test loss: 1.895e+08, MSE(e): 5.781e-05, MSE(pi1): 8.542e-02, MSE(pi2): 2.500e-05, MSE(pi3): 3.453e-03\n",
      "Epoch 45600, Train loss: 1.777e+06, Test loss: 1.898e+08, MSE(e): 5.780e-05, MSE(pi1): 8.537e-02, MSE(pi2): 2.501e-05, MSE(pi3): 3.455e-03\n",
      "Epoch 45700, Train loss: 1.777e+06, Test loss: 1.894e+08, MSE(e): 5.776e-05, MSE(pi1): 8.538e-02, MSE(pi2): 2.497e-05, MSE(pi3): 3.452e-03\n",
      "Epoch 45800, Train loss: 1.776e+06, Test loss: 1.892e+08, MSE(e): 5.775e-05, MSE(pi1): 8.540e-02, MSE(pi2): 2.494e-05, MSE(pi3): 3.448e-03\n",
      "Epoch 45900, Train loss: 1.776e+06, Test loss: 1.894e+08, MSE(e): 5.770e-05, MSE(pi1): 8.533e-02, MSE(pi2): 2.494e-05, MSE(pi3): 3.452e-03\n",
      "Epoch 46000, Train loss: 1.775e+06, Test loss: 1.891e+08, MSE(e): 5.768e-05, MSE(pi1): 8.533e-02, MSE(pi2): 2.492e-05, MSE(pi3): 3.450e-03\n",
      "Epoch 46100, Train loss: 1.775e+06, Test loss: 1.893e+08, MSE(e): 5.765e-05, MSE(pi1): 8.529e-02, MSE(pi2): 2.491e-05, MSE(pi3): 3.451e-03\n",
      "Epoch 46200, Train loss: 1.774e+06, Test loss: 1.895e+08, MSE(e): 5.763e-05, MSE(pi1): 8.524e-02, MSE(pi2): 2.491e-05, MSE(pi3): 3.453e-03\n",
      "Epoch 46300, Train loss: 1.773e+06, Test loss: 1.892e+08, MSE(e): 5.759e-05, MSE(pi1): 8.524e-02, MSE(pi2): 2.488e-05, MSE(pi3): 3.451e-03\n",
      "Epoch 46400, Train loss: 1.773e+06, Test loss: 1.891e+08, MSE(e): 5.759e-05, MSE(pi1): 8.526e-02, MSE(pi2): 2.485e-05, MSE(pi3): 3.447e-03\n",
      "Epoch 46500, Train loss: 1.772e+06, Test loss: 1.891e+08, MSE(e): 5.754e-05, MSE(pi1): 8.519e-02, MSE(pi2): 2.485e-05, MSE(pi3): 3.451e-03\n",
      "Epoch 46600, Train loss: 1.772e+06, Test loss: 1.891e+08, MSE(e): 5.754e-05, MSE(pi1): 8.514e-02, MSE(pi2): 2.486e-05, MSE(pi3): 3.453e-03\n",
      "Epoch 46700, Train loss: 1.771e+06, Test loss: 1.891e+08, MSE(e): 5.749e-05, MSE(pi1): 8.515e-02, MSE(pi2): 2.482e-05, MSE(pi3): 3.450e-03\n",
      "Epoch 46800, Train loss: 1.772e+06, Test loss: 1.878e+08, MSE(e): 5.761e-05, MSE(pi1): 8.523e-02, MSE(pi2): 2.481e-05, MSE(pi3): 3.441e-03\n",
      "Epoch 46900, Train loss: 1.770e+06, Test loss: 1.890e+08, MSE(e): 5.744e-05, MSE(pi1): 8.510e-02, MSE(pi2): 2.479e-05, MSE(pi3): 3.450e-03\n",
      "Epoch 47000, Train loss: 1.770e+06, Test loss: 1.890e+08, MSE(e): 5.742e-05, MSE(pi1): 8.508e-02, MSE(pi2): 2.478e-05, MSE(pi3): 3.450e-03\n",
      "Epoch 47100, Train loss: 1.770e+06, Test loss: 1.880e+08, MSE(e): 5.739e-05, MSE(pi1): 8.508e-02, MSE(pi2): 2.476e-05, MSE(pi3): 3.448e-03\n",
      "Epoch 47200, Train loss: 1.769e+06, Test loss: 1.889e+08, MSE(e): 5.737e-05, MSE(pi1): 8.504e-02, MSE(pi2): 2.475e-05, MSE(pi3): 3.450e-03\n",
      "Epoch 47300, Train loss: 1.769e+06, Test loss: 1.902e+08, MSE(e): 5.738e-05, MSE(pi1): 8.497e-02, MSE(pi2): 2.478e-05, MSE(pi3): 3.454e-03\n",
      "Epoch 47400, Train loss: 1.768e+06, Test loss: 1.888e+08, MSE(e): 5.732e-05, MSE(pi1): 8.499e-02, MSE(pi2): 2.472e-05, MSE(pi3): 3.449e-03\n",
      "Epoch 47500, Train loss: 1.784e+06, Test loss: 1.858e+08, MSE(e): 5.891e-05, MSE(pi1): 8.532e-02, MSE(pi2): 2.520e-05, MSE(pi3): 3.419e-03\n",
      "Epoch 47600, Train loss: 1.767e+06, Test loss: 1.887e+08, MSE(e): 5.727e-05, MSE(pi1): 8.495e-02, MSE(pi2): 2.469e-05, MSE(pi3): 3.448e-03\n",
      "Epoch 47700, Train loss: 1.767e+06, Test loss: 1.887e+08, MSE(e): 5.725e-05, MSE(pi1): 8.492e-02, MSE(pi2): 2.468e-05, MSE(pi3): 3.449e-03\n",
      "Epoch 47800, Train loss: 1.767e+06, Test loss: 1.883e+08, MSE(e): 5.731e-05, MSE(pi1): 8.499e-02, MSE(pi2): 2.466e-05, MSE(pi3): 3.441e-03\n",
      "Epoch 47900, Train loss: 1.766e+06, Test loss: 1.887e+08, MSE(e): 5.720e-05, MSE(pi1): 8.488e-02, MSE(pi2): 2.466e-05, MSE(pi3): 3.448e-03\n",
      "Epoch 48000, Train loss: 1.765e+06, Test loss: 1.882e+08, MSE(e): 5.718e-05, MSE(pi1): 8.486e-02, MSE(pi2): 2.464e-05, MSE(pi3): 3.448e-03\n",
      "Epoch 48100, Train loss: 1.765e+06, Test loss: 1.886e+08, MSE(e): 5.715e-05, MSE(pi1): 8.484e-02, MSE(pi2): 2.463e-05, MSE(pi3): 3.448e-03\n",
      "Epoch 48200, Train loss: 1.765e+06, Test loss: 1.888e+08, MSE(e): 5.719e-05, MSE(pi1): 8.475e-02, MSE(pi2): 2.467e-05, MSE(pi3): 3.453e-03\n",
      "Epoch 48300, Train loss: 1.764e+06, Test loss: 1.885e+08, MSE(e): 5.711e-05, MSE(pi1): 8.479e-02, MSE(pi2): 2.460e-05, MSE(pi3): 3.448e-03\n",
      "Epoch 48400, Train loss: 1.763e+06, Test loss: 1.890e+08, MSE(e): 5.711e-05, MSE(pi1): 8.474e-02, MSE(pi2): 2.462e-05, MSE(pi3): 3.450e-03\n",
      "Epoch 48500, Train loss: 1.763e+06, Test loss: 1.885e+08, MSE(e): 5.706e-05, MSE(pi1): 8.475e-02, MSE(pi2): 2.458e-05, MSE(pi3): 3.447e-03\n",
      "Epoch 48600, Train loss: 1.762e+06, Test loss: 1.882e+08, MSE(e): 5.704e-05, MSE(pi1): 8.474e-02, MSE(pi2): 2.456e-05, MSE(pi3): 3.445e-03\n",
      "Epoch 48700, Train loss: 1.762e+06, Test loss: 1.884e+08, MSE(e): 5.702e-05, MSE(pi1): 8.470e-02, MSE(pi2): 2.455e-05, MSE(pi3): 3.447e-03\n",
      "Epoch 48800, Train loss: 1.762e+06, Test loss: 1.888e+08, MSE(e): 5.702e-05, MSE(pi1): 8.465e-02, MSE(pi2): 2.457e-05, MSE(pi3): 3.450e-03\n",
      "Epoch 48900, Train loss: 1.761e+06, Test loss: 1.884e+08, MSE(e): 5.698e-05, MSE(pi1): 8.466e-02, MSE(pi2): 2.453e-05, MSE(pi3): 3.446e-03\n",
      "Epoch 49000, Train loss: 1.765e+06, Test loss: 1.900e+08, MSE(e): 5.747e-05, MSE(pi1): 8.445e-02, MSE(pi2): 2.482e-05, MSE(pi3): 3.463e-03\n",
      "Epoch 49100, Train loss: 1.760e+06, Test loss: 1.883e+08, MSE(e): 5.693e-05, MSE(pi1): 8.462e-02, MSE(pi2): 2.451e-05, MSE(pi3): 3.446e-03\n",
      "Epoch 49200, Train loss: 1.760e+06, Test loss: 1.886e+08, MSE(e): 5.691e-05, MSE(pi1): 8.459e-02, MSE(pi2): 2.450e-05, MSE(pi3): 3.446e-03\n",
      "Epoch 49300, Train loss: 1.759e+06, Test loss: 1.882e+08, MSE(e): 5.689e-05, MSE(pi1): 8.458e-02, MSE(pi2): 2.448e-05, MSE(pi3): 3.446e-03\n",
      "Epoch 49400, Train loss: 1.760e+06, Test loss: 1.876e+08, MSE(e): 5.696e-05, MSE(pi1): 8.464e-02, MSE(pi2): 2.447e-05, MSE(pi3): 3.438e-03\n",
      "Epoch 49500, Train loss: 1.758e+06, Test loss: 1.882e+08, MSE(e): 5.685e-05, MSE(pi1): 8.453e-02, MSE(pi2): 2.446e-05, MSE(pi3): 3.445e-03\n",
      "Epoch 49600, Train loss: 1.760e+06, Test loss: 1.878e+08, MSE(e): 5.698e-05, MSE(pi1): 8.462e-02, MSE(pi2): 2.446e-05, MSE(pi3): 3.436e-03\n",
      "Epoch 49700, Train loss: 1.757e+06, Test loss: 1.881e+08, MSE(e): 5.681e-05, MSE(pi1): 8.449e-02, MSE(pi2): 2.444e-05, MSE(pi3): 3.445e-03\n",
      "Epoch 49800, Train loss: 1.757e+06, Test loss: 1.883e+08, MSE(e): 5.679e-05, MSE(pi1): 8.446e-02, MSE(pi2): 2.443e-05, MSE(pi3): 3.445e-03\n",
      "Epoch 49900, Train loss: 1.757e+06, Test loss: 1.881e+08, MSE(e): 5.677e-05, MSE(pi1): 8.444e-02, MSE(pi2): 2.442e-05, MSE(pi3): 3.445e-03\n",
      "Epoch 50000, Train loss: 1.757e+06, Test loss: 1.887e+08, MSE(e): 5.681e-05, MSE(pi1): 8.436e-02, MSE(pi2): 2.446e-05, MSE(pi3): 3.450e-03\n",
      "Epoch 50100, Train loss: 1.756e+06, Test loss: 1.880e+08, MSE(e): 5.673e-05, MSE(pi1): 8.440e-02, MSE(pi2): 2.439e-05, MSE(pi3): 3.444e-03\n",
      "Epoch 50200, Train loss: 1.756e+06, Test loss: 1.875e+08, MSE(e): 5.675e-05, MSE(pi1): 8.444e-02, MSE(pi2): 2.437e-05, MSE(pi3): 3.439e-03\n",
      "Epoch 50300, Train loss: 1.755e+06, Test loss: 1.880e+08, MSE(e): 5.669e-05, MSE(pi1): 8.436e-02, MSE(pi2): 2.437e-05, MSE(pi3): 3.444e-03\n",
      "Epoch 50400, Train loss: 1.755e+06, Test loss: 1.882e+08, MSE(e): 5.672e-05, MSE(pi1): 8.428e-02, MSE(pi2): 2.441e-05, MSE(pi3): 3.449e-03\n",
      "Epoch 50500, Train loss: 1.754e+06, Test loss: 1.879e+08, MSE(e): 5.665e-05, MSE(pi1): 8.432e-02, MSE(pi2): 2.435e-05, MSE(pi3): 3.444e-03\n",
      "Epoch 50600, Train loss: 1.754e+06, Test loss: 1.882e+08, MSE(e): 5.667e-05, MSE(pi1): 8.425e-02, MSE(pi2): 2.438e-05, MSE(pi3): 3.448e-03\n",
      "Epoch 50700, Train loss: 1.753e+06, Test loss: 1.878e+08, MSE(e): 5.661e-05, MSE(pi1): 8.428e-02, MSE(pi2): 2.433e-05, MSE(pi3): 3.443e-03\n",
      "Epoch 50800, Train loss: 1.758e+06, Test loss: 1.869e+08, MSE(e): 5.710e-05, MSE(pi1): 8.445e-02, MSE(pi2): 2.444e-05, MSE(pi3): 3.426e-03\n",
      "Epoch 50900, Train loss: 1.752e+06, Test loss: 1.878e+08, MSE(e): 5.658e-05, MSE(pi1): 8.424e-02, MSE(pi2): 2.431e-05, MSE(pi3): 3.443e-03\n",
      "Epoch 51000, Train loss: 1.752e+06, Test loss: 1.877e+08, MSE(e): 5.656e-05, MSE(pi1): 8.423e-02, MSE(pi2): 2.430e-05, MSE(pi3): 3.442e-03\n",
      "Epoch 51100, Train loss: 1.752e+06, Test loss: 1.878e+08, MSE(e): 5.654e-05, MSE(pi1): 8.418e-02, MSE(pi2): 2.430e-05, MSE(pi3): 3.444e-03\n",
      "Epoch 51200, Train loss: 1.751e+06, Test loss: 1.877e+08, MSE(e): 5.652e-05, MSE(pi1): 8.418e-02, MSE(pi2): 2.428e-05, MSE(pi3): 3.442e-03\n",
      "Epoch 51300, Train loss: 1.751e+06, Test loss: 1.878e+08, MSE(e): 5.650e-05, MSE(pi1): 8.414e-02, MSE(pi2): 2.428e-05, MSE(pi3): 3.443e-03\n",
      "Epoch 51400, Train loss: 1.750e+06, Test loss: 1.876e+08, MSE(e): 5.648e-05, MSE(pi1): 8.413e-02, MSE(pi2): 2.426e-05, MSE(pi3): 3.442e-03\n",
      "Epoch 51500, Train loss: 1.750e+06, Test loss: 1.876e+08, MSE(e): 5.647e-05, MSE(pi1): 8.411e-02, MSE(pi2): 2.425e-05, MSE(pi3): 3.442e-03\n",
      "Epoch 51600, Train loss: 1.750e+06, Test loss: 1.876e+08, MSE(e): 5.645e-05, MSE(pi1): 8.409e-02, MSE(pi2): 2.424e-05, MSE(pi3): 3.442e-03\n",
      "Epoch 51700, Train loss: 1.749e+06, Test loss: 1.875e+08, MSE(e): 5.644e-05, MSE(pi1): 8.405e-02, MSE(pi2): 2.425e-05, MSE(pi3): 3.444e-03\n",
      "Epoch 51800, Train loss: 1.749e+06, Test loss: 1.876e+08, MSE(e): 5.641e-05, MSE(pi1): 8.405e-02, MSE(pi2): 2.422e-05, MSE(pi3): 3.442e-03\n",
      "Epoch 51900, Train loss: 1.752e+06, Test loss: 1.885e+08, MSE(e): 5.674e-05, MSE(pi1): 8.388e-02, MSE(pi2): 2.442e-05, MSE(pi3): 3.455e-03\n",
      "Epoch 52000, Train loss: 1.748e+06, Test loss: 1.875e+08, MSE(e): 5.638e-05, MSE(pi1): 8.401e-02, MSE(pi2): 2.421e-05, MSE(pi3): 3.441e-03\n",
      "Epoch 52100, Train loss: 1.784e+06, Test loss: 1.883e+08, MSE(e): 6.006e-05, MSE(pi1): 8.348e-02, MSE(pi2): 2.595e-05, MSE(pi3): 3.486e-03\n",
      "Epoch 52200, Train loss: 1.747e+06, Test loss: 1.875e+08, MSE(e): 5.635e-05, MSE(pi1): 8.397e-02, MSE(pi2): 2.419e-05, MSE(pi3): 3.441e-03\n",
      "Epoch 52300, Train loss: 1.747e+06, Test loss: 1.874e+08, MSE(e): 5.633e-05, MSE(pi1): 8.395e-02, MSE(pi2): 2.418e-05, MSE(pi3): 3.441e-03\n",
      "Epoch 52400, Train loss: 1.769e+06, Test loss: 1.842e+08, MSE(e): 5.849e-05, MSE(pi1): 8.433e-02, MSE(pi2): 2.489e-05, MSE(pi3): 3.407e-03\n",
      "Epoch 52500, Train loss: 1.746e+06, Test loss: 1.874e+08, MSE(e): 5.630e-05, MSE(pi1): 8.391e-02, MSE(pi2): 2.416e-05, MSE(pi3): 3.440e-03\n",
      "Epoch 52600, Train loss: 1.777e+06, Test loss: 1.911e+08, MSE(e): 5.950e-05, MSE(pi1): 8.342e-02, MSE(pi2): 2.569e-05, MSE(pi3): 3.482e-03\n",
      "Epoch 52700, Train loss: 1.745e+06, Test loss: 1.873e+08, MSE(e): 5.626e-05, MSE(pi1): 8.387e-02, MSE(pi2): 2.414e-05, MSE(pi3): 3.440e-03\n",
      "Epoch 52800, Train loss: 1.745e+06, Test loss: 1.874e+08, MSE(e): 5.625e-05, MSE(pi1): 8.385e-02, MSE(pi2): 2.414e-05, MSE(pi3): 3.440e-03\n",
      "Epoch 52900, Train loss: 1.745e+06, Test loss: 1.872e+08, MSE(e): 5.624e-05, MSE(pi1): 8.385e-02, MSE(pi2): 2.412e-05, MSE(pi3): 3.438e-03\n",
      "Epoch 53000, Train loss: 1.744e+06, Test loss: 1.873e+08, MSE(e): 5.621e-05, MSE(pi1): 8.380e-02, MSE(pi2): 2.412e-05, MSE(pi3): 3.440e-03\n",
      "Epoch 53100, Train loss: 1.744e+06, Test loss: 1.873e+08, MSE(e): 5.620e-05, MSE(pi1): 8.379e-02, MSE(pi2): 2.411e-05, MSE(pi3): 3.439e-03\n",
      "Epoch 53200, Train loss: 1.743e+06, Test loss: 1.872e+08, MSE(e): 5.618e-05, MSE(pi1): 8.377e-02, MSE(pi2): 2.410e-05, MSE(pi3): 3.439e-03\n",
      "Epoch 53300, Train loss: 1.744e+06, Test loss: 1.865e+08, MSE(e): 5.627e-05, MSE(pi1): 8.384e-02, MSE(pi2): 2.410e-05, MSE(pi3): 3.431e-03\n",
      "Epoch 53400, Train loss: 1.743e+06, Test loss: 1.872e+08, MSE(e): 5.615e-05, MSE(pi1): 8.373e-02, MSE(pi2): 2.408e-05, MSE(pi3): 3.439e-03\n",
      "Epoch 53500, Train loss: 1.755e+06, Test loss: 1.898e+08, MSE(e): 5.741e-05, MSE(pi1): 8.341e-02, MSE(pi2): 2.473e-05, MSE(pi3): 3.465e-03\n",
      "Epoch 53600, Train loss: 1.742e+06, Test loss: 1.872e+08, MSE(e): 5.612e-05, MSE(pi1): 8.368e-02, MSE(pi2): 2.407e-05, MSE(pi3): 3.439e-03\n",
      "Epoch 53700, Train loss: 1.742e+06, Test loss: 1.871e+08, MSE(e): 5.610e-05, MSE(pi1): 8.367e-02, MSE(pi2): 2.406e-05, MSE(pi3): 3.439e-03\n",
      "Epoch 53800, Train loss: 1.741e+06, Test loss: 1.870e+08, MSE(e): 5.609e-05, MSE(pi1): 8.365e-02, MSE(pi2): 2.405e-05, MSE(pi3): 3.439e-03\n",
      "Epoch 53900, Train loss: 1.741e+06, Test loss: 1.871e+08, MSE(e): 5.607e-05, MSE(pi1): 8.363e-02, MSE(pi2): 2.404e-05, MSE(pi3): 3.438e-03\n",
      "Epoch 54000, Train loss: 1.741e+06, Test loss: 1.865e+08, MSE(e): 5.606e-05, MSE(pi1): 8.359e-02, MSE(pi2): 2.404e-05, MSE(pi3): 3.439e-03\n",
      "Epoch 54100, Train loss: 1.740e+06, Test loss: 1.870e+08, MSE(e): 5.604e-05, MSE(pi1): 8.359e-02, MSE(pi2): 2.403e-05, MSE(pi3): 3.438e-03\n",
      "Epoch 54200, Train loss: 1.743e+06, Test loss: 1.892e+08, MSE(e): 5.633e-05, MSE(pi1): 8.343e-02, MSE(pi2): 2.420e-05, MSE(pi3): 3.450e-03\n",
      "Epoch 54300, Train loss: 1.739e+06, Test loss: 1.870e+08, MSE(e): 5.601e-05, MSE(pi1): 8.355e-02, MSE(pi2): 2.401e-05, MSE(pi3): 3.438e-03\n",
      "Epoch 54400, Train loss: 1.773e+06, Test loss: 1.848e+08, MSE(e): 5.933e-05, MSE(pi1): 8.403e-02, MSE(pi2): 2.517e-05, MSE(pi3): 3.396e-03\n",
      "Epoch 54500, Train loss: 1.739e+06, Test loss: 1.869e+08, MSE(e): 5.598e-05, MSE(pi1): 8.351e-02, MSE(pi2): 2.400e-05, MSE(pi3): 3.437e-03\n",
      "Epoch 54600, Train loss: 1.739e+06, Test loss: 1.884e+08, MSE(e): 5.605e-05, MSE(pi1): 8.342e-02, MSE(pi2): 2.405e-05, MSE(pi3): 3.443e-03\n",
      "Epoch 54700, Train loss: 1.738e+06, Test loss: 1.869e+08, MSE(e): 5.596e-05, MSE(pi1): 8.347e-02, MSE(pi2): 2.398e-05, MSE(pi3): 3.437e-03\n",
      "Epoch 54800, Train loss: 1.739e+06, Test loss: 1.874e+08, MSE(e): 5.606e-05, MSE(pi1): 8.336e-02, MSE(pi2): 2.406e-05, MSE(pi3): 3.445e-03\n",
      "Epoch 54900, Train loss: 1.737e+06, Test loss: 1.869e+08, MSE(e): 5.593e-05, MSE(pi1): 8.343e-02, MSE(pi2): 2.397e-05, MSE(pi3): 3.437e-03\n",
      "Epoch 55000, Train loss: 1.737e+06, Test loss: 1.871e+08, MSE(e): 5.591e-05, MSE(pi1): 8.341e-02, MSE(pi2): 2.396e-05, MSE(pi3): 3.437e-03\n",
      "Epoch 55100, Train loss: 1.737e+06, Test loss: 1.868e+08, MSE(e): 5.590e-05, MSE(pi1): 8.339e-02, MSE(pi2): 2.395e-05, MSE(pi3): 3.437e-03\n",
      "Epoch 55200, Train loss: 1.760e+06, Test loss: 1.856e+08, MSE(e): 5.815e-05, MSE(pi1): 8.379e-02, MSE(pi2): 2.471e-05, MSE(pi3): 3.402e-03\n",
      "Epoch 55300, Train loss: 1.736e+06, Test loss: 1.868e+08, MSE(e): 5.587e-05, MSE(pi1): 8.335e-02, MSE(pi2): 2.394e-05, MSE(pi3): 3.436e-03\n",
      "Epoch 55400, Train loss: 1.736e+06, Test loss: 1.867e+08, MSE(e): 5.586e-05, MSE(pi1): 8.335e-02, MSE(pi2): 2.393e-05, MSE(pi3): 3.435e-03\n",
      "Epoch 55500, Train loss: 1.735e+06, Test loss: 1.867e+08, MSE(e): 5.584e-05, MSE(pi1): 8.331e-02, MSE(pi2): 2.393e-05, MSE(pi3): 3.436e-03\n",
      "Epoch 55600, Train loss: 1.735e+06, Test loss: 1.867e+08, MSE(e): 5.583e-05, MSE(pi1): 8.330e-02, MSE(pi2): 2.392e-05, MSE(pi3): 3.436e-03\n",
      "Epoch 55700, Train loss: 1.734e+06, Test loss: 1.865e+08, MSE(e): 5.582e-05, MSE(pi1): 8.328e-02, MSE(pi2): 2.391e-05, MSE(pi3): 3.435e-03\n",
      "Epoch 55800, Train loss: 1.734e+06, Test loss: 1.867e+08, MSE(e): 5.580e-05, MSE(pi1): 8.326e-02, MSE(pi2): 2.390e-05, MSE(pi3): 3.436e-03\n",
      "Epoch 55900, Train loss: 1.734e+06, Test loss: 1.868e+08, MSE(e): 5.581e-05, MSE(pi1): 8.321e-02, MSE(pi2): 2.391e-05, MSE(pi3): 3.438e-03\n",
      "Epoch 56000, Train loss: 1.733e+06, Test loss: 1.867e+08, MSE(e): 5.578e-05, MSE(pi1): 8.322e-02, MSE(pi2): 2.389e-05, MSE(pi3): 3.435e-03\n",
      "Epoch 56100, Train loss: 1.733e+06, Test loss: 1.865e+08, MSE(e): 5.576e-05, MSE(pi1): 8.319e-02, MSE(pi2): 2.389e-05, MSE(pi3): 3.436e-03\n",
      "Epoch 56200, Train loss: 1.733e+06, Test loss: 1.866e+08, MSE(e): 5.575e-05, MSE(pi1): 8.318e-02, MSE(pi2): 2.388e-05, MSE(pi3): 3.435e-03\n",
      "Epoch 56300, Train loss: 1.734e+06, Test loss: 1.873e+08, MSE(e): 5.593e-05, MSE(pi1): 8.305e-02, MSE(pi2): 2.399e-05, MSE(pi3): 3.444e-03\n",
      "Epoch 56400, Train loss: 1.732e+06, Test loss: 1.866e+08, MSE(e): 5.572e-05, MSE(pi1): 8.315e-02, MSE(pi2): 2.386e-05, MSE(pi3): 3.434e-03\n",
      "Epoch 56500, Train loss: 1.732e+06, Test loss: 1.865e+08, MSE(e): 5.571e-05, MSE(pi1): 8.314e-02, MSE(pi2): 2.385e-05, MSE(pi3): 3.433e-03\n",
      "Epoch 56600, Train loss: 1.732e+06, Test loss: 1.861e+08, MSE(e): 5.573e-05, MSE(pi1): 8.316e-02, MSE(pi2): 2.384e-05, MSE(pi3): 3.430e-03\n",
      "Epoch 56700, Train loss: 1.731e+06, Test loss: 1.865e+08, MSE(e): 5.568e-05, MSE(pi1): 8.310e-02, MSE(pi2): 2.384e-05, MSE(pi3): 3.433e-03\n",
      "Epoch 56800, Train loss: 1.731e+06, Test loss: 1.863e+08, MSE(e): 5.568e-05, MSE(pi1): 8.310e-02, MSE(pi2): 2.383e-05, MSE(pi3): 3.432e-03\n",
      "Epoch 56900, Train loss: 1.730e+06, Test loss: 1.864e+08, MSE(e): 5.566e-05, MSE(pi1): 8.306e-02, MSE(pi2): 2.383e-05, MSE(pi3): 3.433e-03\n",
      "Epoch 57000, Train loss: 1.730e+06, Test loss: 1.864e+08, MSE(e): 5.565e-05, MSE(pi1): 8.304e-02, MSE(pi2): 2.382e-05, MSE(pi3): 3.432e-03\n",
      "Epoch 57100, Train loss: 1.730e+06, Test loss: 1.865e+08, MSE(e): 5.564e-05, MSE(pi1): 8.300e-02, MSE(pi2): 2.382e-05, MSE(pi3): 3.434e-03\n",
      "Epoch 57200, Train loss: 1.729e+06, Test loss: 1.865e+08, MSE(e): 5.562e-05, MSE(pi1): 8.299e-02, MSE(pi2): 2.381e-05, MSE(pi3): 3.434e-03\n",
      "Epoch 57300, Train loss: 1.729e+06, Test loss: 1.865e+08, MSE(e): 5.561e-05, MSE(pi1): 8.296e-02, MSE(pi2): 2.381e-05, MSE(pi3): 3.435e-03\n",
      "Epoch 57400, Train loss: 1.729e+06, Test loss: 1.864e+08, MSE(e): 5.560e-05, MSE(pi1): 8.295e-02, MSE(pi2): 2.380e-05, MSE(pi3): 3.433e-03\n",
      "Epoch 57500, Train loss: 1.729e+06, Test loss: 1.863e+08, MSE(e): 5.559e-05, MSE(pi1): 8.292e-02, MSE(pi2): 2.380e-05, MSE(pi3): 3.434e-03\n",
      "Epoch 57600, Train loss: 1.728e+06, Test loss: 1.864e+08, MSE(e): 5.557e-05, MSE(pi1): 8.292e-02, MSE(pi2): 2.379e-05, MSE(pi3): 3.433e-03\n",
      "Epoch 57700, Train loss: 1.728e+06, Test loss: 1.862e+08, MSE(e): 5.558e-05, MSE(pi1): 8.294e-02, MSE(pi2): 2.377e-05, MSE(pi3): 3.429e-03\n",
      "Epoch 57800, Train loss: 1.728e+06, Test loss: 1.863e+08, MSE(e): 5.555e-05, MSE(pi1): 8.288e-02, MSE(pi2): 2.377e-05, MSE(pi3): 3.432e-03\n",
      "Epoch 57900, Train loss: 1.728e+06, Test loss: 1.866e+08, MSE(e): 5.558e-05, MSE(pi1): 8.281e-02, MSE(pi2): 2.381e-05, MSE(pi3): 3.437e-03\n",
      "Epoch 58000, Train loss: 1.727e+06, Test loss: 1.863e+08, MSE(e): 5.552e-05, MSE(pi1): 8.284e-02, MSE(pi2): 2.376e-05, MSE(pi3): 3.433e-03\n",
      "Epoch 58100, Train loss: 1.727e+06, Test loss: 1.864e+08, MSE(e): 5.552e-05, MSE(pi1): 8.281e-02, MSE(pi2): 2.376e-05, MSE(pi3): 3.433e-03\n",
      "Epoch 58200, Train loss: 1.727e+06, Test loss: 1.867e+08, MSE(e): 5.555e-05, MSE(pi1): 8.275e-02, MSE(pi2): 2.379e-05, MSE(pi3): 3.436e-03\n",
      "Epoch 58300, Train loss: 1.726e+06, Test loss: 1.863e+08, MSE(e): 5.549e-05, MSE(pi1): 8.279e-02, MSE(pi2): 2.374e-05, MSE(pi3): 3.432e-03\n",
      "Epoch 58400, Train loss: 1.726e+06, Test loss: 1.850e+08, MSE(e): 5.552e-05, MSE(pi1): 8.283e-02, MSE(pi2): 2.373e-05, MSE(pi3): 3.426e-03\n",
      "Epoch 58500, Train loss: 1.725e+06, Test loss: 1.862e+08, MSE(e): 5.546e-05, MSE(pi1): 8.275e-02, MSE(pi2): 2.373e-05, MSE(pi3): 3.431e-03\n",
      "Epoch 58600, Train loss: 1.726e+06, Test loss: 1.860e+08, MSE(e): 5.557e-05, MSE(pi1): 8.283e-02, MSE(pi2): 2.374e-05, MSE(pi3): 3.423e-03\n",
      "Epoch 58700, Train loss: 1.725e+06, Test loss: 1.862e+08, MSE(e): 5.544e-05, MSE(pi1): 8.272e-02, MSE(pi2): 2.372e-05, MSE(pi3): 3.431e-03\n",
      "Epoch 58800, Train loss: 1.725e+06, Test loss: 1.857e+08, MSE(e): 5.552e-05, MSE(pi1): 8.278e-02, MSE(pi2): 2.372e-05, MSE(pi3): 3.424e-03\n",
      "Epoch 58900, Train loss: 1.724e+06, Test loss: 1.862e+08, MSE(e): 5.542e-05, MSE(pi1): 8.268e-02, MSE(pi2): 2.371e-05, MSE(pi3): 3.431e-03\n",
      "Epoch 59000, Train loss: 1.724e+06, Test loss: 1.863e+08, MSE(e): 5.545e-05, MSE(pi1): 8.272e-02, MSE(pi2): 2.370e-05, MSE(pi3): 3.426e-03\n",
      "Epoch 59100, Train loss: 1.723e+06, Test loss: 1.861e+08, MSE(e): 5.540e-05, MSE(pi1): 8.264e-02, MSE(pi2): 2.370e-05, MSE(pi3): 3.430e-03\n",
      "Epoch 59200, Train loss: 1.724e+06, Test loss: 1.856e+08, MSE(e): 5.547e-05, MSE(pi1): 8.271e-02, MSE(pi2): 2.370e-05, MSE(pi3): 3.423e-03\n",
      "Epoch 59300, Train loss: 1.723e+06, Test loss: 1.861e+08, MSE(e): 5.537e-05, MSE(pi1): 8.261e-02, MSE(pi2): 2.369e-05, MSE(pi3): 3.430e-03\n",
      "Epoch 59400, Train loss: 1.723e+06, Test loss: 1.860e+08, MSE(e): 5.538e-05, MSE(pi1): 8.256e-02, MSE(pi2): 2.370e-05, MSE(pi3): 3.432e-03\n",
      "Epoch 59500, Train loss: 1.722e+06, Test loss: 1.861e+08, MSE(e): 5.535e-05, MSE(pi1): 8.257e-02, MSE(pi2): 2.368e-05, MSE(pi3): 3.430e-03\n",
      "Epoch 59600, Train loss: 1.766e+06, Test loss: 1.904e+08, MSE(e): 5.985e-05, MSE(pi1): 8.200e-02, MSE(pi2): 2.574e-05, MSE(pi3): 3.479e-03\n",
      "Epoch 59700, Train loss: 1.722e+06, Test loss: 1.860e+08, MSE(e): 5.533e-05, MSE(pi1): 8.253e-02, MSE(pi2): 2.367e-05, MSE(pi3): 3.430e-03\n",
      "Epoch 59800, Train loss: 1.733e+06, Test loss: 1.860e+08, MSE(e): 5.641e-05, MSE(pi1): 8.280e-02, MSE(pi2): 2.401e-05, MSE(pi3): 3.405e-03\n",
      "Epoch 59900, Train loss: 1.721e+06, Test loss: 1.860e+08, MSE(e): 5.531e-05, MSE(pi1): 8.250e-02, MSE(pi2): 2.366e-05, MSE(pi3): 3.429e-03\n",
      "Epoch 60000, Train loss: 1.721e+06, Test loss: 1.860e+08, MSE(e): 5.530e-05, MSE(pi1): 8.248e-02, MSE(pi2): 2.365e-05, MSE(pi3): 3.429e-03\n",
      "Epoch 60100, Train loss: 1.720e+06, Test loss: 1.858e+08, MSE(e): 5.529e-05, MSE(pi1): 8.248e-02, MSE(pi2): 2.364e-05, MSE(pi3): 3.427e-03\n",
      "Epoch 60200, Train loss: 1.720e+06, Test loss: 1.860e+08, MSE(e): 5.528e-05, MSE(pi1): 8.244e-02, MSE(pi2): 2.364e-05, MSE(pi3): 3.429e-03\n",
      "Epoch 60300, Train loss: 1.747e+06, Test loss: 1.878e+08, MSE(e): 5.803e-05, MSE(pi1): 8.199e-02, MSE(pi2): 2.493e-05, MSE(pi3): 3.467e-03\n",
      "Epoch 60400, Train loss: 1.719e+06, Test loss: 1.859e+08, MSE(e): 5.526e-05, MSE(pi1): 8.241e-02, MSE(pi2): 2.363e-05, MSE(pi3): 3.428e-03\n",
      "Epoch 60500, Train loss: 1.719e+06, Test loss: 1.859e+08, MSE(e): 5.525e-05, MSE(pi1): 8.239e-02, MSE(pi2): 2.363e-05, MSE(pi3): 3.429e-03\n",
      "Epoch 60600, Train loss: 1.719e+06, Test loss: 1.860e+08, MSE(e): 5.524e-05, MSE(pi1): 8.235e-02, MSE(pi2): 2.363e-05, MSE(pi3): 3.430e-03\n",
      "Epoch 60700, Train loss: 1.719e+06, Test loss: 1.859e+08, MSE(e): 5.522e-05, MSE(pi1): 8.236e-02, MSE(pi2): 2.361e-05, MSE(pi3): 3.428e-03\n",
      "Epoch 60800, Train loss: 1.718e+06, Test loss: 1.858e+08, MSE(e): 5.522e-05, MSE(pi1): 8.235e-02, MSE(pi2): 2.361e-05, MSE(pi3): 3.427e-03\n",
      "Epoch 60900, Train loss: 1.718e+06, Test loss: 1.859e+08, MSE(e): 5.521e-05, MSE(pi1): 8.231e-02, MSE(pi2): 2.361e-05, MSE(pi3): 3.429e-03\n",
      "Epoch 61000, Train loss: 1.718e+06, Test loss: 1.858e+08, MSE(e): 5.519e-05, MSE(pi1): 8.231e-02, MSE(pi2): 2.360e-05, MSE(pi3): 3.427e-03\n",
      "Epoch 61100, Train loss: 1.717e+06, Test loss: 1.858e+08, MSE(e): 5.518e-05, MSE(pi1): 8.229e-02, MSE(pi2): 2.359e-05, MSE(pi3): 3.427e-03\n",
      "Epoch 61200, Train loss: 1.717e+06, Test loss: 1.859e+08, MSE(e): 5.518e-05, MSE(pi1): 8.225e-02, MSE(pi2): 2.360e-05, MSE(pi3): 3.429e-03\n",
      "Epoch 61300, Train loss: 1.717e+06, Test loss: 1.858e+08, MSE(e): 5.516e-05, MSE(pi1): 8.224e-02, MSE(pi2): 2.359e-05, MSE(pi3): 3.427e-03\n",
      "Epoch 61400, Train loss: 1.717e+06, Test loss: 1.860e+08, MSE(e): 5.519e-05, MSE(pi1): 8.218e-02, MSE(pi2): 2.361e-05, MSE(pi3): 3.431e-03\n",
      "Epoch 61500, Train loss: 1.716e+06, Test loss: 1.858e+08, MSE(e): 5.514e-05, MSE(pi1): 8.221e-02, MSE(pi2): 2.358e-05, MSE(pi3): 3.427e-03\n",
      "Epoch 61600, Train loss: 1.716e+06, Test loss: 1.858e+08, MSE(e): 5.513e-05, MSE(pi1): 8.219e-02, MSE(pi2): 2.357e-05, MSE(pi3): 3.427e-03\n",
      "Epoch 61700, Train loss: 1.716e+06, Test loss: 1.856e+08, MSE(e): 5.513e-05, MSE(pi1): 8.220e-02, MSE(pi2): 2.356e-05, MSE(pi3): 3.425e-03\n",
      "Epoch 61800, Train loss: 1.715e+06, Test loss: 1.858e+08, MSE(e): 5.512e-05, MSE(pi1): 8.215e-02, MSE(pi2): 2.356e-05, MSE(pi3): 3.427e-03\n",
      "Epoch 61900, Train loss: 1.715e+06, Test loss: 1.860e+08, MSE(e): 5.513e-05, MSE(pi1): 8.210e-02, MSE(pi2): 2.358e-05, MSE(pi3): 3.429e-03\n",
      "Epoch 62000, Train loss: 1.715e+06, Test loss: 1.857e+08, MSE(e): 5.510e-05, MSE(pi1): 8.212e-02, MSE(pi2): 2.355e-05, MSE(pi3): 3.426e-03\n",
      "Epoch 62100, Train loss: 1.751e+06, Test loss: 1.903e+08, MSE(e): 5.876e-05, MSE(pi1): 8.160e-02, MSE(pi2): 2.523e-05, MSE(pi3): 3.471e-03\n",
      "Epoch 62200, Train loss: 1.714e+06, Test loss: 1.857e+08, MSE(e): 5.508e-05, MSE(pi1): 8.209e-02, MSE(pi2): 2.354e-05, MSE(pi3): 3.426e-03\n",
      "Epoch 62300, Train loss: 1.714e+06, Test loss: 1.857e+08, MSE(e): 5.507e-05, MSE(pi1): 8.207e-02, MSE(pi2): 2.354e-05, MSE(pi3): 3.426e-03\n",
      "Epoch 62400, Train loss: 1.715e+06, Test loss: 1.864e+08, MSE(e): 5.517e-05, MSE(pi1): 8.196e-02, MSE(pi2): 2.361e-05, MSE(pi3): 3.433e-03\n",
      "Epoch 62500, Train loss: 1.713e+06, Test loss: 1.857e+08, MSE(e): 5.505e-05, MSE(pi1): 8.204e-02, MSE(pi2): 2.353e-05, MSE(pi3): 3.425e-03\n",
      "Epoch 62600, Train loss: 1.713e+06, Test loss: 1.855e+08, MSE(e): 5.504e-05, MSE(pi1): 8.203e-02, MSE(pi2): 2.352e-05, MSE(pi3): 3.424e-03\n",
      "Epoch 62700, Train loss: 1.713e+06, Test loss: 1.856e+08, MSE(e): 5.503e-05, MSE(pi1): 8.201e-02, MSE(pi2): 2.352e-05, MSE(pi3): 3.425e-03\n",
      "Epoch 62800, Train loss: 1.768e+06, Test loss: 1.815e+08, MSE(e): 6.049e-05, MSE(pi1): 8.261e-02, MSE(pi2): 2.557e-05, MSE(pi3): 3.372e-03\n",
      "Epoch 62900, Train loss: 1.712e+06, Test loss: 1.857e+08, MSE(e): 5.501e-05, MSE(pi1): 8.196e-02, MSE(pi2): 2.351e-05, MSE(pi3): 3.425e-03\n",
      "Epoch 63000, Train loss: 1.777e+06, Test loss: 1.909e+08, MSE(e): 6.154e-05, MSE(pi1): 8.128e-02, MSE(pi2): 2.642e-05, MSE(pi3): 3.485e-03\n",
      "Epoch 63100, Train loss: 1.712e+06, Test loss: 1.856e+08, MSE(e): 5.499e-05, MSE(pi1): 8.192e-02, MSE(pi2): 2.351e-05, MSE(pi3): 3.425e-03\n",
      "Epoch 63200, Train loss: 1.711e+06, Test loss: 1.856e+08, MSE(e): 5.498e-05, MSE(pi1): 8.190e-02, MSE(pi2): 2.350e-05, MSE(pi3): 3.425e-03\n",
      "Epoch 63300, Train loss: 1.711e+06, Test loss: 1.855e+08, MSE(e): 5.497e-05, MSE(pi1): 8.190e-02, MSE(pi2): 2.349e-05, MSE(pi3): 3.423e-03\n",
      "Epoch 63400, Train loss: 1.711e+06, Test loss: 1.856e+08, MSE(e): 5.497e-05, MSE(pi1): 8.186e-02, MSE(pi2): 2.349e-05, MSE(pi3): 3.425e-03\n",
      "Epoch 63500, Train loss: 1.711e+06, Test loss: 1.856e+08, MSE(e): 5.496e-05, MSE(pi1): 8.185e-02, MSE(pi2): 2.349e-05, MSE(pi3): 3.425e-03\n",
      "Epoch 63600, Train loss: 1.710e+06, Test loss: 1.855e+08, MSE(e): 5.495e-05, MSE(pi1): 8.184e-02, MSE(pi2): 2.348e-05, MSE(pi3): 3.424e-03\n",
      "Epoch 63700, Train loss: 1.710e+06, Test loss: 1.856e+08, MSE(e): 5.494e-05, MSE(pi1): 8.180e-02, MSE(pi2): 2.349e-05, MSE(pi3): 3.425e-03\n",
      "Epoch 63800, Train loss: 1.710e+06, Test loss: 1.851e+08, MSE(e): 5.496e-05, MSE(pi1): 8.185e-02, MSE(pi2): 2.347e-05, MSE(pi3): 3.420e-03\n",
      "Epoch 63900, Train loss: 1.709e+06, Test loss: 1.855e+08, MSE(e): 5.492e-05, MSE(pi1): 8.178e-02, MSE(pi2): 2.347e-05, MSE(pi3): 3.424e-03\n",
      "Epoch 64000, Train loss: 1.709e+06, Test loss: 1.853e+08, MSE(e): 5.491e-05, MSE(pi1): 8.178e-02, MSE(pi2): 2.346e-05, MSE(pi3): 3.422e-03\n",
      "Epoch 64100, Train loss: 1.709e+06, Test loss: 1.855e+08, MSE(e): 5.490e-05, MSE(pi1): 8.174e-02, MSE(pi2): 2.347e-05, MSE(pi3): 3.424e-03\n",
      "Epoch 64200, Train loss: 1.710e+06, Test loss: 1.862e+08, MSE(e): 5.502e-05, MSE(pi1): 8.164e-02, MSE(pi2): 2.354e-05, MSE(pi3): 3.431e-03\n",
      "Epoch 64300, Train loss: 1.708e+06, Test loss: 1.854e+08, MSE(e): 5.488e-05, MSE(pi1): 8.171e-02, MSE(pi2): 2.345e-05, MSE(pi3): 3.423e-03\n",
      "Epoch 64400, Train loss: 1.729e+06, Test loss: 1.834e+08, MSE(e): 5.691e-05, MSE(pi1): 8.208e-02, MSE(pi2): 2.417e-05, MSE(pi3): 3.390e-03\n",
      "Epoch 64500, Train loss: 1.708e+06, Test loss: 1.855e+08, MSE(e): 5.487e-05, MSE(pi1): 8.167e-02, MSE(pi2): 2.345e-05, MSE(pi3): 3.423e-03\n",
      "Epoch 64600, Train loss: 1.711e+06, Test loss: 1.836e+08, MSE(e): 5.516e-05, MSE(pi1): 8.181e-02, MSE(pi2): 2.352e-05, MSE(pi3): 3.410e-03\n",
      "Epoch 64700, Train loss: 1.707e+06, Test loss: 1.854e+08, MSE(e): 5.485e-05, MSE(pi1): 8.164e-02, MSE(pi2): 2.344e-05, MSE(pi3): 3.423e-03\n",
      "Epoch 64800, Train loss: 1.732e+06, Test loss: 1.837e+08, MSE(e): 5.729e-05, MSE(pi1): 8.204e-02, MSE(pi2): 2.432e-05, MSE(pi3): 3.387e-03\n",
      "Epoch 64900, Train loss: 1.707e+06, Test loss: 1.854e+08, MSE(e): 5.483e-05, MSE(pi1): 8.160e-02, MSE(pi2): 2.343e-05, MSE(pi3): 3.422e-03\n",
      "Epoch 65000, Train loss: 1.707e+06, Test loss: 1.842e+08, MSE(e): 5.488e-05, MSE(pi1): 8.165e-02, MSE(pi2): 2.343e-05, MSE(pi3): 3.416e-03\n",
      "Epoch 65100, Train loss: 1.706e+06, Test loss: 1.854e+08, MSE(e): 5.482e-05, MSE(pi1): 8.157e-02, MSE(pi2): 2.342e-05, MSE(pi3): 3.422e-03\n",
      "Epoch 65200, Train loss: 1.751e+06, Test loss: 1.890e+08, MSE(e): 5.936e-05, MSE(pi1): 8.100e-02, MSE(pi2): 2.547e-05, MSE(pi3): 3.471e-03\n",
      "Epoch 65300, Train loss: 1.705e+06, Test loss: 1.854e+08, MSE(e): 5.480e-05, MSE(pi1): 8.153e-02, MSE(pi2): 2.342e-05, MSE(pi3): 3.422e-03\n",
      "Epoch 65400, Train loss: 1.707e+06, Test loss: 1.850e+08, MSE(e): 5.492e-05, MSE(pi1): 8.161e-02, MSE(pi2): 2.344e-05, MSE(pi3): 3.413e-03\n",
      "Epoch 65500, Train loss: 1.705e+06, Test loss: 1.854e+08, MSE(e): 5.478e-05, MSE(pi1): 8.150e-02, MSE(pi2): 2.341e-05, MSE(pi3): 3.422e-03\n",
      "Epoch 65600, Train loss: 1.705e+06, Test loss: 1.846e+08, MSE(e): 5.478e-05, MSE(pi1): 8.150e-02, MSE(pi2): 2.340e-05, MSE(pi3): 3.419e-03\n",
      "Epoch 65700, Train loss: 1.704e+06, Test loss: 1.854e+08, MSE(e): 5.477e-05, MSE(pi1): 8.146e-02, MSE(pi2): 2.340e-05, MSE(pi3): 3.422e-03\n",
      "Epoch 65800, Train loss: 1.704e+06, Test loss: 1.853e+08, MSE(e): 5.476e-05, MSE(pi1): 8.144e-02, MSE(pi2): 2.340e-05, MSE(pi3): 3.421e-03\n",
      "Epoch 65900, Train loss: 1.704e+06, Test loss: 1.851e+08, MSE(e): 5.476e-05, MSE(pi1): 8.146e-02, MSE(pi2): 2.339e-05, MSE(pi3): 3.418e-03\n",
      "Epoch 66000, Train loss: 1.704e+06, Test loss: 1.853e+08, MSE(e): 5.474e-05, MSE(pi1): 8.141e-02, MSE(pi2): 2.339e-05, MSE(pi3): 3.421e-03\n",
      "Epoch 66100, Train loss: 1.708e+06, Test loss: 1.869e+08, MSE(e): 5.525e-05, MSE(pi1): 8.121e-02, MSE(pi2): 2.366e-05, MSE(pi3): 3.437e-03\n",
      "Epoch 66200, Train loss: 1.703e+06, Test loss: 1.853e+08, MSE(e): 5.472e-05, MSE(pi1): 8.138e-02, MSE(pi2): 2.338e-05, MSE(pi3): 3.420e-03\n",
      "Epoch 66300, Train loss: 1.716e+06, Test loss: 1.833e+08, MSE(e): 5.599e-05, MSE(pi1): 8.166e-02, MSE(pi2): 2.382e-05, MSE(pi3): 3.395e-03\n",
      "Epoch 66400, Train loss: 1.703e+06, Test loss: 1.853e+08, MSE(e): 5.471e-05, MSE(pi1): 8.134e-02, MSE(pi2): 2.338e-05, MSE(pi3): 3.421e-03\n",
      "Epoch 66500, Train loss: 1.750e+06, Test loss: 1.897e+08, MSE(e): 5.953e-05, MSE(pi1): 8.075e-02, MSE(pi2): 2.553e-05, MSE(pi3): 3.471e-03\n",
      "Epoch 66600, Train loss: 1.702e+06, Test loss: 1.853e+08, MSE(e): 5.469e-05, MSE(pi1): 8.131e-02, MSE(pi2): 2.337e-05, MSE(pi3): 3.420e-03\n",
      "Epoch 66700, Train loss: 1.728e+06, Test loss: 1.817e+08, MSE(e): 5.726e-05, MSE(pi1): 8.172e-02, MSE(pi2): 2.431e-05, MSE(pi3): 3.384e-03\n",
      "Epoch 66800, Train loss: 1.701e+06, Test loss: 1.853e+08, MSE(e): 5.468e-05, MSE(pi1): 8.126e-02, MSE(pi2): 2.336e-05, MSE(pi3): 3.420e-03\n",
      "Epoch 66900, Train loss: 1.701e+06, Test loss: 1.853e+08, MSE(e): 5.467e-05, MSE(pi1): 8.125e-02, MSE(pi2): 2.336e-05, MSE(pi3): 3.420e-03\n",
      "Epoch 67000, Train loss: 1.701e+06, Test loss: 1.852e+08, MSE(e): 5.466e-05, MSE(pi1): 8.123e-02, MSE(pi2): 2.336e-05, MSE(pi3): 3.420e-03\n",
      "Epoch 67100, Train loss: 1.701e+06, Test loss: 1.852e+08, MSE(e): 5.465e-05, MSE(pi1): 8.122e-02, MSE(pi2): 2.335e-05, MSE(pi3): 3.420e-03\n",
      "Epoch 67200, Train loss: 1.704e+06, Test loss: 1.861e+08, MSE(e): 5.500e-05, MSE(pi1): 8.105e-02, MSE(pi2): 2.354e-05, MSE(pi3): 3.433e-03\n",
      "Epoch 67300, Train loss: 1.700e+06, Test loss: 1.852e+08, MSE(e): 5.464e-05, MSE(pi1): 8.118e-02, MSE(pi2): 2.334e-05, MSE(pi3): 3.419e-03\n",
      "Epoch 67400, Train loss: 1.704e+06, Test loss: 1.840e+08, MSE(e): 5.506e-05, MSE(pi1): 8.134e-02, MSE(pi2): 2.347e-05, MSE(pi3): 3.404e-03\n",
      "Epoch 67500, Train loss: 1.700e+06, Test loss: 1.852e+08, MSE(e): 5.462e-05, MSE(pi1): 8.115e-02, MSE(pi2): 2.334e-05, MSE(pi3): 3.419e-03\n",
      "Epoch 67600, Train loss: 1.701e+06, Test loss: 1.843e+08, MSE(e): 5.478e-05, MSE(pi1): 8.124e-02, MSE(pi2): 2.337e-05, MSE(pi3): 3.410e-03\n",
      "Epoch 67700, Train loss: 1.699e+06, Test loss: 1.852e+08, MSE(e): 5.461e-05, MSE(pi1): 8.111e-02, MSE(pi2): 2.333e-05, MSE(pi3): 3.419e-03\n",
      "Epoch 67800, Train loss: 1.699e+06, Test loss: 1.839e+08, MSE(e): 5.464e-05, MSE(pi1): 8.116e-02, MSE(pi2): 2.333e-05, MSE(pi3): 3.414e-03\n",
      "Epoch 67900, Train loss: 1.699e+06, Test loss: 1.852e+08, MSE(e): 5.459e-05, MSE(pi1): 8.108e-02, MSE(pi2): 2.332e-05, MSE(pi3): 3.419e-03\n",
      "Epoch 68000, Train loss: 1.710e+06, Test loss: 1.830e+08, MSE(e): 5.575e-05, MSE(pi1): 8.135e-02, MSE(pi2): 2.372e-05, MSE(pi3): 3.394e-03\n",
      "Epoch 68100, Train loss: 1.698e+06, Test loss: 1.852e+08, MSE(e): 5.458e-05, MSE(pi1): 8.104e-02, MSE(pi2): 2.332e-05, MSE(pi3): 3.419e-03\n",
      "Epoch 68200, Train loss: 1.698e+06, Test loss: 1.851e+08, MSE(e): 5.457e-05, MSE(pi1): 8.104e-02, MSE(pi2): 2.331e-05, MSE(pi3): 3.418e-03\n",
      "Epoch 68300, Train loss: 1.698e+06, Test loss: 1.852e+08, MSE(e): 5.456e-05, MSE(pi1): 8.101e-02, MSE(pi2): 2.331e-05, MSE(pi3): 3.418e-03\n",
      "Epoch 68400, Train loss: 1.697e+06, Test loss: 1.851e+08, MSE(e): 5.455e-05, MSE(pi1): 8.099e-02, MSE(pi2): 2.331e-05, MSE(pi3): 3.418e-03\n",
      "Epoch 68500, Train loss: 1.697e+06, Test loss: 1.841e+08, MSE(e): 5.457e-05, MSE(pi1): 8.094e-02, MSE(pi2): 2.332e-05, MSE(pi3): 3.421e-03\n",
      "Epoch 68600, Train loss: 1.697e+06, Test loss: 1.851e+08, MSE(e): 5.454e-05, MSE(pi1): 8.096e-02, MSE(pi2): 2.330e-05, MSE(pi3): 3.418e-03\n",
      "Epoch 68700, Train loss: 1.697e+06, Test loss: 1.848e+08, MSE(e): 5.455e-05, MSE(pi1): 8.098e-02, MSE(pi2): 2.330e-05, MSE(pi3): 3.415e-03\n",
      "Epoch 68800, Train loss: 1.696e+06, Test loss: 1.851e+08, MSE(e): 5.452e-05, MSE(pi1): 8.092e-02, MSE(pi2): 2.330e-05, MSE(pi3): 3.418e-03\n",
      "Epoch 68900, Train loss: 1.715e+06, Test loss: 1.882e+08, MSE(e): 5.650e-05, MSE(pi1): 8.055e-02, MSE(pi2): 2.421e-05, MSE(pi3): 3.450e-03\n",
      "Epoch 69000, Train loss: 1.696e+06, Test loss: 1.851e+08, MSE(e): 5.451e-05, MSE(pi1): 8.090e-02, MSE(pi2): 2.329e-05, MSE(pi3): 3.417e-03\n",
      "Epoch 69100, Train loss: 1.716e+06, Test loss: 1.883e+08, MSE(e): 5.655e-05, MSE(pi1): 8.051e-02, MSE(pi2): 2.423e-05, MSE(pi3): 3.450e-03\n",
      "Epoch 69200, Train loss: 1.695e+06, Test loss: 1.851e+08, MSE(e): 5.449e-05, MSE(pi1): 8.085e-02, MSE(pi2): 2.328e-05, MSE(pi3): 3.418e-03\n",
      "Epoch 69300, Train loss: 1.695e+06, Test loss: 1.855e+08, MSE(e): 5.453e-05, MSE(pi1): 8.079e-02, MSE(pi2): 2.331e-05, MSE(pi3): 3.422e-03\n",
      "Epoch 69400, Train loss: 1.695e+06, Test loss: 1.851e+08, MSE(e): 5.448e-05, MSE(pi1): 8.082e-02, MSE(pi2): 2.328e-05, MSE(pi3): 3.417e-03\n",
      "Epoch 69500, Train loss: 1.695e+06, Test loss: 1.851e+08, MSE(e): 5.447e-05, MSE(pi1): 8.080e-02, MSE(pi2): 2.328e-05, MSE(pi3): 3.418e-03\n",
      "Epoch 69600, Train loss: 1.694e+06, Test loss: 1.851e+08, MSE(e): 5.446e-05, MSE(pi1): 8.079e-02, MSE(pi2): 2.327e-05, MSE(pi3): 3.417e-03\n",
      "Epoch 69700, Train loss: 1.694e+06, Test loss: 1.850e+08, MSE(e): 5.446e-05, MSE(pi1): 8.078e-02, MSE(pi2): 2.327e-05, MSE(pi3): 3.417e-03\n",
      "Epoch 69800, Train loss: 1.694e+06, Test loss: 1.851e+08, MSE(e): 5.445e-05, MSE(pi1): 8.076e-02, MSE(pi2): 2.327e-05, MSE(pi3): 3.417e-03\n",
      "Epoch 69900, Train loss: 1.694e+06, Test loss: 1.851e+08, MSE(e): 5.444e-05, MSE(pi1): 8.074e-02, MSE(pi2): 2.326e-05, MSE(pi3): 3.417e-03\n",
      "Epoch 70000, Train loss: 1.693e+06, Test loss: 1.850e+08, MSE(e): 5.444e-05, MSE(pi1): 8.072e-02, MSE(pi2): 2.326e-05, MSE(pi3): 3.417e-03\n",
      "Epoch 70100, Train loss: 1.693e+06, Test loss: 1.850e+08, MSE(e): 5.443e-05, MSE(pi1): 8.071e-02, MSE(pi2): 2.326e-05, MSE(pi3): 3.416e-03\n",
      "Epoch 70200, Train loss: 1.693e+06, Test loss: 1.852e+08, MSE(e): 5.445e-05, MSE(pi1): 8.065e-02, MSE(pi2): 2.327e-05, MSE(pi3): 3.420e-03\n",
      "Epoch 70300, Train loss: 1.693e+06, Test loss: 1.850e+08, MSE(e): 5.442e-05, MSE(pi1): 8.068e-02, MSE(pi2): 2.325e-05, MSE(pi3): 3.416e-03\n",
      "Epoch 70400, Train loss: 1.693e+06, Test loss: 1.844e+08, MSE(e): 5.451e-05, MSE(pi1): 8.075e-02, MSE(pi2): 2.327e-05, MSE(pi3): 3.409e-03\n",
      "Epoch 70500, Train loss: 1.692e+06, Test loss: 1.850e+08, MSE(e): 5.440e-05, MSE(pi1): 8.064e-02, MSE(pi2): 2.324e-05, MSE(pi3): 3.416e-03\n",
      "Epoch 70600, Train loss: 1.692e+06, Test loss: 1.849e+08, MSE(e): 5.440e-05, MSE(pi1): 8.062e-02, MSE(pi2): 2.324e-05, MSE(pi3): 3.417e-03\n",
      "Epoch 70700, Train loss: 1.692e+06, Test loss: 1.850e+08, MSE(e): 5.439e-05, MSE(pi1): 8.061e-02, MSE(pi2): 2.324e-05, MSE(pi3): 3.416e-03\n",
      "Epoch 70800, Train loss: 1.693e+06, Test loss: 1.844e+08, MSE(e): 5.452e-05, MSE(pi1): 8.070e-02, MSE(pi2): 2.327e-05, MSE(pi3): 3.407e-03\n",
      "Epoch 70900, Train loss: 1.691e+06, Test loss: 1.850e+08, MSE(e): 5.437e-05, MSE(pi1): 8.058e-02, MSE(pi2): 2.323e-05, MSE(pi3): 3.416e-03\n",
      "Epoch 71000, Train loss: 1.692e+06, Test loss: 1.846e+08, MSE(e): 5.444e-05, MSE(pi1): 8.064e-02, MSE(pi2): 2.324e-05, MSE(pi3): 3.409e-03\n",
      "Epoch 71100, Train loss: 1.691e+06, Test loss: 1.850e+08, MSE(e): 5.436e-05, MSE(pi1): 8.055e-02, MSE(pi2): 2.323e-05, MSE(pi3): 3.415e-03\n",
      "Epoch 71200, Train loss: 1.690e+06, Test loss: 1.851e+08, MSE(e): 5.437e-05, MSE(pi1): 8.050e-02, MSE(pi2): 2.324e-05, MSE(pi3): 3.418e-03\n",
      "Epoch 71300, Train loss: 1.690e+06, Test loss: 1.849e+08, MSE(e): 5.435e-05, MSE(pi1): 8.052e-02, MSE(pi2): 2.322e-05, MSE(pi3): 3.415e-03\n",
      "Epoch 71400, Train loss: 1.690e+06, Test loss: 1.848e+08, MSE(e): 5.434e-05, MSE(pi1): 8.051e-02, MSE(pi2): 2.322e-05, MSE(pi3): 3.414e-03\n",
      "Epoch 71500, Train loss: 1.710e+06, Test loss: 1.820e+08, MSE(e): 5.634e-05, MSE(pi1): 8.086e-02, MSE(pi2): 2.395e-05, MSE(pi3): 3.383e-03\n",
      "Epoch 71600, Train loss: 1.689e+06, Test loss: 1.849e+08, MSE(e): 5.433e-05, MSE(pi1): 8.047e-02, MSE(pi2): 2.321e-05, MSE(pi3): 3.415e-03\n",
      "Epoch 71700, Train loss: 1.690e+06, Test loss: 1.858e+08, MSE(e): 5.437e-05, MSE(pi1): 8.040e-02, MSE(pi2): 2.324e-05, MSE(pi3): 3.419e-03\n",
      "Epoch 71800, Train loss: 1.689e+06, Test loss: 1.850e+08, MSE(e): 5.431e-05, MSE(pi1): 8.043e-02, MSE(pi2): 2.321e-05, MSE(pi3): 3.415e-03\n",
      "Epoch 71900, Train loss: 1.689e+06, Test loss: 1.844e+08, MSE(e): 5.433e-05, MSE(pi1): 8.038e-02, MSE(pi2): 2.322e-05, MSE(pi3): 3.418e-03\n",
      "Epoch 72000, Train loss: 1.688e+06, Test loss: 1.849e+08, MSE(e): 5.430e-05, MSE(pi1): 8.040e-02, MSE(pi2): 2.320e-05, MSE(pi3): 3.415e-03\n",
      "Epoch 72100, Train loss: 1.688e+06, Test loss: 1.849e+08, MSE(e): 5.429e-05, MSE(pi1): 8.039e-02, MSE(pi2): 2.320e-05, MSE(pi3): 3.415e-03\n",
      "Epoch 72200, Train loss: 1.690e+06, Test loss: 1.858e+08, MSE(e): 5.450e-05, MSE(pi1): 8.026e-02, MSE(pi2): 2.331e-05, MSE(pi3): 3.425e-03\n",
      "Epoch 72300, Train loss: 1.688e+06, Test loss: 1.849e+08, MSE(e): 5.428e-05, MSE(pi1): 8.036e-02, MSE(pi2): 2.320e-05, MSE(pi3): 3.414e-03\n",
      "Epoch 72400, Train loss: 1.688e+06, Test loss: 1.846e+08, MSE(e): 5.432e-05, MSE(pi1): 8.040e-02, MSE(pi2): 2.320e-05, MSE(pi3): 3.409e-03\n",
      "Epoch 72500, Train loss: 1.687e+06, Test loss: 1.849e+08, MSE(e): 5.427e-05, MSE(pi1): 8.033e-02, MSE(pi2): 2.319e-05, MSE(pi3): 3.414e-03\n",
      "Epoch 72600, Train loss: 1.687e+06, Test loss: 1.842e+08, MSE(e): 5.428e-05, MSE(pi1): 8.035e-02, MSE(pi2): 2.319e-05, MSE(pi3): 3.410e-03\n",
      "Epoch 72700, Train loss: 1.687e+06, Test loss: 1.849e+08, MSE(e): 5.425e-05, MSE(pi1): 8.029e-02, MSE(pi2): 2.319e-05, MSE(pi3): 3.414e-03\n",
      "Epoch 72800, Train loss: 1.702e+06, Test loss: 1.877e+08, MSE(e): 5.584e-05, MSE(pi1): 7.995e-02, MSE(pi2): 2.392e-05, MSE(pi3): 3.442e-03\n",
      "Epoch 72900, Train loss: 1.686e+06, Test loss: 1.849e+08, MSE(e): 5.424e-05, MSE(pi1): 8.026e-02, MSE(pi2): 2.318e-05, MSE(pi3): 3.414e-03\n",
      "Epoch 73000, Train loss: 1.687e+06, Test loss: 1.839e+08, MSE(e): 5.432e-05, MSE(pi1): 8.018e-02, MSE(pi2): 2.323e-05, MSE(pi3): 3.420e-03\n",
      "Epoch 73100, Train loss: 1.686e+06, Test loss: 1.849e+08, MSE(e): 5.423e-05, MSE(pi1): 8.023e-02, MSE(pi2): 2.317e-05, MSE(pi3): 3.413e-03\n",
      "Epoch 73200, Train loss: 1.686e+06, Test loss: 1.849e+08, MSE(e): 5.422e-05, MSE(pi1): 8.021e-02, MSE(pi2): 2.317e-05, MSE(pi3): 3.414e-03\n",
      "Epoch 73300, Train loss: 1.685e+06, Test loss: 1.849e+08, MSE(e): 5.422e-05, MSE(pi1): 8.019e-02, MSE(pi2): 2.317e-05, MSE(pi3): 3.414e-03\n",
      "Epoch 73400, Train loss: 1.685e+06, Test loss: 1.849e+08, MSE(e): 5.421e-05, MSE(pi1): 8.018e-02, MSE(pi2): 2.317e-05, MSE(pi3): 3.413e-03\n",
      "Epoch 73500, Train loss: 1.685e+06, Test loss: 1.849e+08, MSE(e): 5.420e-05, MSE(pi1): 8.017e-02, MSE(pi2): 2.316e-05, MSE(pi3): 3.413e-03\n",
      "Epoch 73600, Train loss: 1.685e+06, Test loss: 1.848e+08, MSE(e): 5.420e-05, MSE(pi1): 8.016e-02, MSE(pi2): 2.316e-05, MSE(pi3): 3.413e-03\n",
      "Epoch 73700, Train loss: 1.685e+06, Test loss: 1.849e+08, MSE(e): 5.419e-05, MSE(pi1): 8.014e-02, MSE(pi2): 2.316e-05, MSE(pi3): 3.413e-03\n",
      "Epoch 73800, Train loss: 1.684e+06, Test loss: 1.849e+08, MSE(e): 5.418e-05, MSE(pi1): 8.013e-02, MSE(pi2): 2.316e-05, MSE(pi3): 3.413e-03\n",
      "Epoch 73900, Train loss: 1.684e+06, Test loss: 1.848e+08, MSE(e): 5.418e-05, MSE(pi1): 8.011e-02, MSE(pi2): 2.315e-05, MSE(pi3): 3.413e-03\n",
      "Epoch 74000, Train loss: 1.684e+06, Test loss: 1.848e+08, MSE(e): 5.417e-05, MSE(pi1): 8.009e-02, MSE(pi2): 2.315e-05, MSE(pi3): 3.413e-03\n",
      "Epoch 74100, Train loss: 1.684e+06, Test loss: 1.847e+08, MSE(e): 5.417e-05, MSE(pi1): 8.010e-02, MSE(pi2): 2.315e-05, MSE(pi3): 3.410e-03\n",
      "Epoch 74200, Train loss: 1.683e+06, Test loss: 1.849e+08, MSE(e): 5.416e-05, MSE(pi1): 8.006e-02, MSE(pi2): 2.315e-05, MSE(pi3): 3.413e-03\n",
      "Epoch 74300, Train loss: 1.683e+06, Test loss: 1.853e+08, MSE(e): 5.416e-05, MSE(pi1): 8.002e-02, MSE(pi2): 2.316e-05, MSE(pi3): 3.415e-03\n",
      "Epoch 74400, Train loss: 1.683e+06, Test loss: 1.848e+08, MSE(e): 5.415e-05, MSE(pi1): 8.003e-02, MSE(pi2): 2.314e-05, MSE(pi3): 3.412e-03\n",
      "Epoch 74500, Train loss: 1.687e+06, Test loss: 1.839e+08, MSE(e): 5.451e-05, MSE(pi1): 8.018e-02, MSE(pi2): 2.326e-05, MSE(pi3): 3.398e-03\n",
      "Epoch 74600, Train loss: 1.683e+06, Test loss: 1.848e+08, MSE(e): 5.413e-05, MSE(pi1): 8.000e-02, MSE(pi2): 2.314e-05, MSE(pi3): 3.412e-03\n",
      "Epoch 74700, Train loss: 1.697e+06, Test loss: 1.836e+08, MSE(e): 5.555e-05, MSE(pi1): 8.030e-02, MSE(pi2): 2.365e-05, MSE(pi3): 3.385e-03\n",
      "Epoch 74800, Train loss: 1.682e+06, Test loss: 1.848e+08, MSE(e): 5.412e-05, MSE(pi1): 7.997e-02, MSE(pi2): 2.313e-05, MSE(pi3): 3.412e-03\n",
      "Epoch 74900, Train loss: 1.685e+06, Test loss: 1.846e+08, MSE(e): 5.442e-05, MSE(pi1): 8.010e-02, MSE(pi2): 2.322e-05, MSE(pi3): 3.399e-03\n",
      "Epoch 75000, Train loss: 1.682e+06, Test loss: 1.849e+08, MSE(e): 5.411e-05, MSE(pi1): 7.994e-02, MSE(pi2): 2.313e-05, MSE(pi3): 3.412e-03\n",
      "Epoch 75100, Train loss: 1.682e+06, Test loss: 1.851e+08, MSE(e): 5.417e-05, MSE(pi1): 7.986e-02, MSE(pi2): 2.316e-05, MSE(pi3): 3.417e-03\n",
      "Epoch 75200, Train loss: 1.681e+06, Test loss: 1.848e+08, MSE(e): 5.410e-05, MSE(pi1): 7.991e-02, MSE(pi2): 2.312e-05, MSE(pi3): 3.412e-03\n",
      "Epoch 75300, Train loss: 1.681e+06, Test loss: 1.846e+08, MSE(e): 5.412e-05, MSE(pi1): 7.994e-02, MSE(pi2): 2.312e-05, MSE(pi3): 3.407e-03\n",
      "Epoch 75400, Train loss: 1.681e+06, Test loss: 1.848e+08, MSE(e): 5.409e-05, MSE(pi1): 7.988e-02, MSE(pi2): 2.312e-05, MSE(pi3): 3.411e-03\n",
      "Epoch 75500, Train loss: 1.681e+06, Test loss: 1.847e+08, MSE(e): 5.408e-05, MSE(pi1): 7.987e-02, MSE(pi2): 2.311e-05, MSE(pi3): 3.411e-03\n",
      "Epoch 75600, Train loss: 1.684e+06, Test loss: 1.862e+08, MSE(e): 5.448e-05, MSE(pi1): 7.968e-02, MSE(pi2): 2.331e-05, MSE(pi3): 3.425e-03\n",
      "Epoch 75700, Train loss: 1.680e+06, Test loss: 1.848e+08, MSE(e): 5.407e-05, MSE(pi1): 7.983e-02, MSE(pi2): 2.311e-05, MSE(pi3): 3.411e-03\n",
      "Epoch 75800, Train loss: 1.680e+06, Test loss: 1.848e+08, MSE(e): 5.406e-05, MSE(pi1): 7.982e-02, MSE(pi2): 2.311e-05, MSE(pi3): 3.411e-03\n",
      "Epoch 75900, Train loss: 1.680e+06, Test loss: 1.848e+08, MSE(e): 5.406e-05, MSE(pi1): 7.980e-02, MSE(pi2): 2.311e-05, MSE(pi3): 3.411e-03\n",
      "Epoch 76000, Train loss: 1.679e+06, Test loss: 1.848e+08, MSE(e): 5.405e-05, MSE(pi1): 7.978e-02, MSE(pi2): 2.311e-05, MSE(pi3): 3.411e-03\n",
      "Epoch 76100, Train loss: 1.679e+06, Test loss: 1.846e+08, MSE(e): 5.405e-05, MSE(pi1): 7.980e-02, MSE(pi2): 2.310e-05, MSE(pi3): 3.408e-03\n",
      "Epoch 76200, Train loss: 1.679e+06, Test loss: 1.848e+08, MSE(e): 5.404e-05, MSE(pi1): 7.975e-02, MSE(pi2): 2.310e-05, MSE(pi3): 3.410e-03\n",
      "Epoch 76300, Train loss: 1.679e+06, Test loss: 1.847e+08, MSE(e): 5.403e-05, MSE(pi1): 7.974e-02, MSE(pi2): 2.310e-05, MSE(pi3): 3.410e-03\n",
      "Epoch 76400, Train loss: 1.679e+06, Test loss: 1.847e+08, MSE(e): 5.403e-05, MSE(pi1): 7.974e-02, MSE(pi2): 2.309e-05, MSE(pi3): 3.409e-03\n",
      "Epoch 76500, Train loss: 1.678e+06, Test loss: 1.847e+08, MSE(e): 5.402e-05, MSE(pi1): 7.972e-02, MSE(pi2): 2.309e-05, MSE(pi3): 3.409e-03\n",
      "Epoch 76600, Train loss: 1.702e+06, Test loss: 1.830e+08, MSE(e): 5.630e-05, MSE(pi1): 8.009e-02, MSE(pi2): 2.396e-05, MSE(pi3): 3.376e-03\n",
      "Epoch 76700, Train loss: 1.678e+06, Test loss: 1.848e+08, MSE(e): 5.401e-05, MSE(pi1): 7.967e-02, MSE(pi2): 2.309e-05, MSE(pi3): 3.411e-03\n",
      "Epoch 76800, Train loss: 1.678e+06, Test loss: 1.850e+08, MSE(e): 5.407e-05, MSE(pi1): 7.960e-02, MSE(pi2): 2.313e-05, MSE(pi3): 3.415e-03\n",
      "Epoch 76900, Train loss: 1.677e+06, Test loss: 1.848e+08, MSE(e): 5.400e-05, MSE(pi1): 7.965e-02, MSE(pi2): 2.309e-05, MSE(pi3): 3.410e-03\n",
      "Epoch 77000, Train loss: 1.677e+06, Test loss: 1.845e+08, MSE(e): 5.399e-05, MSE(pi1): 7.964e-02, MSE(pi2): 2.308e-05, MSE(pi3): 3.409e-03\n",
      "Epoch 77100, Train loss: 1.677e+06, Test loss: 1.848e+08, MSE(e): 5.399e-05, MSE(pi1): 7.962e-02, MSE(pi2): 2.308e-05, MSE(pi3): 3.410e-03\n",
      "Epoch 77200, Train loss: 1.677e+06, Test loss: 1.844e+08, MSE(e): 5.400e-05, MSE(pi1): 7.964e-02, MSE(pi2): 2.308e-05, MSE(pi3): 3.406e-03\n",
      "Epoch 77300, Train loss: 1.677e+06, Test loss: 1.848e+08, MSE(e): 5.397e-05, MSE(pi1): 7.958e-02, MSE(pi2): 2.308e-05, MSE(pi3): 3.409e-03\n",
      "Epoch 77400, Train loss: 1.676e+06, Test loss: 1.848e+08, MSE(e): 5.397e-05, MSE(pi1): 7.955e-02, MSE(pi2): 2.308e-05, MSE(pi3): 3.411e-03\n",
      "Epoch 77500, Train loss: 1.676e+06, Test loss: 1.848e+08, MSE(e): 5.396e-05, MSE(pi1): 7.955e-02, MSE(pi2): 2.307e-05, MSE(pi3): 3.409e-03\n",
      "Epoch 77600, Train loss: 1.676e+06, Test loss: 1.847e+08, MSE(e): 5.396e-05, MSE(pi1): 7.956e-02, MSE(pi2): 2.307e-05, MSE(pi3): 3.408e-03\n",
      "Epoch 77700, Train loss: 1.676e+06, Test loss: 1.848e+08, MSE(e): 5.395e-05, MSE(pi1): 7.952e-02, MSE(pi2): 2.307e-05, MSE(pi3): 3.409e-03\n",
      "Epoch 77800, Train loss: 1.676e+06, Test loss: 1.853e+08, MSE(e): 5.401e-05, MSE(pi1): 7.944e-02, MSE(pi2): 2.311e-05, MSE(pi3): 3.415e-03\n",
      "Epoch 77900, Train loss: 1.675e+06, Test loss: 1.848e+08, MSE(e): 5.394e-05, MSE(pi1): 7.949e-02, MSE(pi2): 2.307e-05, MSE(pi3): 3.409e-03\n",
      "Epoch 78000, Train loss: 1.675e+06, Test loss: 1.845e+08, MSE(e): 5.393e-05, MSE(pi1): 7.948e-02, MSE(pi2): 2.306e-05, MSE(pi3): 3.409e-03\n",
      "Epoch 78100, Train loss: 1.675e+06, Test loss: 1.848e+08, MSE(e): 5.393e-05, MSE(pi1): 7.946e-02, MSE(pi2): 2.306e-05, MSE(pi3): 3.409e-03\n",
      "Epoch 78200, Train loss: 1.675e+06, Test loss: 1.848e+08, MSE(e): 5.398e-05, MSE(pi1): 7.939e-02, MSE(pi2): 2.309e-05, MSE(pi3): 3.414e-03\n",
      "Epoch 78300, Train loss: 1.674e+06, Test loss: 1.848e+08, MSE(e): 5.392e-05, MSE(pi1): 7.943e-02, MSE(pi2): 2.306e-05, MSE(pi3): 3.409e-03\n",
      "Epoch 78400, Train loss: 1.674e+06, Test loss: 1.847e+08, MSE(e): 5.392e-05, MSE(pi1): 7.944e-02, MSE(pi2): 2.305e-05, MSE(pi3): 3.406e-03\n",
      "Epoch 78500, Train loss: 1.674e+06, Test loss: 1.848e+08, MSE(e): 5.391e-05, MSE(pi1): 7.940e-02, MSE(pi2): 2.305e-05, MSE(pi3): 3.408e-03\n",
      "Epoch 78600, Train loss: 1.674e+06, Test loss: 1.852e+08, MSE(e): 5.398e-05, MSE(pi1): 7.931e-02, MSE(pi2): 2.310e-05, MSE(pi3): 3.414e-03\n",
      "Epoch 78700, Train loss: 1.673e+06, Test loss: 1.848e+08, MSE(e): 5.390e-05, MSE(pi1): 7.937e-02, MSE(pi2): 2.305e-05, MSE(pi3): 3.408e-03\n",
      "Epoch 78800, Train loss: 1.673e+06, Test loss: 1.855e+08, MSE(e): 5.391e-05, MSE(pi1): 7.932e-02, MSE(pi2): 2.306e-05, MSE(pi3): 3.411e-03\n",
      "Epoch 78900, Train loss: 1.673e+06, Test loss: 1.847e+08, MSE(e): 5.389e-05, MSE(pi1): 7.934e-02, MSE(pi2): 2.304e-05, MSE(pi3): 3.408e-03\n",
      "Epoch 79000, Train loss: 1.673e+06, Test loss: 1.847e+08, MSE(e): 5.390e-05, MSE(pi1): 7.937e-02, MSE(pi2): 2.304e-05, MSE(pi3): 3.404e-03\n",
      "Epoch 79100, Train loss: 1.673e+06, Test loss: 1.848e+08, MSE(e): 5.388e-05, MSE(pi1): 7.930e-02, MSE(pi2): 2.304e-05, MSE(pi3): 3.408e-03\n",
      "Epoch 79200, Train loss: 1.672e+06, Test loss: 1.845e+08, MSE(e): 5.388e-05, MSE(pi1): 7.932e-02, MSE(pi2): 2.304e-05, MSE(pi3): 3.406e-03\n",
      "Epoch 79300, Train loss: 1.672e+06, Test loss: 1.848e+08, MSE(e): 5.387e-05, MSE(pi1): 7.927e-02, MSE(pi2): 2.304e-05, MSE(pi3): 3.408e-03\n",
      "Epoch 79400, Train loss: 1.672e+06, Test loss: 1.848e+08, MSE(e): 5.386e-05, MSE(pi1): 7.925e-02, MSE(pi2): 2.304e-05, MSE(pi3): 3.408e-03\n",
      "Epoch 79500, Train loss: 1.672e+06, Test loss: 1.847e+08, MSE(e): 5.385e-05, MSE(pi1): 7.925e-02, MSE(pi2): 2.303e-05, MSE(pi3): 3.407e-03\n",
      "Epoch 79600, Train loss: 1.672e+06, Test loss: 1.847e+08, MSE(e): 5.385e-05, MSE(pi1): 7.924e-02, MSE(pi2): 2.303e-05, MSE(pi3): 3.407e-03\n",
      "Epoch 79700, Train loss: 1.671e+06, Test loss: 1.848e+08, MSE(e): 5.384e-05, MSE(pi1): 7.920e-02, MSE(pi2): 2.303e-05, MSE(pi3): 3.408e-03\n",
      "Epoch 79800, Train loss: 1.671e+06, Test loss: 1.847e+08, MSE(e): 5.384e-05, MSE(pi1): 7.920e-02, MSE(pi2): 2.303e-05, MSE(pi3): 3.407e-03\n",
      "Epoch 79900, Train loss: 1.671e+06, Test loss: 1.847e+08, MSE(e): 5.383e-05, MSE(pi1): 7.919e-02, MSE(pi2): 2.302e-05, MSE(pi3): 3.407e-03\n",
      "Epoch 80000, Train loss: 1.671e+06, Test loss: 1.846e+08, MSE(e): 5.383e-05, MSE(pi1): 7.918e-02, MSE(pi2): 2.302e-05, MSE(pi3): 3.405e-03\n",
      "Epoch 80100, Train loss: 1.670e+06, Test loss: 1.847e+08, MSE(e): 5.382e-05, MSE(pi1): 7.915e-02, MSE(pi2): 2.302e-05, MSE(pi3): 3.407e-03\n",
      "Epoch 80200, Train loss: 1.671e+06, Test loss: 1.840e+08, MSE(e): 5.384e-05, MSE(pi1): 7.918e-02, MSE(pi2): 2.302e-05, MSE(pi3): 3.403e-03\n",
      "Epoch 80300, Train loss: 1.670e+06, Test loss: 1.847e+08, MSE(e): 5.381e-05, MSE(pi1): 7.912e-02, MSE(pi2): 2.302e-05, MSE(pi3): 3.407e-03\n",
      "Epoch 80400, Train loss: 1.670e+06, Test loss: 1.847e+08, MSE(e): 5.381e-05, MSE(pi1): 7.911e-02, MSE(pi2): 2.302e-05, MSE(pi3): 3.407e-03\n",
      "Epoch 80500, Train loss: 1.672e+06, Test loss: 1.866e+08, MSE(e): 5.411e-05, MSE(pi1): 7.895e-02, MSE(pi2): 2.316e-05, MSE(pi3): 3.419e-03\n",
      "Epoch 80600, Train loss: 1.669e+06, Test loss: 1.847e+08, MSE(e): 5.380e-05, MSE(pi1): 7.907e-02, MSE(pi2): 2.301e-05, MSE(pi3): 3.407e-03\n",
      "Epoch 80700, Train loss: 1.669e+06, Test loss: 1.847e+08, MSE(e): 5.379e-05, MSE(pi1): 7.906e-02, MSE(pi2): 2.301e-05, MSE(pi3): 3.406e-03\n",
      "Epoch 80800, Train loss: 1.722e+06, Test loss: 1.811e+08, MSE(e): 5.896e-05, MSE(pi1): 7.965e-02, MSE(pi2): 2.505e-05, MSE(pi3): 3.355e-03\n",
      "Epoch 80900, Train loss: 1.669e+06, Test loss: 1.847e+08, MSE(e): 5.378e-05, MSE(pi1): 7.903e-02, MSE(pi2): 2.301e-05, MSE(pi3): 3.406e-03\n",
      "Epoch 81000, Train loss: 1.696e+06, Test loss: 1.819e+08, MSE(e): 5.648e-05, MSE(pi1): 7.945e-02, MSE(pi2): 2.405e-05, MSE(pi3): 3.369e-03\n",
      "Epoch 81100, Train loss: 1.668e+06, Test loss: 1.847e+08, MSE(e): 5.377e-05, MSE(pi1): 7.899e-02, MSE(pi2): 2.300e-05, MSE(pi3): 3.406e-03\n",
      "Epoch 81200, Train loss: 1.671e+06, Test loss: 1.835e+08, MSE(e): 5.409e-05, MSE(pi1): 7.913e-02, MSE(pi2): 2.311e-05, MSE(pi3): 3.393e-03\n",
      "Epoch 81300, Train loss: 1.668e+06, Test loss: 1.847e+08, MSE(e): 5.376e-05, MSE(pi1): 7.896e-02, MSE(pi2): 2.300e-05, MSE(pi3): 3.406e-03\n",
      "Epoch 81400, Train loss: 1.668e+06, Test loss: 1.840e+08, MSE(e): 5.377e-05, MSE(pi1): 7.892e-02, MSE(pi2): 2.301e-05, MSE(pi3): 3.408e-03\n",
      "Epoch 81500, Train loss: 1.667e+06, Test loss: 1.847e+08, MSE(e): 5.375e-05, MSE(pi1): 7.893e-02, MSE(pi2): 2.300e-05, MSE(pi3): 3.406e-03\n",
      "Epoch 81600, Train loss: 1.670e+06, Test loss: 1.856e+08, MSE(e): 5.408e-05, MSE(pi1): 7.877e-02, MSE(pi2): 2.315e-05, MSE(pi3): 3.418e-03\n",
      "Epoch 81700, Train loss: 1.667e+06, Test loss: 1.847e+08, MSE(e): 5.374e-05, MSE(pi1): 7.890e-02, MSE(pi2): 2.299e-05, MSE(pi3): 3.406e-03\n",
      "Epoch 81800, Train loss: 1.682e+06, Test loss: 1.835e+08, MSE(e): 5.522e-05, MSE(pi1): 7.920e-02, MSE(pi2): 2.355e-05, MSE(pi3): 3.378e-03\n",
      "Epoch 81900, Train loss: 1.667e+06, Test loss: 1.847e+08, MSE(e): 5.373e-05, MSE(pi1): 7.886e-02, MSE(pi2): 2.299e-05, MSE(pi3): 3.406e-03\n",
      "Epoch 82000, Train loss: 1.666e+06, Test loss: 1.847e+08, MSE(e): 5.373e-05, MSE(pi1): 7.885e-02, MSE(pi2): 2.299e-05, MSE(pi3): 3.405e-03\n",
      "Epoch 82100, Train loss: 1.666e+06, Test loss: 1.845e+08, MSE(e): 5.373e-05, MSE(pi1): 7.887e-02, MSE(pi2): 2.299e-05, MSE(pi3): 3.402e-03\n",
      "Epoch 82200, Train loss: 1.666e+06, Test loss: 1.847e+08, MSE(e): 5.372e-05, MSE(pi1): 7.882e-02, MSE(pi2): 2.298e-05, MSE(pi3): 3.405e-03\n",
      "Epoch 82300, Train loss: 1.666e+06, Test loss: 1.842e+08, MSE(e): 5.374e-05, MSE(pi1): 7.886e-02, MSE(pi2): 2.299e-05, MSE(pi3): 3.400e-03\n",
      "Epoch 82400, Train loss: 1.665e+06, Test loss: 1.847e+08, MSE(e): 5.371e-05, MSE(pi1): 7.879e-02, MSE(pi2): 2.298e-05, MSE(pi3): 3.405e-03\n",
      "Epoch 82500, Train loss: 1.665e+06, Test loss: 1.847e+08, MSE(e): 5.370e-05, MSE(pi1): 7.878e-02, MSE(pi2): 2.298e-05, MSE(pi3): 3.405e-03\n",
      "Epoch 82600, Train loss: 1.665e+06, Test loss: 1.848e+08, MSE(e): 5.370e-05, MSE(pi1): 7.874e-02, MSE(pi2): 2.298e-05, MSE(pi3): 3.406e-03\n",
      "Epoch 82700, Train loss: 1.665e+06, Test loss: 1.847e+08, MSE(e): 5.369e-05, MSE(pi1): 7.874e-02, MSE(pi2): 2.297e-05, MSE(pi3): 3.405e-03\n",
      "Epoch 82800, Train loss: 1.688e+06, Test loss: 1.876e+08, MSE(e): 5.611e-05, MSE(pi1): 7.833e-02, MSE(pi2): 2.403e-05, MSE(pi3): 3.440e-03\n",
      "Epoch 82900, Train loss: 1.664e+06, Test loss: 1.847e+08, MSE(e): 5.368e-05, MSE(pi1): 7.871e-02, MSE(pi2): 2.297e-05, MSE(pi3): 3.405e-03\n",
      "Epoch 83000, Train loss: 1.683e+06, Test loss: 1.860e+08, MSE(e): 5.559e-05, MSE(pi1): 7.834e-02, MSE(pi2): 2.381e-05, MSE(pi3): 3.436e-03\n",
      "Epoch 83100, Train loss: 1.664e+06, Test loss: 1.847e+08, MSE(e): 5.367e-05, MSE(pi1): 7.867e-02, MSE(pi2): 2.297e-05, MSE(pi3): 3.404e-03\n",
      "Epoch 83200, Train loss: 1.670e+06, Test loss: 1.860e+08, MSE(e): 5.428e-05, MSE(pi1): 7.846e-02, MSE(pi2): 2.325e-05, MSE(pi3): 3.422e-03\n",
      "Epoch 83300, Train loss: 1.663e+06, Test loss: 1.847e+08, MSE(e): 5.366e-05, MSE(pi1): 7.865e-02, MSE(pi2): 2.296e-05, MSE(pi3): 3.404e-03\n",
      "Epoch 83400, Train loss: 1.663e+06, Test loss: 1.840e+08, MSE(e): 5.367e-05, MSE(pi1): 7.860e-02, MSE(pi2): 2.297e-05, MSE(pi3): 3.406e-03\n",
      "Epoch 83500, Train loss: 1.663e+06, Test loss: 1.847e+08, MSE(e): 5.365e-05, MSE(pi1): 7.861e-02, MSE(pi2): 2.296e-05, MSE(pi3): 3.404e-03\n",
      "Epoch 83600, Train loss: 1.668e+06, Test loss: 1.864e+08, MSE(e): 5.417e-05, MSE(pi1): 7.841e-02, MSE(pi2): 2.320e-05, MSE(pi3): 3.420e-03\n",
      "Epoch 83700, Train loss: 1.663e+06, Test loss: 1.847e+08, MSE(e): 5.364e-05, MSE(pi1): 7.859e-02, MSE(pi2): 2.296e-05, MSE(pi3): 3.403e-03\n",
      "Epoch 83800, Train loss: 1.662e+06, Test loss: 1.847e+08, MSE(e): 5.364e-05, MSE(pi1): 7.856e-02, MSE(pi2): 2.296e-05, MSE(pi3): 3.404e-03\n",
      "Epoch 83900, Train loss: 1.662e+06, Test loss: 1.848e+08, MSE(e): 5.365e-05, MSE(pi1): 7.852e-02, MSE(pi2): 2.296e-05, MSE(pi3): 3.406e-03\n",
      "Epoch 84000, Train loss: 1.662e+06, Test loss: 1.847e+08, MSE(e): 5.363e-05, MSE(pi1): 7.853e-02, MSE(pi2): 2.295e-05, MSE(pi3): 3.404e-03\n",
      "Epoch 84100, Train loss: 1.662e+06, Test loss: 1.845e+08, MSE(e): 5.363e-05, MSE(pi1): 7.855e-02, MSE(pi2): 2.295e-05, MSE(pi3): 3.401e-03\n",
      "Epoch 84200, Train loss: 1.662e+06, Test loss: 1.847e+08, MSE(e): 5.362e-05, MSE(pi1): 7.850e-02, MSE(pi2): 2.295e-05, MSE(pi3): 3.404e-03\n",
      "Epoch 84300, Train loss: 1.661e+06, Test loss: 1.848e+08, MSE(e): 5.362e-05, MSE(pi1): 7.847e-02, MSE(pi2): 2.295e-05, MSE(pi3): 3.404e-03\n",
      "Epoch 84400, Train loss: 1.661e+06, Test loss: 1.847e+08, MSE(e): 5.361e-05, MSE(pi1): 7.846e-02, MSE(pi2): 2.295e-05, MSE(pi3): 3.403e-03\n",
      "Epoch 84500, Train loss: 1.661e+06, Test loss: 1.847e+08, MSE(e): 5.361e-05, MSE(pi1): 7.847e-02, MSE(pi2): 2.294e-05, MSE(pi3): 3.402e-03\n",
      "Epoch 84600, Train loss: 1.661e+06, Test loss: 1.847e+08, MSE(e): 5.360e-05, MSE(pi1): 7.843e-02, MSE(pi2): 2.294e-05, MSE(pi3): 3.403e-03\n",
      "Epoch 84700, Train loss: 1.660e+06, Test loss: 1.847e+08, MSE(e): 5.360e-05, MSE(pi1): 7.841e-02, MSE(pi2): 2.294e-05, MSE(pi3): 3.404e-03\n",
      "Epoch 84800, Train loss: 1.660e+06, Test loss: 1.848e+08, MSE(e): 5.359e-05, MSE(pi1): 7.840e-02, MSE(pi2): 2.294e-05, MSE(pi3): 3.403e-03\n",
      "Epoch 84900, Train loss: 1.660e+06, Test loss: 1.846e+08, MSE(e): 5.359e-05, MSE(pi1): 7.840e-02, MSE(pi2): 2.294e-05, MSE(pi3): 3.401e-03\n",
      "Epoch 85000, Train loss: 1.660e+06, Test loss: 1.846e+08, MSE(e): 5.358e-05, MSE(pi1): 7.838e-02, MSE(pi2): 2.294e-05, MSE(pi3): 3.401e-03\n",
      "Epoch 85100, Train loss: 1.660e+06, Test loss: 1.847e+08, MSE(e): 5.358e-05, MSE(pi1): 7.836e-02, MSE(pi2): 2.293e-05, MSE(pi3): 3.402e-03\n",
      "Epoch 85200, Train loss: 1.659e+06, Test loss: 1.848e+08, MSE(e): 5.357e-05, MSE(pi1): 7.833e-02, MSE(pi2): 2.294e-05, MSE(pi3): 3.403e-03\n",
      "Epoch 85300, Train loss: 1.659e+06, Test loss: 1.846e+08, MSE(e): 5.357e-05, MSE(pi1): 7.834e-02, MSE(pi2): 2.293e-05, MSE(pi3): 3.401e-03\n",
      "Epoch 85400, Train loss: 1.664e+06, Test loss: 1.832e+08, MSE(e): 5.400e-05, MSE(pi1): 7.848e-02, MSE(pi2): 2.309e-05, MSE(pi3): 3.387e-03\n",
      "Epoch 85500, Train loss: 1.659e+06, Test loss: 1.847e+08, MSE(e): 5.356e-05, MSE(pi1): 7.829e-02, MSE(pi2): 2.293e-05, MSE(pi3): 3.403e-03\n",
      "Epoch 85600, Train loss: 1.659e+06, Test loss: 1.844e+08, MSE(e): 5.357e-05, MSE(pi1): 7.832e-02, MSE(pi2): 2.293e-05, MSE(pi3): 3.399e-03\n",
      "Epoch 85700, Train loss: 1.658e+06, Test loss: 1.848e+08, MSE(e): 5.355e-05, MSE(pi1): 7.825e-02, MSE(pi2): 2.293e-05, MSE(pi3): 3.403e-03\n",
      "Epoch 85800, Train loss: 1.663e+06, Test loss: 1.849e+08, MSE(e): 5.404e-05, MSE(pi1): 7.806e-02, MSE(pi2): 2.315e-05, MSE(pi3): 3.418e-03\n",
      "Epoch 85900, Train loss: 1.658e+06, Test loss: 1.847e+08, MSE(e): 5.354e-05, MSE(pi1): 7.822e-02, MSE(pi2): 2.292e-05, MSE(pi3): 3.402e-03\n",
      "Epoch 86000, Train loss: 1.658e+06, Test loss: 1.846e+08, MSE(e): 5.356e-05, MSE(pi1): 7.825e-02, MSE(pi2): 2.293e-05, MSE(pi3): 3.398e-03\n",
      "Epoch 86100, Train loss: 1.657e+06, Test loss: 1.848e+08, MSE(e): 5.353e-05, MSE(pi1): 7.819e-02, MSE(pi2): 2.292e-05, MSE(pi3): 3.402e-03\n",
      "Epoch 86200, Train loss: 1.657e+06, Test loss: 1.848e+08, MSE(e): 5.353e-05, MSE(pi1): 7.816e-02, MSE(pi2): 2.292e-05, MSE(pi3): 3.403e-03\n",
      "Epoch 86300, Train loss: 1.657e+06, Test loss: 1.847e+08, MSE(e): 5.352e-05, MSE(pi1): 7.817e-02, MSE(pi2): 2.292e-05, MSE(pi3): 3.401e-03\n",
      "Epoch 86400, Train loss: 1.657e+06, Test loss: 1.847e+08, MSE(e): 5.352e-05, MSE(pi1): 7.815e-02, MSE(pi2): 2.291e-05, MSE(pi3): 3.401e-03\n",
      "Epoch 86500, Train loss: 1.657e+06, Test loss: 1.846e+08, MSE(e): 5.351e-05, MSE(pi1): 7.814e-02, MSE(pi2): 2.291e-05, MSE(pi3): 3.400e-03\n",
      "Epoch 86600, Train loss: 1.656e+06, Test loss: 1.847e+08, MSE(e): 5.351e-05, MSE(pi1): 7.812e-02, MSE(pi2): 2.291e-05, MSE(pi3): 3.401e-03\n",
      "Epoch 86700, Train loss: 1.656e+06, Test loss: 1.847e+08, MSE(e): 5.350e-05, MSE(pi1): 7.809e-02, MSE(pi2): 2.291e-05, MSE(pi3): 3.402e-03\n",
      "Epoch 86800, Train loss: 1.656e+06, Test loss: 1.852e+08, MSE(e): 5.353e-05, MSE(pi1): 7.804e-02, MSE(pi2): 2.293e-05, MSE(pi3): 3.405e-03\n",
      "Epoch 86900, Train loss: 1.656e+06, Test loss: 1.847e+08, MSE(e): 5.349e-05, MSE(pi1): 7.806e-02, MSE(pi2): 2.291e-05, MSE(pi3): 3.402e-03\n",
      "Epoch 87000, Train loss: 1.656e+06, Test loss: 1.852e+08, MSE(e): 5.353e-05, MSE(pi1): 7.799e-02, MSE(pi2): 2.293e-05, MSE(pi3): 3.406e-03\n",
      "Epoch 87100, Train loss: 1.655e+06, Test loss: 1.847e+08, MSE(e): 5.348e-05, MSE(pi1): 7.803e-02, MSE(pi2): 2.290e-05, MSE(pi3): 3.401e-03\n",
      "Epoch 87200, Train loss: 1.655e+06, Test loss: 1.849e+08, MSE(e): 5.350e-05, MSE(pi1): 7.798e-02, MSE(pi2): 2.291e-05, MSE(pi3): 3.404e-03\n",
      "Epoch 87300, Train loss: 1.655e+06, Test loss: 1.847e+08, MSE(e): 5.348e-05, MSE(pi1): 7.799e-02, MSE(pi2): 2.290e-05, MSE(pi3): 3.401e-03\n",
      "Epoch 87400, Train loss: 1.655e+06, Test loss: 1.848e+08, MSE(e): 5.347e-05, MSE(pi1): 7.798e-02, MSE(pi2): 2.290e-05, MSE(pi3): 3.401e-03\n",
      "Epoch 87500, Train loss: 1.654e+06, Test loss: 1.847e+08, MSE(e): 5.347e-05, MSE(pi1): 7.796e-02, MSE(pi2): 2.290e-05, MSE(pi3): 3.401e-03\n",
      "Epoch 87600, Train loss: 1.654e+06, Test loss: 1.837e+08, MSE(e): 5.347e-05, MSE(pi1): 7.796e-02, MSE(pi2): 2.290e-05, MSE(pi3): 3.399e-03\n",
      "Epoch 87700, Train loss: 1.654e+06, Test loss: 1.847e+08, MSE(e): 5.346e-05, MSE(pi1): 7.793e-02, MSE(pi2): 2.290e-05, MSE(pi3): 3.401e-03\n",
      "Epoch 87800, Train loss: 1.655e+06, Test loss: 1.846e+08, MSE(e): 5.357e-05, MSE(pi1): 7.800e-02, MSE(pi2): 2.293e-05, MSE(pi3): 3.393e-03\n",
      "Epoch 87900, Train loss: 1.654e+06, Test loss: 1.847e+08, MSE(e): 5.345e-05, MSE(pi1): 7.790e-02, MSE(pi2): 2.289e-05, MSE(pi3): 3.401e-03\n",
      "Epoch 88000, Train loss: 1.653e+06, Test loss: 1.845e+08, MSE(e): 5.344e-05, MSE(pi1): 7.788e-02, MSE(pi2): 2.289e-05, MSE(pi3): 3.400e-03\n",
      "Epoch 88100, Train loss: 1.653e+06, Test loss: 1.847e+08, MSE(e): 5.344e-05, MSE(pi1): 7.786e-02, MSE(pi2): 2.289e-05, MSE(pi3): 3.401e-03\n",
      "Epoch 88200, Train loss: 1.653e+06, Test loss: 1.845e+08, MSE(e): 5.349e-05, MSE(pi1): 7.791e-02, MSE(pi2): 2.290e-05, MSE(pi3): 3.395e-03\n",
      "Epoch 88300, Train loss: 1.653e+06, Test loss: 1.848e+08, MSE(e): 5.343e-05, MSE(pi1): 7.783e-02, MSE(pi2): 2.289e-05, MSE(pi3): 3.401e-03\n",
      "Epoch 88400, Train loss: 1.653e+06, Test loss: 1.848e+08, MSE(e): 5.345e-05, MSE(pi1): 7.778e-02, MSE(pi2): 2.290e-05, MSE(pi3): 3.404e-03\n",
      "Epoch 88500, Train loss: 1.652e+06, Test loss: 1.847e+08, MSE(e): 5.342e-05, MSE(pi1): 7.780e-02, MSE(pi2): 2.288e-05, MSE(pi3): 3.400e-03\n",
      "Epoch 88600, Train loss: 1.652e+06, Test loss: 1.848e+08, MSE(e): 5.342e-05, MSE(pi1): 7.779e-02, MSE(pi2): 2.288e-05, MSE(pi3): 3.400e-03\n",
      "Epoch 88700, Train loss: 1.652e+06, Test loss: 1.848e+08, MSE(e): 5.341e-05, MSE(pi1): 7.776e-02, MSE(pi2): 2.288e-05, MSE(pi3): 3.400e-03\n",
      "Epoch 88800, Train loss: 1.654e+06, Test loss: 1.856e+08, MSE(e): 5.366e-05, MSE(pi1): 7.762e-02, MSE(pi2): 2.299e-05, MSE(pi3): 3.411e-03\n",
      "Epoch 88900, Train loss: 1.651e+06, Test loss: 1.848e+08, MSE(e): 5.341e-05, MSE(pi1): 7.773e-02, MSE(pi2): 2.288e-05, MSE(pi3): 3.400e-03\n",
      "Epoch 89000, Train loss: 1.657e+06, Test loss: 1.870e+08, MSE(e): 5.404e-05, MSE(pi1): 7.751e-02, MSE(pi2): 2.316e-05, MSE(pi3): 3.418e-03\n",
      "Epoch 89100, Train loss: 1.651e+06, Test loss: 1.848e+08, MSE(e): 5.340e-05, MSE(pi1): 7.769e-02, MSE(pi2): 2.288e-05, MSE(pi3): 3.400e-03\n",
      "Epoch 89200, Train loss: 1.655e+06, Test loss: 1.844e+08, MSE(e): 5.379e-05, MSE(pi1): 7.784e-02, MSE(pi2): 2.302e-05, MSE(pi3): 3.386e-03\n",
      "Epoch 89300, Train loss: 1.650e+06, Test loss: 1.848e+08, MSE(e): 5.339e-05, MSE(pi1): 7.766e-02, MSE(pi2): 2.287e-05, MSE(pi3): 3.400e-03\n",
      "Epoch 89400, Train loss: 1.671e+06, Test loss: 1.884e+08, MSE(e): 5.550e-05, MSE(pi1): 7.727e-02, MSE(pi2): 2.378e-05, MSE(pi3): 3.433e-03\n",
      "Epoch 89500, Train loss: 1.650e+06, Test loss: 1.848e+08, MSE(e): 5.338e-05, MSE(pi1): 7.762e-02, MSE(pi2): 2.287e-05, MSE(pi3): 3.400e-03\n",
      "Epoch 89600, Train loss: 1.650e+06, Test loss: 1.848e+08, MSE(e): 5.338e-05, MSE(pi1): 7.760e-02, MSE(pi2): 2.287e-05, MSE(pi3): 3.400e-03\n",
      "Epoch 89700, Train loss: 1.650e+06, Test loss: 1.849e+08, MSE(e): 5.337e-05, MSE(pi1): 7.759e-02, MSE(pi2): 2.287e-05, MSE(pi3): 3.400e-03\n",
      "Epoch 89800, Train loss: 1.649e+06, Test loss: 1.847e+08, MSE(e): 5.337e-05, MSE(pi1): 7.758e-02, MSE(pi2): 2.287e-05, MSE(pi3): 3.399e-03\n",
      "Epoch 89900, Train loss: 1.649e+06, Test loss: 1.847e+08, MSE(e): 5.336e-05, MSE(pi1): 7.756e-02, MSE(pi2): 2.286e-05, MSE(pi3): 3.399e-03\n",
      "Epoch 90000, Train loss: 1.649e+06, Test loss: 1.848e+08, MSE(e): 5.336e-05, MSE(pi1): 7.754e-02, MSE(pi2): 2.286e-05, MSE(pi3): 3.400e-03\n",
      "Epoch 90100, Train loss: 1.649e+06, Test loss: 1.849e+08, MSE(e): 5.336e-05, MSE(pi1): 7.750e-02, MSE(pi2): 2.287e-05, MSE(pi3): 3.401e-03\n",
      "Epoch 90200, Train loss: 1.648e+06, Test loss: 1.848e+08, MSE(e): 5.335e-05, MSE(pi1): 7.750e-02, MSE(pi2): 2.286e-05, MSE(pi3): 3.399e-03\n",
      "Epoch 90300, Train loss: 1.649e+06, Test loss: 1.845e+08, MSE(e): 5.344e-05, MSE(pi1): 7.757e-02, MSE(pi2): 2.289e-05, MSE(pi3): 3.392e-03\n",
      "Epoch 90400, Train loss: 1.648e+06, Test loss: 1.848e+08, MSE(e): 5.334e-05, MSE(pi1): 7.747e-02, MSE(pi2): 2.286e-05, MSE(pi3): 3.399e-03\n",
      "Epoch 90500, Train loss: 1.653e+06, Test loss: 1.861e+08, MSE(e): 5.386e-05, MSE(pi1): 7.727e-02, MSE(pi2): 2.309e-05, MSE(pi3): 3.415e-03\n",
      "Epoch 90600, Train loss: 1.648e+06, Test loss: 1.848e+08, MSE(e): 5.333e-05, MSE(pi1): 7.744e-02, MSE(pi2): 2.286e-05, MSE(pi3): 3.399e-03\n",
      "Epoch 90700, Train loss: 1.647e+06, Test loss: 1.847e+08, MSE(e): 5.333e-05, MSE(pi1): 7.743e-02, MSE(pi2): 2.285e-05, MSE(pi3): 3.398e-03\n",
      "Epoch 90800, Train loss: 1.647e+06, Test loss: 1.848e+08, MSE(e): 5.332e-05, MSE(pi1): 7.740e-02, MSE(pi2): 2.285e-05, MSE(pi3): 3.399e-03\n",
      "Epoch 90900, Train loss: 1.647e+06, Test loss: 1.846e+08, MSE(e): 5.333e-05, MSE(pi1): 7.740e-02, MSE(pi2): 2.285e-05, MSE(pi3): 3.397e-03\n",
      "Epoch 91000, Train loss: 1.647e+06, Test loss: 1.847e+08, MSE(e): 5.332e-05, MSE(pi1): 7.737e-02, MSE(pi2): 2.285e-05, MSE(pi3): 3.398e-03\n",
      "Epoch 91100, Train loss: 1.647e+06, Test loss: 1.832e+08, MSE(e): 5.335e-05, MSE(pi1): 7.740e-02, MSE(pi2): 2.286e-05, MSE(pi3): 3.394e-03\n",
      "Epoch 91200, Train loss: 1.646e+06, Test loss: 1.848e+08, MSE(e): 5.331e-05, MSE(pi1): 7.733e-02, MSE(pi2): 2.285e-05, MSE(pi3): 3.399e-03\n",
      "Epoch 91300, Train loss: 1.649e+06, Test loss: 1.850e+08, MSE(e): 5.367e-05, MSE(pi1): 7.716e-02, MSE(pi2): 2.301e-05, MSE(pi3): 3.412e-03\n",
      "Epoch 91400, Train loss: 1.646e+06, Test loss: 1.848e+08, MSE(e): 5.330e-05, MSE(pi1): 7.729e-02, MSE(pi2): 2.285e-05, MSE(pi3): 3.399e-03\n",
      "Epoch 91500, Train loss: 1.646e+06, Test loss: 1.845e+08, MSE(e): 5.330e-05, MSE(pi1): 7.727e-02, MSE(pi2): 2.285e-05, MSE(pi3): 3.399e-03\n",
      "Epoch 91600, Train loss: 1.645e+06, Test loss: 1.848e+08, MSE(e): 5.329e-05, MSE(pi1): 7.726e-02, MSE(pi2): 2.284e-05, MSE(pi3): 3.399e-03\n",
      "Epoch 91700, Train loss: 1.647e+06, Test loss: 1.840e+08, MSE(e): 5.346e-05, MSE(pi1): 7.735e-02, MSE(pi2): 2.290e-05, MSE(pi3): 3.389e-03\n",
      "Epoch 91800, Train loss: 1.645e+06, Test loss: 1.848e+08, MSE(e): 5.328e-05, MSE(pi1): 7.722e-02, MSE(pi2): 2.284e-05, MSE(pi3): 3.399e-03\n",
      "Epoch 91900, Train loss: 1.650e+06, Test loss: 1.867e+08, MSE(e): 5.384e-05, MSE(pi1): 7.702e-02, MSE(pi2): 2.309e-05, MSE(pi3): 3.415e-03\n",
      "Epoch 92000, Train loss: 1.644e+06, Test loss: 1.848e+08, MSE(e): 5.328e-05, MSE(pi1): 7.719e-02, MSE(pi2): 2.284e-05, MSE(pi3): 3.398e-03\n",
      "Epoch 92100, Train loss: 1.654e+06, Test loss: 1.842e+08, MSE(e): 5.420e-05, MSE(pi1): 7.742e-02, MSE(pi2): 2.320e-05, MSE(pi3): 3.376e-03\n",
      "Epoch 92200, Train loss: 1.644e+06, Test loss: 1.848e+08, MSE(e): 5.327e-05, MSE(pi1): 7.716e-02, MSE(pi2): 2.284e-05, MSE(pi3): 3.398e-03\n",
      "Epoch 92300, Train loss: 1.651e+06, Test loss: 1.868e+08, MSE(e): 5.402e-05, MSE(pi1): 7.691e-02, MSE(pi2): 2.316e-05, MSE(pi3): 3.417e-03\n",
      "Epoch 92400, Train loss: 1.644e+06, Test loss: 1.849e+08, MSE(e): 5.326e-05, MSE(pi1): 7.711e-02, MSE(pi2): 2.283e-05, MSE(pi3): 3.399e-03\n",
      "Epoch 92500, Train loss: 1.643e+06, Test loss: 1.848e+08, MSE(e): 5.326e-05, MSE(pi1): 7.709e-02, MSE(pi2): 2.283e-05, MSE(pi3): 3.398e-03\n",
      "Epoch 92600, Train loss: 1.643e+06, Test loss: 1.851e+08, MSE(e): 5.330e-05, MSE(pi1): 7.702e-02, MSE(pi2): 2.285e-05, MSE(pi3): 3.402e-03\n",
      "Epoch 92700, Train loss: 1.643e+06, Test loss: 1.848e+08, MSE(e): 5.325e-05, MSE(pi1): 7.707e-02, MSE(pi2): 2.283e-05, MSE(pi3): 3.397e-03\n",
      "Epoch 92800, Train loss: 1.643e+06, Test loss: 1.847e+08, MSE(e): 5.325e-05, MSE(pi1): 7.706e-02, MSE(pi2): 2.283e-05, MSE(pi3): 3.397e-03\n",
      "Epoch 92900, Train loss: 1.643e+06, Test loss: 1.853e+08, MSE(e): 5.329e-05, MSE(pi1): 7.697e-02, MSE(pi2): 2.285e-05, MSE(pi3): 3.403e-03\n",
      "Epoch 93000, Train loss: 1.642e+06, Test loss: 1.848e+08, MSE(e): 5.324e-05, MSE(pi1): 7.701e-02, MSE(pi2): 2.283e-05, MSE(pi3): 3.397e-03\n",
      "Epoch 93100, Train loss: 1.694e+06, Test loss: 1.815e+08, MSE(e): 5.839e-05, MSE(pi1): 7.758e-02, MSE(pi2): 2.491e-05, MSE(pi3): 3.347e-03\n",
      "Epoch 93200, Train loss: 1.642e+06, Test loss: 1.848e+08, MSE(e): 5.323e-05, MSE(pi1): 7.697e-02, MSE(pi2): 2.282e-05, MSE(pi3): 3.398e-03\n",
      "Epoch 93300, Train loss: 1.642e+06, Test loss: 1.859e+08, MSE(e): 5.324e-05, MSE(pi1): 7.698e-02, MSE(pi2): 2.283e-05, MSE(pi3): 3.394e-03\n",
      "Epoch 93400, Train loss: 1.641e+06, Test loss: 1.848e+08, MSE(e): 5.322e-05, MSE(pi1): 7.693e-02, MSE(pi2): 2.282e-05, MSE(pi3): 3.397e-03\n",
      "Epoch 93500, Train loss: 1.661e+06, Test loss: 1.841e+08, MSE(e): 5.513e-05, MSE(pi1): 7.727e-02, MSE(pi2): 2.358e-05, MSE(pi3): 3.366e-03\n",
      "Epoch 93600, Train loss: 1.641e+06, Test loss: 1.848e+08, MSE(e): 5.321e-05, MSE(pi1): 7.690e-02, MSE(pi2): 2.282e-05, MSE(pi3): 3.397e-03\n",
      "Epoch 93700, Train loss: 1.641e+06, Test loss: 1.848e+08, MSE(e): 5.321e-05, MSE(pi1): 7.687e-02, MSE(pi2): 2.282e-05, MSE(pi3): 3.397e-03\n",
      "Epoch 93800, Train loss: 1.640e+06, Test loss: 1.850e+08, MSE(e): 5.322e-05, MSE(pi1): 7.683e-02, MSE(pi2): 2.283e-05, MSE(pi3): 3.400e-03\n",
      "Epoch 93900, Train loss: 1.640e+06, Test loss: 1.848e+08, MSE(e): 5.320e-05, MSE(pi1): 7.684e-02, MSE(pi2): 2.282e-05, MSE(pi3): 3.397e-03\n",
      "Epoch 94000, Train loss: 1.681e+06, Test loss: 1.891e+08, MSE(e): 5.733e-05, MSE(pi1): 7.631e-02, MSE(pi2): 2.455e-05, MSE(pi3): 3.443e-03\n",
      "Epoch 94100, Train loss: 1.640e+06, Test loss: 1.849e+08, MSE(e): 5.319e-05, MSE(pi1): 7.679e-02, MSE(pi2): 2.281e-05, MSE(pi3): 3.397e-03\n",
      "Epoch 94200, Train loss: 1.639e+06, Test loss: 1.850e+08, MSE(e): 5.320e-05, MSE(pi1): 7.676e-02, MSE(pi2): 2.282e-05, MSE(pi3): 3.398e-03\n",
      "Epoch 94300, Train loss: 1.639e+06, Test loss: 1.848e+08, MSE(e): 5.319e-05, MSE(pi1): 7.676e-02, MSE(pi2): 2.281e-05, MSE(pi3): 3.397e-03\n",
      "Epoch 94400, Train loss: 1.639e+06, Test loss: 1.849e+08, MSE(e): 5.318e-05, MSE(pi1): 7.674e-02, MSE(pi2): 2.281e-05, MSE(pi3): 3.397e-03\n",
      "Epoch 94500, Train loss: 1.639e+06, Test loss: 1.849e+08, MSE(e): 5.318e-05, MSE(pi1): 7.673e-02, MSE(pi2): 2.281e-05, MSE(pi3): 3.396e-03\n",
      "Epoch 94600, Train loss: 1.638e+06, Test loss: 1.849e+08, MSE(e): 5.317e-05, MSE(pi1): 7.670e-02, MSE(pi2): 2.281e-05, MSE(pi3): 3.396e-03\n",
      "Epoch 94700, Train loss: 1.639e+06, Test loss: 1.846e+08, MSE(e): 5.325e-05, MSE(pi1): 7.676e-02, MSE(pi2): 2.284e-05, MSE(pi3): 3.389e-03\n",
      "Epoch 94800, Train loss: 1.638e+06, Test loss: 1.849e+08, MSE(e): 5.317e-05, MSE(pi1): 7.666e-02, MSE(pi2): 2.281e-05, MSE(pi3): 3.396e-03\n",
      "Epoch 94900, Train loss: 1.638e+06, Test loss: 1.849e+08, MSE(e): 5.316e-05, MSE(pi1): 7.665e-02, MSE(pi2): 2.280e-05, MSE(pi3): 3.396e-03\n",
      "Epoch 95000, Train loss: 1.644e+06, Test loss: 1.868e+08, MSE(e): 5.389e-05, MSE(pi1): 7.641e-02, MSE(pi2): 2.312e-05, MSE(pi3): 3.415e-03\n",
      "Epoch 95100, Train loss: 1.637e+06, Test loss: 1.849e+08, MSE(e): 5.316e-05, MSE(pi1): 7.660e-02, MSE(pi2): 2.280e-05, MSE(pi3): 3.396e-03\n",
      "Epoch 95200, Train loss: 1.637e+06, Test loss: 1.849e+08, MSE(e): 5.315e-05, MSE(pi1): 7.659e-02, MSE(pi2): 2.280e-05, MSE(pi3): 3.396e-03\n",
      "Epoch 95300, Train loss: 1.640e+06, Test loss: 1.855e+08, MSE(e): 5.347e-05, MSE(pi1): 7.642e-02, MSE(pi2): 2.294e-05, MSE(pi3): 3.408e-03\n",
      "Epoch 95400, Train loss: 1.636e+06, Test loss: 1.849e+08, MSE(e): 5.314e-05, MSE(pi1): 7.655e-02, MSE(pi2): 2.280e-05, MSE(pi3): 3.396e-03\n",
      "Epoch 95500, Train loss: 1.641e+06, Test loss: 1.864e+08, MSE(e): 5.359e-05, MSE(pi1): 7.636e-02, MSE(pi2): 2.299e-05, MSE(pi3): 3.410e-03\n",
      "Epoch 95600, Train loss: 1.636e+06, Test loss: 1.849e+08, MSE(e): 5.314e-05, MSE(pi1): 7.650e-02, MSE(pi2): 2.280e-05, MSE(pi3): 3.396e-03\n",
      "Epoch 95700, Train loss: 1.636e+06, Test loss: 1.847e+08, MSE(e): 5.313e-05, MSE(pi1): 7.650e-02, MSE(pi2): 2.280e-05, MSE(pi3): 3.394e-03\n",
      "Epoch 95800, Train loss: 1.636e+06, Test loss: 1.848e+08, MSE(e): 5.313e-05, MSE(pi1): 7.647e-02, MSE(pi2): 2.279e-05, MSE(pi3): 3.395e-03\n",
      "Epoch 95900, Train loss: 1.635e+06, Test loss: 1.850e+08, MSE(e): 5.313e-05, MSE(pi1): 7.643e-02, MSE(pi2): 2.280e-05, MSE(pi3): 3.397e-03\n",
      "Epoch 96000, Train loss: 1.635e+06, Test loss: 1.849e+08, MSE(e): 5.312e-05, MSE(pi1): 7.642e-02, MSE(pi2): 2.279e-05, MSE(pi3): 3.395e-03\n",
      "Epoch 96100, Train loss: 1.635e+06, Test loss: 1.849e+08, MSE(e): 5.312e-05, MSE(pi1): 7.642e-02, MSE(pi2): 2.279e-05, MSE(pi3): 3.394e-03\n",
      "Epoch 96200, Train loss: 1.635e+06, Test loss: 1.849e+08, MSE(e): 5.311e-05, MSE(pi1): 7.639e-02, MSE(pi2): 2.279e-05, MSE(pi3): 3.395e-03\n",
      "Epoch 96300, Train loss: 1.634e+06, Test loss: 1.849e+08, MSE(e): 5.312e-05, MSE(pi1): 7.635e-02, MSE(pi2): 2.279e-05, MSE(pi3): 3.396e-03\n",
      "Epoch 96400, Train loss: 1.634e+06, Test loss: 1.849e+08, MSE(e): 5.311e-05, MSE(pi1): 7.635e-02, MSE(pi2): 2.279e-05, MSE(pi3): 3.395e-03\n",
      "Epoch 96500, Train loss: 1.635e+06, Test loss: 1.831e+08, MSE(e): 5.323e-05, MSE(pi1): 7.642e-02, MSE(pi2): 2.283e-05, MSE(pi3): 3.387e-03\n",
      "Epoch 96600, Train loss: 1.634e+06, Test loss: 1.849e+08, MSE(e): 5.310e-05, MSE(pi1): 7.630e-02, MSE(pi2): 2.279e-05, MSE(pi3): 3.395e-03\n",
      "Epoch 96700, Train loss: 1.684e+06, Test loss: 1.880e+08, MSE(e): 5.820e-05, MSE(pi1): 7.572e-02, MSE(pi2): 2.492e-05, MSE(pi3): 3.446e-03\n",
      "Epoch 96800, Train loss: 1.633e+06, Test loss: 1.849e+08, MSE(e): 5.309e-05, MSE(pi1): 7.627e-02, MSE(pi2): 2.278e-05, MSE(pi3): 3.395e-03\n",
      "Epoch 96900, Train loss: 1.633e+06, Test loss: 1.844e+08, MSE(e): 5.313e-05, MSE(pi1): 7.630e-02, MSE(pi2): 2.280e-05, MSE(pi3): 3.389e-03\n",
      "Epoch 97000, Train loss: 1.633e+06, Test loss: 1.850e+08, MSE(e): 5.309e-05, MSE(pi1): 7.622e-02, MSE(pi2): 2.278e-05, MSE(pi3): 3.395e-03\n",
      "Epoch 97100, Train loss: 1.632e+06, Test loss: 1.849e+08, MSE(e): 5.308e-05, MSE(pi1): 7.621e-02, MSE(pi2): 2.278e-05, MSE(pi3): 3.394e-03\n",
      "Epoch 97200, Train loss: 1.632e+06, Test loss: 1.849e+08, MSE(e): 5.308e-05, MSE(pi1): 7.619e-02, MSE(pi2): 2.278e-05, MSE(pi3): 3.394e-03\n",
      "Epoch 97300, Train loss: 1.632e+06, Test loss: 1.849e+08, MSE(e): 5.307e-05, MSE(pi1): 7.616e-02, MSE(pi2): 2.278e-05, MSE(pi3): 3.394e-03\n",
      "Epoch 97400, Train loss: 1.632e+06, Test loss: 1.849e+08, MSE(e): 5.307e-05, MSE(pi1): 7.615e-02, MSE(pi2): 2.278e-05, MSE(pi3): 3.394e-03\n",
      "Epoch 97500, Train loss: 1.631e+06, Test loss: 1.849e+08, MSE(e): 5.307e-05, MSE(pi1): 7.612e-02, MSE(pi2): 2.278e-05, MSE(pi3): 3.393e-03\n",
      "Epoch 97600, Train loss: 1.631e+06, Test loss: 1.848e+08, MSE(e): 5.306e-05, MSE(pi1): 7.611e-02, MSE(pi2): 2.278e-05, MSE(pi3): 3.393e-03\n",
      "Epoch 97700, Train loss: 1.631e+06, Test loss: 1.844e+08, MSE(e): 5.311e-05, MSE(pi1): 7.614e-02, MSE(pi2): 2.279e-05, MSE(pi3): 3.388e-03\n",
      "Epoch 97800, Train loss: 1.631e+06, Test loss: 1.850e+08, MSE(e): 5.306e-05, MSE(pi1): 7.605e-02, MSE(pi2): 2.277e-05, MSE(pi3): 3.394e-03\n",
      "Epoch 97900, Train loss: 1.693e+06, Test loss: 1.805e+08, MSE(e): 5.926e-05, MSE(pi1): 7.669e-02, MSE(pi2): 2.530e-05, MSE(pi3): 3.339e-03\n",
      "Epoch 98000, Train loss: 1.630e+06, Test loss: 1.849e+08, MSE(e): 5.305e-05, MSE(pi1): 7.602e-02, MSE(pi2): 2.277e-05, MSE(pi3): 3.393e-03\n",
      "Epoch 98100, Train loss: 1.631e+06, Test loss: 1.858e+08, MSE(e): 5.323e-05, MSE(pi1): 7.589e-02, MSE(pi2): 2.285e-05, MSE(pi3): 3.403e-03\n",
      "Epoch 98200, Train loss: 1.629e+06, Test loss: 1.849e+08, MSE(e): 5.304e-05, MSE(pi1): 7.598e-02, MSE(pi2): 2.277e-05, MSE(pi3): 3.393e-03\n",
      "Epoch 98300, Train loss: 1.642e+06, Test loss: 1.834e+08, MSE(e): 5.424e-05, MSE(pi1): 7.624e-02, MSE(pi2): 2.325e-05, MSE(pi3): 3.368e-03\n",
      "Epoch 98400, Train loss: 1.629e+06, Test loss: 1.849e+08, MSE(e): 5.303e-05, MSE(pi1): 7.593e-02, MSE(pi2): 2.277e-05, MSE(pi3): 3.393e-03\n",
      "Epoch 98500, Train loss: 1.636e+06, Test loss: 1.850e+08, MSE(e): 5.376e-05, MSE(pi1): 7.570e-02, MSE(pi2): 2.308e-05, MSE(pi3): 3.412e-03\n",
      "Epoch 98600, Train loss: 1.628e+06, Test loss: 1.850e+08, MSE(e): 5.303e-05, MSE(pi1): 7.589e-02, MSE(pi2): 2.277e-05, MSE(pi3): 3.393e-03\n",
      "Epoch 98700, Train loss: 1.687e+06, Test loss: 1.892e+08, MSE(e): 5.895e-05, MSE(pi1): 7.526e-02, MSE(pi2): 2.523e-05, MSE(pi3): 3.448e-03\n",
      "Epoch 98800, Train loss: 1.628e+06, Test loss: 1.850e+08, MSE(e): 5.302e-05, MSE(pi1): 7.585e-02, MSE(pi2): 2.276e-05, MSE(pi3): 3.393e-03\n",
      "Epoch 98900, Train loss: 1.669e+06, Test loss: 1.893e+08, MSE(e): 5.723e-05, MSE(pi1): 7.531e-02, MSE(pi2): 2.452e-05, MSE(pi3): 3.439e-03\n",
      "Epoch 99000, Train loss: 1.627e+06, Test loss: 1.850e+08, MSE(e): 5.301e-05, MSE(pi1): 7.580e-02, MSE(pi2): 2.276e-05, MSE(pi3): 3.393e-03\n",
      "Epoch 99100, Train loss: 1.627e+06, Test loss: 1.850e+08, MSE(e): 5.301e-05, MSE(pi1): 7.578e-02, MSE(pi2): 2.276e-05, MSE(pi3): 3.393e-03\n",
      "Epoch 99200, Train loss: 1.627e+06, Test loss: 1.850e+08, MSE(e): 5.301e-05, MSE(pi1): 7.575e-02, MSE(pi2): 2.276e-05, MSE(pi3): 3.394e-03\n",
      "Epoch 99300, Train loss: 1.627e+06, Test loss: 1.850e+08, MSE(e): 5.300e-05, MSE(pi1): 7.574e-02, MSE(pi2): 2.276e-05, MSE(pi3): 3.392e-03\n",
      "Epoch 99400, Train loss: 1.627e+06, Test loss: 1.847e+08, MSE(e): 5.301e-05, MSE(pi1): 7.575e-02, MSE(pi2): 2.276e-05, MSE(pi3): 3.390e-03\n",
      "Epoch 99500, Train loss: 1.626e+06, Test loss: 1.850e+08, MSE(e): 5.299e-05, MSE(pi1): 7.570e-02, MSE(pi2): 2.276e-05, MSE(pi3): 3.392e-03\n",
      "Epoch 99600, Train loss: 1.626e+06, Test loss: 1.843e+08, MSE(e): 5.302e-05, MSE(pi1): 7.572e-02, MSE(pi2): 2.277e-05, MSE(pi3): 3.388e-03\n",
      "Epoch 99700, Train loss: 1.626e+06, Test loss: 1.850e+08, MSE(e): 5.299e-05, MSE(pi1): 7.565e-02, MSE(pi2): 2.275e-05, MSE(pi3): 3.392e-03\n",
      "Epoch 99800, Train loss: 1.625e+06, Test loss: 1.852e+08, MSE(e): 5.299e-05, MSE(pi1): 7.561e-02, MSE(pi2): 2.276e-05, MSE(pi3): 3.394e-03\n",
      "Epoch 99900, Train loss: 1.625e+06, Test loss: 1.850e+08, MSE(e): 5.298e-05, MSE(pi1): 7.561e-02, MSE(pi2): 2.275e-05, MSE(pi3): 3.392e-03\n",
      "\n",
      "Training process finished after 100000 epochs\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Parametros de entrenamiento\n",
    "start_epoch = 18000\n",
    "n_epochs = 100000\n",
    "\n",
    "batch_size = 64 \n",
    "n_checkpoints = 100\n",
    "\n",
    "second_lr = 1e-4\n",
    "\n",
    "train_loop(model, optimizer, X_train_NN, y_train_NN, f_train_NN, X_test_NN, y_test_NN, f_test_NN,\n",
    "           D, n_checkpoints, start_epoch=start_epoch, n_epochs=n_epochs, batch_size=batch_size, \n",
    "           model_results_path=MODEL_RESULTS_PGNNIV_PATH, device=DEVICE, new_lr=second_lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SciML_test_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
