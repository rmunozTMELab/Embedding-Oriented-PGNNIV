{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import sys\n",
    "\n",
    "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), \"../../\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Own library imports\n",
    "from vecopsciml.utils import TensOps\n",
    "from vecopsciml.operators.zero_order import Mx, My\n",
    "from vecopsciml.kernels.derivative import DerivativeKernels\n",
    "\n",
    "# Function from this project\n",
    "from utils.folders import create_folder\n",
    "from utils.load_data import load_data\n",
    "from trainers.train import train_loop\n",
    "\n",
    "# Import model\n",
    "from architectures.pgnniv_pod import PGNNIVPOD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset\n",
    "dataset = 'non_linear'\n",
    "N_data = 1000\n",
    "noise = 0\n",
    "\n",
    "data_name = dataset + '_' + str(N_data) + '_' + str(noise)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model\n",
    "model = 'POD'\n",
    "n_modes = 10\n",
    "\n",
    "model_name = model + '_model_' + str(n_modes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "ROOT_PATH = os.path.abspath(os.path.join(os.getcwd(), \"../../\"))\n",
    "DATA_PATH = os.path.join(ROOT_PATH, r'data/', data_name, data_name) + '.pkl'\n",
    "RESULTS_FOLDER_PATH = os.path.join(ROOT_PATH, r'results/', data_name)\n",
    "MODEL_RESULTS_PATH = os.path.join(ROOT_PATH, r'results/', data_name, model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Folder successfully created at: /home/rmunoz/Escritorio/rmunozTMELab/Physically-Guided-Machine-Learning/results/non_linear_1000_0\n",
      "Folder successfully created at: /home/rmunoz/Escritorio/rmunozTMELab/Physically-Guided-Machine-Learning/results/non_linear_1000_0/POD_model_10\n"
     ]
    }
   ],
   "source": [
    "create_folder(RESULTS_FOLDER_PATH)\n",
    "create_folder(MODEL_RESULTS_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully loaded from: /home/rmunoz/Escritorio/rmunozTMELab/Physically-Guided-Machine-Learning/data/non_linear_1000_0/non_linear_1000_0.pkl\n"
     ]
    }
   ],
   "source": [
    "# Load dataset\n",
    "dataset = load_data(DATA_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convolutional filters to derivate\n",
    "dx = dataset['x_step_size']\n",
    "dy = dataset['y_step_size']\n",
    "D = DerivativeKernels(dx, dy, 0).grad_kernels_two_dimensions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "print(f\"Using device: {DEVICE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train data splitting in train/test\n",
    "X = torch.tensor(dataset['X_train'], dtype=torch.float32).unsqueeze(1)\n",
    "y = torch.tensor(dataset['y_train'], dtype=torch.float32).unsqueeze(1)\n",
    "K = torch.tensor(dataset['k_train'], dtype=torch.float32).unsqueeze(1)\n",
    "f = torch.tensor(dataset['f_train'], dtype=torch.float32).unsqueeze(1)\n",
    "\n",
    "X_train, X_test, y_train, y_test, K_train, K_test, f_train, f_test = train_test_split(X, y, K, f, test_size=0.3, random_state=42)\n",
    "\n",
    "# Data processing and adequacy with our TensOps library\n",
    "X_train = X_train.to(DEVICE)\n",
    "X_test = X_test.to(DEVICE)\n",
    "\n",
    "y_train = TensOps(y_train.to(DEVICE).requires_grad_(True), space_dimension=2, contravariance=0, covariance=0)\n",
    "y_test = TensOps(y_test.to(DEVICE).requires_grad_(True), space_dimension=2, contravariance=0, covariance=0)\n",
    "\n",
    "K_train = TensOps(K_train.to(DEVICE).requires_grad_(True), space_dimension=2, contravariance=0, covariance=0)\n",
    "K_test = TensOps(K_test.to(DEVICE).requires_grad_(True), space_dimension=2, contravariance=0, covariance=0)\n",
    "\n",
    "f_train = TensOps(f_train.to(DEVICE).requires_grad_(True), space_dimension=2, contravariance=0, covariance=0)\n",
    "f_test = TensOps(f_test.to(DEVICE).requires_grad_(True), space_dimension=2, contravariance=0, covariance=0)\n",
    "\n",
    "# Loading and processing validation data\n",
    "X_val = torch.tensor(dataset['X_val'], dtype=torch.float32).unsqueeze(1)\n",
    "y_val = TensOps(torch.tensor(dataset['y_val'], dtype=torch.float32, requires_grad=True).unsqueeze(1), space_dimension=2, contravariance=0, covariance=0)\n",
    "K_val = TensOps(torch.tensor(dataset['k_val'], dtype=torch.float32, requires_grad=True).unsqueeze(1), space_dimension=2, contravariance=0, covariance=0)\n",
    "f_val = TensOps(torch.tensor(dataset['f_val'], dtype=torch.float32, requires_grad=True).unsqueeze(1), space_dimension=2, contravariance=0, covariance=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_modes = 20\n",
    "\n",
    "U_train, S_train, Vt_train = torch.linalg.svd(y_train.values.detach().squeeze().to('cpu').view(y_train.values.detach().shape[0], -1), full_matrices=False)\n",
    "\n",
    "U_reduced_train = U_train[:, :num_modes]\n",
    "S_reduced_train = S_train[:num_modes]\n",
    "Vt_reduced_train = Vt_train[:num_modes, :]\n",
    "\n",
    "POD_base = Vt_reduced_train.to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predictive network architecture\n",
    "input_shape = X_train[0].shape\n",
    "predictive_layers = [20, 10, num_modes]\n",
    "predictive_output = y_train.values[0].shape\n",
    "\n",
    "# Explanatory network architecture\n",
    "explanatory_input = Mx(My(y_train)).values[0].shape\n",
    "explanatory_layers = [10, 10]\n",
    "explanatory_output = Mx(My(f_train)).values[0].shape\n",
    "\n",
    "# Other parameters \n",
    "n_filters_explanatory = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training from scratch.\n",
      "Epoch 0, Train loss: 7.620e+08, Test loss: 9.960e+08, MSE(e): 7.476e+01, MSE(pi1): 1.198e+03, MSE(pi2): 2.841e+01, MSE(pi3): 2.415e+01\n",
      "Epoch 100, Train loss: 1.989e+07, Test loss: 3.581e+07, MSE(e): 1.982e+00, MSE(pi1): 4.432e+00, MSE(pi2): 9.572e-01, MSE(pi3): 3.275e-01\n",
      "Epoch 200, Train loss: 1.529e+06, Test loss: 2.542e+06, MSE(e): 1.493e-01, MSE(pi1): 1.813e+00, MSE(pi2): 8.966e-02, MSE(pi3): 1.695e-01\n",
      "Epoch 300, Train loss: 2.127e+05, Test loss: 5.032e+05, MSE(e): 1.817e-02, MSE(pi1): 1.411e+00, MSE(pi2): 1.044e-02, MSE(pi3): 1.689e-01\n",
      "Epoch 400, Train loss: 9.998e+04, Test loss: 2.365e+05, MSE(e): 7.072e-03, MSE(pi1): 1.271e+00, MSE(pi2): 4.426e-03, MSE(pi3): 1.655e-01\n",
      "Epoch 500, Train loss: 7.234e+04, Test loss: 1.326e+05, MSE(e): 4.646e-03, MSE(pi1): 1.120e+00, MSE(pi2): 3.096e-03, MSE(pi3): 1.468e-01\n",
      "Epoch 600, Train loss: 4.049e+04, Test loss: 7.784e+04, MSE(e): 2.931e-03, MSE(pi1): 5.339e-01, MSE(pi2): 1.995e-03, MSE(pi3): 5.841e-02\n",
      "Epoch 700, Train loss: 2.453e+04, Test loss: 4.851e+04, MSE(e): 2.234e-03, MSE(pi1): 1.123e-01, MSE(pi2): 1.525e-03, MSE(pi3): 1.066e-02\n",
      "Epoch 800, Train loss: 1.848e+04, Test loss: 3.578e+04, MSE(e): 1.744e-03, MSE(pi1): 5.504e-02, MSE(pi2): 1.207e-03, MSE(pi3): 4.896e-03\n",
      "Epoch 900, Train loss: 1.462e+04, Test loss: 2.958e+04, MSE(e): 1.393e-03, MSE(pi1): 3.536e-02, MSE(pi2): 9.847e-04, MSE(pi3): 3.326e-03\n",
      "Epoch 1000, Train loss: 1.229e+04, Test loss: 2.558e+04, MSE(e): 1.171e-03, MSE(pi1): 2.996e-02, MSE(pi2): 8.404e-04, MSE(pi3): 2.768e-03\n",
      "Epoch 1100, Train loss: 1.076e+04, Test loss: 2.236e+04, MSE(e): 1.025e-03, MSE(pi1): 2.642e-02, MSE(pi2): 7.212e-04, MSE(pi3): 2.420e-03\n",
      "Epoch 1200, Train loss: 9.690e+03, Test loss: 1.991e+04, MSE(e): 9.290e-04, MSE(pi1): 1.907e-02, MSE(pi2): 6.330e-04, MSE(pi3): 2.089e-03\n",
      "Epoch 1300, Train loss: 7.807e+03, Test loss: 1.312e+04, MSE(e): 7.479e-04, MSE(pi1): 1.403e-02, MSE(pi2): 5.304e-04, MSE(pi3): 1.869e-03\n",
      "Epoch 1400, Train loss: 6.605e+03, Test loss: 1.384e+04, MSE(e): 6.315e-04, MSE(pi1): 1.223e-02, MSE(pi2): 4.613e-04, MSE(pi3): 1.672e-03\n",
      "Epoch 1500, Train loss: 6.149e+03, Test loss: 1.296e+04, MSE(e): 5.882e-04, MSE(pi1): 1.058e-02, MSE(pi2): 4.232e-04, MSE(pi3): 1.604e-03\n",
      "Epoch 1600, Train loss: 5.701e+03, Test loss: 1.068e+04, MSE(e): 5.450e-04, MSE(pi1): 9.732e-03, MSE(pi2): 3.847e-04, MSE(pi3): 1.527e-03\n",
      "Epoch 1700, Train loss: 5.001e+03, Test loss: 1.003e+04, MSE(e): 4.746e-04, MSE(pi1): 1.069e-02, MSE(pi2): 3.363e-04, MSE(pi3): 1.485e-03\n",
      "Epoch 1800, Train loss: 5.584e+03, Test loss: 1.172e+04, MSE(e): 5.343e-04, MSE(pi1): 9.353e-03, MSE(pi2): 3.385e-04, MSE(pi3): 1.480e-03\n",
      "Epoch 1900, Train loss: 3.777e+03, Test loss: 8.513e+03, MSE(e): 3.552e-04, MSE(pi1): 8.697e-03, MSE(pi2): 2.718e-04, MSE(pi3): 1.369e-03\n",
      "Epoch 2000, Train loss: 4.723e+03, Test loss: 7.913e+03, MSE(e): 4.496e-04, MSE(pi1): 8.529e-03, MSE(pi2): 2.921e-04, MSE(pi3): 1.423e-03\n",
      "Epoch 2100, Train loss: 4.444e+03, Test loss: 8.101e+03, MSE(e): 4.222e-04, MSE(pi1): 8.366e-03, MSE(pi2): 2.686e-04, MSE(pi3): 1.376e-03\n",
      "Epoch 2200, Train loss: 3.469e+03, Test loss: 7.350e+03, MSE(e): 3.240e-04, MSE(pi1): 9.361e-03, MSE(pi2): 2.218e-04, MSE(pi3): 1.351e-03\n",
      "Epoch 2300, Train loss: 3.285e+03, Test loss: 1.253e+04, MSE(e): 3.075e-04, MSE(pi1): 9.692e-03, MSE(pi2): 2.108e-04, MSE(pi3): 1.128e-03\n",
      "Epoch 2400, Train loss: 2.969e+03, Test loss: 6.843e+03, MSE(e): 2.760e-04, MSE(pi1): 8.070e-03, MSE(pi2): 1.957e-04, MSE(pi3): 1.282e-03\n",
      "Epoch 2500, Train loss: 2.684e+03, Test loss: 6.667e+03, MSE(e): 2.477e-04, MSE(pi1): 8.043e-03, MSE(pi2): 1.750e-04, MSE(pi3): 1.271e-03\n",
      "Epoch 2600, Train loss: 2.925e+03, Test loss: 6.521e+03, MSE(e): 2.699e-04, MSE(pi1): 9.733e-03, MSE(pi2): 1.785e-04, MSE(pi3): 1.286e-03\n",
      "Epoch 2700, Train loss: 2.387e+03, Test loss: 6.146e+03, MSE(e): 2.176e-04, MSE(pi1): 8.774e-03, MSE(pi2): 1.585e-04, MSE(pi3): 1.233e-03\n",
      "Epoch 2800, Train loss: 2.387e+03, Test loss: 6.112e+03, MSE(e): 2.163e-04, MSE(pi1): 1.054e-02, MSE(pi2): 1.482e-04, MSE(pi3): 1.192e-03\n",
      "Epoch 2900, Train loss: 2.353e+03, Test loss: 5.624e+03, MSE(e): 2.146e-04, MSE(pi1): 8.908e-03, MSE(pi2): 1.494e-04, MSE(pi3): 1.180e-03\n",
      "Epoch 3000, Train loss: 2.114e+03, Test loss: 5.564e+03, MSE(e): 1.913e-04, MSE(pi1): 8.216e-03, MSE(pi2): 1.337e-04, MSE(pi3): 1.184e-03\n",
      "Epoch 3100, Train loss: 2.082e+03, Test loss: 5.509e+03, MSE(e): 1.862e-04, MSE(pi1): 9.988e-03, MSE(pi2): 1.281e-04, MSE(pi3): 1.195e-03\n",
      "Epoch 3200, Train loss: 1.893e+03, Test loss: 5.849e+03, MSE(e): 1.696e-04, MSE(pi1): 8.429e-03, MSE(pi2): 1.146e-04, MSE(pi3): 1.121e-03\n",
      "Epoch 3300, Train loss: 2.272e+03, Test loss: 7.438e+03, MSE(e): 2.083e-04, MSE(pi1): 9.166e-03, MSE(pi2): 1.302e-04, MSE(pi3): 9.695e-04\n",
      "Epoch 3400, Train loss: 1.706e+03, Test loss: 5.322e+03, MSE(e): 1.509e-04, MSE(pi1): 8.629e-03, MSE(pi2): 1.096e-04, MSE(pi3): 1.107e-03\n",
      "Epoch 3500, Train loss: 1.654e+03, Test loss: 4.850e+03, MSE(e): 1.470e-04, MSE(pi1): 6.918e-03, MSE(pi2): 1.079e-04, MSE(pi3): 1.139e-03\n",
      "Epoch 3600, Train loss: 6.981e+03, Test loss: 6.545e+03, MSE(e): 6.783e-04, MSE(pi1): 9.383e-03, MSE(pi2): 3.027e-04, MSE(pi3): 1.032e-03\n",
      "Epoch 3700, Train loss: 3.526e+03, Test loss: 5.601e+03, MSE(e): 3.328e-04, MSE(pi1): 9.699e-03, MSE(pi2): 1.755e-04, MSE(pi3): 1.007e-03\n",
      "Epoch 3800, Train loss: 1.547e+03, Test loss: 4.781e+03, MSE(e): 1.352e-04, MSE(pi1): 8.801e-03, MSE(pi2): 9.690e-05, MSE(pi3): 1.070e-03\n",
      "Epoch 3900, Train loss: 4.225e+03, Test loss: 5.640e+03, MSE(e): 4.020e-04, MSE(pi1): 1.045e-02, MSE(pi2): 1.985e-04, MSE(pi3): 1.002e-03\n",
      "Epoch 4000, Train loss: 1.456e+03, Test loss: 4.742e+03, MSE(e): 1.273e-04, MSE(pi1): 8.006e-03, MSE(pi2): 9.025e-05, MSE(pi3): 1.025e-03\n",
      "Epoch 4100, Train loss: 1.792e+03, Test loss: 4.785e+03, MSE(e): 1.600e-04, MSE(pi1): 9.075e-03, MSE(pi2): 1.180e-04, MSE(pi3): 1.014e-03\n",
      "Epoch 4200, Train loss: 1.402e+03, Test loss: 4.534e+03, MSE(e): 1.209e-04, MSE(pi1): 8.766e-03, MSE(pi2): 8.977e-05, MSE(pi3): 1.055e-03\n",
      "Epoch 4300, Train loss: 1.376e+03, Test loss: 4.616e+03, MSE(e): 1.171e-04, MSE(pi1): 1.010e-02, MSE(pi2): 8.780e-05, MSE(pi3): 1.044e-03\n",
      "Epoch 4400, Train loss: 1.930e+03, Test loss: 8.557e+03, MSE(e): 1.748e-04, MSE(pi1): 8.724e-03, MSE(pi2): 1.075e-04, MSE(pi3): 9.551e-04\n",
      "Epoch 4500, Train loss: 1.493e+03, Test loss: 4.608e+03, MSE(e): 1.310e-04, MSE(pi1): 8.149e-03, MSE(pi2): 9.229e-05, MSE(pi3): 1.014e-03\n",
      "Epoch 4600, Train loss: 2.075e+03, Test loss: 1.013e+04, MSE(e): 1.902e-04, MSE(pi1): 8.110e-03, MSE(pi2): 1.002e-04, MSE(pi3): 9.246e-04\n",
      "Epoch 4700, Train loss: 2.081e+03, Test loss: 1.659e+04, MSE(e): 1.883e-04, MSE(pi1): 9.984e-03, MSE(pi2): 1.076e-04, MSE(pi3): 9.801e-04\n",
      "Epoch 4800, Train loss: 3.712e+03, Test loss: 7.047e+03, MSE(e): 3.521e-04, MSE(pi1): 9.512e-03, MSE(pi2): 1.617e-04, MSE(pi3): 9.659e-04\n",
      "Epoch 4900, Train loss: 9.951e+03, Test loss: 9.296e+03, MSE(e): 9.785e-04, MSE(pi1): 6.688e-03, MSE(pi2): 4.023e-04, MSE(pi3): 9.810e-04\n",
      "Epoch 5000, Train loss: 1.345e+03, Test loss: 4.629e+03, MSE(e): 1.135e-04, MSE(pi1): 1.075e-02, MSE(pi2): 8.344e-05, MSE(pi3): 1.020e-03\n",
      "Epoch 5100, Train loss: 1.609e+03, Test loss: 4.962e+03, MSE(e): 1.435e-04, MSE(pi1): 7.909e-03, MSE(pi2): 9.082e-05, MSE(pi3): 9.478e-04\n",
      "Epoch 5200, Train loss: 2.070e+03, Test loss: 4.693e+03, MSE(e): 1.889e-04, MSE(pi1): 8.621e-03, MSE(pi2): 9.657e-05, MSE(pi3): 9.422e-04\n",
      "Epoch 5300, Train loss: 1.373e+03, Test loss: 4.458e+03, MSE(e): 1.178e-04, MSE(pi1): 8.226e-03, MSE(pi2): 7.700e-05, MSE(pi3): 1.123e-03\n",
      "Epoch 5400, Train loss: 1.174e+03, Test loss: 4.506e+03, MSE(e): 9.937e-05, MSE(pi1): 8.003e-03, MSE(pi2): 6.971e-05, MSE(pi3): 1.002e-03\n",
      "Epoch 5500, Train loss: 1.304e+03, Test loss: 4.407e+03, MSE(e): 1.135e-04, MSE(pi1): 7.375e-03, MSE(pi2): 7.939e-05, MSE(pi3): 9.522e-04\n",
      "Epoch 5600, Train loss: 1.522e+03, Test loss: 4.475e+03, MSE(e): 1.338e-04, MSE(pi1): 9.114e-03, MSE(pi2): 9.032e-05, MSE(pi3): 9.297e-04\n",
      "Epoch 5700, Train loss: 1.147e+03, Test loss: 4.564e+03, MSE(e): 9.692e-05, MSE(pi1): 8.120e-03, MSE(pi2): 7.106e-05, MSE(pi3): 9.661e-04\n",
      "Epoch 5800, Train loss: 1.509e+03, Test loss: 4.458e+03, MSE(e): 1.311e-04, MSE(pi1): 9.925e-03, MSE(pi2): 8.845e-05, MSE(pi3): 9.803e-04\n",
      "Epoch 5900, Train loss: 1.143e+03, Test loss: 4.325e+03, MSE(e): 9.745e-05, MSE(pi1): 7.307e-03, MSE(pi2): 7.419e-05, MSE(pi3): 9.496e-04\n",
      "Epoch 6000, Train loss: 1.087e+03, Test loss: 4.318e+03, MSE(e): 9.220e-05, MSE(pi1): 7.227e-03, MSE(pi2): 6.658e-05, MSE(pi3): 9.283e-04\n",
      "Epoch 6100, Train loss: 1.302e+03, Test loss: 4.073e+03, MSE(e): 1.131e-04, MSE(pi1): 7.394e-03, MSE(pi2): 7.026e-05, MSE(pi3): 9.673e-04\n",
      "Epoch 6200, Train loss: 3.018e+03, Test loss: 7.968e+03, MSE(e): 2.845e-04, MSE(pi1): 7.362e-03, MSE(pi2): 1.817e-04, MSE(pi3): 9.876e-04\n",
      "Epoch 6300, Train loss: 1.023e+03, Test loss: 4.090e+03, MSE(e): 8.618e-05, MSE(pi1): 6.910e-03, MSE(pi2): 6.557e-05, MSE(pi3): 9.159e-04\n",
      "Epoch 6400, Train loss: 6.501e+03, Test loss: 6.980e+03, MSE(e): 6.332e-04, MSE(pi1): 7.304e-03, MSE(pi2): 2.855e-04, MSE(pi3): 9.503e-04\n",
      "Epoch 6500, Train loss: 1.043e+03, Test loss: 4.100e+03, MSE(e): 8.690e-05, MSE(pi1): 8.041e-03, MSE(pi2): 6.668e-05, MSE(pi3): 9.369e-04\n",
      "Epoch 6600, Train loss: 2.604e+03, Test loss: 1.012e+04, MSE(e): 2.423e-04, MSE(pi1): 8.346e-03, MSE(pi2): 1.404e-04, MSE(pi3): 9.719e-04\n",
      "Epoch 6700, Train loss: 1.075e+03, Test loss: 3.773e+03, MSE(e): 9.029e-05, MSE(pi1): 7.727e-03, MSE(pi2): 7.154e-05, MSE(pi3): 9.425e-04\n",
      "Epoch 6800, Train loss: 3.074e+03, Test loss: 4.913e+03, MSE(e): 2.881e-04, MSE(pi1): 9.690e-03, MSE(pi2): 1.555e-04, MSE(pi3): 9.590e-04\n",
      "Epoch 6900, Train loss: 1.011e+03, Test loss: 3.839e+03, MSE(e): 8.235e-05, MSE(pi1): 9.084e-03, MSE(pi2): 6.351e-05, MSE(pi3): 9.690e-04\n",
      "Epoch 7000, Train loss: 3.468e+03, Test loss: 4.867e+03, MSE(e): 3.277e-04, MSE(pi1): 9.272e-03, MSE(pi2): 1.728e-04, MSE(pi3): 9.865e-04\n",
      "Epoch 7100, Train loss: 9.512e+02, Test loss: 3.712e+03, MSE(e): 7.782e-05, MSE(pi1): 7.618e-03, MSE(pi2): 5.855e-05, MSE(pi3): 9.681e-04\n",
      "Epoch 7200, Train loss: 2.354e+03, Test loss: 9.575e+03, MSE(e): 2.192e-04, MSE(pi1): 6.570e-03, MSE(pi2): 1.409e-04, MSE(pi3): 9.649e-04\n",
      "Epoch 7300, Train loss: 9.288e+02, Test loss: 3.555e+03, MSE(e): 7.746e-05, MSE(pi1): 5.791e-03, MSE(pi2): 6.076e-05, MSE(pi3): 9.616e-04\n",
      "Epoch 7400, Train loss: 1.037e+03, Test loss: 3.730e+03, MSE(e): 8.729e-05, MSE(pi1): 7.082e-03, MSE(pi2): 5.858e-05, MSE(pi3): 9.320e-04\n",
      "Epoch 7500, Train loss: 3.052e+03, Test loss: 5.242e+03, MSE(e): 2.872e-04, MSE(pi1): 8.078e-03, MSE(pi2): 1.613e-04, MSE(pi3): 9.815e-04\n",
      "Epoch 7600, Train loss: 1.111e+03, Test loss: 3.678e+03, MSE(e): 9.353e-05, MSE(pi1): 8.486e-03, MSE(pi2): 7.468e-05, MSE(pi3): 9.123e-04\n",
      "Epoch 7700, Train loss: 1.776e+03, Test loss: 3.732e+03, MSE(e): 1.604e-04, MSE(pi1): 7.846e-03, MSE(pi2): 8.685e-05, MSE(pi3): 9.326e-04\n",
      "Epoch 7800, Train loss: 1.814e+03, Test loss: 4.130e+03, MSE(e): 1.571e-04, MSE(pi1): 1.320e-02, MSE(pi2): 1.129e-04, MSE(pi3): 1.109e-03\n",
      "Epoch 7900, Train loss: 9.167e+02, Test loss: 3.476e+03, MSE(e): 7.621e-05, MSE(pi1): 5.681e-03, MSE(pi2): 5.825e-05, MSE(pi3): 9.773e-04\n",
      "Epoch 8000, Train loss: 4.566e+03, Test loss: 4.131e+03, MSE(e): 4.378e-04, MSE(pi1): 9.206e-03, MSE(pi2): 1.852e-04, MSE(pi3): 9.502e-04\n",
      "Epoch 8100, Train loss: 1.012e+03, Test loss: 3.992e+03, MSE(e): 7.946e-05, MSE(pi1): 1.123e-02, MSE(pi2): 5.795e-05, MSE(pi3): 1.046e-03\n",
      "Epoch 8200, Train loss: 8.300e+02, Test loss: 3.444e+03, MSE(e): 6.853e-05, MSE(pi1): 4.971e-03, MSE(pi2): 5.321e-05, MSE(pi3): 9.492e-04\n",
      "Epoch 8300, Train loss: 1.780e+03, Test loss: 3.929e+03, MSE(e): 1.595e-04, MSE(pi1): 9.092e-03, MSE(pi2): 8.340e-05, MSE(pi3): 9.437e-04\n",
      "Epoch 8400, Train loss: 3.109e+03, Test loss: 4.174e+03, MSE(e): 2.954e-04, MSE(pi1): 6.148e-03, MSE(pi2): 1.612e-04, MSE(pi3): 9.376e-04\n",
      "Epoch 8500, Train loss: 9.774e+02, Test loss: 3.404e+03, MSE(e): 8.295e-05, MSE(pi1): 5.611e-03, MSE(pi2): 6.316e-05, MSE(pi3): 9.180e-04\n",
      "Epoch 8600, Train loss: 8.854e+02, Test loss: 3.361e+03, MSE(e): 7.374e-05, MSE(pi1): 5.352e-03, MSE(pi2): 5.352e-05, MSE(pi3): 9.439e-04\n",
      "Epoch 8700, Train loss: 3.465e+03, Test loss: 7.892e+03, MSE(e): 3.304e-04, MSE(pi1): 6.564e-03, MSE(pi2): 1.775e-04, MSE(pi3): 9.549e-04\n",
      "Epoch 8800, Train loss: 9.420e+02, Test loss: 3.465e+03, MSE(e): 7.348e-05, MSE(pi1): 1.028e-02, MSE(pi2): 5.626e-05, MSE(pi3): 1.043e-03\n",
      "Epoch 8900, Train loss: 8.562e+02, Test loss: 3.230e+03, MSE(e): 7.105e-05, MSE(pi1): 4.686e-03, MSE(pi2): 5.474e-05, MSE(pi3): 9.880e-04\n",
      "Epoch 9000, Train loss: 2.593e+03, Test loss: 3.541e+03, MSE(e): 2.436e-04, MSE(pi1): 6.080e-03, MSE(pi2): 1.159e-04, MSE(pi3): 9.604e-04\n",
      "Epoch 9100, Train loss: 1.062e+03, Test loss: 3.589e+03, MSE(e): 8.574e-05, MSE(pi1): 9.813e-03, MSE(pi2): 6.193e-05, MSE(pi3): 1.067e-03\n",
      "Epoch 9200, Train loss: 8.090e+02, Test loss: 3.227e+03, MSE(e): 6.649e-05, MSE(pi1): 4.650e-03, MSE(pi2): 5.241e-05, MSE(pi3): 9.748e-04\n",
      "Epoch 9300, Train loss: 1.384e+03, Test loss: 3.540e+03, MSE(e): 1.222e-04, MSE(pi1): 6.718e-03, MSE(pi2): 6.733e-05, MSE(pi3): 9.457e-04\n",
      "Epoch 9400, Train loss: 2.857e+03, Test loss: 4.549e+03, MSE(e): 2.697e-04, MSE(pi1): 6.479e-03, MSE(pi2): 1.538e-04, MSE(pi3): 9.530e-04\n",
      "Epoch 9500, Train loss: 9.584e+02, Test loss: 3.531e+03, MSE(e): 7.899e-05, MSE(pi1): 6.648e-03, MSE(pi2): 5.650e-05, MSE(pi3): 1.020e-03\n",
      "Epoch 9600, Train loss: 9.722e+02, Test loss: 3.385e+03, MSE(e): 7.990e-05, MSE(pi1): 7.170e-03, MSE(pi2): 6.124e-05, MSE(pi3): 1.015e-03\n",
      "Epoch 9700, Train loss: 1.373e+03, Test loss: 3.456e+03, MSE(e): 1.185e-04, MSE(pi1): 9.181e-03, MSE(pi2): 6.597e-05, MSE(pi3): 9.615e-04\n",
      "Epoch 9800, Train loss: 1.670e+03, Test loss: 5.296e+03, MSE(e): 1.524e-04, MSE(pi1): 4.791e-03, MSE(pi2): 1.070e-04, MSE(pi3): 9.813e-04\n",
      "Epoch 9900, Train loss: 8.025e+02, Test loss: 3.186e+03, MSE(e): 6.597e-05, MSE(pi1): 4.116e-03, MSE(pi2): 5.258e-05, MSE(pi3): 1.016e-03\n",
      "\n",
      "Training process finished after 10000 epochs\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load model and the optimizer\n",
    "model = PGNNIVPOD(input_shape, predictive_layers, POD_base, predictive_output, explanatory_input, explanatory_layers, explanatory_output, n_filters_explanatory).to(DEVICE)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=3e-3)\n",
    "\n",
    "# Parametros de entrenamiento\n",
    "start_epoch = 0\n",
    "n_epochs = 10000\n",
    "\n",
    "batch_size = 64\n",
    "n_checkpoints = 5\n",
    "\n",
    "train_loop(model, optimizer, X_train, y_train, f_train, X_test, y_test, f_test,\n",
    "        D,  n_checkpoints, start_epoch=start_epoch, n_epochs=n_epochs, batch_size=batch_size, \n",
    "        model_results_path=MODEL_RESULTS_PATH, device=DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training from a checkpoint. Epoch 800.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Parametros de entrenamiento\n",
    "start_epoch = 800\n",
    "n_epochs = 1000\n",
    "\n",
    "batch_size = 64 \n",
    "n_checkpoints = 10\n",
    "\n",
    "second_lr = 3e-4\n",
    "\n",
    "train_loop(model, optimizer, X_train, y_train, f_train, X_test, y_test, f_test,\n",
    "        D,  n_checkpoints, start_epoch=start_epoch, n_epochs=n_epochs, batch_size=batch_size, \n",
    "        model_results_path=MODEL_RESULTS_PATH, device=DEVICE, new_lr=second_lr)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SciML_test_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
