{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import sys\n",
    "\n",
    "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), \"../../\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Own library imports\n",
    "from vecopsciml.utils import TensOps\n",
    "from vecopsciml.operators.zero_order import Mx, My\n",
    "from vecopsciml.kernels.derivative import DerivativeKernels\n",
    "\n",
    "# Function from this project\n",
    "from utils.folders import create_folder\n",
    "from utils.load_data import load_data\n",
    "from trainers.train import train_loop, train_autoencoder_loop\n",
    "\n",
    "# Import model\n",
    "from architectures.autoencoder import Autoencoder\n",
    "from architectures.pgnniv_decoder import PGNNIVAutoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset\n",
    "dataset = 'non_linear'\n",
    "N_data = 100\n",
    "noise = 1\n",
    "\n",
    "data_name = dataset + '_' + str(N_data) + '_' + str(noise)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model\n",
    "model = 'autoencoder'\n",
    "n_modes = 10\n",
    "\n",
    "model_name = model + '_model_' + str(n_modes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Folder already exists at: /home/rmunoz/Escritorio/rmunozTMELab/Physically-Guided-Machine-Learning/results/non_linear_100_1\n",
      "Folder already exists at: /home/rmunoz/Escritorio/rmunozTMELab/Physically-Guided-Machine-Learning/results/non_linear_100_1/autoencoder_model_10_AE\n",
      "Folder already exists at: /home/rmunoz/Escritorio/rmunozTMELab/Physically-Guided-Machine-Learning/results/non_linear_100_1/autoencoder_model_10_NN\n"
     ]
    }
   ],
   "source": [
    "ROOT_PATH = os.path.abspath(os.path.join(os.getcwd(), \"../../\"))\n",
    "DATA_PATH = os.path.join(ROOT_PATH, r'data/', data_name, data_name) + '.pkl'\n",
    "RESULTS_FOLDER_PATH = os.path.join(ROOT_PATH, r'results/', data_name)\n",
    "\n",
    "MODEL_RESULTS_AE_PATH = os.path.join(ROOT_PATH, r'results/', data_name, model_name) + '_AE'\n",
    "MODEL_RESULTS_PGNNIV_PATH = os.path.join(ROOT_PATH, r'results/', data_name, model_name) + '_NN'\n",
    "\n",
    "# Creamos las carpetas que sean necesarias (si ya están creadas se avisará de ello)\n",
    "create_folder(RESULTS_FOLDER_PATH)\n",
    "create_folder(MODEL_RESULTS_AE_PATH)\n",
    "create_folder(MODEL_RESULTS_PGNNIV_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully loaded from: /home/rmunoz/Escritorio/rmunozTMELab/Physically-Guided-Machine-Learning/data/non_linear_100_1/non_linear_100_1.pkl\n"
     ]
    }
   ],
   "source": [
    "# Load dataset\n",
    "dataset = load_data(DATA_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convolutional filters to derivate\n",
    "dx = dataset['x_step_size']\n",
    "dy = dataset['y_step_size']\n",
    "D = DerivativeKernels(dx, dy, 0).grad_kernels_two_dimensions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "print(f\"Using device: {DEVICE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### División de los datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset length: 80\n",
      "Validation dataset length: 20\n"
     ]
    }
   ],
   "source": [
    "X_train = torch.Tensor(dataset['X_train']).unsqueeze(1)\n",
    "y_train = torch.Tensor(dataset['y_train']).unsqueeze(1)\n",
    "K_train = torch.tensor(dataset['k_train']).unsqueeze(1)\n",
    "f_train = torch.tensor(dataset['f_train']).unsqueeze(1).to(torch.float32)\n",
    "\n",
    "X_val = torch.Tensor(dataset['X_val']).unsqueeze(1)\n",
    "y_val = TensOps(torch.Tensor(dataset['y_val']).unsqueeze(1).requires_grad_(True), space_dimension=2, contravariance=0, covariance=0)\n",
    "K_val = TensOps(torch.tensor(dataset['k_val']).unsqueeze(1).requires_grad_(True), space_dimension=2, contravariance=0, covariance=0)\n",
    "f_val = TensOps(torch.tensor(dataset['f_val']).to(torch.float32).unsqueeze(1).requires_grad_(True), space_dimension=2, contravariance=0, covariance=0)\n",
    "\n",
    "print(\"Train dataset length:\", len(X_train))\n",
    "print(\"Validation dataset length:\", len(X_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset length for the autoencoder: 40\n",
      "Dataset length for the PGNNIV: 40\n"
     ]
    }
   ],
   "source": [
    "N_data_AE = len(X_train)//2\n",
    "N_data_NN = len(X_train) - len(X_train)//2\n",
    "prop_data_NN = 1 - N_data_AE/(N_data_NN + N_data_AE)\n",
    "\n",
    "print(\"Dataset length for the autoencoder:\", N_data_AE)\n",
    "print(\"Dataset length for the PGNNIV:\", N_data_NN)\n",
    "\n",
    "X_AE, X_NN, y_AE, y_NN, K_AE, K_NN, f_AE, f_NN = train_test_split(X_train, y_train, K_train, f_train, test_size=prop_data_NN, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Datos para el autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_AE, y_test_AE = train_test_split(y_AE, test_size=0.2, random_state=42)\n",
    "\n",
    "y_train_AE = TensOps(y_train_AE.requires_grad_(True).to(DEVICE), space_dimension=2, contravariance=0, covariance=0)\n",
    "y_test_AE = TensOps(y_test_AE.requires_grad_(True).to(DEVICE), space_dimension=2, contravariance=0, covariance=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Datos para la PGNNIV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_NN, X_test_NN, y_train_NN, y_test_NN, K_train_NN, K_test_NN, f_train_NN, f_test_NN = train_test_split(X_NN, y_NN, K_NN, f_NN, test_size=0.2, random_state=42)\n",
    "\n",
    "X_train_NN = X_train_NN.to(DEVICE)\n",
    "X_test_NN = X_test_NN.to(DEVICE)\n",
    "\n",
    "y_train_NN = TensOps(y_train_NN.requires_grad_(True).to(DEVICE), space_dimension=2, contravariance=0, covariance=0)\n",
    "y_test_NN = TensOps(y_test_NN.requires_grad_(True).to(DEVICE), space_dimension=2, contravariance=0, covariance=0)\n",
    "\n",
    "K_train_NN = TensOps(K_train_NN.to(DEVICE), space_dimension=2, contravariance=0, covariance=0)\n",
    "K_test_NN = TensOps(K_test_NN.to(DEVICE), space_dimension=2, contravariance=0, covariance=0)\n",
    "\n",
    "f_train_NN = TensOps(f_train_NN.to(DEVICE), space_dimension=2, contravariance=0, covariance=0)\n",
    "f_test_NN = TensOps(f_test_NN.to(DEVICE), space_dimension=2, contravariance=0, covariance=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "autoencoder_input_shape = y_train_AE.values[0].shape\n",
    "latent_space_dim = [15, 10, n_modes, 10, 15]\n",
    "autoencoder_output_shape = y_train_AE.values[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = y_train_AE.values\n",
    "y_train = y_train_AE\n",
    "\n",
    "X_test = y_test_AE.values\n",
    "y_test = y_test_AE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training from scratch.\n",
      "Epoch 0, Train loss: 5.446e+01, Test loss: 8.564e+01\n",
      "Epoch 10, Train loss: 8.212e+00, Test loss: 1.227e+01\n",
      "Epoch 20, Train loss: 3.451e+00, Test loss: 7.113e+00\n",
      "Epoch 30, Train loss: 2.433e+00, Test loss: 4.520e+00\n",
      "Epoch 40, Train loss: 2.064e+00, Test loss: 4.114e+00\n",
      "Epoch 50, Train loss: 1.932e+00, Test loss: 3.814e+00\n",
      "Epoch 60, Train loss: 1.888e+00, Test loss: 3.679e+00\n",
      "Epoch 70, Train loss: 1.863e+00, Test loss: 3.682e+00\n",
      "Epoch 80, Train loss: 1.853e+00, Test loss: 3.668e+00\n",
      "Epoch 90, Train loss: 1.835e+00, Test loss: 3.670e+00\n",
      "Epoch 100, Train loss: 1.784e+00, Test loss: 3.664e+00\n",
      "Epoch 110, Train loss: 1.618e+00, Test loss: 3.611e+00\n",
      "Epoch 120, Train loss: 1.145e+00, Test loss: 3.432e+00\n",
      "Epoch 130, Train loss: 8.745e-01, Test loss: 3.903e+00\n",
      "Epoch 140, Train loss: 7.375e-01, Test loss: 3.176e+00\n",
      "Epoch 150, Train loss: 8.005e-01, Test loss: 2.504e+00\n",
      "Epoch 160, Train loss: 6.026e-01, Test loss: 2.298e+00\n",
      "Epoch 170, Train loss: 5.625e-01, Test loss: 2.129e+00\n",
      "Epoch 180, Train loss: 5.367e-01, Test loss: 1.916e+00\n",
      "Epoch 190, Train loss: 4.649e-01, Test loss: 1.044e+00\n",
      "Epoch 200, Train loss: 3.851e-01, Test loss: 1.309e+00\n",
      "Epoch 210, Train loss: 3.258e-01, Test loss: 7.168e-01\n",
      "Epoch 220, Train loss: 3.448e-01, Test loss: 9.080e-01\n",
      "Epoch 230, Train loss: 3.377e-01, Test loss: 6.801e-01\n",
      "Epoch 240, Train loss: 3.011e-01, Test loss: 7.266e-01\n",
      "Epoch 250, Train loss: 3.499e-01, Test loss: 7.138e-01\n",
      "Epoch 260, Train loss: 2.769e-01, Test loss: 6.558e-01\n",
      "Epoch 270, Train loss: 2.802e-01, Test loss: 7.240e-01\n",
      "Epoch 280, Train loss: 2.869e-01, Test loss: 7.218e-01\n",
      "Epoch 290, Train loss: 2.786e-01, Test loss: 6.607e-01\n",
      "Epoch 300, Train loss: 2.950e-01, Test loss: 8.117e-01\n",
      "Epoch 310, Train loss: 2.704e-01, Test loss: 6.509e-01\n",
      "Epoch 320, Train loss: 2.591e-01, Test loss: 6.357e-01\n",
      "Epoch 330, Train loss: 2.607e-01, Test loss: 6.714e-01\n",
      "Epoch 340, Train loss: 3.039e-01, Test loss: 8.822e-01\n",
      "Epoch 350, Train loss: 2.583e-01, Test loss: 6.518e-01\n",
      "Epoch 360, Train loss: 2.535e-01, Test loss: 6.708e-01\n",
      "Epoch 370, Train loss: 2.503e-01, Test loss: 6.467e-01\n",
      "Epoch 380, Train loss: 2.503e-01, Test loss: 6.279e-01\n",
      "Epoch 390, Train loss: 3.635e-01, Test loss: 7.617e-01\n",
      "Epoch 400, Train loss: 2.726e-01, Test loss: 6.286e-01\n",
      "Epoch 410, Train loss: 2.467e-01, Test loss: 6.450e-01\n",
      "Epoch 420, Train loss: 2.469e-01, Test loss: 6.450e-01\n",
      "Epoch 430, Train loss: 2.440e-01, Test loss: 6.265e-01\n",
      "Epoch 440, Train loss: 2.481e-01, Test loss: 6.585e-01\n",
      "Epoch 450, Train loss: 2.825e-01, Test loss: 6.505e-01\n",
      "Epoch 460, Train loss: 2.430e-01, Test loss: 6.135e-01\n",
      "Epoch 470, Train loss: 2.492e-01, Test loss: 6.125e-01\n",
      "Epoch 480, Train loss: 2.424e-01, Test loss: 6.182e-01\n",
      "Epoch 490, Train loss: 2.417e-01, Test loss: 6.182e-01\n",
      "Epoch 500, Train loss: 2.404e-01, Test loss: 6.300e-01\n",
      "Epoch 510, Train loss: 3.360e-01, Test loss: 9.842e-01\n",
      "Epoch 520, Train loss: 2.629e-01, Test loss: 7.131e-01\n",
      "Epoch 530, Train loss: 2.412e-01, Test loss: 6.066e-01\n",
      "Epoch 540, Train loss: 2.404e-01, Test loss: 6.156e-01\n",
      "Epoch 550, Train loss: 2.382e-01, Test loss: 6.225e-01\n",
      "Epoch 560, Train loss: 2.380e-01, Test loss: 6.155e-01\n",
      "Epoch 570, Train loss: 2.373e-01, Test loss: 6.181e-01\n",
      "Epoch 580, Train loss: 2.365e-01, Test loss: 6.154e-01\n",
      "Epoch 590, Train loss: 3.704e-01, Test loss: 1.185e+00\n",
      "Epoch 600, Train loss: 2.667e-01, Test loss: 6.001e-01\n",
      "Epoch 610, Train loss: 2.503e-01, Test loss: 6.554e-01\n",
      "Epoch 620, Train loss: 2.409e-01, Test loss: 6.155e-01\n",
      "Epoch 630, Train loss: 2.375e-01, Test loss: 6.107e-01\n",
      "Epoch 640, Train loss: 2.368e-01, Test loss: 6.128e-01\n",
      "Epoch 650, Train loss: 2.357e-01, Test loss: 6.114e-01\n",
      "Epoch 660, Train loss: 2.347e-01, Test loss: 6.089e-01\n",
      "Epoch 670, Train loss: 2.343e-01, Test loss: 6.078e-01\n",
      "Epoch 680, Train loss: 2.339e-01, Test loss: 6.023e-01\n",
      "Epoch 690, Train loss: 2.337e-01, Test loss: 5.992e-01\n",
      "Epoch 700, Train loss: 2.710e-01, Test loss: 6.984e-01\n",
      "Epoch 710, Train loss: 2.468e-01, Test loss: 5.933e-01\n",
      "Epoch 720, Train loss: 2.474e-01, Test loss: 5.896e-01\n",
      "Epoch 730, Train loss: 2.396e-01, Test loss: 6.070e-01\n",
      "Epoch 740, Train loss: 2.334e-01, Test loss: 5.987e-01\n",
      "Epoch 750, Train loss: 2.328e-01, Test loss: 6.002e-01\n",
      "Epoch 760, Train loss: 2.328e-01, Test loss: 6.029e-01\n",
      "Epoch 770, Train loss: 2.323e-01, Test loss: 5.961e-01\n",
      "Epoch 780, Train loss: 2.319e-01, Test loss: 5.966e-01\n",
      "Epoch 790, Train loss: 2.332e-01, Test loss: 6.031e-01\n",
      "Epoch 800, Train loss: 3.146e-01, Test loss: 7.735e-01\n",
      "Epoch 810, Train loss: 2.506e-01, Test loss: 6.372e-01\n",
      "Epoch 820, Train loss: 2.354e-01, Test loss: 5.896e-01\n",
      "Epoch 830, Train loss: 2.326e-01, Test loss: 5.914e-01\n",
      "Epoch 840, Train loss: 2.321e-01, Test loss: 5.991e-01\n",
      "Epoch 850, Train loss: 2.315e-01, Test loss: 5.902e-01\n",
      "Epoch 860, Train loss: 2.306e-01, Test loss: 5.891e-01\n",
      "Epoch 870, Train loss: 2.303e-01, Test loss: 5.886e-01\n",
      "Epoch 880, Train loss: 2.368e-01, Test loss: 6.241e-01\n",
      "Epoch 890, Train loss: 2.908e-01, Test loss: 6.596e-01\n",
      "Epoch 900, Train loss: 2.501e-01, Test loss: 6.566e-01\n",
      "Epoch 910, Train loss: 2.416e-01, Test loss: 5.833e-01\n",
      "Epoch 920, Train loss: 2.338e-01, Test loss: 6.072e-01\n",
      "Epoch 930, Train loss: 2.321e-01, Test loss: 5.969e-01\n",
      "Epoch 940, Train loss: 2.307e-01, Test loss: 5.944e-01\n",
      "Epoch 950, Train loss: 2.298e-01, Test loss: 5.891e-01\n",
      "Epoch 960, Train loss: 2.296e-01, Test loss: 5.843e-01\n",
      "Epoch 970, Train loss: 2.294e-01, Test loss: 5.830e-01\n",
      "Epoch 980, Train loss: 2.292e-01, Test loss: 5.816e-01\n",
      "Epoch 990, Train loss: 2.291e-01, Test loss: 5.812e-01\n"
     ]
    }
   ],
   "source": [
    "autoencoder = Autoencoder(autoencoder_input_shape, latent_space_dim, autoencoder_output_shape).to(DEVICE)\n",
    "optimizer = torch.optim.Adam(autoencoder.parameters(), lr=1e-2)\n",
    "\n",
    "start_epoch = 0\n",
    "n_epochs = 1000\n",
    "batch_size = 64\n",
    "n_checkpoint = 10\n",
    "new_lr = None\n",
    "\n",
    "train_autoencoder_loop(autoencoder, optimizer, X_train, y_train, X_test, y_test,  \n",
    "                       n_checkpoint, start_epoch, n_epochs, batch_size, MODEL_RESULTS_AE_PATH, DEVICE, new_lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training from a checkpoint. Epoch 900.\n",
      "Epoch 900, Train loss: 2.523e-01, Test loss: 6.554e-01\n",
      "Epoch 1000, Train loss: 2.334e-01, Test loss: 5.859e-01\n",
      "Epoch 1100, Train loss: 2.327e-01, Test loss: 5.852e-01\n",
      "Epoch 1200, Train loss: 2.322e-01, Test loss: 5.850e-01\n",
      "Epoch 1300, Train loss: 2.317e-01, Test loss: 5.850e-01\n",
      "Epoch 1400, Train loss: 2.313e-01, Test loss: 5.852e-01\n",
      "Epoch 1500, Train loss: 2.311e-01, Test loss: 5.855e-01\n",
      "Epoch 1600, Train loss: 2.308e-01, Test loss: 5.858e-01\n",
      "Epoch 1700, Train loss: 2.306e-01, Test loss: 5.862e-01\n",
      "Epoch 1800, Train loss: 2.305e-01, Test loss: 5.865e-01\n",
      "Epoch 1900, Train loss: 2.303e-01, Test loss: 5.868e-01\n"
     ]
    }
   ],
   "source": [
    "start_epoch = 900\n",
    "n_epochs = 2000\n",
    "batch_size = 64\n",
    "n_checkpoint = 10\n",
    "new_lr = 1e-4\n",
    "4\n",
    "train_autoencoder_loop(autoencoder, optimizer, X_train, y_train, X_test, y_test,  \n",
    "                       n_checkpoint, start_epoch, n_epochs, batch_size, MODEL_RESULTS_AE_PATH, DEVICE, new_lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PGNNIV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predictive network architecture\n",
    "input_shape = X_train_NN[0].shape\n",
    "predictive_layers = [15, 10, n_modes]\n",
    "predictive_output = y_train_NN.values[0].shape\n",
    "\n",
    "# Explanatory network architecture\n",
    "explanatory_input = Mx(My(y_train_NN)).values[0].shape\n",
    "explanatory_layers = [10, 10]\n",
    "explanatory_output = Mx(My(f_train_NN)).values[0].shape\n",
    "\n",
    "# Other parameters\n",
    "n_filters_explanatory = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hidden1_layer.weight: requires_grad=False\n",
      "hidden1_layer.bias: requires_grad=False\n",
      "hidden2_layer.weight: requires_grad=False\n",
      "hidden2_layer.bias: requires_grad=False\n",
      "output_layer.weight: requires_grad=False\n",
      "output_layer.bias: requires_grad=False\n"
     ]
    }
   ],
   "source": [
    "pretrained_decoder = autoencoder.decoder\n",
    "\n",
    "for param in pretrained_decoder.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "for name, param in pretrained_decoder.named_parameters():\n",
    "    print(f\"{name}: requires_grad={param.requires_grad}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training from scratch.\n",
      "Epoch 0, Train loss: 3.215e+07, Test loss: 2.633e+07, MSE(e): 3.207e+00, MSE(pi1): 5.922e+00, MSE(pi2): 1.433e+00, MSE(pi3): 2.240e-01\n",
      "Epoch 10, Train loss: 1.863e+07, Test loss: 2.269e+07, MSE(e): 1.860e+00, MSE(pi1): 1.241e+00, MSE(pi2): 8.586e-01, MSE(pi3): 9.997e-02\n",
      "Epoch 20, Train loss: 1.724e+07, Test loss: 1.291e+07, MSE(e): 1.722e+00, MSE(pi1): 1.154e+00, MSE(pi2): 7.925e-01, MSE(pi3): 1.026e-01\n",
      "Epoch 30, Train loss: 1.407e+07, Test loss: 1.106e+07, MSE(e): 1.405e+00, MSE(pi1): 1.141e+00, MSE(pi2): 6.623e-01, MSE(pi3): 1.032e-01\n",
      "Epoch 40, Train loss: 9.204e+06, Test loss: 7.417e+06, MSE(e): 9.182e-01, MSE(pi1): 1.154e+00, MSE(pi2): 4.561e-01, MSE(pi3): 9.995e-02\n",
      "Epoch 50, Train loss: 5.276e+06, Test loss: 1.001e+07, MSE(e): 5.254e-01, MSE(pi1): 1.116e+00, MSE(pi2): 2.886e-01, MSE(pi3): 1.002e-01\n",
      "Epoch 60, Train loss: 4.926e+06, Test loss: 1.113e+07, MSE(e): 4.905e-01, MSE(pi1): 1.109e+00, MSE(pi2): 2.773e-01, MSE(pi3): 9.693e-02\n",
      "Epoch 70, Train loss: 4.283e+06, Test loss: 6.744e+06, MSE(e): 4.263e-01, MSE(pi1): 1.078e+00, MSE(pi2): 2.537e-01, MSE(pi3): 9.679e-02\n",
      "Epoch 80, Train loss: 4.001e+06, Test loss: 6.523e+06, MSE(e): 3.981e-01, MSE(pi1): 1.057e+00, MSE(pi2): 2.437e-01, MSE(pi3): 9.421e-02\n",
      "Epoch 90, Train loss: 3.839e+06, Test loss: 6.095e+06, MSE(e): 3.819e-01, MSE(pi1): 1.018e+00, MSE(pi2): 2.383e-01, MSE(pi3): 9.192e-02\n",
      "Epoch 100, Train loss: 3.709e+06, Test loss: 6.032e+06, MSE(e): 3.690e-01, MSE(pi1): 9.783e-01, MSE(pi2): 2.337e-01, MSE(pi3): 8.822e-02\n",
      "Epoch 110, Train loss: 3.581e+06, Test loss: 5.915e+06, MSE(e): 3.563e-01, MSE(pi1): 9.342e-01, MSE(pi2): 2.288e-01, MSE(pi3): 8.380e-02\n",
      "Epoch 120, Train loss: 3.451e+06, Test loss: 5.872e+06, MSE(e): 3.434e-01, MSE(pi1): 9.076e-01, MSE(pi2): 2.238e-01, MSE(pi3): 7.873e-02\n",
      "Epoch 130, Train loss: 3.325e+06, Test loss: 5.807e+06, MSE(e): 3.308e-01, MSE(pi1): 9.094e-01, MSE(pi2): 2.188e-01, MSE(pi3): 7.446e-02\n",
      "Epoch 140, Train loss: 3.218e+06, Test loss: 5.745e+06, MSE(e): 3.201e-01, MSE(pi1): 9.346e-01, MSE(pi2): 2.143e-01, MSE(pi3): 7.174e-02\n",
      "Epoch 150, Train loss: 3.137e+06, Test loss: 5.708e+06, MSE(e): 3.120e-01, MSE(pi1): 9.536e-01, MSE(pi2): 2.111e-01, MSE(pi3): 7.088e-02\n",
      "Epoch 160, Train loss: 3.081e+06, Test loss: 5.697e+06, MSE(e): 3.064e-01, MSE(pi1): 9.556e-01, MSE(pi2): 2.090e-01, MSE(pi3): 7.130e-02\n",
      "Epoch 170, Train loss: 3.043e+06, Test loss: 5.691e+06, MSE(e): 3.026e-01, MSE(pi1): 9.513e-01, MSE(pi2): 2.075e-01, MSE(pi3): 7.200e-02\n",
      "Epoch 180, Train loss: 3.017e+06, Test loss: 5.689e+06, MSE(e): 3.000e-01, MSE(pi1): 9.490e-01, MSE(pi2): 2.066e-01, MSE(pi3): 7.245e-02\n",
      "Epoch 190, Train loss: 2.997e+06, Test loss: 5.681e+06, MSE(e): 2.980e-01, MSE(pi1): 9.488e-01, MSE(pi2): 2.058e-01, MSE(pi3): 7.254e-02\n",
      "Epoch 200, Train loss: 2.981e+06, Test loss: 5.671e+06, MSE(e): 2.964e-01, MSE(pi1): 9.492e-01, MSE(pi2): 2.052e-01, MSE(pi3): 7.239e-02\n",
      "Epoch 210, Train loss: 2.967e+06, Test loss: 5.657e+06, MSE(e): 2.950e-01, MSE(pi1): 9.489e-01, MSE(pi2): 2.046e-01, MSE(pi3): 7.214e-02\n",
      "Epoch 220, Train loss: 2.954e+06, Test loss: 5.644e+06, MSE(e): 2.938e-01, MSE(pi1): 9.474e-01, MSE(pi2): 2.041e-01, MSE(pi3): 7.187e-02\n",
      "Epoch 230, Train loss: 2.943e+06, Test loss: 5.632e+06, MSE(e): 2.927e-01, MSE(pi1): 9.453e-01, MSE(pi2): 2.037e-01, MSE(pi3): 7.157e-02\n",
      "Epoch 240, Train loss: 2.933e+06, Test loss: 5.622e+06, MSE(e): 2.917e-01, MSE(pi1): 9.428e-01, MSE(pi2): 2.033e-01, MSE(pi3): 7.124e-02\n",
      "Epoch 250, Train loss: 2.925e+06, Test loss: 5.613e+06, MSE(e): 2.908e-01, MSE(pi1): 9.403e-01, MSE(pi2): 2.029e-01, MSE(pi3): 7.087e-02\n",
      "Epoch 260, Train loss: 2.917e+06, Test loss: 5.606e+06, MSE(e): 2.900e-01, MSE(pi1): 9.377e-01, MSE(pi2): 2.026e-01, MSE(pi3): 7.046e-02\n",
      "Epoch 270, Train loss: 2.910e+06, Test loss: 5.601e+06, MSE(e): 2.893e-01, MSE(pi1): 9.350e-01, MSE(pi2): 2.023e-01, MSE(pi3): 7.002e-02\n",
      "Epoch 280, Train loss: 2.903e+06, Test loss: 5.596e+06, MSE(e): 2.887e-01, MSE(pi1): 9.322e-01, MSE(pi2): 2.021e-01, MSE(pi3): 6.955e-02\n",
      "Epoch 290, Train loss: 2.898e+06, Test loss: 5.592e+06, MSE(e): 2.881e-01, MSE(pi1): 9.292e-01, MSE(pi2): 2.018e-01, MSE(pi3): 6.905e-02\n",
      "Epoch 300, Train loss: 2.892e+06, Test loss: 5.589e+06, MSE(e): 2.876e-01, MSE(pi1): 9.262e-01, MSE(pi2): 2.016e-01, MSE(pi3): 6.853e-02\n",
      "Epoch 310, Train loss: 2.888e+06, Test loss: 5.586e+06, MSE(e): 2.871e-01, MSE(pi1): 9.231e-01, MSE(pi2): 2.014e-01, MSE(pi3): 6.798e-02\n",
      "Epoch 320, Train loss: 2.883e+06, Test loss: 5.584e+06, MSE(e): 2.867e-01, MSE(pi1): 9.200e-01, MSE(pi2): 2.013e-01, MSE(pi3): 6.741e-02\n",
      "Epoch 330, Train loss: 2.880e+06, Test loss: 5.582e+06, MSE(e): 2.863e-01, MSE(pi1): 9.168e-01, MSE(pi2): 2.011e-01, MSE(pi3): 6.682e-02\n",
      "Epoch 340, Train loss: 2.876e+06, Test loss: 5.581e+06, MSE(e): 2.860e-01, MSE(pi1): 9.137e-01, MSE(pi2): 2.010e-01, MSE(pi3): 6.622e-02\n",
      "Epoch 350, Train loss: 2.873e+06, Test loss: 5.579e+06, MSE(e): 2.857e-01, MSE(pi1): 9.105e-01, MSE(pi2): 2.009e-01, MSE(pi3): 6.562e-02\n",
      "Epoch 360, Train loss: 2.870e+06, Test loss: 5.578e+06, MSE(e): 2.854e-01, MSE(pi1): 9.074e-01, MSE(pi2): 2.008e-01, MSE(pi3): 6.501e-02\n",
      "Epoch 370, Train loss: 2.867e+06, Test loss: 5.577e+06, MSE(e): 2.851e-01, MSE(pi1): 9.043e-01, MSE(pi2): 2.007e-01, MSE(pi3): 6.442e-02\n",
      "Epoch 380, Train loss: 2.865e+06, Test loss: 5.577e+06, MSE(e): 2.849e-01, MSE(pi1): 9.013e-01, MSE(pi2): 2.006e-01, MSE(pi3): 6.384e-02\n",
      "Epoch 390, Train loss: 2.862e+06, Test loss: 5.576e+06, MSE(e): 2.847e-01, MSE(pi1): 8.982e-01, MSE(pi2): 2.005e-01, MSE(pi3): 6.329e-02\n",
      "Epoch 400, Train loss: 2.860e+06, Test loss: 5.576e+06, MSE(e): 2.845e-01, MSE(pi1): 8.952e-01, MSE(pi2): 2.004e-01, MSE(pi3): 6.276e-02\n",
      "Epoch 410, Train loss: 2.858e+06, Test loss: 5.575e+06, MSE(e): 2.843e-01, MSE(pi1): 8.924e-01, MSE(pi2): 2.003e-01, MSE(pi3): 6.225e-02\n",
      "Epoch 420, Train loss: 2.857e+06, Test loss: 5.580e+06, MSE(e): 2.841e-01, MSE(pi1): 8.908e-01, MSE(pi2): 2.002e-01, MSE(pi3): 6.178e-02\n",
      "Epoch 430, Train loss: 2.859e+06, Test loss: 5.578e+06, MSE(e): 2.844e-01, MSE(pi1): 8.874e-01, MSE(pi2): 1.999e-01, MSE(pi3): 6.133e-02\n",
      "Epoch 440, Train loss: 2.864e+06, Test loss: 5.596e+06, MSE(e): 2.849e-01, MSE(pi1): 8.761e-01, MSE(pi2): 2.012e-01, MSE(pi3): 6.089e-02\n",
      "Epoch 450, Train loss: 2.851e+06, Test loss: 5.583e+06, MSE(e): 2.836e-01, MSE(pi1): 8.781e-01, MSE(pi2): 2.002e-01, MSE(pi3): 6.043e-02\n",
      "Epoch 460, Train loss: 2.851e+06, Test loss: 5.575e+06, MSE(e): 2.836e-01, MSE(pi1): 8.783e-01, MSE(pi2): 1.999e-01, MSE(pi3): 6.008e-02\n",
      "Epoch 470, Train loss: 2.848e+06, Test loss: 5.573e+06, MSE(e): 2.833e-01, MSE(pi1): 8.728e-01, MSE(pi2): 2.001e-01, MSE(pi3): 5.980e-02\n",
      "Epoch 480, Train loss: 2.847e+06, Test loss: 5.575e+06, MSE(e): 2.832e-01, MSE(pi1): 8.720e-01, MSE(pi2): 2.000e-01, MSE(pi3): 5.952e-02\n",
      "Epoch 490, Train loss: 2.845e+06, Test loss: 5.576e+06, MSE(e): 2.831e-01, MSE(pi1): 8.712e-01, MSE(pi2): 1.999e-01, MSE(pi3): 5.929e-02\n",
      "Epoch 500, Train loss: 2.844e+06, Test loss: 5.576e+06, MSE(e): 2.829e-01, MSE(pi1): 8.693e-01, MSE(pi2): 1.998e-01, MSE(pi3): 5.908e-02\n",
      "Epoch 510, Train loss: 2.843e+06, Test loss: 5.576e+06, MSE(e): 2.828e-01, MSE(pi1): 8.676e-01, MSE(pi2): 1.998e-01, MSE(pi3): 5.890e-02\n",
      "Epoch 520, Train loss: 2.842e+06, Test loss: 5.576e+06, MSE(e): 2.827e-01, MSE(pi1): 8.660e-01, MSE(pi2): 1.997e-01, MSE(pi3): 5.873e-02\n",
      "Epoch 530, Train loss: 2.841e+06, Test loss: 5.577e+06, MSE(e): 2.826e-01, MSE(pi1): 8.646e-01, MSE(pi2): 1.997e-01, MSE(pi3): 5.859e-02\n",
      "Epoch 540, Train loss: 2.840e+06, Test loss: 5.577e+06, MSE(e): 2.825e-01, MSE(pi1): 8.634e-01, MSE(pi2): 1.997e-01, MSE(pi3): 5.845e-02\n",
      "Epoch 550, Train loss: 2.839e+06, Test loss: 5.578e+06, MSE(e): 2.824e-01, MSE(pi1): 8.622e-01, MSE(pi2): 1.996e-01, MSE(pi3): 5.833e-02\n",
      "Epoch 560, Train loss: 2.838e+06, Test loss: 5.579e+06, MSE(e): 2.823e-01, MSE(pi1): 8.611e-01, MSE(pi2): 1.996e-01, MSE(pi3): 5.823e-02\n",
      "Epoch 570, Train loss: 2.837e+06, Test loss: 5.579e+06, MSE(e): 2.822e-01, MSE(pi1): 8.602e-01, MSE(pi2): 1.995e-01, MSE(pi3): 5.813e-02\n",
      "Epoch 580, Train loss: 2.836e+06, Test loss: 5.580e+06, MSE(e): 2.821e-01, MSE(pi1): 8.593e-01, MSE(pi2): 1.995e-01, MSE(pi3): 5.804e-02\n",
      "Epoch 590, Train loss: 2.835e+06, Test loss: 5.580e+06, MSE(e): 2.821e-01, MSE(pi1): 8.585e-01, MSE(pi2): 1.995e-01, MSE(pi3): 5.796e-02\n",
      "Epoch 600, Train loss: 2.834e+06, Test loss: 5.581e+06, MSE(e): 2.820e-01, MSE(pi1): 8.578e-01, MSE(pi2): 1.994e-01, MSE(pi3): 5.789e-02\n",
      "Epoch 610, Train loss: 2.834e+06, Test loss: 5.581e+06, MSE(e): 2.819e-01, MSE(pi1): 8.571e-01, MSE(pi2): 1.994e-01, MSE(pi3): 5.783e-02\n",
      "Epoch 620, Train loss: 2.833e+06, Test loss: 5.581e+06, MSE(e): 2.818e-01, MSE(pi1): 8.565e-01, MSE(pi2): 1.993e-01, MSE(pi3): 5.777e-02\n",
      "Epoch 630, Train loss: 2.832e+06, Test loss: 5.582e+06, MSE(e): 2.817e-01, MSE(pi1): 8.559e-01, MSE(pi2): 1.993e-01, MSE(pi3): 5.771e-02\n",
      "Epoch 640, Train loss: 2.831e+06, Test loss: 5.582e+06, MSE(e): 2.817e-01, MSE(pi1): 8.552e-01, MSE(pi2): 1.993e-01, MSE(pi3): 5.766e-02\n",
      "Epoch 650, Train loss: 2.850e+06, Test loss: 5.640e+06, MSE(e): 2.835e-01, MSE(pi1): 8.477e-01, MSE(pi2): 2.008e-01, MSE(pi3): 5.767e-02\n",
      "Epoch 660, Train loss: 2.851e+06, Test loss: 5.581e+06, MSE(e): 2.836e-01, MSE(pi1): 8.604e-01, MSE(pi2): 1.993e-01, MSE(pi3): 5.753e-02\n",
      "Epoch 670, Train loss: 2.830e+06, Test loss: 5.591e+06, MSE(e): 2.816e-01, MSE(pi1): 8.481e-01, MSE(pi2): 1.993e-01, MSE(pi3): 5.743e-02\n",
      "Epoch 680, Train loss: 2.833e+06, Test loss: 5.584e+06, MSE(e): 2.818e-01, MSE(pi1): 8.475e-01, MSE(pi2): 1.997e-01, MSE(pi3): 5.736e-02\n",
      "Epoch 690, Train loss: 2.828e+06, Test loss: 5.586e+06, MSE(e): 2.814e-01, MSE(pi1): 8.510e-01, MSE(pi2): 1.992e-01, MSE(pi3): 5.733e-02\n",
      "Epoch 700, Train loss: 2.828e+06, Test loss: 5.586e+06, MSE(e): 2.813e-01, MSE(pi1): 8.524e-01, MSE(pi2): 1.990e-01, MSE(pi3): 5.729e-02\n",
      "Epoch 710, Train loss: 2.827e+06, Test loss: 5.584e+06, MSE(e): 2.813e-01, MSE(pi1): 8.517e-01, MSE(pi2): 1.990e-01, MSE(pi3): 5.729e-02\n",
      "Epoch 720, Train loss: 2.826e+06, Test loss: 5.585e+06, MSE(e): 2.812e-01, MSE(pi1): 8.516e-01, MSE(pi2): 1.990e-01, MSE(pi3): 5.727e-02\n",
      "Epoch 730, Train loss: 2.826e+06, Test loss: 5.585e+06, MSE(e): 2.811e-01, MSE(pi1): 8.514e-01, MSE(pi2): 1.990e-01, MSE(pi3): 5.726e-02\n",
      "Epoch 740, Train loss: 2.825e+06, Test loss: 5.586e+06, MSE(e): 2.811e-01, MSE(pi1): 8.513e-01, MSE(pi2): 1.990e-01, MSE(pi3): 5.725e-02\n",
      "Epoch 750, Train loss: 2.825e+06, Test loss: 5.587e+06, MSE(e): 2.810e-01, MSE(pi1): 8.513e-01, MSE(pi2): 1.990e-01, MSE(pi3): 5.724e-02\n",
      "Epoch 760, Train loss: 2.824e+06, Test loss: 5.587e+06, MSE(e): 2.810e-01, MSE(pi1): 8.513e-01, MSE(pi2): 1.989e-01, MSE(pi3): 5.723e-02\n",
      "Epoch 770, Train loss: 2.824e+06, Test loss: 5.588e+06, MSE(e): 2.809e-01, MSE(pi1): 8.512e-01, MSE(pi2): 1.989e-01, MSE(pi3): 5.722e-02\n",
      "Epoch 780, Train loss: 2.823e+06, Test loss: 5.588e+06, MSE(e): 2.809e-01, MSE(pi1): 8.511e-01, MSE(pi2): 1.989e-01, MSE(pi3): 5.721e-02\n",
      "Epoch 790, Train loss: 2.822e+06, Test loss: 5.589e+06, MSE(e): 2.808e-01, MSE(pi1): 8.511e-01, MSE(pi2): 1.989e-01, MSE(pi3): 5.720e-02\n",
      "Epoch 800, Train loss: 2.822e+06, Test loss: 5.589e+06, MSE(e): 2.808e-01, MSE(pi1): 8.510e-01, MSE(pi2): 1.989e-01, MSE(pi3): 5.720e-02\n",
      "Epoch 810, Train loss: 2.822e+06, Test loss: 5.590e+06, MSE(e): 2.807e-01, MSE(pi1): 8.510e-01, MSE(pi2): 1.988e-01, MSE(pi3): 5.719e-02\n",
      "Epoch 820, Train loss: 2.821e+06, Test loss: 5.590e+06, MSE(e): 2.807e-01, MSE(pi1): 8.509e-01, MSE(pi2): 1.988e-01, MSE(pi3): 5.718e-02\n",
      "Epoch 830, Train loss: 2.821e+06, Test loss: 5.591e+06, MSE(e): 2.806e-01, MSE(pi1): 8.509e-01, MSE(pi2): 1.988e-01, MSE(pi3): 5.717e-02\n",
      "Epoch 840, Train loss: 2.820e+06, Test loss: 5.592e+06, MSE(e): 2.806e-01, MSE(pi1): 8.508e-01, MSE(pi2): 1.988e-01, MSE(pi3): 5.717e-02\n",
      "Epoch 850, Train loss: 2.820e+06, Test loss: 5.592e+06, MSE(e): 2.805e-01, MSE(pi1): 8.508e-01, MSE(pi2): 1.987e-01, MSE(pi3): 5.716e-02\n",
      "Epoch 860, Train loss: 2.819e+06, Test loss: 5.593e+06, MSE(e): 2.805e-01, MSE(pi1): 8.506e-01, MSE(pi2): 1.987e-01, MSE(pi3): 5.716e-02\n",
      "Epoch 870, Train loss: 2.821e+06, Test loss: 5.598e+06, MSE(e): 2.806e-01, MSE(pi1): 8.485e-01, MSE(pi2): 1.990e-01, MSE(pi3): 5.718e-02\n",
      "Epoch 880, Train loss: 2.833e+06, Test loss: 5.591e+06, MSE(e): 2.819e-01, MSE(pi1): 8.402e-01, MSE(pi2): 1.997e-01, MSE(pi3): 5.724e-02\n",
      "Epoch 890, Train loss: 2.818e+06, Test loss: 5.618e+06, MSE(e): 2.804e-01, MSE(pi1): 8.489e-01, MSE(pi2): 1.986e-01, MSE(pi3): 5.701e-02\n",
      "Epoch 900, Train loss: 2.825e+06, Test loss: 5.610e+06, MSE(e): 2.810e-01, MSE(pi1): 8.520e-01, MSE(pi2): 1.986e-01, MSE(pi3): 5.694e-02\n",
      "Epoch 910, Train loss: 2.819e+06, Test loss: 5.595e+06, MSE(e): 2.804e-01, MSE(pi1): 8.490e-01, MSE(pi2): 1.985e-01, MSE(pi3): 5.699e-02\n",
      "Epoch 920, Train loss: 2.817e+06, Test loss: 5.596e+06, MSE(e): 2.803e-01, MSE(pi1): 8.483e-01, MSE(pi2): 1.987e-01, MSE(pi3): 5.701e-02\n",
      "Epoch 930, Train loss: 2.817e+06, Test loss: 5.596e+06, MSE(e): 2.802e-01, MSE(pi1): 8.480e-01, MSE(pi2): 1.987e-01, MSE(pi3): 5.704e-02\n",
      "Epoch 940, Train loss: 2.816e+06, Test loss: 5.597e+06, MSE(e): 2.802e-01, MSE(pi1): 8.485e-01, MSE(pi2): 1.986e-01, MSE(pi3): 5.705e-02\n",
      "Epoch 950, Train loss: 2.816e+06, Test loss: 5.597e+06, MSE(e): 2.801e-01, MSE(pi1): 8.491e-01, MSE(pi2): 1.986e-01, MSE(pi3): 5.707e-02\n",
      "Epoch 960, Train loss: 2.815e+06, Test loss: 5.598e+06, MSE(e): 2.801e-01, MSE(pi1): 8.496e-01, MSE(pi2): 1.986e-01, MSE(pi3): 5.708e-02\n",
      "Epoch 970, Train loss: 2.815e+06, Test loss: 5.598e+06, MSE(e): 2.801e-01, MSE(pi1): 8.499e-01, MSE(pi2): 1.985e-01, MSE(pi3): 5.709e-02\n",
      "Epoch 980, Train loss: 2.815e+06, Test loss: 5.599e+06, MSE(e): 2.800e-01, MSE(pi1): 8.502e-01, MSE(pi2): 1.985e-01, MSE(pi3): 5.709e-02\n",
      "Epoch 990, Train loss: 2.814e+06, Test loss: 5.600e+06, MSE(e): 2.800e-01, MSE(pi1): 8.505e-01, MSE(pi2): 1.985e-01, MSE(pi3): 5.710e-02\n",
      "\n",
      "Training process finished after 1000 epochs\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = PGNNIVAutoencoder(input_shape, predictive_layers, pretrained_decoder, predictive_output, explanatory_input,\n",
    "                                   explanatory_layers, explanatory_output, n_filters_explanatory).to(DEVICE)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-2)\n",
    "\n",
    "# Parametros de entrenamiento\n",
    "start_epoch = 0\n",
    "n_epochs = 1000\n",
    "\n",
    "batch_size = 64\n",
    "n_checkpoints = 10\n",
    "\n",
    "train_loop(model, optimizer, X_train_NN, y_train_NN, f_train_NN, X_test_NN, y_test_NN, f_test_NN,\n",
    "           D, n_checkpoints, start_epoch=start_epoch, n_epochs=n_epochs, batch_size=batch_size, \n",
    "           model_results_path=MODEL_RESULTS_PGNNIV_PATH, device=DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training from a checkpoint. Epoch 900.\n",
      "Epoch 900, Train loss: 2.824e+06, Test loss: 5.610e+06, MSE(e): 2.810e-01, MSE(pi1): 8.432e-01, MSE(pi2): 1.993e-01, MSE(pi3): 5.704e-02\n",
      "Epoch 1000, Train loss: 2.818e+06, Test loss: 5.594e+06, MSE(e): 2.803e-01, MSE(pi1): 8.477e-01, MSE(pi2): 1.987e-01, MSE(pi3): 5.698e-02\n",
      "Epoch 1100, Train loss: 2.818e+06, Test loss: 5.595e+06, MSE(e): 2.803e-01, MSE(pi1): 8.478e-01, MSE(pi2): 1.987e-01, MSE(pi3): 5.699e-02\n",
      "Epoch 1200, Train loss: 2.817e+06, Test loss: 5.595e+06, MSE(e): 2.803e-01, MSE(pi1): 8.480e-01, MSE(pi2): 1.987e-01, MSE(pi3): 5.699e-02\n",
      "Epoch 1300, Train loss: 2.817e+06, Test loss: 5.595e+06, MSE(e): 2.803e-01, MSE(pi1): 8.481e-01, MSE(pi2): 1.987e-01, MSE(pi3): 5.700e-02\n",
      "Epoch 1400, Train loss: 2.817e+06, Test loss: 5.595e+06, MSE(e): 2.803e-01, MSE(pi1): 8.482e-01, MSE(pi2): 1.987e-01, MSE(pi3): 5.700e-02\n",
      "Epoch 1500, Train loss: 2.817e+06, Test loss: 5.595e+06, MSE(e): 2.803e-01, MSE(pi1): 8.483e-01, MSE(pi2): 1.986e-01, MSE(pi3): 5.701e-02\n",
      "Epoch 1600, Train loss: 2.817e+06, Test loss: 5.595e+06, MSE(e): 2.803e-01, MSE(pi1): 8.484e-01, MSE(pi2): 1.986e-01, MSE(pi3): 5.701e-02\n",
      "Epoch 1700, Train loss: 2.817e+06, Test loss: 5.595e+06, MSE(e): 2.803e-01, MSE(pi1): 8.485e-01, MSE(pi2): 1.986e-01, MSE(pi3): 5.702e-02\n",
      "Epoch 1800, Train loss: 2.817e+06, Test loss: 5.595e+06, MSE(e): 2.803e-01, MSE(pi1): 8.486e-01, MSE(pi2): 1.986e-01, MSE(pi3): 5.702e-02\n",
      "Epoch 1900, Train loss: 2.817e+06, Test loss: 5.595e+06, MSE(e): 2.803e-01, MSE(pi1): 8.487e-01, MSE(pi2): 1.986e-01, MSE(pi3): 5.702e-02\n",
      "\n",
      "Training process finished after 2000 epochs\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Parametros de entrenamiento\n",
    "start_epoch = 900\n",
    "n_epochs = 2000\n",
    "\n",
    "batch_size = 64 \n",
    "n_checkpoints = 100\n",
    "\n",
    "second_lr = 1e-4\n",
    "\n",
    "train_loop(model, optimizer, X_train_NN, y_train_NN, f_train_NN, X_test_NN, y_test_NN, f_test_NN,\n",
    "           D, n_checkpoints, start_epoch=start_epoch, n_epochs=n_epochs, batch_size=batch_size, \n",
    "           model_results_path=MODEL_RESULTS_PGNNIV_PATH, device=DEVICE, new_lr=second_lr)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SciML_test_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
