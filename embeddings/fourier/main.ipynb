{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import sys\n",
    "\n",
    "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), \"../../\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Own library imports\n",
    "from vecopsciml.utils import TensOps\n",
    "from vecopsciml.operators.zero_order import Mx, My\n",
    "from vecopsciml.kernels.derivative import DerivativeKernels\n",
    "\n",
    "# Function from this project\n",
    "from utils.folders import create_folder\n",
    "from utils.load_data import load_data\n",
    "from trainers.train import train_loop\n",
    "from utils.fourier_base import compute_fourier_base\n",
    "\n",
    "# Import model\n",
    "from architectures.pgnniv_fourier import PGNNIVFourier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset\n",
    "dataset = 'non_linear'\n",
    "N_data = 100\n",
    "noise = 1\n",
    "\n",
    "data_name = dataset + '_' + str(N_data) + '_' + str(noise)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model\n",
    "model = 'fourier'\n",
    "n_modes = 10\n",
    "\n",
    "model_name = model + '_model_' + str(n_modes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "ROOT_PATH = os.path.abspath(os.path.join(os.getcwd(), \"../../\"))\n",
    "DATA_PATH = os.path.join(ROOT_PATH, r'data/', data_name, data_name) + '.pkl'\n",
    "RESULTS_FOLDER_PATH = os.path.join(ROOT_PATH, r'results/', data_name)\n",
    "MODEL_RESULTS_PATH = os.path.join(ROOT_PATH, r'results/', data_name, model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Folder successfully created at: /home/rmunoz/Escritorio/rmunozTMELab/Physically-Guided-Machine-Learning/results/non_linear_100_1\n",
      "Folder successfully created at: /home/rmunoz/Escritorio/rmunozTMELab/Physically-Guided-Machine-Learning/results/non_linear_100_1/fourier_model_10\n"
     ]
    }
   ],
   "source": [
    "create_folder(RESULTS_FOLDER_PATH)\n",
    "create_folder(MODEL_RESULTS_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully loaded from: /home/rmunoz/Escritorio/rmunozTMELab/Physically-Guided-Machine-Learning/data/non_linear_100_1/non_linear_100_1.pkl\n"
     ]
    }
   ],
   "source": [
    "# Load dataset\n",
    "dataset = load_data(DATA_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convolutional filters to derivate\n",
    "dx = dataset['x_step_size']\n",
    "dy = dataset['y_step_size']\n",
    "D = DerivativeKernels(dx, dy, 0).grad_kernels_two_dimensions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "print(f\"Using device: {DEVICE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train data splitting in train/test\n",
    "X = torch.tensor(dataset['X_train'], dtype=torch.float32).unsqueeze(1)\n",
    "y = torch.tensor(dataset['y_train'], dtype=torch.float32).unsqueeze(1)\n",
    "K = torch.tensor(dataset['k_train'], dtype=torch.float32).unsqueeze(1)\n",
    "f = torch.tensor(dataset['f_train'], dtype=torch.float32).unsqueeze(1)\n",
    "\n",
    "X_train, X_test, y_train, y_test, K_train, K_test, f_train, f_test = train_test_split(X, y, K, f, test_size=0.3, random_state=42)\n",
    "\n",
    "# Data processing and adequacy with our TensOps library\n",
    "X_train = X_train.to(DEVICE)\n",
    "X_test = X_test.to(DEVICE)\n",
    "\n",
    "y_train = TensOps(y_train.to(DEVICE).requires_grad_(True), space_dimension=2, contravariance=0, covariance=0)\n",
    "y_test = TensOps(y_test.to(DEVICE).requires_grad_(True), space_dimension=2, contravariance=0, covariance=0)\n",
    "\n",
    "K_train = TensOps(K_train.to(DEVICE).requires_grad_(True), space_dimension=2, contravariance=0, covariance=0)\n",
    "K_test = TensOps(K_test.to(DEVICE).requires_grad_(True), space_dimension=2, contravariance=0, covariance=0)\n",
    "\n",
    "f_train = TensOps(f_train.to(DEVICE).requires_grad_(True), space_dimension=2, contravariance=0, covariance=0)\n",
    "f_test = TensOps(f_test.to(DEVICE).requires_grad_(True), space_dimension=2, contravariance=0, covariance=0)\n",
    "\n",
    "# Loading and processing validation data\n",
    "X_val = torch.tensor(dataset['X_val'], dtype=torch.float32).unsqueeze(1)\n",
    "y_val = TensOps(torch.tensor(dataset['y_val'], dtype=torch.float32, requires_grad=True).unsqueeze(1), space_dimension=2, contravariance=0, covariance=0)\n",
    "K_val = TensOps(torch.tensor(dataset['k_val'], dtype=torch.float32, requires_grad=True).unsqueeze(1), space_dimension=2, contravariance=0, covariance=0)\n",
    "f_val = TensOps(torch.tensor(dataset['f_val'], dtype=torch.float32, requires_grad=True).unsqueeze(1), space_dimension=2, contravariance=0, covariance=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_modes = 20 \n",
    "\n",
    "X_mesh = torch.tensor(dataset['X_mesh'])\n",
    "Y_mesh = torch.tensor(dataset['Y_mesh'])\n",
    "\n",
    "base = compute_fourier_base(num_modes, X_mesh, Y_mesh).to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predictive network architecture\n",
    "input_shape = X_train[0].shape\n",
    "predictive_layers = [20, 10, num_modes]\n",
    "predictive_output = y_train.values[0].shape\n",
    "\n",
    "# Explanatory network architecture\n",
    "explanatory_input = Mx(My(y_train)).values[0].shape\n",
    "explanatory_layers = [10, 10]\n",
    "explanatory_output = Mx(My(f_train)).values[0].shape\n",
    "\n",
    "# Other parameters \n",
    "n_filters_explanatory = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training from scratch.\n",
      "Epoch 0, Train loss: 1.591e+09, Test loss: 1.574e+09, MSE(e): 1.562e+02, MSE(pi1): 2.647e+03, MSE(pi2): 3.999e+01, MSE(pi3): 1.762e+01\n",
      "Epoch 10, Train loss: 4.066e+08, Test loss: 4.468e+08, MSE(e): 4.044e+01, MSE(pi1): 1.795e+02, MSE(pi2): 3.287e+01, MSE(pi3): 3.747e+00\n",
      "Epoch 20, Train loss: 1.313e+08, Test loss: 1.484e+08, MSE(e): 1.310e+01, MSE(pi1): 2.496e+01, MSE(pi2): 1.122e+01, MSE(pi3): 5.247e-01\n",
      "Epoch 30, Train loss: 9.892e+07, Test loss: 1.065e+08, MSE(e): 9.886e+00, MSE(pi1): 3.163e+00, MSE(pi2): 6.118e+00, MSE(pi3): 1.907e-01\n",
      "Epoch 40, Train loss: 6.683e+07, Test loss: 7.455e+07, MSE(e): 6.676e+00, MSE(pi1): 5.079e+00, MSE(pi2): 5.561e+00, MSE(pi3): 1.557e-01\n",
      "Epoch 50, Train loss: 5.489e+07, Test loss: 7.039e+07, MSE(e): 5.481e+00, MSE(pi1): 5.252e+00, MSE(pi2): 4.035e+00, MSE(pi3): 1.619e-01\n",
      "Epoch 60, Train loss: 4.865e+07, Test loss: 6.036e+07, MSE(e): 4.859e+00, MSE(pi1): 3.390e+00, MSE(pi2): 3.889e+00, MSE(pi3): 1.627e-01\n",
      "Epoch 70, Train loss: 4.352e+07, Test loss: 5.325e+07, MSE(e): 4.347e+00, MSE(pi1): 2.750e+00, MSE(pi2): 3.231e+00, MSE(pi3): 1.726e-01\n",
      "Epoch 80, Train loss: 4.035e+07, Test loss: 4.898e+07, MSE(e): 4.030e+00, MSE(pi1): 2.468e+00, MSE(pi2): 2.875e+00, MSE(pi3): 1.815e-01\n",
      "Epoch 90, Train loss: 3.743e+07, Test loss: 4.601e+07, MSE(e): 3.739e+00, MSE(pi1): 2.321e+00, MSE(pi2): 2.702e+00, MSE(pi3): 1.866e-01\n",
      "Epoch 100, Train loss: 3.472e+07, Test loss: 4.299e+07, MSE(e): 3.467e+00, MSE(pi1): 2.307e+00, MSE(pi2): 2.442e+00, MSE(pi3): 1.898e-01\n",
      "Epoch 110, Train loss: 3.208e+07, Test loss: 3.965e+07, MSE(e): 3.204e+00, MSE(pi1): 2.297e+00, MSE(pi2): 2.211e+00, MSE(pi3): 1.910e-01\n",
      "Epoch 120, Train loss: 2.958e+07, Test loss: 3.676e+07, MSE(e): 2.954e+00, MSE(pi1): 2.298e+00, MSE(pi2): 1.992e+00, MSE(pi3): 1.917e-01\n",
      "Epoch 130, Train loss: 2.719e+07, Test loss: 3.402e+07, MSE(e): 2.715e+00, MSE(pi1): 2.289e+00, MSE(pi2): 1.812e+00, MSE(pi3): 1.920e-01\n",
      "Epoch 140, Train loss: 2.490e+07, Test loss: 3.130e+07, MSE(e): 2.486e+00, MSE(pi1): 2.286e+00, MSE(pi2): 1.639e+00, MSE(pi3): 1.924e-01\n",
      "Epoch 150, Train loss: 2.271e+07, Test loss: 2.869e+07, MSE(e): 2.267e+00, MSE(pi1): 2.284e+00, MSE(pi2): 1.481e+00, MSE(pi3): 1.927e-01\n",
      "Epoch 160, Train loss: 2.060e+07, Test loss: 2.620e+07, MSE(e): 2.056e+00, MSE(pi1): 2.281e+00, MSE(pi2): 1.342e+00, MSE(pi3): 1.931e-01\n",
      "Epoch 170, Train loss: 1.859e+07, Test loss: 2.381e+07, MSE(e): 1.855e+00, MSE(pi1): 2.280e+00, MSE(pi2): 1.214e+00, MSE(pi3): 1.936e-01\n",
      "Epoch 180, Train loss: 1.670e+07, Test loss: 2.153e+07, MSE(e): 1.666e+00, MSE(pi1): 2.279e+00, MSE(pi2): 1.098e+00, MSE(pi3): 1.940e-01\n",
      "Epoch 190, Train loss: 1.496e+07, Test loss: 1.941e+07, MSE(e): 1.492e+00, MSE(pi1): 2.280e+00, MSE(pi2): 9.929e-01, MSE(pi3): 1.945e-01\n",
      "Epoch 200, Train loss: 1.340e+07, Test loss: 1.747e+07, MSE(e): 1.335e+00, MSE(pi1): 2.281e+00, MSE(pi2): 8.991e-01, MSE(pi3): 1.949e-01\n",
      "Epoch 210, Train loss: 1.204e+07, Test loss: 1.575e+07, MSE(e): 1.199e+00, MSE(pi1): 2.282e+00, MSE(pi2): 8.160e-01, MSE(pi3): 1.954e-01\n",
      "Epoch 220, Train loss: 1.088e+07, Test loss: 1.427e+07, MSE(e): 1.084e+00, MSE(pi1): 2.284e+00, MSE(pi2): 7.438e-01, MSE(pi3): 1.958e-01\n",
      "Epoch 230, Train loss: 9.933e+06, Test loss: 1.302e+07, MSE(e): 9.890e-01, MSE(pi1): 2.287e+00, MSE(pi2): 6.821e-01, MSE(pi3): 1.962e-01\n",
      "Epoch 240, Train loss: 9.159e+06, Test loss: 1.197e+07, MSE(e): 9.116e-01, MSE(pi1): 2.289e+00, MSE(pi2): 6.301e-01, MSE(pi3): 1.965e-01\n",
      "Epoch 250, Train loss: 8.532e+06, Test loss: 1.112e+07, MSE(e): 8.489e-01, MSE(pi1): 2.291e+00, MSE(pi2): 5.867e-01, MSE(pi3): 1.968e-01\n",
      "Epoch 260, Train loss: 8.022e+06, Test loss: 1.041e+07, MSE(e): 7.978e-01, MSE(pi1): 2.293e+00, MSE(pi2): 5.507e-01, MSE(pi3): 1.970e-01\n",
      "Epoch 270, Train loss: 7.601e+06, Test loss: 9.823e+06, MSE(e): 7.557e-01, MSE(pi1): 2.296e+00, MSE(pi2): 5.208e-01, MSE(pi3): 1.972e-01\n",
      "Epoch 280, Train loss: 7.248e+06, Test loss: 9.335e+06, MSE(e): 7.205e-01, MSE(pi1): 2.297e+00, MSE(pi2): 4.961e-01, MSE(pi3): 1.973e-01\n",
      "Epoch 290, Train loss: 6.949e+06, Test loss: 8.924e+06, MSE(e): 6.906e-01, MSE(pi1): 2.299e+00, MSE(pi2): 4.755e-01, MSE(pi3): 1.974e-01\n",
      "Epoch 300, Train loss: 6.691e+06, Test loss: 8.572e+06, MSE(e): 6.648e-01, MSE(pi1): 2.300e+00, MSE(pi2): 4.581e-01, MSE(pi3): 1.974e-01\n",
      "Epoch 310, Train loss: 6.466e+06, Test loss: 8.268e+06, MSE(e): 6.423e-01, MSE(pi1): 2.301e+00, MSE(pi2): 4.434e-01, MSE(pi3): 1.974e-01\n",
      "Epoch 320, Train loss: 6.267e+06, Test loss: 8.002e+06, MSE(e): 6.224e-01, MSE(pi1): 2.300e+00, MSE(pi2): 4.306e-01, MSE(pi3): 1.974e-01\n",
      "Epoch 330, Train loss: 6.088e+06, Test loss: 7.766e+06, MSE(e): 6.045e-01, MSE(pi1): 2.300e+00, MSE(pi2): 4.193e-01, MSE(pi3): 1.973e-01\n",
      "Epoch 340, Train loss: 5.925e+06, Test loss: 7.554e+06, MSE(e): 5.882e-01, MSE(pi1): 2.298e+00, MSE(pi2): 4.092e-01, MSE(pi3): 1.972e-01\n",
      "Epoch 350, Train loss: 5.774e+06, Test loss: 7.363e+06, MSE(e): 5.731e-01, MSE(pi1): 2.296e+00, MSE(pi2): 4.000e-01, MSE(pi3): 1.970e-01\n",
      "Epoch 360, Train loss: 5.634e+06, Test loss: 7.188e+06, MSE(e): 5.591e-01, MSE(pi1): 2.293e+00, MSE(pi2): 3.915e-01, MSE(pi3): 1.969e-01\n",
      "Epoch 370, Train loss: 5.503e+06, Test loss: 7.027e+06, MSE(e): 5.460e-01, MSE(pi1): 2.290e+00, MSE(pi2): 3.836e-01, MSE(pi3): 1.968e-01\n",
      "Epoch 380, Train loss: 5.380e+06, Test loss: 6.878e+06, MSE(e): 5.337e-01, MSE(pi1): 2.287e+00, MSE(pi2): 3.762e-01, MSE(pi3): 1.967e-01\n",
      "Epoch 390, Train loss: 5.264e+06, Test loss: 6.738e+06, MSE(e): 5.222e-01, MSE(pi1): 2.283e+00, MSE(pi2): 3.692e-01, MSE(pi3): 1.966e-01\n",
      "Epoch 400, Train loss: 5.155e+06, Test loss: 6.605e+06, MSE(e): 5.112e-01, MSE(pi1): 2.280e+00, MSE(pi2): 3.625e-01, MSE(pi3): 1.965e-01\n",
      "Epoch 410, Train loss: 5.052e+06, Test loss: 6.478e+06, MSE(e): 5.009e-01, MSE(pi1): 2.277e+00, MSE(pi2): 3.562e-01, MSE(pi3): 1.964e-01\n",
      "Epoch 420, Train loss: 4.954e+06, Test loss: 6.357e+06, MSE(e): 4.911e-01, MSE(pi1): 2.274e+00, MSE(pi2): 3.501e-01, MSE(pi3): 1.963e-01\n",
      "Epoch 430, Train loss: 4.860e+06, Test loss: 6.241e+06, MSE(e): 4.817e-01, MSE(pi1): 2.271e+00, MSE(pi2): 3.442e-01, MSE(pi3): 1.963e-01\n",
      "Epoch 440, Train loss: 4.770e+06, Test loss: 6.131e+06, MSE(e): 4.727e-01, MSE(pi1): 2.269e+00, MSE(pi2): 3.386e-01, MSE(pi3): 1.962e-01\n",
      "Epoch 450, Train loss: 4.684e+06, Test loss: 6.025e+06, MSE(e): 4.641e-01, MSE(pi1): 2.267e+00, MSE(pi2): 3.331e-01, MSE(pi3): 1.962e-01\n",
      "Epoch 460, Train loss: 4.600e+06, Test loss: 5.923e+06, MSE(e): 4.557e-01, MSE(pi1): 2.265e+00, MSE(pi2): 3.278e-01, MSE(pi3): 1.961e-01\n",
      "Epoch 470, Train loss: 4.519e+06, Test loss: 5.824e+06, MSE(e): 4.476e-01, MSE(pi1): 2.263e+00, MSE(pi2): 3.227e-01, MSE(pi3): 1.961e-01\n",
      "Epoch 480, Train loss: 4.440e+06, Test loss: 5.728e+06, MSE(e): 4.398e-01, MSE(pi1): 2.261e+00, MSE(pi2): 3.176e-01, MSE(pi3): 1.961e-01\n",
      "Epoch 490, Train loss: 4.363e+06, Test loss: 5.635e+06, MSE(e): 4.321e-01, MSE(pi1): 2.259e+00, MSE(pi2): 3.127e-01, MSE(pi3): 1.961e-01\n",
      "Epoch 500, Train loss: 4.288e+06, Test loss: 5.544e+06, MSE(e): 4.246e-01, MSE(pi1): 2.258e+00, MSE(pi2): 3.079e-01, MSE(pi3): 1.962e-01\n",
      "Epoch 510, Train loss: 4.214e+06, Test loss: 5.454e+06, MSE(e): 4.172e-01, MSE(pi1): 2.257e+00, MSE(pi2): 3.032e-01, MSE(pi3): 1.962e-01\n",
      "Epoch 520, Train loss: 4.142e+06, Test loss: 5.366e+06, MSE(e): 4.099e-01, MSE(pi1): 2.256e+00, MSE(pi2): 2.985e-01, MSE(pi3): 1.962e-01\n",
      "Epoch 530, Train loss: 4.070e+06, Test loss: 5.279e+06, MSE(e): 4.028e-01, MSE(pi1): 2.255e+00, MSE(pi2): 2.940e-01, MSE(pi3): 1.963e-01\n",
      "Epoch 540, Train loss: 4.000e+06, Test loss: 5.194e+06, MSE(e): 3.958e-01, MSE(pi1): 2.254e+00, MSE(pi2): 2.895e-01, MSE(pi3): 1.963e-01\n",
      "Epoch 550, Train loss: 3.931e+06, Test loss: 5.109e+06, MSE(e): 3.889e-01, MSE(pi1): 2.254e+00, MSE(pi2): 2.850e-01, MSE(pi3): 1.964e-01\n",
      "Epoch 560, Train loss: 3.863e+06, Test loss: 5.026e+06, MSE(e): 3.820e-01, MSE(pi1): 2.254e+00, MSE(pi2): 2.807e-01, MSE(pi3): 1.965e-01\n",
      "Epoch 570, Train loss: 3.795e+06, Test loss: 4.944e+06, MSE(e): 3.753e-01, MSE(pi1): 2.254e+00, MSE(pi2): 2.763e-01, MSE(pi3): 1.966e-01\n",
      "Epoch 580, Train loss: 3.729e+06, Test loss: 4.862e+06, MSE(e): 3.687e-01, MSE(pi1): 2.254e+00, MSE(pi2): 2.721e-01, MSE(pi3): 1.967e-01\n",
      "Epoch 590, Train loss: 3.664e+06, Test loss: 4.782e+06, MSE(e): 3.621e-01, MSE(pi1): 2.255e+00, MSE(pi2): 2.679e-01, MSE(pi3): 1.968e-01\n",
      "Epoch 600, Train loss: 3.599e+06, Test loss: 4.703e+06, MSE(e): 3.557e-01, MSE(pi1): 2.255e+00, MSE(pi2): 2.638e-01, MSE(pi3): 1.969e-01\n",
      "Epoch 610, Train loss: 3.536e+06, Test loss: 4.625e+06, MSE(e): 3.494e-01, MSE(pi1): 2.256e+00, MSE(pi2): 2.597e-01, MSE(pi3): 1.970e-01\n",
      "Epoch 620, Train loss: 3.474e+06, Test loss: 4.548e+06, MSE(e): 3.431e-01, MSE(pi1): 2.257e+00, MSE(pi2): 2.557e-01, MSE(pi3): 1.972e-01\n",
      "Epoch 630, Train loss: 3.413e+06, Test loss: 4.472e+06, MSE(e): 3.370e-01, MSE(pi1): 2.259e+00, MSE(pi2): 2.518e-01, MSE(pi3): 1.973e-01\n",
      "Epoch 640, Train loss: 3.353e+06, Test loss: 4.398e+06, MSE(e): 3.310e-01, MSE(pi1): 2.260e+00, MSE(pi2): 2.479e-01, MSE(pi3): 1.974e-01\n",
      "Epoch 650, Train loss: 3.294e+06, Test loss: 4.325e+06, MSE(e): 3.251e-01, MSE(pi1): 2.262e+00, MSE(pi2): 2.441e-01, MSE(pi3): 1.975e-01\n",
      "Epoch 660, Train loss: 3.236e+06, Test loss: 4.253e+06, MSE(e): 3.194e-01, MSE(pi1): 2.263e+00, MSE(pi2): 2.404e-01, MSE(pi3): 1.977e-01\n",
      "Epoch 670, Train loss: 3.180e+06, Test loss: 4.182e+06, MSE(e): 3.137e-01, MSE(pi1): 2.265e+00, MSE(pi2): 2.368e-01, MSE(pi3): 1.978e-01\n",
      "Epoch 680, Train loss: 3.125e+06, Test loss: 4.113e+06, MSE(e): 3.082e-01, MSE(pi1): 2.267e+00, MSE(pi2): 2.333e-01, MSE(pi3): 1.979e-01\n",
      "Epoch 690, Train loss: 3.072e+06, Test loss: 4.045e+06, MSE(e): 3.029e-01, MSE(pi1): 2.269e+00, MSE(pi2): 2.298e-01, MSE(pi3): 1.980e-01\n",
      "Epoch 700, Train loss: 3.019e+06, Test loss: 3.979e+06, MSE(e): 2.976e-01, MSE(pi1): 2.272e+00, MSE(pi2): 2.265e-01, MSE(pi3): 1.981e-01\n",
      "Epoch 710, Train loss: 2.968e+06, Test loss: 3.914e+06, MSE(e): 2.926e-01, MSE(pi1): 2.274e+00, MSE(pi2): 2.232e-01, MSE(pi3): 1.983e-01\n",
      "Epoch 720, Train loss: 2.919e+06, Test loss: 3.850e+06, MSE(e): 2.876e-01, MSE(pi1): 2.276e+00, MSE(pi2): 2.200e-01, MSE(pi3): 1.984e-01\n",
      "Epoch 730, Train loss: 2.871e+06, Test loss: 3.787e+06, MSE(e): 2.828e-01, MSE(pi1): 2.278e+00, MSE(pi2): 2.169e-01, MSE(pi3): 1.984e-01\n",
      "Epoch 740, Train loss: 2.824e+06, Test loss: 3.726e+06, MSE(e): 2.781e-01, MSE(pi1): 2.280e+00, MSE(pi2): 2.138e-01, MSE(pi3): 1.985e-01\n",
      "Epoch 750, Train loss: 2.778e+06, Test loss: 3.666e+06, MSE(e): 2.735e-01, MSE(pi1): 2.283e+00, MSE(pi2): 2.109e-01, MSE(pi3): 1.986e-01\n",
      "Epoch 760, Train loss: 2.734e+06, Test loss: 3.607e+06, MSE(e): 2.691e-01, MSE(pi1): 2.285e+00, MSE(pi2): 2.080e-01, MSE(pi3): 1.987e-01\n",
      "Epoch 770, Train loss: 2.691e+06, Test loss: 3.550e+06, MSE(e): 2.648e-01, MSE(pi1): 2.287e+00, MSE(pi2): 2.052e-01, MSE(pi3): 1.988e-01\n",
      "Epoch 780, Train loss: 2.649e+06, Test loss: 3.494e+06, MSE(e): 2.606e-01, MSE(pi1): 2.289e+00, MSE(pi2): 2.025e-01, MSE(pi3): 1.988e-01\n",
      "Epoch 790, Train loss: 2.609e+06, Test loss: 3.439e+06, MSE(e): 2.566e-01, MSE(pi1): 2.291e+00, MSE(pi2): 1.999e-01, MSE(pi3): 1.989e-01\n",
      "Epoch 800, Train loss: 2.569e+06, Test loss: 3.386e+06, MSE(e): 2.526e-01, MSE(pi1): 2.293e+00, MSE(pi2): 1.973e-01, MSE(pi3): 1.989e-01\n",
      "Epoch 810, Train loss: 2.531e+06, Test loss: 3.333e+06, MSE(e): 2.488e-01, MSE(pi1): 2.295e+00, MSE(pi2): 1.949e-01, MSE(pi3): 1.989e-01\n",
      "Epoch 820, Train loss: 2.494e+06, Test loss: 3.282e+06, MSE(e): 2.451e-01, MSE(pi1): 2.297e+00, MSE(pi2): 1.924e-01, MSE(pi3): 1.990e-01\n",
      "Epoch 830, Train loss: 2.458e+06, Test loss: 3.232e+06, MSE(e): 2.415e-01, MSE(pi1): 2.298e+00, MSE(pi2): 1.901e-01, MSE(pi3): 1.990e-01\n",
      "Epoch 840, Train loss: 2.423e+06, Test loss: 3.183e+06, MSE(e): 2.380e-01, MSE(pi1): 2.300e+00, MSE(pi2): 1.878e-01, MSE(pi3): 1.990e-01\n",
      "Epoch 850, Train loss: 2.389e+06, Test loss: 3.135e+06, MSE(e): 2.346e-01, MSE(pi1): 2.301e+00, MSE(pi2): 1.855e-01, MSE(pi3): 1.991e-01\n",
      "Epoch 860, Train loss: 2.356e+06, Test loss: 3.089e+06, MSE(e): 2.313e-01, MSE(pi1): 2.303e+00, MSE(pi2): 1.833e-01, MSE(pi3): 1.991e-01\n",
      "Epoch 870, Train loss: 2.324e+06, Test loss: 3.043e+06, MSE(e): 2.280e-01, MSE(pi1): 2.304e+00, MSE(pi2): 1.812e-01, MSE(pi3): 1.991e-01\n",
      "Epoch 880, Train loss: 2.292e+06, Test loss: 2.999e+06, MSE(e): 2.249e-01, MSE(pi1): 2.305e+00, MSE(pi2): 1.790e-01, MSE(pi3): 1.991e-01\n",
      "Epoch 890, Train loss: 2.261e+06, Test loss: 2.955e+06, MSE(e): 2.218e-01, MSE(pi1): 2.306e+00, MSE(pi2): 1.770e-01, MSE(pi3): 1.991e-01\n",
      "Epoch 900, Train loss: 2.231e+06, Test loss: 2.913e+06, MSE(e): 2.188e-01, MSE(pi1): 2.307e+00, MSE(pi2): 1.750e-01, MSE(pi3): 1.991e-01\n",
      "Epoch 910, Train loss: 2.202e+06, Test loss: 2.871e+06, MSE(e): 2.159e-01, MSE(pi1): 2.308e+00, MSE(pi2): 1.730e-01, MSE(pi3): 1.991e-01\n",
      "Epoch 920, Train loss: 2.174e+06, Test loss: 2.830e+06, MSE(e): 2.131e-01, MSE(pi1): 2.308e+00, MSE(pi2): 1.711e-01, MSE(pi3): 1.991e-01\n",
      "Epoch 930, Train loss: 2.146e+06, Test loss: 2.791e+06, MSE(e): 2.103e-01, MSE(pi1): 2.309e+00, MSE(pi2): 1.692e-01, MSE(pi3): 1.991e-01\n",
      "Epoch 940, Train loss: 2.119e+06, Test loss: 2.752e+06, MSE(e): 2.075e-01, MSE(pi1): 2.310e+00, MSE(pi2): 1.673e-01, MSE(pi3): 1.990e-01\n",
      "Epoch 950, Train loss: 2.092e+06, Test loss: 2.714e+06, MSE(e): 2.049e-01, MSE(pi1): 2.310e+00, MSE(pi2): 1.655e-01, MSE(pi3): 1.990e-01\n",
      "Epoch 960, Train loss: 2.066e+06, Test loss: 2.677e+06, MSE(e): 2.023e-01, MSE(pi1): 2.311e+00, MSE(pi2): 1.636e-01, MSE(pi3): 1.990e-01\n",
      "Epoch 970, Train loss: 2.040e+06, Test loss: 2.641e+06, MSE(e): 1.997e-01, MSE(pi1): 2.311e+00, MSE(pi2): 1.619e-01, MSE(pi3): 1.990e-01\n",
      "Epoch 980, Train loss: 2.015e+06, Test loss: 2.605e+06, MSE(e): 1.972e-01, MSE(pi1): 2.311e+00, MSE(pi2): 1.601e-01, MSE(pi3): 1.989e-01\n",
      "Epoch 990, Train loss: 1.991e+06, Test loss: 2.571e+06, MSE(e): 1.947e-01, MSE(pi1): 2.312e+00, MSE(pi2): 1.584e-01, MSE(pi3): 1.989e-01\n",
      "\n",
      "Training process finished after 1000 epochs\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load model and the optimizer\n",
    "model = PGNNIVFourier(input_shape, predictive_layers, base, predictive_output, explanatory_input, explanatory_layers, explanatory_output, n_filters_explanatory, DEVICE).to(DEVICE)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=3e-3)\n",
    "\n",
    "# Parametros de entrenamiento\n",
    "start_epoch = 0\n",
    "n_epochs = 1000\n",
    "\n",
    "batch_size = 64\n",
    "n_checkpoints = 5\n",
    "\n",
    "train_loop(model, optimizer, X_train, y_train, f_train, X_test, y_test, f_test,\n",
    "        D,  n_checkpoints, start_epoch=start_epoch, n_epochs=n_epochs, batch_size=batch_size, \n",
    "        model_results_path=MODEL_RESULTS_PATH, device=DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training from a checkpoint. Epoch 800.\n",
      "Epoch 800, Train loss: 3.937e+06, Test loss: 5.202e+06, MSE(e): 3.890e-01, MSE(pi1): 2.490e+00, MSE(pi2): 2.889e-01, MSE(pi3): 2.119e-01\n",
      "Epoch 900, Train loss: 3.868e+06, Test loss: 5.113e+06, MSE(e): 3.822e-01, MSE(pi1): 2.489e+00, MSE(pi2): 2.843e-01, MSE(pi3): 2.119e-01\n",
      "Epoch 1000, Train loss: 3.796e+06, Test loss: 5.020e+06, MSE(e): 3.749e-01, MSE(pi1): 2.489e+00, MSE(pi2): 2.795e-01, MSE(pi3): 2.119e-01\n",
      "Epoch 1100, Train loss: 3.720e+06, Test loss: 4.921e+06, MSE(e): 3.673e-01, MSE(pi1): 2.488e+00, MSE(pi2): 2.743e-01, MSE(pi3): 2.119e-01\n",
      "Epoch 1200, Train loss: 3.641e+06, Test loss: 4.819e+06, MSE(e): 3.594e-01, MSE(pi1): 2.487e+00, MSE(pi2): 2.690e-01, MSE(pi3): 2.119e-01\n",
      "Epoch 1300, Train loss: 3.559e+06, Test loss: 4.713e+06, MSE(e): 3.513e-01, MSE(pi1): 2.487e+00, MSE(pi2): 2.635e-01, MSE(pi3): 2.119e-01\n",
      "Epoch 1400, Train loss: 3.475e+06, Test loss: 4.603e+06, MSE(e): 3.429e-01, MSE(pi1): 2.487e+00, MSE(pi2): 2.577e-01, MSE(pi3): 2.119e-01\n",
      "Epoch 1500, Train loss: 3.389e+06, Test loss: 4.491e+06, MSE(e): 3.343e-01, MSE(pi1): 2.486e+00, MSE(pi2): 2.519e-01, MSE(pi3): 2.119e-01\n",
      "Epoch 1600, Train loss: 3.302e+06, Test loss: 4.376e+06, MSE(e): 3.256e-01, MSE(pi1): 2.486e+00, MSE(pi2): 2.460e-01, MSE(pi3): 2.119e-01\n",
      "Epoch 1700, Train loss: 3.214e+06, Test loss: 4.260e+06, MSE(e): 3.168e-01, MSE(pi1): 2.486e+00, MSE(pi2): 2.400e-01, MSE(pi3): 2.119e-01\n",
      "Epoch 1800, Train loss: 3.126e+06, Test loss: 4.143e+06, MSE(e): 3.079e-01, MSE(pi1): 2.486e+00, MSE(pi2): 2.339e-01, MSE(pi3): 2.119e-01\n",
      "Epoch 1900, Train loss: 3.038e+06, Test loss: 4.026e+06, MSE(e): 2.992e-01, MSE(pi1): 2.486e+00, MSE(pi2): 2.279e-01, MSE(pi3): 2.120e-01\n",
      "\n",
      "Training process finished after 2000 epochs\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Parametros de entrenamiento\n",
    "start_epoch = 800\n",
    "n_epochs = 2000\n",
    "\n",
    "batch_size = 64 \n",
    "n_checkpoints = 10\n",
    "\n",
    "second_lr = 3e-4\n",
    "\n",
    "train_loop(model, optimizer, X_train, y_train, f_train, X_test, y_test, f_test,\n",
    "        D,  n_checkpoints, start_epoch=start_epoch, n_epochs=n_epochs, batch_size=batch_size, \n",
    "        model_results_path=MODEL_RESULTS_PATH, device=DEVICE, new_lr=second_lr)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SciML_test_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
