{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import GPUtil\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Imports de la libreria propia\n",
    "from vecopsciml.kernels.derivative import DerivativeKernels\n",
    "from vecopsciml.utils import TensOps\n",
    "\n",
    "# Imports de las funciones creadas para este programa\n",
    "from utils.folders import create_folder\n",
    "from utils.load_data import load_data\n",
    "from trainers.train import train_loop\n",
    "\n",
    "from vecopsciml.operators.zero_order import Mx, My\n",
    "from utils.checkpoints import load_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Folder already exists at: /home/rmunoz/Escritorio/rmunozTMELab/Physically-Guided-Machine-Learning/transfer_learning/results/sigmoid_training_decoder\n",
      "Folder already exists at: /home/rmunoz/Escritorio/rmunozTMELab/Physically-Guided-Machine-Learning/transfer_learning/results/non_linear/baseline\n",
      "Folder already exists at: /home/rmunoz/Escritorio/rmunozTMELab/Physically-Guided-Machine-Learning/transfer_learning/results/non_linear/baseline\n"
     ]
    }
   ],
   "source": [
    "# Creamos los paths para las distintas carpetas\n",
    "ROOT_PATH = r'/home/rmunoz/Escritorio/rmunozTMELab/Physically-Guided-Machine-Learning'\n",
    "DATA_PATH = os.path.join(ROOT_PATH, r'data/sigmoid_nonlinear/sigmoid_nonlinear.pkl')\n",
    "RESULTS_FOLDER_PATH = os.path.join(ROOT_PATH, r'transfer_learning/results/sigmoid_training_decoder')\n",
    "\n",
    "MODEL_RESULTS_AE_PATH = os.path.join(ROOT_PATH, r'transfer_learning/results/non_linear/baseline')\n",
    "MODEL_RESULTS_PGNNIV_PATH = os.path.join(ROOT_PATH, r'transfer_learning/results/non_linear/baseline')\n",
    "MODEL_RESULTS_TRANSFERLEARNING_PATH = os.path.join(ROOT_PATH, r'transfer_learning/results/sigmoid_training_decoder')\n",
    "\n",
    "# Creamos las carpetas que sean necesarias (si ya están creadas se avisará de ello)\n",
    "create_folder(RESULTS_FOLDER_PATH)\n",
    "create_folder(MODEL_RESULTS_AE_PATH)\n",
    "create_folder(MODEL_RESULTS_PGNNIV_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully loaded from: /home/rmunoz/Escritorio/rmunozTMELab/Physically-Guided-Machine-Learning/data/sigmoid_nonlinear/sigmoid_nonlinear.pkl\n"
     ]
    }
   ],
   "source": [
    "# Load dataset\n",
    "dataset = load_data(DATA_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convolutional filters to derivate\n",
    "dx = dataset['x_step_size']\n",
    "dy = dataset['y_step_size']\n",
    "D = DerivativeKernels(dx, dy, 0).grad_kernels_two_dimensions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "print(f\"Using device: {DEVICE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### División de los datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset length: 80\n",
      "Validation dataset length: 20\n"
     ]
    }
   ],
   "source": [
    "X_train = torch.Tensor(dataset['X_train']).unsqueeze(1)\n",
    "y_train = torch.Tensor(dataset['y_train']).unsqueeze(1)\n",
    "K_train = torch.tensor(dataset['k_train']).unsqueeze(1)\n",
    "f_train = torch.tensor(dataset['f_train']).unsqueeze(1).to(torch.float32)\n",
    "\n",
    "X_val = torch.Tensor(dataset['X_val']).unsqueeze(1)\n",
    "y_val = TensOps(torch.Tensor(dataset['y_val']).unsqueeze(1).requires_grad_(True), space_dimension=2, contravariance=0, covariance=0)\n",
    "K_val = TensOps(torch.tensor(dataset['k_val']).unsqueeze(1).requires_grad_(True), space_dimension=2, contravariance=0, covariance=0)\n",
    "f_val = TensOps(torch.tensor(dataset['f_val']).to(torch.float32).unsqueeze(1).requires_grad_(True), space_dimension=2, contravariance=0, covariance=0)\n",
    "\n",
    "print(\"Train dataset length:\", len(X_train))\n",
    "print(\"Validation dataset length:\", len(X_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test, K_train, K_test, f_train, f_test = train_test_split(X_train, y_train, K_train, f_train, test_size=0.2, random_state=42)\n",
    "\n",
    "X_train = X_train.to(DEVICE)\n",
    "X_test = X_test.to(DEVICE)\n",
    "\n",
    "y_train = TensOps(y_train.requires_grad_(True).to(DEVICE), space_dimension=2, contravariance=0, covariance=0)\n",
    "y_test = TensOps(y_test.requires_grad_(True).to(DEVICE), space_dimension=2, contravariance=0, covariance=0)\n",
    "\n",
    "K_train = TensOps(K_train.to(DEVICE), space_dimension=2, contravariance=0, covariance=0)\n",
    "K_test = TensOps(K_test.to(DEVICE), space_dimension=2, contravariance=0, covariance=0)\n",
    "\n",
    "f_train = TensOps(f_train.to(DEVICE), space_dimension=2, contravariance=0, covariance=0)\n",
    "f_test = TensOps(f_test.to(DEVICE), space_dimension=2, contravariance=0, covariance=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model import BaselineNonlinearModel as PretrainedAutoencoder\n",
    "from model.ae_nonlinear_model import AutoencoderNonlinearModel as PretrainedPGNNNIV\n",
    "from model.transfer_learnign_ae import AutoencoderTransferLearning as TransferLearningAutoencoder\n",
    "from trainers.train import train_autoencoder_loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vecopsciml.operators.zero_order import Mx, My\n",
    "from model.ae_nonlinear_model import AutoencoderNonlinearModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Other parameters\n",
    "n_filters_explanatory = 5\n",
    "n_modes = 10\n",
    "\n",
    "# Predictive network architecture\n",
    "input_shape = X_train[0].shape\n",
    "predictive_layers = [20, 10, n_modes, 10, 20]\n",
    "predictive_output = y_train.values[0].shape\n",
    "\n",
    "# Explanatory network architecture\n",
    "explanatory_input = Mx(My(y_train)).values[0].shape\n",
    "explanatory_layers = [10]\n",
    "explanatory_output = Mx(My(f_train)).values[0].shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load autoencoder\n",
    "autoencoder_input_shape = y_train.values[0].shape\n",
    "latent_space_dim = [20, 10, n_modes, 10, 20]\n",
    "autoencoder_output_shape = y_train.values[0].shape\n",
    "\n",
    "# pretrained_autoencoder = PretrainedAutoencoder(autoencoder_input_shape, latent_space_dim, autoencoder_output_shape).to(DEVICE)\n",
    "pretrained_autoencoder = PretrainedAutoencoder(input_shape, predictive_layers, predictive_output, explanatory_input, explanatory_layers, explanatory_output, n_filters_explanatory).to(DEVICE)\n",
    "optimizer = torch.optim.Adam(pretrained_autoencoder.parameters(), lr=1e-4)\n",
    "pretrained_autoencoder, optimizer, lists = load_results(pretrained_autoencoder, optimizer, MODEL_RESULTS_AE_PATH, map_location=torch.device('cpu'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pretrained PGNNIV\n",
    "pretrained_encoder = pretrained_autoencoder.encoder\n",
    "pretrained_decoder = pretrained_autoencoder.decoder\n",
    "\n",
    "pretrained_pgnniv = AutoencoderNonlinearModel(input_shape, predictive_layers, pretrained_decoder, predictive_output, explanatory_input,\n",
    "                        explanatory_layers, explanatory_output, n_filters_explanatory).to(DEVICE)\n",
    "optimizer = torch.optim.Adam(pretrained_pgnniv.parameters(), lr=1e-4)\n",
    "pretrained_pgnniv, optimizer, lists = load_results(pretrained_pgnniv, optimizer, MODEL_RESULTS_PGNNIV_PATH, map_location=torch.device('cpu'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x7f4871026ff0>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.scatter(Mx(My(TensOps(pretrained_pgnniv(X_train)[0], space_dimension=2, contravariance=0, covariance=0))).values.cpu().detach().numpy().flatten(), \n",
    "            pretrained_pgnniv(X_train)[1].cpu().detach().numpy().flatten())\n",
    "\n",
    "plt.scatter(y_train.values.cpu().detach().numpy().flatten(), \n",
    "           K_train.values.cpu().detach().numpy().flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hidden1_layer.weight: requires_grad=True\n",
      "hidden1_layer.bias: requires_grad=True\n",
      "hidden2_layer.weight: requires_grad=True\n",
      "hidden2_layer.bias: requires_grad=True\n",
      "latent_space_layer.weight: requires_grad=True\n",
      "latent_space_layer.bias: requires_grad=True\n"
     ]
    }
   ],
   "source": [
    "pgnniv_pretrained_encoder = pretrained_pgnniv.encoder\n",
    "\n",
    "# for param in pgnniv_pretrained_encoder.parameters():\n",
    "#     param.requires_grad = False\n",
    "\n",
    "for name, param in pgnniv_pretrained_encoder.named_parameters():\n",
    "    print(f\"{name}: requires_grad={param.requires_grad}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training from scratch.\n",
      "Epoch 0, Train loss: 1.235e+12, Test loss: 1.539e+12, MSE(e): 1.141e+02, MSE(pi1): 9.251e+03, MSE(pi2): 4.467e+01, MSE(pi3): 1.114e+01\n",
      "Epoch 100, Train loss: 5.473e+10, Test loss: 5.651e+10, MSE(e): 5.437e+00, MSE(pi1): 1.834e+01, MSE(pi2): 2.644e+00, MSE(pi3): 1.741e+00\n",
      "Epoch 200, Train loss: 2.699e+10, Test loss: 3.249e+10, MSE(e): 2.664e+00, MSE(pi1): 1.823e+01, MSE(pi2): 1.521e+00, MSE(pi3): 1.710e+00\n",
      "Epoch 300, Train loss: 2.044e+10, Test loss: 2.708e+10, MSE(e): 2.009e+00, MSE(pi1): 1.825e+01, MSE(pi2): 1.253e+00, MSE(pi3): 1.684e+00\n",
      "Epoch 400, Train loss: 1.798e+10, Test loss: 2.479e+10, MSE(e): 1.763e+00, MSE(pi1): 1.829e+01, MSE(pi2): 1.119e+00, MSE(pi3): 1.662e+00\n",
      "Epoch 500, Train loss: 1.508e+10, Test loss: 2.018e+10, MSE(e): 1.473e+00, MSE(pi1): 1.838e+01, MSE(pi2): 9.278e-01, MSE(pi3): 1.645e+00\n",
      "Epoch 600, Train loss: 1.161e+10, Test loss: 1.573e+10, MSE(e): 1.127e+00, MSE(pi1): 1.840e+01, MSE(pi2): 7.297e-01, MSE(pi3): 1.637e+00\n",
      "Epoch 700, Train loss: 9.659e+09, Test loss: 1.369e+10, MSE(e): 9.314e-01, MSE(pi1): 1.833e+01, MSE(pi2): 6.142e-01, MSE(pi3): 1.617e+00\n",
      "Epoch 800, Train loss: 8.569e+09, Test loss: 1.277e+10, MSE(e): 8.228e-01, MSE(pi1): 1.829e+01, MSE(pi2): 5.448e-01, MSE(pi3): 1.578e+00\n",
      "Epoch 900, Train loss: 7.892e+09, Test loss: 1.114e+10, MSE(e): 7.558e-01, MSE(pi1): 1.818e+01, MSE(pi2): 5.011e-01, MSE(pi3): 1.521e+00\n",
      "Epoch 1000, Train loss: 7.129e+09, Test loss: 1.002e+10, MSE(e): 6.804e-01, MSE(pi1): 1.802e+01, MSE(pi2): 4.546e-01, MSE(pi3): 1.449e+00\n",
      "Epoch 1100, Train loss: 5.947e+09, Test loss: 8.220e+09, MSE(e): 5.630e-01, MSE(pi1): 1.801e+01, MSE(pi2): 3.751e-01, MSE(pi3): 1.367e+00\n",
      "Epoch 1200, Train loss: 3.420e+09, Test loss: 5.809e+09, MSE(e): 3.086e-01, MSE(pi1): 1.961e+01, MSE(pi2): 1.973e-01, MSE(pi3): 1.371e+00\n",
      "Epoch 1300, Train loss: 1.736e+09, Test loss: 3.889e+09, MSE(e): 1.413e-01, MSE(pi1): 1.813e+01, MSE(pi2): 8.559e-02, MSE(pi3): 1.422e+00\n",
      "Epoch 1400, Train loss: 1.030e+09, Test loss: 2.958e+09, MSE(e): 7.220e-02, MSE(pi1): 1.741e+01, MSE(pi2): 4.370e-02, MSE(pi3): 1.337e+00\n",
      "Epoch 1500, Train loss: 7.462e+08, Test loss: 2.440e+09, MSE(e): 4.617e-02, MSE(pi1): 1.657e+01, MSE(pi2): 2.856e-02, MSE(pi3): 1.188e+00\n",
      "Epoch 1600, Train loss: 5.991e+08, Test loss: 2.032e+09, MSE(e): 3.401e-02, MSE(pi1): 1.557e+01, MSE(pi2): 2.130e-02, MSE(pi3): 1.034e+00\n",
      "Epoch 1700, Train loss: 5.109e+08, Test loss: 1.683e+09, MSE(e): 2.727e-02, MSE(pi1): 1.469e+01, MSE(pi2): 1.744e-02, MSE(pi3): 9.129e-01\n",
      "Epoch 1800, Train loss: 4.445e+08, Test loss: 1.500e+09, MSE(e): 2.215e-02, MSE(pi1): 1.400e+01, MSE(pi2): 1.380e-02, MSE(pi3): 8.307e-01\n",
      "Epoch 1900, Train loss: 6.002e+08, Test loss: 1.337e+09, MSE(e): 3.890e-02, MSE(pi1): 1.339e+01, MSE(pi2): 2.324e-02, MSE(pi3): 7.734e-01\n",
      "Epoch 2000, Train loss: 3.475e+08, Test loss: 1.196e+09, MSE(e): 1.476e-02, MSE(pi1): 1.278e+01, MSE(pi2): 9.041e-03, MSE(pi3): 7.213e-01\n",
      "Epoch 2100, Train loss: 3.109e+08, Test loss: 1.107e+09, MSE(e): 1.227e-02, MSE(pi1): 1.210e+01, MSE(pi2): 7.475e-03, MSE(pi3): 6.722e-01\n",
      "Epoch 2200, Train loss: 2.806e+08, Test loss: 1.050e+09, MSE(e): 1.059e-02, MSE(pi1): 1.130e+01, MSE(pi2): 6.396e-03, MSE(pi3): 6.172e-01\n",
      "Epoch 2300, Train loss: 2.511e+08, Test loss: 9.978e+08, MSE(e): 9.277e-03, MSE(pi1): 1.031e+01, MSE(pi2): 5.602e-03, MSE(pi3): 5.525e-01\n",
      "Epoch 2400, Train loss: 2.235e+08, Test loss: 9.741e+08, MSE(e): 8.449e-03, MSE(pi1): 9.106e+00, MSE(pi2): 5.265e-03, MSE(pi3): 4.796e-01\n",
      "Epoch 2500, Train loss: 1.908e+08, Test loss: 8.902e+08, MSE(e): 7.404e-03, MSE(pi1): 7.672e+00, MSE(pi2): 4.451e-03, MSE(pi3): 4.006e-01\n",
      "Epoch 2600, Train loss: 1.603e+08, Test loss: 8.294e+08, MSE(e): 6.634e-03, MSE(pi1): 6.137e+00, MSE(pi2): 3.985e-03, MSE(pi3): 3.255e-01\n",
      "Epoch 2700, Train loss: 1.338e+08, Test loss: 7.901e+08, MSE(e): 6.163e-03, MSE(pi1): 4.624e+00, MSE(pi2): 3.494e-03, MSE(pi3): 2.595e-01\n",
      "Epoch 2800, Train loss: 1.078e+08, Test loss: 7.123e+08, MSE(e): 5.375e-03, MSE(pi1): 3.352e+00, MSE(pi2): 3.217e-03, MSE(pi3): 2.058e-01\n",
      "Epoch 2900, Train loss: 8.858e+07, Test loss: 6.618e+08, MSE(e): 4.832e-03, MSE(pi1): 2.392e+00, MSE(pi2): 2.879e-03, MSE(pi3): 1.634e-01\n",
      "Epoch 3000, Train loss: 7.544e+07, Test loss: 6.017e+08, MSE(e): 4.471e-03, MSE(pi1): 1.767e+00, MSE(pi2): 2.716e-03, MSE(pi3): 1.306e-01\n",
      "Epoch 3100, Train loss: 6.415e+07, Test loss: 5.867e+08, MSE(e): 3.993e-03, MSE(pi1): 1.367e+00, MSE(pi2): 2.354e-03, MSE(pi3): 1.055e-01\n",
      "Epoch 3200, Train loss: 5.641e+07, Test loss: 5.603e+08, MSE(e): 3.648e-03, MSE(pi1): 1.131e+00, MSE(pi2): 2.139e-03, MSE(pi3): 8.614e-02\n",
      "Epoch 3300, Train loss: 5.106e+07, Test loss: 5.183e+08, MSE(e): 3.409e-03, MSE(pi1): 9.844e-01, MSE(pi2): 2.016e-03, MSE(pi3): 7.119e-02\n",
      "Epoch 3400, Train loss: 4.604e+07, Test loss: 5.234e+08, MSE(e): 3.133e-03, MSE(pi1): 8.742e-01, MSE(pi2): 1.823e-03, MSE(pi3): 5.971e-02\n",
      "Epoch 3500, Train loss: 4.205e+07, Test loss: 5.108e+08, MSE(e): 2.909e-03, MSE(pi1): 7.890e-01, MSE(pi2): 1.688e-03, MSE(pi3): 5.075e-02\n",
      "Epoch 3600, Train loss: 3.859e+07, Test loss: 4.981e+08, MSE(e): 2.707e-03, MSE(pi1): 7.143e-01, MSE(pi2): 1.569e-03, MSE(pi3): 4.370e-02\n",
      "Epoch 3700, Train loss: 3.580e+07, Test loss: 4.841e+08, MSE(e): 2.552e-03, MSE(pi1): 6.479e-01, MSE(pi2): 1.486e-03, MSE(pi3): 3.802e-02\n",
      "Epoch 3800, Train loss: 3.301e+07, Test loss: 4.792e+08, MSE(e): 2.383e-03, MSE(pi1): 5.839e-01, MSE(pi2): 1.375e-03, MSE(pi3): 3.344e-02\n",
      "Epoch 3900, Train loss: 4.553e+07, Test loss: 4.771e+08, MSE(e): 3.692e-03, MSE(pi1): 5.644e-01, MSE(pi2): 2.005e-03, MSE(pi3): 2.967e-02\n",
      "Epoch 4000, Train loss: 2.855e+07, Test loss: 4.621e+08, MSE(e): 2.114e-03, MSE(pi1): 4.769e-01, MSE(pi2): 1.216e-03, MSE(pi3): 2.641e-02\n",
      "Epoch 4100, Train loss: 2.656e+07, Test loss: 4.560e+08, MSE(e): 1.988e-03, MSE(pi1): 4.320e-01, MSE(pi2): 1.140e-03, MSE(pi3): 2.364e-02\n",
      "Epoch 4200, Train loss: 6.619e+07, Test loss: 4.142e+08, MSE(e): 5.745e-03, MSE(pi1): 6.725e-01, MSE(pi2): 2.821e-03, MSE(pi3): 2.016e-02\n",
      "Epoch 4300, Train loss: 2.331e+07, Test loss: 4.425e+08, MSE(e): 1.780e-03, MSE(pi1): 3.581e-01, MSE(pi2): 1.018e-03, MSE(pi3): 1.927e-02\n",
      "Epoch 4400, Train loss: 2.181e+07, Test loss: 4.382e+08, MSE(e): 1.679e-03, MSE(pi1): 3.258e-01, MSE(pi2): 9.555e-04, MSE(pi3): 1.759e-02\n",
      "Epoch 4500, Train loss: 2.696e+07, Test loss: 4.935e+08, MSE(e): 2.203e-03, MSE(pi1): 3.189e-01, MSE(pi2): 1.106e-03, MSE(pi3): 1.735e-02\n",
      "Epoch 4600, Train loss: 4.597e+07, Test loss: 3.879e+08, MSE(e): 3.983e-03, MSE(pi1): 4.626e-01, MSE(pi2): 1.947e-03, MSE(pi3): 1.520e-02\n",
      "Epoch 4700, Train loss: 7.758e+07, Test loss: 5.574e+08, MSE(e): 6.828e-03, MSE(pi1): 7.870e-01, MSE(pi2): 2.871e-03, MSE(pi3): 1.427e-02\n",
      "Epoch 4800, Train loss: 2.374e+07, Test loss: 4.077e+08, MSE(e): 1.951e-03, MSE(pi1): 2.865e-01, MSE(pi2): 1.058e-03, MSE(pi3): 1.366e-02\n",
      "Epoch 4900, Train loss: 1.701e+07, Test loss: 3.981e+08, MSE(e): 1.287e-03, MSE(pi1): 2.930e-01, MSE(pi2): 7.263e-04, MSE(pi3): 1.216e-02\n",
      "Epoch 5000, Train loss: 1.570e+07, Test loss: 4.023e+08, MSE(e): 1.241e-03, MSE(pi1): 2.133e-01, MSE(pi2): 6.946e-04, MSE(pi3): 1.163e-02\n",
      "Epoch 5100, Train loss: 1.688e+07, Test loss: 3.936e+08, MSE(e): 1.288e-03, MSE(pi1): 3.097e-01, MSE(pi2): 7.304e-04, MSE(pi3): 9.002e-03\n",
      "Epoch 5200, Train loss: 1.464e+07, Test loss: 3.975e+08, MSE(e): 1.104e-03, MSE(pi1): 2.472e-01, MSE(pi2): 6.055e-04, MSE(pi3): 1.130e-02\n",
      "Epoch 5300, Train loss: 1.426e+07, Test loss: 3.861e+08, MSE(e): 1.105e-03, MSE(pi1): 2.319e-01, MSE(pi2): 6.173e-04, MSE(pi3): 8.990e-03\n",
      "Epoch 5400, Train loss: 1.536e+07, Test loss: 3.637e+08, MSE(e): 1.200e-03, MSE(pi1): 2.423e-01, MSE(pi2): 6.394e-04, MSE(pi3): 9.293e-03\n",
      "Epoch 5500, Train loss: 1.977e+07, Test loss: 3.561e+08, MSE(e): 1.697e-03, MSE(pi1): 1.714e-01, MSE(pi2): 8.799e-04, MSE(pi3): 1.080e-02\n",
      "Epoch 5600, Train loss: 1.514e+07, Test loss: 3.799e+08, MSE(e): 9.783e-04, MSE(pi1): 4.563e-01, MSE(pi2): 5.210e-04, MSE(pi3): 7.901e-03\n",
      "Epoch 5700, Train loss: 3.270e+07, Test loss: 4.396e+08, MSE(e): 2.822e-03, MSE(pi1): 3.245e-01, MSE(pi2): 1.260e-03, MSE(pi3): 1.239e-02\n",
      "Epoch 5800, Train loss: 1.460e+07, Test loss: 4.036e+08, MSE(e): 9.000e-04, MSE(pi1): 4.891e-01, MSE(pi2): 4.724e-04, MSE(pi3): 7.099e-03\n",
      "Epoch 5900, Train loss: 6.340e+07, Test loss: 3.412e+08, MSE(e): 5.674e-03, MSE(pi1): 5.988e-01, MSE(pi2): 2.588e-03, MSE(pi3): 6.717e-03\n",
      "Epoch 6000, Train loss: 1.313e+07, Test loss: 3.736e+08, MSE(e): 8.430e-04, MSE(pi1): 4.069e-01, MSE(pi2): 4.715e-04, MSE(pi3): 6.336e-03\n",
      "Epoch 6100, Train loss: 5.094e+07, Test loss: 4.181e+08, MSE(e): 1.319e-03, MSE(pi1): 3.562e+00, MSE(pi2): 6.269e-04, MSE(pi3): 2.140e-02\n",
      "Epoch 6200, Train loss: 9.113e+06, Test loss: 3.629e+08, MSE(e): 7.272e-04, MSE(pi1): 1.087e-01, MSE(pi2): 3.926e-04, MSE(pi3): 7.534e-03\n",
      "Epoch 6300, Train loss: 8.846e+06, Test loss: 3.638e+08, MSE(e): 7.090e-04, MSE(pi1): 9.995e-02, MSE(pi2): 3.804e-04, MSE(pi3): 7.565e-03\n",
      "Epoch 6400, Train loss: 1.633e+07, Test loss: 4.126e+08, MSE(e): 1.389e-03, MSE(pi1): 1.418e-01, MSE(pi2): 6.584e-04, MSE(pi3): 1.023e-02\n",
      "Epoch 6500, Train loss: 1.602e+07, Test loss: 4.101e+08, MSE(e): 1.418e-03, MSE(pi1): 1.052e-01, MSE(pi2): 6.585e-04, MSE(pi3): 7.868e-03\n",
      "Epoch 6600, Train loss: 3.477e+07, Test loss: 3.113e+08, MSE(e): 3.256e-03, MSE(pi1): 1.624e-01, MSE(pi2): 1.508e-03, MSE(pi3): 5.901e-03\n",
      "Epoch 6700, Train loss: 2.220e+07, Test loss: 3.282e+08, MSE(e): 1.430e-03, MSE(pi1): 7.157e-01, MSE(pi2): 6.851e-04, MSE(pi3): 7.423e-03\n",
      "Epoch 6800, Train loss: 1.257e+07, Test loss: 3.674e+08, MSE(e): 7.102e-04, MSE(pi1): 4.418e-01, MSE(pi2): 3.540e-04, MSE(pi3): 1.048e-02\n",
      "Epoch 6900, Train loss: 7.905e+06, Test loss: 3.438e+08, MSE(e): 6.098e-04, MSE(pi1): 1.101e-01, MSE(pi2): 3.235e-04, MSE(pi3): 7.059e-03\n",
      "Epoch 7000, Train loss: 9.413e+06, Test loss: 3.443e+08, MSE(e): 6.093e-04, MSE(pi1): 2.765e-01, MSE(pi2): 3.332e-04, MSE(pi3): 5.559e-03\n",
      "Epoch 7100, Train loss: 2.328e+07, Test loss: 4.107e+08, MSE(e): 7.535e-04, MSE(pi1): 1.503e+00, MSE(pi2): 3.424e-04, MSE(pi3): 7.126e-03\n",
      "Epoch 7200, Train loss: 7.252e+06, Test loss: 3.536e+08, MSE(e): 5.769e-04, MSE(pi1): 7.935e-02, MSE(pi2): 3.021e-04, MSE(pi3): 6.893e-03\n",
      "Epoch 7300, Train loss: 7.617e+06, Test loss: 3.515e+08, MSE(e): 5.374e-04, MSE(pi1): 1.131e-01, MSE(pi2): 2.869e-04, MSE(pi3): 1.112e-02\n",
      "Epoch 7400, Train loss: 8.710e+06, Test loss: 3.568e+08, MSE(e): 6.044e-04, MSE(pi1): 1.851e-01, MSE(pi2): 3.032e-04, MSE(pi3): 8.155e-03\n",
      "Epoch 7500, Train loss: 1.191e+08, Test loss: 5.833e+08, MSE(e): 5.782e-03, MSE(pi1): 5.789e+00, MSE(pi2): 2.806e-03, MSE(pi3): 3.381e-02\n",
      "Epoch 7600, Train loss: 8.337e+06, Test loss: 3.449e+08, MSE(e): 5.092e-04, MSE(pi1): 2.474e-01, MSE(pi2): 2.641e-04, MSE(pi3): 7.715e-03\n",
      "Epoch 7700, Train loss: 2.122e+07, Test loss: 3.709e+08, MSE(e): 6.238e-04, MSE(pi1): 1.415e+00, MSE(pi2): 2.895e-04, MSE(pi3): 8.346e-03\n",
      "Epoch 7800, Train loss: 7.538e+06, Test loss: 3.421e+08, MSE(e): 4.869e-04, MSE(pi1): 1.835e-01, MSE(pi2): 2.582e-04, MSE(pi3): 8.338e-03\n",
      "Epoch 7900, Train loss: 1.663e+07, Test loss: 3.635e+08, MSE(e): 5.954e-04, MSE(pi1): 9.442e-01, MSE(pi2): 3.066e-04, MSE(pi3): 1.231e-02\n",
      "Epoch 8000, Train loss: 1.882e+07, Test loss: 3.183e+08, MSE(e): 1.413e-03, MSE(pi1): 3.286e-01, MSE(pi2): 6.765e-04, MSE(pi3): 1.398e-02\n",
      "Epoch 8100, Train loss: 1.117e+07, Test loss: 3.251e+08, MSE(e): 6.267e-04, MSE(pi1): 4.256e-01, MSE(pi2): 3.183e-04, MSE(pi3): 6.510e-03\n",
      "Epoch 8200, Train loss: 9.323e+06, Test loss: 3.274e+08, MSE(e): 5.590e-04, MSE(pi1): 3.061e-01, MSE(pi2): 2.897e-04, MSE(pi3): 6.718e-03\n",
      "Epoch 8300, Train loss: 1.853e+07, Test loss: 3.625e+08, MSE(e): 5.763e-04, MSE(pi1): 1.136e+00, MSE(pi2): 2.901e-04, MSE(pi3): 1.407e-02\n",
      "Epoch 8400, Train loss: 6.648e+06, Test loss: 3.451e+08, MSE(e): 4.242e-04, MSE(pi1): 1.522e-01, MSE(pi2): 2.181e-04, MSE(pi3): 8.833e-03\n",
      "Epoch 8500, Train loss: 2.725e+07, Test loss: 3.593e+08, MSE(e): 7.857e-04, MSE(pi1): 1.879e+00, MSE(pi2): 4.357e-04, MSE(pi3): 6.021e-03\n",
      "Epoch 8600, Train loss: 6.486e+06, Test loss: 3.396e+08, MSE(e): 4.515e-04, MSE(pi1): 1.150e-01, MSE(pi2): 2.487e-04, MSE(pi3): 8.212e-03\n",
      "Epoch 8700, Train loss: 1.017e+07, Test loss: 3.477e+08, MSE(e): 4.575e-04, MSE(pi1): 5.019e-01, MSE(pi2): 2.364e-04, MSE(pi3): 5.708e-03\n",
      "Epoch 8800, Train loss: 7.830e+06, Test loss: 3.580e+08, MSE(e): 5.014e-04, MSE(pi1): 1.622e-01, MSE(pi2): 2.618e-04, MSE(pi3): 1.194e-02\n",
      "Epoch 8900, Train loss: 5.174e+06, Test loss: 3.408e+08, MSE(e): 3.682e-04, MSE(pi1): 9.279e-02, MSE(pi2): 1.947e-04, MSE(pi3): 5.634e-03\n",
      "Epoch 9000, Train loss: 7.659e+06, Test loss: 3.470e+08, MSE(e): 3.831e-04, MSE(pi1): 2.865e-01, MSE(pi2): 1.968e-04, MSE(pi3): 9.626e-03\n",
      "Epoch 9100, Train loss: 5.864e+06, Test loss: 3.658e+08, MSE(e): 4.129e-04, MSE(pi1): 1.004e-01, MSE(pi2): 2.085e-04, MSE(pi3): 7.312e-03\n",
      "Epoch 9200, Train loss: 1.216e+07, Test loss: 3.437e+08, MSE(e): 5.111e-04, MSE(pi1): 6.483e-01, MSE(pi2): 3.033e-04, MSE(pi3): 5.614e-03\n",
      "Epoch 9300, Train loss: 1.189e+07, Test loss: 4.034e+08, MSE(e): 7.107e-04, MSE(pi1): 3.953e-01, MSE(pi2): 3.280e-04, MSE(pi3): 8.252e-03\n",
      "Epoch 9400, Train loss: 1.240e+07, Test loss: 3.700e+08, MSE(e): 4.613e-04, MSE(pi1): 6.728e-01, MSE(pi2): 2.338e-04, MSE(pi3): 1.064e-02\n",
      "Epoch 9500, Train loss: 1.478e+07, Test loss: 3.171e+08, MSE(e): 1.222e-03, MSE(pi1): 1.846e-01, MSE(pi2): 6.035e-04, MSE(pi3): 7.185e-03\n",
      "Epoch 9600, Train loss: 7.695e+06, Test loss: 3.733e+08, MSE(e): 6.353e-04, MSE(pi1): 5.497e-02, MSE(pi2): 2.968e-04, MSE(pi3): 7.923e-03\n",
      "Epoch 9700, Train loss: 6.588e+06, Test loss: 3.695e+08, MSE(e): 4.797e-04, MSE(pi1): 4.400e-02, MSE(pi2): 2.400e-04, MSE(pi3): 1.351e-02\n",
      "Epoch 9800, Train loss: 1.453e+07, Test loss: 3.664e+08, MSE(e): 4.383e-04, MSE(pi1): 9.057e-01, MSE(pi2): 2.260e-04, MSE(pi3): 1.093e-02\n",
      "Epoch 9900, Train loss: 1.816e+07, Test loss: 3.415e+08, MSE(e): 5.742e-04, MSE(pi1): 1.067e+00, MSE(pi2): 2.781e-04, MSE(pi3): 1.754e-02\n",
      "Epoch 10000, Train loss: 3.705e+07, Test loss: 3.058e+08, MSE(e): 3.456e-03, MSE(pi1): 1.676e-01, MSE(pi2): 1.589e-03, MSE(pi3): 8.134e-03\n",
      "Epoch 10100, Train loss: 8.546e+06, Test loss: 3.374e+08, MSE(e): 7.159e-04, MSE(pi1): 7.355e-02, MSE(pi2): 3.564e-04, MSE(pi3): 6.517e-03\n",
      "Epoch 10200, Train loss: 1.248e+07, Test loss: 4.055e+08, MSE(e): 1.055e-03, MSE(pi1): 1.127e-01, MSE(pi2): 4.705e-04, MSE(pi3): 8.027e-03\n",
      "Epoch 10300, Train loss: 5.809e+06, Test loss: 3.451e+08, MSE(e): 3.281e-04, MSE(pi1): 1.840e-01, MSE(pi2): 1.682e-04, MSE(pi3): 6.883e-03\n",
      "Epoch 10400, Train loss: 1.639e+07, Test loss: 4.044e+08, MSE(e): 1.456e-03, MSE(pi1): 1.052e-01, MSE(pi2): 6.652e-04, MSE(pi3): 7.715e-03\n",
      "Epoch 10500, Train loss: 6.359e+06, Test loss: 3.351e+08, MSE(e): 4.817e-04, MSE(pi1): 9.392e-02, MSE(pi2): 2.420e-04, MSE(pi3): 6.028e-03\n",
      "Epoch 10600, Train loss: 9.500e+06, Test loss: 3.638e+08, MSE(e): 3.208e-04, MSE(pi1): 5.108e-01, MSE(pi2): 1.584e-04, MSE(pi3): 1.184e-02\n",
      "Epoch 10700, Train loss: 1.752e+07, Test loss: 3.669e+08, MSE(e): 4.483e-04, MSE(pi1): 1.215e+00, MSE(pi2): 1.957e-04, MSE(pi3): 8.951e-03\n",
      "Epoch 10800, Train loss: 9.304e+06, Test loss: 3.668e+08, MSE(e): 4.316e-04, MSE(pi1): 4.060e-01, MSE(pi2): 2.294e-04, MSE(pi3): 9.280e-03\n",
      "Epoch 10900, Train loss: 9.781e+06, Test loss: 3.547e+08, MSE(e): 5.611e-04, MSE(pi1): 2.764e-01, MSE(pi2): 3.469e-04, MSE(pi3): 1.405e-02\n",
      "Epoch 11000, Train loss: 3.589e+06, Test loss: 3.600e+08, MSE(e): 2.585e-04, MSE(pi1): 3.417e-02, MSE(pi2): 1.350e-04, MSE(pi3): 6.620e-03\n",
      "Epoch 11100, Train loss: 1.183e+07, Test loss: 4.071e+08, MSE(e): 1.056e-03, MSE(pi1): 4.606e-02, MSE(pi2): 4.723e-04, MSE(pi3): 8.113e-03\n",
      "Epoch 11200, Train loss: 4.278e+06, Test loss: 3.795e+08, MSE(e): 2.959e-04, MSE(pi1): 5.458e-02, MSE(pi2): 1.489e-04, MSE(pi3): 7.724e-03\n",
      "Epoch 11300, Train loss: 4.695e+06, Test loss: 3.562e+08, MSE(e): 3.113e-04, MSE(pi1): 7.285e-02, MSE(pi2): 1.671e-04, MSE(pi3): 8.534e-03\n",
      "Epoch 11400, Train loss: 8.747e+06, Test loss: 3.866e+08, MSE(e): 4.426e-04, MSE(pi1): 3.663e-01, MSE(pi2): 2.143e-04, MSE(pi3): 6.582e-03\n",
      "Epoch 11500, Train loss: 7.660e+06, Test loss: 3.634e+08, MSE(e): 5.160e-04, MSE(pi1): 1.348e-01, MSE(pi2): 3.142e-04, MSE(pi3): 1.152e-02\n",
      "Epoch 11600, Train loss: 1.231e+07, Test loss: 3.624e+08, MSE(e): 9.548e-04, MSE(pi1): 1.966e-01, MSE(pi2): 5.247e-04, MSE(pi3): 7.915e-03\n",
      "Epoch 11700, Train loss: 6.467e+06, Test loss: 3.641e+08, MSE(e): 4.140e-04, MSE(pi1): 1.181e-01, MSE(pi2): 2.641e-04, MSE(pi3): 1.146e-02\n",
      "Epoch 11800, Train loss: 8.543e+06, Test loss: 4.167e+08, MSE(e): 7.271e-04, MSE(pi1): 2.929e-02, MSE(pi2): 3.227e-04, MSE(pi3): 9.790e-03\n",
      "Epoch 11900, Train loss: 3.791e+06, Test loss: 3.700e+08, MSE(e): 2.362e-04, MSE(pi1): 4.933e-02, MSE(pi2): 1.248e-04, MSE(pi3): 9.353e-03\n",
      "Epoch 12000, Train loss: 6.430e+06, Test loss: 3.810e+08, MSE(e): 4.032e-04, MSE(pi1): 9.032e-02, MSE(pi2): 2.234e-04, MSE(pi3): 1.494e-02\n",
      "Epoch 12100, Train loss: 4.161e+06, Test loss: 3.707e+08, MSE(e): 2.457e-04, MSE(pi1): 1.061e-01, MSE(pi2): 1.325e-04, MSE(pi3): 6.430e-03\n",
      "Epoch 12200, Train loss: 7.171e+06, Test loss: 3.826e+08, MSE(e): 2.868e-04, MSE(pi1): 3.765e-01, MSE(pi2): 1.535e-04, MSE(pi3): 5.383e-03\n",
      "Epoch 12300, Train loss: 1.454e+07, Test loss: 4.085e+08, MSE(e): 9.046e-04, MSE(pi1): 3.144e-01, MSE(pi2): 5.411e-04, MSE(pi3): 2.354e-02\n",
      "Epoch 12400, Train loss: 3.174e+06, Test loss: 3.727e+08, MSE(e): 2.224e-04, MSE(pi1): 3.302e-02, MSE(pi2): 1.158e-04, MSE(pi3): 6.203e-03\n",
      "Epoch 12500, Train loss: 2.228e+07, Test loss: 4.246e+08, MSE(e): 4.355e-04, MSE(pi1): 1.719e+00, MSE(pi2): 2.044e-04, MSE(pi3): 7.441e-03\n",
      "Epoch 12600, Train loss: 8.716e+06, Test loss: 4.198e+08, MSE(e): 6.184e-04, MSE(pi1): 1.969e-01, MSE(pi2): 2.564e-04, MSE(pi3): 5.622e-03\n",
      "Epoch 12700, Train loss: 5.231e+06, Test loss: 3.581e+08, MSE(e): 3.745e-04, MSE(pi1): 8.267e-02, MSE(pi2): 1.817e-04, MSE(pi3): 6.596e-03\n",
      "Epoch 12800, Train loss: 1.056e+07, Test loss: 3.621e+08, MSE(e): 4.014e-04, MSE(pi1): 5.985e-01, MSE(pi2): 1.918e-04, MSE(pi3): 5.641e-03\n",
      "Epoch 12900, Train loss: 3.984e+06, Test loss: 3.730e+08, MSE(e): 2.701e-04, MSE(pi1): 5.582e-02, MSE(pi2): 1.447e-04, MSE(pi3): 7.256e-03\n",
      "Epoch 13000, Train loss: 3.074e+07, Test loss: 4.387e+08, MSE(e): 1.089e-03, MSE(pi1): 1.744e+00, MSE(pi2): 6.592e-04, MSE(pi3): 2.419e-02\n",
      "Epoch 13100, Train loss: 4.223e+06, Test loss: 3.936e+08, MSE(e): 2.610e-04, MSE(pi1): 6.207e-02, MSE(pi2): 1.303e-04, MSE(pi3): 9.926e-03\n",
      "Epoch 13200, Train loss: 7.105e+06, Test loss: 3.602e+08, MSE(e): 5.274e-04, MSE(pi1): 1.028e-01, MSE(pi2): 3.157e-04, MSE(pi3): 8.024e-03\n",
      "Epoch 13300, Train loss: 2.925e+06, Test loss: 3.853e+08, MSE(e): 2.004e-04, MSE(pi1): 2.957e-02, MSE(pi2): 1.039e-04, MSE(pi3): 6.246e-03\n",
      "Epoch 13400, Train loss: 3.962e+06, Test loss: 3.748e+08, MSE(e): 2.875e-04, MSE(pi1): 5.088e-02, MSE(pi2): 1.488e-04, MSE(pi3): 5.779e-03\n",
      "Epoch 13500, Train loss: 5.452e+06, Test loss: 3.908e+08, MSE(e): 3.946e-04, MSE(pi1): 4.387e-02, MSE(pi2): 2.327e-04, MSE(pi3): 1.068e-02\n",
      "Epoch 13600, Train loss: 4.956e+06, Test loss: 4.029e+08, MSE(e): 2.698e-04, MSE(pi1): 1.547e-01, MSE(pi2): 1.333e-04, MSE(pi3): 7.113e-03\n",
      "Epoch 13700, Train loss: 3.644e+06, Test loss: 4.000e+08, MSE(e): 2.551e-04, MSE(pi1): 4.182e-02, MSE(pi2): 1.284e-04, MSE(pi3): 6.752e-03\n",
      "Epoch 13800, Train loss: 1.313e+07, Test loss: 3.938e+08, MSE(e): 3.996e-04, MSE(pi1): 8.436e-01, MSE(pi2): 2.121e-04, MSE(pi3): 6.948e-03\n",
      "Epoch 13900, Train loss: 3.566e+07, Test loss: 3.473e+08, MSE(e): 3.176e-03, MSE(pi1): 3.437e-01, MSE(pi2): 1.497e-03, MSE(pi3): 4.652e-03\n",
      "Epoch 14000, Train loss: 2.514e+07, Test loss: 3.468e+08, MSE(e): 2.332e-03, MSE(pi1): 9.232e-02, MSE(pi2): 1.058e-03, MSE(pi3): 8.908e-03\n",
      "Epoch 14100, Train loss: 1.033e+07, Test loss: 3.576e+08, MSE(e): 8.830e-04, MSE(pi1): 9.028e-02, MSE(pi2): 4.394e-04, MSE(pi3): 5.993e-03\n",
      "Epoch 14200, Train loss: 6.027e+06, Test loss: 3.758e+08, MSE(e): 4.216e-04, MSE(pi1): 1.215e-01, MSE(pi2): 2.190e-04, MSE(pi3): 5.969e-03\n",
      "Epoch 14300, Train loss: 1.163e+07, Test loss: 4.186e+08, MSE(e): 4.764e-04, MSE(pi1): 4.778e-01, MSE(pi2): 2.111e-04, MSE(pi3): 2.088e-02\n",
      "Epoch 14400, Train loss: 3.058e+06, Test loss: 3.938e+08, MSE(e): 1.953e-04, MSE(pi1): 4.421e-02, MSE(pi2): 1.012e-04, MSE(pi3): 6.629e-03\n",
      "Epoch 14500, Train loss: 7.116e+06, Test loss: 4.014e+08, MSE(e): 2.199e-04, MSE(pi1): 3.826e-01, MSE(pi2): 1.081e-04, MSE(pi3): 1.091e-02\n",
      "Epoch 14600, Train loss: 1.744e+07, Test loss: 4.074e+08, MSE(e): 4.233e-04, MSE(pi1): 1.266e+00, MSE(pi2): 2.803e-04, MSE(pi3): 5.518e-03\n",
      "Epoch 14700, Train loss: 1.640e+07, Test loss: 3.907e+08, MSE(e): 5.911e-04, MSE(pi1): 1.002e+00, MSE(pi2): 3.701e-04, MSE(pi3): 4.724e-03\n",
      "Epoch 14800, Train loss: 3.022e+06, Test loss: 3.993e+08, MSE(e): 1.832e-04, MSE(pi1): 6.081e-02, MSE(pi2): 9.627e-05, MSE(pi3): 5.822e-03\n",
      "Epoch 14900, Train loss: 5.193e+07, Test loss: 3.886e+08, MSE(e): 4.022e-03, MSE(pi1): 3.789e-01, MSE(pi2): 1.947e-03, MSE(pi3): 7.923e-02\n",
      "Epoch 15000, Train loss: 9.247e+06, Test loss: 4.529e+08, MSE(e): 8.302e-04, MSE(pi1): 2.649e-02, MSE(pi2): 3.800e-04, MSE(pi3): 6.801e-03\n",
      "Epoch 15100, Train loss: 2.320e+07, Test loss: 4.951e+08, MSE(e): 1.637e-03, MSE(pi1): 5.840e-01, MSE(pi2): 7.293e-04, MSE(pi3): 9.899e-03\n",
      "Epoch 15200, Train loss: 3.368e+07, Test loss: 5.003e+08, MSE(e): 2.755e-03, MSE(pi1): 5.003e-01, MSE(pi2): 1.344e-03, MSE(pi3): 1.130e-02\n",
      "Epoch 15300, Train loss: 5.491e+06, Test loss: 4.067e+08, MSE(e): 2.007e-04, MSE(pi1): 2.486e-01, MSE(pi2): 9.931e-05, MSE(pi3): 9.982e-03\n",
      "Epoch 15400, Train loss: 1.314e+07, Test loss: 4.762e+08, MSE(e): 1.128e-03, MSE(pi1): 9.986e-02, MSE(pi2): 5.218e-04, MSE(pi3): 8.676e-03\n",
      "Epoch 15500, Train loss: 3.281e+07, Test loss: 5.404e+08, MSE(e): 2.664e-03, MSE(pi1): 4.926e-01, MSE(pi2): 1.199e-03, MSE(pi3): 1.246e-02\n",
      "Epoch 15600, Train loss: 1.086e+08, Test loss: 6.095e+08, MSE(e): 1.069e-02, MSE(pi1): 7.868e-02, MSE(pi2): 4.776e-03, MSE(pi3): 9.635e-03\n",
      "Epoch 15700, Train loss: 4.471e+07, Test loss: 5.096e+08, MSE(e): 5.200e-04, MSE(pi1): 3.884e+00, MSE(pi2): 1.713e-04, MSE(pi3): 6.783e-03\n",
      "Epoch 15800, Train loss: 5.528e+06, Test loss: 4.371e+08, MSE(e): 3.788e-04, MSE(pi1): 9.951e-02, MSE(pi2): 1.826e-04, MSE(pi3): 7.451e-03\n",
      "Epoch 15900, Train loss: 2.621e+07, Test loss: 3.746e+08, MSE(e): 2.141e-03, MSE(pi1): 4.211e-01, MSE(pi2): 1.056e-03, MSE(pi3): 5.892e-03\n",
      "Epoch 16000, Train loss: 3.302e+06, Test loss: 4.111e+08, MSE(e): 1.759e-04, MSE(pi1): 9.813e-02, MSE(pi2): 9.381e-05, MSE(pi3): 5.617e-03\n",
      "Epoch 16100, Train loss: 1.184e+07, Test loss: 4.783e+08, MSE(e): 8.365e-04, MSE(pi1): 2.490e-01, MSE(pi2): 3.583e-04, MSE(pi3): 9.839e-03\n",
      "Epoch 16200, Train loss: 4.257e+06, Test loss: 4.132e+08, MSE(e): 1.864e-04, MSE(pi1): 1.729e-01, MSE(pi2): 9.387e-05, MSE(pi3): 6.644e-03\n",
      "Epoch 16300, Train loss: 5.213e+06, Test loss: 4.185e+08, MSE(e): 2.020e-04, MSE(pi1): 2.388e-01, MSE(pi2): 9.954e-05, MSE(pi3): 8.054e-03\n",
      "Epoch 16400, Train loss: 1.105e+08, Test loss: 6.010e+08, MSE(e): 1.066e-02, MSE(pi1): 2.161e-01, MSE(pi2): 4.739e-03, MSE(pi3): 1.792e-02\n",
      "Epoch 16500, Train loss: 4.022e+06, Test loss: 4.184e+08, MSE(e): 2.083e-04, MSE(pi1): 1.012e-01, MSE(pi2): 1.178e-04, MSE(pi3): 9.269e-03\n",
      "Epoch 16600, Train loss: 1.061e+07, Test loss: 4.663e+08, MSE(e): 7.466e-04, MSE(pi1): 2.190e-01, MSE(pi2): 3.332e-04, MSE(pi3): 9.584e-03\n",
      "Epoch 16700, Train loss: 4.274e+06, Test loss: 4.050e+08, MSE(e): 3.235e-04, MSE(pi1): 4.815e-02, MSE(pi2): 1.643e-04, MSE(pi3): 5.584e-03\n",
      "Epoch 16800, Train loss: 1.089e+08, Test loss: 7.217e+08, MSE(e): 1.005e-02, MSE(pi1): 6.791e-01, MSE(pi2): 4.643e-03, MSE(pi3): 1.563e-02\n",
      "Epoch 16900, Train loss: 3.432e+06, Test loss: 4.283e+08, MSE(e): 1.679e-04, MSE(pi1): 7.338e-02, MSE(pi2): 8.956e-05, MSE(pi3): 1.019e-02\n",
      "Epoch 17000, Train loss: 8.465e+06, Test loss: 4.500e+08, MSE(e): 4.840e-04, MSE(pi1): 3.006e-01, MSE(pi2): 1.985e-04, MSE(pi3): 6.186e-03\n",
      "Epoch 17100, Train loss: 3.482e+07, Test loss: 5.472e+08, MSE(e): 2.665e-03, MSE(pi1): 4.143e-01, MSE(pi2): 9.728e-04, MSE(pi3): 4.025e-02\n",
      "Epoch 17200, Train loss: 2.558e+06, Test loss: 4.155e+08, MSE(e): 1.582e-04, MSE(pi1): 3.632e-02, MSE(pi2): 8.246e-05, MSE(pi3): 6.124e-03\n",
      "Epoch 17300, Train loss: 1.354e+07, Test loss: 4.203e+08, MSE(e): 9.934e-04, MSE(pi1): 8.622e-02, MSE(pi2): 6.141e-04, MSE(pi3): 2.748e-02\n",
      "Epoch 17400, Train loss: 1.421e+07, Test loss: 4.541e+08, MSE(e): 7.943e-04, MSE(pi1): 5.341e-01, MSE(pi2): 4.398e-04, MSE(pi3): 9.231e-03\n",
      "Epoch 17500, Train loss: 2.702e+07, Test loss: 5.181e+08, MSE(e): 2.247e-03, MSE(pi1): 3.963e-01, MSE(pi2): 9.852e-04, MSE(pi3): 5.812e-03\n",
      "Epoch 17600, Train loss: 2.287e+07, Test loss: 4.450e+08, MSE(e): 5.802e-04, MSE(pi1): 1.435e+00, MSE(pi2): 3.307e-04, MSE(pi3): 2.718e-02\n",
      "Epoch 17700, Train loss: 1.162e+07, Test loss: 4.968e+08, MSE(e): 1.052e-03, MSE(pi1): 3.327e-02, MSE(pi2): 4.691e-04, MSE(pi3): 7.647e-03\n",
      "Epoch 17800, Train loss: 2.374e+06, Test loss: 4.246e+08, MSE(e): 1.530e-04, MSE(pi1): 2.407e-02, MSE(pi2): 8.004e-05, MSE(pi3): 6.033e-03\n",
      "Epoch 17900, Train loss: 6.707e+06, Test loss: 4.308e+08, MSE(e): 2.156e-04, MSE(pi1): 3.923e-01, MSE(pi2): 1.234e-04, MSE(pi3): 6.275e-03\n",
      "Epoch 18000, Train loss: 1.867e+07, Test loss: 3.922e+08, MSE(e): 1.523e-03, MSE(pi1): 2.603e-01, MSE(pi2): 7.083e-04, MSE(pi3): 8.441e-03\n",
      "Epoch 18100, Train loss: 8.109e+06, Test loss: 4.774e+08, MSE(e): 6.047e-04, MSE(pi1): 1.325e-01, MSE(pi2): 2.811e-04, MSE(pi3): 7.368e-03\n",
      "Epoch 18200, Train loss: 1.737e+07, Test loss: 4.513e+08, MSE(e): 4.017e-04, MSE(pi1): 1.260e+00, MSE(pi2): 1.701e-04, MSE(pi3): 7.504e-03\n",
      "Epoch 18300, Train loss: 8.433e+06, Test loss: 4.612e+08, MSE(e): 3.691e-04, MSE(pi1): 3.396e-01, MSE(pi2): 1.739e-04, MSE(pi3): 1.346e-02\n",
      "Epoch 18400, Train loss: 7.600e+06, Test loss: 4.833e+08, MSE(e): 6.309e-04, MSE(pi1): 5.863e-02, MSE(pi2): 2.878e-04, MSE(pi3): 7.040e-03\n",
      "Epoch 18500, Train loss: 9.688e+06, Test loss: 4.218e+08, MSE(e): 5.440e-04, MSE(pi1): 3.300e-01, MSE(pi2): 3.909e-04, MSE(pi3): 9.482e-03\n",
      "Epoch 18600, Train loss: 5.241e+06, Test loss: 4.467e+08, MSE(e): 3.202e-04, MSE(pi1): 1.472e-01, MSE(pi2): 2.024e-04, MSE(pi3): 5.672e-03\n",
      "Epoch 18700, Train loss: 6.186e+07, Test loss: 3.793e+08, MSE(e): 5.895e-03, MSE(pi1): 2.247e-01, MSE(pi2): 2.718e-03, MSE(pi3): 6.641e-03\n",
      "Epoch 18800, Train loss: 1.271e+07, Test loss: 4.903e+08, MSE(e): 1.101e-03, MSE(pi1): 3.722e-02, MSE(pi2): 4.935e-04, MSE(pi3): 1.323e-02\n",
      "Epoch 18900, Train loss: 9.611e+06, Test loss: 4.624e+08, MSE(e): 2.089e-04, MSE(pi1): 6.897e-01, MSE(pi2): 9.300e-05, MSE(pi3): 6.253e-03\n",
      "Epoch 19000, Train loss: 3.477e+07, Test loss: 5.501e+08, MSE(e): 3.334e-03, MSE(pi1): 6.101e-02, MSE(pi2): 1.487e-03, MSE(pi3): 8.134e-03\n",
      "Epoch 19100, Train loss: 5.445e+06, Test loss: 4.201e+08, MSE(e): 4.388e-04, MSE(pi1): 3.830e-02, MSE(pi2): 2.136e-04, MSE(pi3): 6.739e-03\n",
      "Epoch 19200, Train loss: 1.670e+07, Test loss: 4.621e+08, MSE(e): 3.397e-04, MSE(pi1): 8.179e-01, MSE(pi2): 1.528e-04, MSE(pi3): 5.123e-02\n",
      "Epoch 19300, Train loss: 2.740e+06, Test loss: 4.359e+08, MSE(e): 1.697e-04, MSE(pi1): 3.774e-02, MSE(pi2): 8.671e-05, MSE(pi3): 6.659e-03\n",
      "Epoch 19400, Train loss: 2.274e+06, Test loss: 4.451e+08, MSE(e): 1.443e-04, MSE(pi1): 2.128e-02, MSE(pi2): 7.446e-05, MSE(pi3): 6.180e-03\n",
      "Epoch 19500, Train loss: 1.859e+07, Test loss: 4.049e+08, MSE(e): 1.615e-03, MSE(pi1): 1.747e-01, MSE(pi2): 7.440e-04, MSE(pi3): 6.957e-03\n",
      "Epoch 19600, Train loss: 1.536e+07, Test loss: 3.991e+08, MSE(e): 1.327e-03, MSE(pi1): 1.379e-01, MSE(pi2): 6.062e-04, MSE(pi3): 7.041e-03\n",
      "Epoch 19700, Train loss: 1.059e+07, Test loss: 5.125e+08, MSE(e): 9.461e-04, MSE(pi1): 3.945e-02, MSE(pi2): 4.353e-04, MSE(pi3): 7.388e-03\n",
      "Epoch 19800, Train loss: 4.653e+06, Test loss: 4.499e+08, MSE(e): 2.140e-04, MSE(pi1): 1.901e-01, MSE(pi2): 1.295e-04, MSE(pi3): 6.132e-03\n",
      "Epoch 19900, Train loss: 3.547e+07, Test loss: 5.648e+08, MSE(e): 3.378e-03, MSE(pi1): 8.945e-02, MSE(pi2): 1.513e-03, MSE(pi3): 7.950e-03\n",
      "\n",
      "Training process finished after 20000 epochs\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# explanatory_layers = [100]\n",
    "\n",
    "model = TransferLearningAutoencoder(input_shape, predictive_layers, pgnniv_pretrained_encoder, predictive_output, explanatory_input,\n",
    "                                   explanatory_layers, explanatory_output, n_filters_explanatory).to(DEVICE)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=3e-3)\n",
    "\n",
    "# Parametros de entrenamiento\n",
    "start_epoch = 0\n",
    "n_epochs = 20000\n",
    "\n",
    "batch_size = 64\n",
    "n_checkpoints = 10\n",
    "\n",
    "train_loop(model, optimizer, X_train, y_train, f_train, X_test, y_test, f_test,\n",
    "           D, n_checkpoints, start_epoch=start_epoch, n_epochs=n_epochs, batch_size=batch_size, \n",
    "           model_results_path=MODEL_RESULTS_TRANSFERLEARNING_PATH, device=DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training from a checkpoint. Epoch 18000.\n",
      "Epoch 18000, Train loss: 1.816e+07, Test loss: 5.015e+08, MSE(e): 1.528e-03, MSE(pi1): 2.038e-01, MSE(pi2): 6.919e-04, MSE(pi3): 8.502e-03\n",
      "Epoch 18100, Train loss: 9.189e+06, Test loss: 4.011e+08, MSE(e): 6.837e-04, MSE(pi1): 1.793e-01, MSE(pi2): 3.296e-04, MSE(pi3): 5.583e-03\n",
      "Epoch 18200, Train loss: 2.017e+07, Test loss: 4.743e+08, MSE(e): 4.006e-04, MSE(pi1): 1.530e+00, MSE(pi2): 1.512e-04, MSE(pi3): 8.544e-03\n",
      "Epoch 18300, Train loss: 4.117e+06, Test loss: 4.202e+08, MSE(e): 2.695e-04, MSE(pi1): 7.824e-02, MSE(pi2): 1.558e-04, MSE(pi3): 6.391e-03\n",
      "Epoch 18400, Train loss: 8.517e+06, Test loss: 4.130e+08, MSE(e): 5.263e-04, MSE(pi1): 2.622e-01, MSE(pi2): 2.563e-04, MSE(pi3): 6.318e-03\n",
      "Epoch 18500, Train loss: 4.246e+07, Test loss: 5.023e+08, MSE(e): 2.031e-03, MSE(pi1): 1.289e+00, MSE(pi2): 1.064e-03, MSE(pi3): 9.263e-02\n",
      "Epoch 18600, Train loss: 5.049e+07, Test loss: 5.998e+08, MSE(e): 4.762e-03, MSE(pi1): 2.025e-01, MSE(pi2): 2.129e-03, MSE(pi3): 8.369e-03\n",
      "Epoch 18700, Train loss: 1.049e+08, Test loss: 7.006e+08, MSE(e): 9.701e-03, MSE(pi1): 6.466e-01, MSE(pi2): 4.403e-03, MSE(pi3): 1.454e-02\n",
      "Epoch 18800, Train loss: 6.293e+06, Test loss: 4.453e+08, MSE(e): 2.665e-04, MSE(pi1): 2.367e-01, MSE(pi2): 1.537e-04, MSE(pi3): 1.261e-02\n",
      "Epoch 18900, Train loss: 3.669e+06, Test loss: 4.396e+08, MSE(e): 1.807e-04, MSE(pi1): 1.233e-01, MSE(pi2): 9.901e-05, MSE(pi3): 6.298e-03\n",
      "Epoch 19000, Train loss: 9.746e+06, Test loss: 4.789e+08, MSE(e): 3.185e-04, MSE(pi1): 4.646e-01, MSE(pi2): 1.468e-04, MSE(pi3): 1.914e-02\n",
      "Epoch 19100, Train loss: 6.115e+06, Test loss: 4.426e+08, MSE(e): 4.448e-04, MSE(pi1): 1.073e-01, MSE(pi2): 3.099e-04, MSE(pi3): 5.943e-03\n",
      "Epoch 19200, Train loss: 1.418e+07, Test loss: 4.953e+08, MSE(e): 1.211e-03, MSE(pi1): 8.239e-02, MSE(pi2): 5.430e-04, MSE(pi3): 1.249e-02\n",
      "Epoch 19300, Train loss: 7.001e+06, Test loss: 4.161e+08, MSE(e): 6.143e-04, MSE(pi1): 2.618e-02, MSE(pi2): 2.913e-04, MSE(pi3): 5.966e-03\n",
      "Epoch 19400, Train loss: 2.623e+07, Test loss: 5.195e+08, MSE(e): 1.548e-03, MSE(pi1): 1.007e+00, MSE(pi2): 5.655e-04, MSE(pi3): 6.842e-03\n",
      "Epoch 19500, Train loss: 1.044e+07, Test loss: 4.017e+08, MSE(e): 9.108e-04, MSE(pi1): 8.193e-02, MSE(pi2): 4.364e-04, MSE(pi3): 5.129e-03\n",
      "Epoch 19600, Train loss: 5.524e+06, Test loss: 4.468e+08, MSE(e): 2.734e-04, MSE(pi1): 1.576e-01, MSE(pi2): 1.631e-04, MSE(pi3): 1.213e-02\n",
      "Epoch 19700, Train loss: 1.351e+07, Test loss: 3.975e+08, MSE(e): 1.254e-03, MSE(pi1): 3.495e-02, MSE(pi2): 5.745e-04, MSE(pi3): 6.191e-03\n",
      "Epoch 19800, Train loss: 2.674e+06, Test loss: 4.456e+08, MSE(e): 1.464e-04, MSE(pi1): 6.434e-02, MSE(pi2): 7.972e-05, MSE(pi3): 5.665e-03\n",
      "Epoch 19900, Train loss: 1.137e+07, Test loss: 4.154e+08, MSE(e): 8.049e-04, MSE(pi1): 2.350e-01, MSE(pi2): 3.565e-04, MSE(pi3): 9.737e-03\n",
      "Epoch 20000, Train loss: 7.410e+06, Test loss: 4.926e+08, MSE(e): 6.332e-04, MSE(pi1): 3.555e-02, MSE(pi2): 2.948e-04, MSE(pi3): 7.226e-03\n",
      "Epoch 20100, Train loss: 1.493e+07, Test loss: 4.160e+08, MSE(e): 1.118e-03, MSE(pi1): 2.640e-01, MSE(pi2): 5.394e-04, MSE(pi3): 1.105e-02\n",
      "Epoch 20200, Train loss: 3.179e+07, Test loss: 3.981e+08, MSE(e): 2.822e-03, MSE(pi1): 2.973e-01, MSE(pi2): 1.356e-03, MSE(pi3): 5.976e-03\n",
      "Epoch 20300, Train loss: 4.932e+06, Test loss: 4.326e+08, MSE(e): 2.384e-04, MSE(pi1): 1.785e-01, MSE(pi2): 1.035e-04, MSE(pi3): 7.630e-03\n",
      "Epoch 20400, Train loss: 2.410e+06, Test loss: 4.635e+08, MSE(e): 1.422e-04, MSE(pi1): 2.835e-02, MSE(pi2): 7.355e-05, MSE(pi3): 7.036e-03\n",
      "Epoch 20500, Train loss: 4.299e+06, Test loss: 4.758e+08, MSE(e): 2.805e-04, MSE(pi1): 7.935e-02, MSE(pi2): 1.325e-04, MSE(pi3): 7.007e-03\n",
      "Epoch 20600, Train loss: 6.818e+06, Test loss: 4.677e+08, MSE(e): 1.738e-04, MSE(pi1): 4.454e-01, MSE(pi2): 7.848e-05, MSE(pi3): 6.266e-03\n",
      "Epoch 20700, Train loss: 1.406e+07, Test loss: 4.493e+08, MSE(e): 2.527e-04, MSE(pi1): 9.961e-01, MSE(pi2): 1.107e-04, MSE(pi3): 1.571e-02\n",
      "Epoch 20800, Train loss: 4.500e+07, Test loss: 6.141e+08, MSE(e): 4.351e-03, MSE(pi1): 6.175e-02, MSE(pi2): 1.960e-03, MSE(pi3): 8.745e-03\n",
      "Epoch 20900, Train loss: 8.549e+06, Test loss: 4.153e+08, MSE(e): 7.340e-04, MSE(pi1): 6.791e-02, MSE(pi2): 3.536e-04, MSE(pi3): 5.297e-03\n",
      "Epoch 21000, Train loss: 1.760e+07, Test loss: 4.758e+08, MSE(e): 4.057e-04, MSE(pi1): 1.302e+00, MSE(pi2): 2.510e-04, MSE(pi3): 5.226e-03\n",
      "Epoch 21100, Train loss: 1.098e+07, Test loss: 4.384e+08, MSE(e): 4.354e-04, MSE(pi1): 5.973e-01, MSE(pi2): 2.488e-04, MSE(pi3): 6.563e-03\n",
      "Epoch 21200, Train loss: 4.463e+06, Test loss: 4.862e+08, MSE(e): 3.399e-04, MSE(pi1): 3.939e-02, MSE(pi2): 1.600e-04, MSE(pi3): 6.701e-03\n",
      "Epoch 21300, Train loss: 6.457e+06, Test loss: 4.836e+08, MSE(e): 3.902e-04, MSE(pi1): 1.775e-01, MSE(pi2): 1.809e-04, MSE(pi3): 7.795e-03\n",
      "Epoch 21400, Train loss: 6.321e+06, Test loss: 4.937e+08, MSE(e): 4.294e-04, MSE(pi1): 1.365e-01, MSE(pi2): 2.021e-04, MSE(pi3): 6.617e-03\n",
      "Epoch 21500, Train loss: 5.291e+07, Test loss: 4.091e+08, MSE(e): 4.166e-03, MSE(pi1): 1.082e+00, MSE(pi2): 2.089e-03, MSE(pi3): 4.319e-03\n",
      "Epoch 21600, Train loss: 7.388e+06, Test loss: 4.362e+08, MSE(e): 5.853e-04, MSE(pi1): 4.266e-02, MSE(pi2): 3.000e-04, MSE(pi3): 1.109e-02\n",
      "Epoch 21700, Train loss: 1.290e+07, Test loss: 4.768e+08, MSE(e): 2.801e-04, MSE(pi1): 9.378e-01, MSE(pi2): 1.162e-04, MSE(pi3): 7.242e-03\n",
      "Epoch 21800, Train loss: 6.837e+07, Test loss: 4.254e+08, MSE(e): 4.088e-03, MSE(pi1): 2.704e+00, MSE(pi2): 2.063e-03, MSE(pi3): 4.529e-03\n",
      "Epoch 21900, Train loss: 1.763e+07, Test loss: 4.119e+08, MSE(e): 1.636e-03, MSE(pi1): 6.999e-02, MSE(pi2): 7.538e-04, MSE(pi3): 5.794e-03\n",
      "Epoch 22000, Train loss: 3.147e+06, Test loss: 4.731e+08, MSE(e): 1.784e-04, MSE(pi1): 5.534e-02, MSE(pi2): 9.103e-05, MSE(pi3): 8.096e-03\n",
      "Epoch 22100, Train loss: 3.112e+06, Test loss: 4.767e+08, MSE(e): 1.881e-04, MSE(pi1): 4.881e-02, MSE(pi2): 9.705e-05, MSE(pi3): 7.434e-03\n",
      "Epoch 22200, Train loss: 7.320e+06, Test loss: 4.764e+08, MSE(e): 1.707e-04, MSE(pi1): 4.821e-01, MSE(pi2): 7.641e-05, MSE(pi3): 7.919e-03\n",
      "Epoch 22300, Train loss: 5.105e+06, Test loss: 4.685e+08, MSE(e): 1.553e-04, MSE(pi1): 2.812e-01, MSE(pi2): 7.315e-05, MSE(pi3): 7.398e-03\n",
      "Epoch 22400, Train loss: 4.656e+06, Test loss: 4.844e+08, MSE(e): 2.075e-04, MSE(pi1): 1.887e-01, MSE(pi2): 9.649e-05, MSE(pi3): 6.944e-03\n",
      "Epoch 22500, Train loss: 3.394e+06, Test loss: 4.644e+08, MSE(e): 1.687e-04, MSE(pi1): 9.421e-02, MSE(pi2): 9.420e-05, MSE(pi3): 7.648e-03\n",
      "Epoch 22600, Train loss: 6.917e+06, Test loss: 4.965e+08, MSE(e): 4.048e-04, MSE(pi1): 1.880e-01, MSE(pi2): 2.227e-04, MSE(pi3): 9.894e-03\n",
      "Epoch 22700, Train loss: 3.349e+07, Test loss: 5.553e+08, MSE(e): 2.794e-03, MSE(pi1): 1.449e-01, MSE(pi2): 1.214e-03, MSE(pi3): 4.105e-02\n",
      "Epoch 22800, Train loss: 2.903e+07, Test loss: 4.124e+08, MSE(e): 2.764e-03, MSE(pi1): 8.848e-02, MSE(pi2): 1.279e-03, MSE(pi3): 5.069e-03\n",
      "Epoch 22900, Train loss: 4.747e+07, Test loss: 4.157e+08, MSE(e): 4.570e-03, MSE(pi1): 7.191e-02, MSE(pi2): 2.137e-03, MSE(pi3): 1.052e-02\n",
      "Epoch 23000, Train loss: 2.115e+07, Test loss: 5.351e+08, MSE(e): 1.694e-03, MSE(pi1): 3.273e-01, MSE(pi2): 7.488e-04, MSE(pi3): 9.335e-03\n",
      "Epoch 23100, Train loss: 3.007e+06, Test loss: 4.844e+08, MSE(e): 2.168e-04, MSE(pi1): 1.802e-02, MSE(pi2): 1.036e-04, MSE(pi3): 6.589e-03\n",
      "Epoch 23200, Train loss: 1.662e+07, Test loss: 4.516e+08, MSE(e): 4.439e-04, MSE(pi1): 1.131e+00, MSE(pi2): 2.254e-04, MSE(pi3): 8.710e-03\n",
      "Epoch 23300, Train loss: 1.312e+07, Test loss: 4.257e+08, MSE(e): 1.217e-03, MSE(pi1): 3.758e-02, MSE(pi2): 5.646e-04, MSE(pi3): 5.695e-03\n",
      "Epoch 23400, Train loss: 8.333e+06, Test loss: 4.890e+08, MSE(e): 1.723e-04, MSE(pi1): 5.863e-01, MSE(pi2): 7.484e-05, MSE(pi3): 7.470e-03\n",
      "Epoch 23500, Train loss: 8.777e+06, Test loss: 4.606e+08, MSE(e): 2.786e-04, MSE(pi1): 4.796e-01, MSE(pi2): 1.435e-04, MSE(pi3): 1.194e-02\n",
      "Epoch 23600, Train loss: 1.116e+07, Test loss: 4.368e+08, MSE(e): 8.525e-04, MSE(pi1): 2.135e-01, MSE(pi2): 4.329e-04, MSE(pi3): 4.979e-03\n",
      "Epoch 23700, Train loss: 5.692e+06, Test loss: 4.897e+08, MSE(e): 1.645e-04, MSE(pi1): 2.391e-01, MSE(pi2): 8.282e-05, MSE(pi3): 1.656e-02\n",
      "Epoch 23800, Train loss: 5.253e+06, Test loss: 4.877e+08, MSE(e): 3.963e-04, MSE(pi1): 3.507e-02, MSE(pi2): 2.419e-04, MSE(pi3): 9.389e-03\n",
      "Epoch 23900, Train loss: 1.135e+07, Test loss: 5.246e+08, MSE(e): 1.019e-03, MSE(pi1): 4.967e-02, MSE(pi2): 4.794e-04, MSE(pi3): 6.640e-03\n",
      "Epoch 24000, Train loss: 4.472e+06, Test loss: 4.823e+08, MSE(e): 1.458e-04, MSE(pi1): 2.259e-01, MSE(pi2): 7.481e-05, MSE(pi3): 7.558e-03\n",
      "Epoch 24100, Train loss: 6.208e+06, Test loss: 4.696e+08, MSE(e): 4.317e-04, MSE(pi1): 8.969e-02, MSE(pi2): 2.663e-04, MSE(pi3): 9.945e-03\n",
      "Epoch 24200, Train loss: 3.521e+06, Test loss: 4.591e+08, MSE(e): 2.673e-04, MSE(pi1): 2.737e-02, MSE(pi2): 1.345e-04, MSE(pi3): 5.740e-03\n",
      "Epoch 24300, Train loss: 1.148e+07, Test loss: 5.178e+08, MSE(e): 4.574e-04, MSE(pi1): 3.859e-01, MSE(pi2): 2.875e-04, MSE(pi3): 3.042e-02\n",
      "Epoch 24400, Train loss: 2.327e+06, Test loss: 4.723e+08, MSE(e): 1.362e-04, MSE(pi1): 3.089e-02, MSE(pi2): 6.890e-05, MSE(pi3): 6.563e-03\n",
      "Epoch 24500, Train loss: 2.476e+06, Test loss: 4.817e+08, MSE(e): 1.204e-04, MSE(pi1): 6.840e-02, MSE(pi2): 6.374e-05, MSE(pi3): 5.881e-03\n",
      "Epoch 24600, Train loss: 1.887e+07, Test loss: 4.984e+08, MSE(e): 3.737e-04, MSE(pi1): 1.429e+00, MSE(pi2): 1.043e-04, MSE(pi3): 8.469e-03\n",
      "Epoch 24700, Train loss: 1.970e+07, Test loss: 5.573e+08, MSE(e): 1.702e-03, MSE(pi1): 4.104e-02, MSE(pi2): 7.409e-04, MSE(pi3): 2.261e-02\n",
      "Epoch 24800, Train loss: 3.767e+06, Test loss: 4.616e+08, MSE(e): 2.962e-04, MSE(pi1): 2.111e-02, MSE(pi2): 1.449e-04, MSE(pi3): 5.946e-03\n",
      "Epoch 24900, Train loss: 4.029e+06, Test loss: 4.795e+08, MSE(e): 1.526e-04, MSE(pi1): 1.911e-01, MSE(pi2): 8.338e-05, MSE(pi3): 5.916e-03\n",
      "Epoch 25000, Train loss: 2.780e+07, Test loss: 4.290e+08, MSE(e): 2.560e-03, MSE(pi1): 1.758e-01, MSE(pi2): 1.182e-03, MSE(pi3): 4.476e-03\n",
      "Epoch 25100, Train loss: 4.218e+06, Test loss: 4.636e+08, MSE(e): 3.253e-04, MSE(pi1): 4.116e-02, MSE(pi2): 1.624e-04, MSE(pi3): 5.542e-03\n",
      "Epoch 25200, Train loss: 3.254e+06, Test loss: 4.910e+08, MSE(e): 2.044e-04, MSE(pi1): 5.998e-02, MSE(pi2): 1.078e-04, MSE(pi3): 6.107e-03\n",
      "Epoch 25300, Train loss: 1.035e+07, Test loss: 5.323e+08, MSE(e): 3.355e-04, MSE(pi1): 6.250e-01, MSE(pi2): 1.618e-04, MSE(pi3): 7.410e-03\n",
      "Epoch 25400, Train loss: 8.743e+06, Test loss: 5.071e+08, MSE(e): 6.966e-04, MSE(pi1): 1.016e-01, MSE(pi2): 3.263e-04, MSE(pi3): 7.615e-03\n",
      "Epoch 25500, Train loss: 1.060e+07, Test loss: 4.448e+08, MSE(e): 7.923e-04, MSE(pi1): 2.143e-01, MSE(pi2): 3.890e-04, MSE(pi3): 5.309e-03\n",
      "Epoch 25600, Train loss: 6.864e+06, Test loss: 5.279e+08, MSE(e): 5.125e-04, MSE(pi1): 1.076e-01, MSE(pi2): 2.664e-04, MSE(pi3): 6.624e-03\n",
      "Epoch 25700, Train loss: 1.401e+07, Test loss: 5.627e+08, MSE(e): 1.186e-03, MSE(pi1): 1.367e-01, MSE(pi2): 5.300e-04, MSE(pi3): 7.824e-03\n",
      "Epoch 25800, Train loss: 2.490e+07, Test loss: 6.078e+08, MSE(e): 2.308e-03, MSE(pi1): 1.201e-01, MSE(pi2): 1.076e-03, MSE(pi3): 6.203e-03\n",
      "Epoch 25900, Train loss: 9.714e+06, Test loss: 5.003e+08, MSE(e): 1.914e-04, MSE(pi1): 6.270e-01, MSE(pi2): 8.265e-05, MSE(pi3): 1.530e-02\n",
      "Epoch 26000, Train loss: 1.228e+07, Test loss: 5.496e+08, MSE(e): 1.069e-03, MSE(pi1): 8.966e-02, MSE(pi2): 4.872e-04, MSE(pi3): 6.996e-03\n",
      "Epoch 26100, Train loss: 8.369e+06, Test loss: 4.893e+08, MSE(e): 2.514e-04, MSE(pi1): 4.945e-01, MSE(pi2): 1.462e-04, MSE(pi3): 9.105e-03\n",
      "Epoch 26200, Train loss: 2.210e+06, Test loss: 4.904e+08, MSE(e): 1.098e-04, MSE(pi1): 2.480e-02, MSE(pi2): 5.878e-05, MSE(pi3): 8.647e-03\n",
      "Epoch 26300, Train loss: 1.956e+06, Test loss: 4.971e+08, MSE(e): 1.143e-04, MSE(pi1): 1.705e-02, MSE(pi2): 5.863e-05, MSE(pi3): 6.427e-03\n",
      "Epoch 26400, Train loss: 4.956e+06, Test loss: 5.094e+08, MSE(e): 3.014e-04, MSE(pi1): 1.201e-01, MSE(pi2): 1.851e-04, MSE(pi3): 7.400e-03\n",
      "Epoch 26500, Train loss: 8.196e+06, Test loss: 4.955e+08, MSE(e): 2.969e-04, MSE(pi1): 4.525e-01, MSE(pi2): 1.915e-04, MSE(pi3): 7.021e-03\n",
      "Epoch 26600, Train loss: 5.728e+06, Test loss: 4.881e+08, MSE(e): 1.748e-04, MSE(pi1): 3.334e-01, MSE(pi2): 8.458e-05, MSE(pi3): 6.467e-03\n",
      "Epoch 26700, Train loss: 6.814e+06, Test loss: 5.046e+08, MSE(e): 1.539e-04, MSE(pi1): 4.467e-01, MSE(pi2): 6.614e-05, MSE(pi3): 8.079e-03\n",
      "Epoch 26800, Train loss: 3.204e+06, Test loss: 4.929e+08, MSE(e): 1.328e-04, MSE(pi1): 1.360e-01, MSE(pi2): 7.508e-05, MSE(pi3): 5.151e-03\n",
      "Epoch 26900, Train loss: 4.704e+07, Test loss: 6.488e+08, MSE(e): 3.014e-03, MSE(pi1): 1.545e+00, MSE(pi2): 1.582e-03, MSE(pi3): 1.453e-02\n",
      "Epoch 27000, Train loss: 6.779e+06, Test loss: 5.386e+08, MSE(e): 5.630e-04, MSE(pi1): 4.586e-02, MSE(pi2): 2.581e-04, MSE(pi3): 6.907e-03\n",
      "Epoch 27100, Train loss: 8.959e+06, Test loss: 5.169e+08, MSE(e): 2.488e-04, MSE(pi1): 5.917e-01, MSE(pi2): 1.403e-04, MSE(pi3): 5.541e-03\n",
      "Epoch 27200, Train loss: 3.189e+06, Test loss: 5.144e+08, MSE(e): 1.866e-04, MSE(pi1): 6.050e-02, MSE(pi2): 9.762e-05, MSE(pi3): 7.175e-03\n",
      "Epoch 27300, Train loss: 3.211e+07, Test loss: 5.476e+08, MSE(e): 5.137e-04, MSE(pi1): 2.609e+00, MSE(pi2): 1.946e-04, MSE(pi3): 8.872e-03\n",
      "Epoch 27400, Train loss: 1.978e+06, Test loss: 5.041e+08, MSE(e): 1.165e-04, MSE(pi1): 1.763e-02, MSE(pi2): 6.053e-05, MSE(pi3): 6.365e-03\n",
      "Epoch 27500, Train loss: 5.958e+06, Test loss: 5.138e+08, MSE(e): 2.214e-04, MSE(pi1): 3.174e-01, MSE(pi2): 1.204e-04, MSE(pi3): 5.703e-03\n",
      "Epoch 27600, Train loss: 4.676e+07, Test loss: 6.271e+08, MSE(e): 3.787e-03, MSE(pi1): 6.780e-01, MSE(pi2): 1.537e-03, MSE(pi3): 2.107e-02\n",
      "Epoch 27700, Train loss: 1.228e+07, Test loss: 5.826e+08, MSE(e): 1.106e-03, MSE(pi1): 4.587e-02, MSE(pi2): 5.054e-04, MSE(pi3): 7.575e-03\n",
      "Epoch 27800, Train loss: 2.078e+06, Test loss: 5.022e+08, MSE(e): 1.112e-04, MSE(pi1): 2.386e-02, MSE(pi2): 5.883e-05, MSE(pi3): 7.275e-03\n",
      "Epoch 27900, Train loss: 2.654e+07, Test loss: 6.238e+08, MSE(e): 2.478e-03, MSE(pi1): 1.046e-01, MSE(pi2): 1.145e-03, MSE(pi3): 7.081e-03\n",
      "Epoch 28000, Train loss: 7.853e+06, Test loss: 4.661e+08, MSE(e): 6.677e-04, MSE(pi1): 6.401e-02, MSE(pi2): 3.213e-04, MSE(pi3): 5.360e-03\n",
      "Epoch 28100, Train loss: 3.039e+06, Test loss: 4.983e+08, MSE(e): 1.197e-04, MSE(pi1): 1.224e-01, MSE(pi2): 6.087e-05, MSE(pi3): 6.187e-03\n",
      "Epoch 28200, Train loss: 8.724e+06, Test loss: 5.573e+08, MSE(e): 7.596e-04, MSE(pi1): 3.707e-02, MSE(pi2): 3.565e-04, MSE(pi3): 7.578e-03\n",
      "Epoch 28300, Train loss: 2.775e+06, Test loss: 5.002e+08, MSE(e): 1.305e-04, MSE(pi1): 8.820e-02, MSE(pi2): 6.617e-05, MSE(pi3): 5.874e-03\n",
      "Epoch 28400, Train loss: 5.657e+06, Test loss: 5.142e+08, MSE(e): 1.552e-04, MSE(pi1): 6.940e-02, MSE(pi2): 9.103e-05, MSE(pi3): 3.411e-02\n",
      "Epoch 28500, Train loss: 2.003e+06, Test loss: 5.039e+08, MSE(e): 1.096e-04, MSE(pi1): 2.433e-02, MSE(pi2): 5.942e-05, MSE(pi3): 6.631e-03\n",
      "Epoch 28600, Train loss: 1.610e+07, Test loss: 5.343e+08, MSE(e): 2.315e-04, MSE(pi1): 1.306e+00, MSE(pi2): 8.909e-05, MSE(pi3): 7.266e-03\n",
      "Epoch 28700, Train loss: 5.813e+06, Test loss: 5.161e+08, MSE(e): 1.905e-04, MSE(pi1): 3.159e-01, MSE(pi2): 8.163e-05, MSE(pi3): 7.489e-03\n",
      "Epoch 28800, Train loss: 4.225e+06, Test loss: 5.317e+08, MSE(e): 2.961e-04, MSE(pi1): 5.275e-02, MSE(pi2): 1.464e-04, MSE(pi3): 7.357e-03\n",
      "Epoch 28900, Train loss: 8.584e+06, Test loss: 4.914e+08, MSE(e): 4.230e-04, MSE(pi1): 3.810e-01, MSE(pi2): 2.062e-04, MSE(pi3): 5.439e-03\n",
      "Epoch 29000, Train loss: 7.266e+06, Test loss: 4.899e+08, MSE(e): 3.573e-04, MSE(pi1): 3.187e-01, MSE(pi2): 1.894e-04, MSE(pi3): 5.058e-03\n",
      "Epoch 29100, Train loss: 1.392e+07, Test loss: 4.777e+08, MSE(e): 7.615e-04, MSE(pi1): 5.601e-01, MSE(pi2): 4.077e-04, MSE(pi3): 7.034e-03\n",
      "Epoch 29200, Train loss: 3.982e+06, Test loss: 5.168e+08, MSE(e): 1.553e-04, MSE(pi1): 1.532e-01, MSE(pi2): 8.011e-05, MSE(pi3): 8.972e-03\n",
      "Epoch 29300, Train loss: 1.667e+07, Test loss: 5.526e+08, MSE(e): 3.615e-04, MSE(pi1): 1.160e+00, MSE(pi2): 1.108e-04, MSE(pi3): 1.450e-02\n",
      "Epoch 29400, Train loss: 1.283e+07, Test loss: 6.002e+08, MSE(e): 1.197e-03, MSE(pi1): 1.527e-02, MSE(pi2): 5.367e-04, MSE(pi3): 7.004e-03\n",
      "Epoch 29500, Train loss: 1.797e+06, Test loss: 5.115e+08, MSE(e): 9.709e-05, MSE(pi1): 2.121e-02, MSE(pi2): 5.166e-05, MSE(pi3): 6.143e-03\n",
      "Epoch 29600, Train loss: 1.486e+07, Test loss: 5.601e+08, MSE(e): 1.077e-03, MSE(pi1): 1.583e-01, MSE(pi2): 6.505e-04, MSE(pi3): 2.514e-02\n",
      "Epoch 29700, Train loss: 3.687e+06, Test loss: 4.954e+08, MSE(e): 2.678e-04, MSE(pi1): 4.359e-02, MSE(pi2): 1.352e-04, MSE(pi3): 5.739e-03\n",
      "Epoch 29800, Train loss: 4.245e+06, Test loss: 5.218e+08, MSE(e): 1.309e-04, MSE(pi1): 2.362e-01, MSE(pi2): 6.248e-05, MSE(pi3): 5.739e-03\n",
      "Epoch 29900, Train loss: 2.055e+07, Test loss: 5.356e+08, MSE(e): 4.748e-04, MSE(pi1): 1.497e+00, MSE(pi2): 1.603e-04, MSE(pi3): 8.337e-03\n",
      "Epoch 30000, Train loss: 6.824e+06, Test loss: 4.815e+08, MSE(e): 5.973e-04, MSE(pi1): 2.935e-02, MSE(pi2): 2.761e-04, MSE(pi3): 5.569e-03\n",
      "Epoch 30100, Train loss: 5.455e+06, Test loss: 5.334e+08, MSE(e): 1.554e-04, MSE(pi1): 3.296e-01, MSE(pi2): 7.347e-05, MSE(pi3): 6.047e-03\n",
      "Epoch 30200, Train loss: 1.034e+07, Test loss: 5.403e+08, MSE(e): 2.003e-04, MSE(pi1): 7.652e-01, MSE(pi2): 8.114e-05, MSE(pi3): 6.860e-03\n",
      "Epoch 30300, Train loss: 3.696e+06, Test loss: 5.137e+08, MSE(e): 1.759e-04, MSE(pi1): 7.947e-02, MSE(pi2): 1.144e-04, MSE(pi3): 1.143e-02\n",
      "Epoch 30400, Train loss: 1.023e+07, Test loss: 5.486e+08, MSE(e): 3.700e-04, MSE(pi1): 5.993e-01, MSE(pi2): 1.893e-04, MSE(pi3): 5.321e-03\n",
      "Epoch 30500, Train loss: 1.165e+07, Test loss: 5.355e+08, MSE(e): 4.727e-04, MSE(pi1): 4.347e-01, MSE(pi2): 2.716e-04, MSE(pi3): 2.571e-02\n",
      "Epoch 30600, Train loss: 2.790e+06, Test loss: 5.405e+08, MSE(e): 1.881e-04, MSE(pi1): 2.256e-02, MSE(pi2): 9.270e-05, MSE(pi3): 6.828e-03\n",
      "Epoch 30700, Train loss: 3.902e+06, Test loss: 5.210e+08, MSE(e): 1.214e-04, MSE(pi1): 2.107e-01, MSE(pi2): 6.627e-05, MSE(pi3): 5.803e-03\n",
      "Epoch 30800, Train loss: 3.456e+06, Test loss: 5.195e+08, MSE(e): 1.162e-04, MSE(pi1): 1.366e-01, MSE(pi2): 6.196e-05, MSE(pi3): 9.290e-03\n",
      "Epoch 30900, Train loss: 6.462e+06, Test loss: 5.059e+08, MSE(e): 3.656e-04, MSE(pi1): 2.193e-01, MSE(pi2): 2.599e-04, MSE(pi3): 6.128e-03\n",
      "Epoch 31000, Train loss: 4.077e+07, Test loss: 6.588e+08, MSE(e): 3.942e-03, MSE(pi1): 3.827e-02, MSE(pi2): 1.736e-03, MSE(pi3): 9.610e-03\n",
      "Epoch 31100, Train loss: 1.257e+07, Test loss: 5.500e+08, MSE(e): 3.538e-04, MSE(pi1): 6.388e-01, MSE(pi2): 1.848e-04, MSE(pi3): 2.646e-02\n",
      "Epoch 31200, Train loss: 1.947e+07, Test loss: 5.616e+08, MSE(e): 1.822e-03, MSE(pi1): 3.322e-02, MSE(pi2): 8.060e-04, MSE(pi3): 9.244e-03\n",
      "Epoch 31300, Train loss: 3.435e+06, Test loss: 5.064e+08, MSE(e): 2.373e-04, MSE(pi1): 1.900e-02, MSE(pi2): 1.242e-04, MSE(pi3): 8.716e-03\n",
      "Epoch 31400, Train loss: 1.123e+07, Test loss: 5.697e+08, MSE(e): 9.098e-04, MSE(pi1): 1.415e-01, MSE(pi2): 4.257e-04, MSE(pi3): 7.135e-03\n",
      "Epoch 31500, Train loss: 4.013e+06, Test loss: 5.481e+08, MSE(e): 3.100e-04, MSE(pi1): 2.921e-02, MSE(pi2): 1.443e-04, MSE(pi3): 6.214e-03\n",
      "Epoch 31600, Train loss: 4.499e+06, Test loss: 5.281e+08, MSE(e): 1.677e-04, MSE(pi1): 2.092e-01, MSE(pi2): 9.790e-05, MSE(pi3): 7.299e-03\n",
      "Epoch 31700, Train loss: 3.629e+06, Test loss: 5.155e+08, MSE(e): 2.349e-04, MSE(pi1): 7.006e-02, MSE(pi2): 1.273e-04, MSE(pi3): 5.790e-03\n",
      "Epoch 31800, Train loss: 2.035e+07, Test loss: 6.198e+08, MSE(e): 1.940e-03, MSE(pi1): 1.940e-02, MSE(pi2): 8.586e-04, MSE(pi3): 7.550e-03\n",
      "Epoch 31900, Train loss: 2.824e+06, Test loss: 5.094e+08, MSE(e): 1.917e-04, MSE(pi1): 2.922e-02, MSE(pi2): 9.333e-05, MSE(pi3): 6.139e-03\n",
      "Epoch 32000, Train loss: 1.289e+07, Test loss: 4.961e+08, MSE(e): 1.156e-03, MSE(pi1): 7.001e-02, MSE(pi2): 5.828e-04, MSE(pi3): 6.327e-03\n",
      "Epoch 32100, Train loss: 8.667e+06, Test loss: 5.655e+08, MSE(e): 6.499e-04, MSE(pi1): 1.153e-01, MSE(pi2): 3.829e-04, MSE(pi3): 1.015e-02\n",
      "Epoch 32200, Train loss: 2.585e+06, Test loss: 5.280e+08, MSE(e): 1.021e-04, MSE(pi1): 1.052e-01, MSE(pi2): 5.213e-05, MSE(pi3): 5.113e-03\n",
      "Epoch 32300, Train loss: 5.555e+06, Test loss: 5.406e+08, MSE(e): 1.304e-04, MSE(pi1): 3.630e-01, MSE(pi2): 5.776e-05, MSE(pi3): 6.217e-03\n",
      "Epoch 32400, Train loss: 1.585e+07, Test loss: 5.849e+08, MSE(e): 7.713e-04, MSE(pi1): 4.946e-01, MSE(pi2): 3.667e-04, MSE(pi3): 3.192e-02\n",
      "Epoch 32500, Train loss: 3.804e+06, Test loss: 5.205e+08, MSE(e): 2.301e-04, MSE(pi1): 9.374e-02, MSE(pi2): 1.356e-04, MSE(pi3): 5.664e-03\n",
      "Epoch 32600, Train loss: 7.079e+06, Test loss: 5.722e+08, MSE(e): 2.798e-04, MSE(pi1): 3.106e-01, MSE(pi2): 1.175e-04, MSE(pi3): 1.175e-02\n",
      "Epoch 32700, Train loss: 7.409e+06, Test loss: 5.548e+08, MSE(e): 4.555e-04, MSE(pi1): 2.099e-01, MSE(pi2): 2.104e-04, MSE(pi3): 7.557e-03\n",
      "Epoch 32800, Train loss: 1.101e+07, Test loss: 5.659e+08, MSE(e): 5.075e-04, MSE(pi1): 4.696e-01, MSE(pi2): 2.220e-04, MSE(pi3): 1.239e-02\n",
      "Epoch 32900, Train loss: 3.356e+07, Test loss: 5.974e+08, MSE(e): 3.129e-03, MSE(pi1): 1.293e-01, MSE(pi2): 1.408e-03, MSE(pi3): 9.777e-03\n",
      "Epoch 33000, Train loss: 8.900e+06, Test loss: 5.503e+08, MSE(e): 7.419e-04, MSE(pi1): 5.942e-02, MSE(pi2): 5.015e-04, MSE(pi3): 8.867e-03\n",
      "Epoch 33100, Train loss: 3.666e+06, Test loss: 5.467e+08, MSE(e): 2.739e-04, MSE(pi1): 2.307e-02, MSE(pi2): 1.309e-04, MSE(pi3): 6.962e-03\n",
      "Epoch 33200, Train loss: 2.174e+06, Test loss: 5.306e+08, MSE(e): 1.293e-04, MSE(pi1): 1.666e-02, MSE(pi2): 7.733e-05, MSE(pi3): 7.143e-03\n",
      "Epoch 33300, Train loss: 8.915e+06, Test loss: 4.992e+08, MSE(e): 7.526e-04, MSE(pi1): 8.109e-02, MSE(pi2): 3.690e-04, MSE(pi3): 5.777e-03\n",
      "Epoch 33400, Train loss: 2.343e+07, Test loss: 4.885e+08, MSE(e): 2.200e-03, MSE(pi1): 6.763e-02, MSE(pi2): 1.051e-03, MSE(pi3): 7.495e-03\n",
      "Epoch 33500, Train loss: 8.372e+06, Test loss: 5.547e+08, MSE(e): 2.877e-04, MSE(pi1): 4.189e-01, MSE(pi2): 1.795e-04, MSE(pi3): 1.306e-02\n",
      "Epoch 33600, Train loss: 7.942e+06, Test loss: 5.798e+08, MSE(e): 5.924e-04, MSE(pi1): 1.328e-01, MSE(pi2): 2.606e-04, MSE(pi3): 6.903e-03\n",
      "Epoch 33700, Train loss: 5.879e+06, Test loss: 5.278e+08, MSE(e): 2.702e-04, MSE(pi1): 2.655e-01, MSE(pi2): 1.552e-04, MSE(pi3): 5.224e-03\n",
      "Epoch 33800, Train loss: 9.800e+06, Test loss: 5.314e+08, MSE(e): 7.673e-04, MSE(pi1): 1.026e-01, MSE(pi2): 5.559e-04, MSE(pi3): 1.101e-02\n",
      "Epoch 33900, Train loss: 5.686e+06, Test loss: 5.343e+08, MSE(e): 2.134e-04, MSE(pi1): 2.778e-01, MSE(pi2): 1.047e-04, MSE(pi3): 7.739e-03\n",
      "Epoch 34000, Train loss: 7.477e+06, Test loss: 5.107e+08, MSE(e): 5.393e-04, MSE(pi1): 1.481e-01, MSE(pi2): 2.629e-04, MSE(pi3): 6.023e-03\n",
      "Epoch 34100, Train loss: 2.896e+06, Test loss: 5.304e+08, MSE(e): 1.742e-04, MSE(pi1): 3.033e-02, MSE(pi2): 9.524e-05, MSE(pi3): 8.506e-03\n",
      "Epoch 34200, Train loss: 1.030e+07, Test loss: 6.027e+08, MSE(e): 9.348e-04, MSE(pi1): 2.469e-02, MSE(pi2): 4.239e-04, MSE(pi3): 7.086e-03\n",
      "Epoch 34300, Train loss: 1.028e+07, Test loss: 5.673e+08, MSE(e): 2.548e-04, MSE(pi1): 7.237e-01, MSE(pi2): 1.661e-04, MSE(pi3): 4.964e-03\n",
      "Epoch 34400, Train loss: 6.011e+06, Test loss: 5.754e+08, MSE(e): 4.239e-04, MSE(pi1): 5.223e-02, MSE(pi2): 2.467e-04, MSE(pi3): 1.249e-02\n",
      "Epoch 34500, Train loss: 5.179e+06, Test loss: 5.215e+08, MSE(e): 3.676e-04, MSE(pi1): 9.165e-02, MSE(pi2): 1.980e-04, MSE(pi3): 5.864e-03\n",
      "Epoch 34600, Train loss: 4.835e+06, Test loss: 5.479e+08, MSE(e): 1.135e-04, MSE(pi1): 3.069e-01, MSE(pi2): 5.315e-05, MSE(pi3): 6.317e-03\n",
      "Epoch 34700, Train loss: 3.288e+06, Test loss: 5.437e+08, MSE(e): 1.059e-04, MSE(pi1): 1.584e-01, MSE(pi2): 5.209e-05, MSE(pi3): 6.449e-03\n",
      "Epoch 34800, Train loss: 1.664e+07, Test loss: 5.786e+08, MSE(e): 5.604e-04, MSE(pi1): 1.052e+00, MSE(pi2): 3.453e-04, MSE(pi3): 5.127e-03\n",
      "Epoch 34900, Train loss: 8.046e+06, Test loss: 5.303e+08, MSE(e): 2.951e-04, MSE(pi1): 4.562e-01, MSE(pi2): 2.137e-04, MSE(pi3): 5.335e-03\n",
      "Epoch 35000, Train loss: 2.665e+06, Test loss: 5.556e+08, MSE(e): 1.513e-04, MSE(pi1): 4.433e-02, MSE(pi2): 7.843e-05, MSE(pi3): 7.081e-03\n",
      "Epoch 35100, Train loss: 1.854e+06, Test loss: 5.462e+08, MSE(e): 9.705e-05, MSE(pi1): 2.544e-02, MSE(pi2): 5.279e-05, MSE(pi3): 6.296e-03\n",
      "Epoch 35200, Train loss: 4.229e+06, Test loss: 5.735e+08, MSE(e): 3.240e-04, MSE(pi1): 2.478e-02, MSE(pi2): 1.585e-04, MSE(pi3): 7.414e-03\n",
      "Epoch 35300, Train loss: 1.143e+07, Test loss: 5.135e+08, MSE(e): 8.846e-04, MSE(pi1): 2.043e-01, MSE(pi2): 4.029e-04, MSE(pi3): 5.428e-03\n",
      "Epoch 35400, Train loss: 1.058e+07, Test loss: 5.431e+08, MSE(e): 1.786e-04, MSE(pi1): 7.773e-01, MSE(pi2): 7.650e-05, MSE(pi3): 1.026e-02\n",
      "Epoch 35500, Train loss: 7.981e+06, Test loss: 5.663e+08, MSE(e): 1.714e-04, MSE(pi1): 2.290e-01, MSE(pi2): 1.014e-04, MSE(pi3): 3.977e-02\n",
      "Epoch 35600, Train loss: 1.626e+06, Test loss: 5.459e+08, MSE(e): 8.518e-05, MSE(pi1): 1.484e-02, MSE(pi2): 4.589e-05, MSE(pi3): 6.255e-03\n",
      "Epoch 35700, Train loss: 6.533e+06, Test loss: 5.535e+08, MSE(e): 1.413e-04, MSE(pi1): 4.539e-01, MSE(pi2): 5.825e-05, MSE(pi3): 5.805e-03\n",
      "Epoch 35800, Train loss: 6.344e+06, Test loss: 5.642e+08, MSE(e): 1.820e-04, MSE(pi1): 3.682e-01, MSE(pi2): 7.348e-05, MSE(pi3): 8.426e-03\n",
      "Epoch 35900, Train loss: 8.495e+06, Test loss: 5.168e+08, MSE(e): 6.655e-04, MSE(pi1): 1.157e-01, MSE(pi2): 3.181e-04, MSE(pi3): 6.829e-03\n",
      "Epoch 36000, Train loss: 2.407e+06, Test loss: 5.535e+08, MSE(e): 1.029e-04, MSE(pi1): 8.159e-02, MSE(pi2): 5.173e-05, MSE(pi3): 5.614e-03\n",
      "Epoch 36100, Train loss: 3.126e+06, Test loss: 5.516e+08, MSE(e): 1.623e-04, MSE(pi1): 7.731e-02, MSE(pi2): 9.570e-05, MSE(pi3): 7.292e-03\n",
      "Epoch 36200, Train loss: 1.129e+07, Test loss: 5.093e+08, MSE(e): 1.032e-03, MSE(pi1): 3.678e-02, MSE(pi2): 5.304e-04, MSE(pi3): 6.047e-03\n",
      "Epoch 36300, Train loss: 4.783e+06, Test loss: 5.778e+08, MSE(e): 3.082e-04, MSE(pi1): 1.103e-01, MSE(pi2): 1.406e-04, MSE(pi3): 5.974e-03\n",
      "Epoch 36400, Train loss: 1.748e+07, Test loss: 5.323e+08, MSE(e): 8.476e-04, MSE(pi1): 7.188e-01, MSE(pi2): 4.115e-04, MSE(pi3): 1.814e-02\n",
      "Epoch 36500, Train loss: 7.000e+06, Test loss: 5.760e+08, MSE(e): 5.675e-04, MSE(pi1): 4.870e-02, MSE(pi2): 2.640e-04, MSE(pi3): 8.376e-03\n",
      "Epoch 36600, Train loss: 2.625e+06, Test loss: 5.491e+08, MSE(e): 1.020e-04, MSE(pi1): 9.449e-02, MSE(pi2): 5.375e-05, MSE(pi3): 6.602e-03\n",
      "Epoch 36700, Train loss: 6.259e+06, Test loss: 5.494e+08, MSE(e): 1.779e-04, MSE(pi1): 3.875e-01, MSE(pi2): 7.993e-05, MSE(pi3): 6.056e-03\n",
      "Epoch 36800, Train loss: 1.296e+07, Test loss: 5.802e+08, MSE(e): 2.370e-04, MSE(pi1): 9.786e-01, MSE(pi2): 9.711e-05, MSE(pi3): 8.083e-03\n",
      "Epoch 36900, Train loss: 7.580e+06, Test loss: 5.708e+08, MSE(e): 2.177e-04, MSE(pi1): 4.373e-01, MSE(pi2): 1.364e-04, MSE(pi3): 1.030e-02\n",
      "Epoch 37000, Train loss: 5.314e+06, Test loss: 5.698e+08, MSE(e): 3.862e-04, MSE(pi1): 3.316e-02, MSE(pi2): 2.504e-04, MSE(pi3): 1.121e-02\n",
      "Epoch 37100, Train loss: 3.485e+06, Test loss: 5.445e+08, MSE(e): 1.426e-04, MSE(pi1): 1.531e-01, MSE(pi2): 7.270e-05, MSE(pi3): 5.276e-03\n",
      "Epoch 37200, Train loss: 3.091e+06, Test loss: 5.567e+08, MSE(e): 1.375e-04, MSE(pi1): 1.140e-01, MSE(pi2): 8.515e-05, MSE(pi3): 5.752e-03\n",
      "Epoch 37300, Train loss: 4.746e+06, Test loss: 5.504e+08, MSE(e): 1.336e-04, MSE(pi1): 2.540e-01, MSE(pi2): 8.523e-05, MSE(pi3): 8.698e-03\n",
      "Epoch 37400, Train loss: 2.222e+06, Test loss: 5.486e+08, MSE(e): 1.046e-04, MSE(pi1): 5.248e-02, MSE(pi2): 6.187e-05, MSE(pi3): 6.513e-03\n",
      "Epoch 37500, Train loss: 8.171e+06, Test loss: 5.723e+08, MSE(e): 2.065e-04, MSE(pi1): 4.627e-01, MSE(pi2): 1.059e-04, MSE(pi3): 1.480e-02\n",
      "Epoch 37600, Train loss: 3.295e+06, Test loss: 5.549e+08, MSE(e): 1.418e-04, MSE(pi1): 1.177e-01, MSE(pi2): 7.441e-05, MSE(pi3): 6.992e-03\n",
      "Epoch 37700, Train loss: 1.061e+07, Test loss: 5.160e+08, MSE(e): 9.020e-04, MSE(pi1): 1.154e-01, MSE(pi2): 4.205e-04, MSE(pi3): 4.337e-03\n",
      "Epoch 37800, Train loss: 4.617e+06, Test loss: 5.499e+08, MSE(e): 2.529e-04, MSE(pi1): 1.328e-01, MSE(pi2): 1.331e-04, MSE(pi3): 7.598e-03\n",
      "Epoch 37900, Train loss: 9.750e+06, Test loss: 5.443e+08, MSE(e): 4.034e-04, MSE(pi1): 5.109e-01, MSE(pi2): 1.812e-04, MSE(pi3): 6.072e-03\n",
      "Epoch 38000, Train loss: 5.266e+07, Test loss: 6.799e+08, MSE(e): 3.963e-03, MSE(pi1): 1.217e+00, MSE(pi2): 1.804e-03, MSE(pi3): 8.615e-03\n",
      "Epoch 38100, Train loss: 1.925e+07, Test loss: 6.106e+08, MSE(e): 1.783e-03, MSE(pi1): 3.896e-02, MSE(pi2): 7.743e-04, MSE(pi3): 1.038e-02\n",
      "Epoch 38200, Train loss: 1.421e+07, Test loss: 5.466e+08, MSE(e): 9.864e-04, MSE(pi1): 3.836e-01, MSE(pi2): 5.332e-04, MSE(pi3): 5.124e-03\n",
      "Epoch 38300, Train loss: 3.303e+07, Test loss: 5.077e+08, MSE(e): 3.050e-03, MSE(pi1): 2.021e-01, MSE(pi2): 1.437e-03, MSE(pi3): 5.073e-03\n",
      "Epoch 38400, Train loss: 6.831e+06, Test loss: 5.904e+08, MSE(e): 3.812e-04, MSE(pi1): 7.875e-02, MSE(pi2): 2.396e-04, MSE(pi3): 2.232e-02\n",
      "Epoch 38500, Train loss: 3.739e+07, Test loss: 6.460e+08, MSE(e): 2.445e-03, MSE(pi1): 1.118e+00, MSE(pi2): 1.205e-03, MSE(pi3): 1.755e-02\n",
      "Epoch 38600, Train loss: 6.465e+06, Test loss: 5.868e+08, MSE(e): 1.664e-04, MSE(pi1): 4.079e-01, MSE(pi2): 7.180e-05, MSE(pi3): 7.223e-03\n",
      "Epoch 38700, Train loss: 2.459e+07, Test loss: 5.586e+08, MSE(e): 7.415e-04, MSE(pi1): 1.645e+00, MSE(pi2): 5.584e-04, MSE(pi3): 7.277e-03\n",
      "Epoch 38800, Train loss: 3.462e+07, Test loss: 6.617e+08, MSE(e): 2.941e-03, MSE(pi1): 3.806e-01, MSE(pi2): 1.357e-03, MSE(pi3): 1.403e-02\n",
      "Epoch 38900, Train loss: 4.537e+07, Test loss: 5.230e+08, MSE(e): 3.905e-03, MSE(pi1): 5.660e-01, MSE(pi2): 1.750e-03, MSE(pi3): 6.598e-03\n",
      "Epoch 39000, Train loss: 2.409e+06, Test loss: 5.726e+08, MSE(e): 1.135e-04, MSE(pi1): 6.585e-02, MSE(pi2): 5.175e-05, MSE(pi3): 6.157e-03\n",
      "Epoch 39100, Train loss: 2.755e+06, Test loss: 5.524e+08, MSE(e): 1.411e-04, MSE(pi1): 8.128e-02, MSE(pi2): 7.550e-05, MSE(pi3): 5.307e-03\n",
      "Epoch 39200, Train loss: 6.599e+06, Test loss: 5.718e+08, MSE(e): 1.627e-04, MSE(pi1): 4.245e-01, MSE(pi2): 6.782e-05, MSE(pi3): 7.260e-03\n",
      "Epoch 39300, Train loss: 9.343e+06, Test loss: 5.703e+08, MSE(e): 1.982e-04, MSE(pi1): 6.851e-01, MSE(pi2): 1.018e-04, MSE(pi3): 5.093e-03\n",
      "Epoch 39400, Train loss: 2.425e+06, Test loss: 5.680e+08, MSE(e): 9.555e-05, MSE(pi1): 9.089e-02, MSE(pi2): 5.259e-05, MSE(pi3): 5.607e-03\n",
      "Epoch 39500, Train loss: 5.878e+06, Test loss: 5.594e+08, MSE(e): 2.437e-04, MSE(pi1): 2.187e-01, MSE(pi2): 1.641e-04, MSE(pi3): 1.255e-02\n",
      "Epoch 39600, Train loss: 8.665e+06, Test loss: 5.462e+08, MSE(e): 5.643e-04, MSE(pi1): 2.170e-01, MSE(pi2): 2.837e-04, MSE(pi3): 8.516e-03\n",
      "Epoch 39700, Train loss: 2.082e+06, Test loss: 5.689e+08, MSE(e): 9.444e-05, MSE(pi1): 3.313e-02, MSE(pi2): 5.148e-05, MSE(pi3): 8.065e-03\n",
      "Epoch 39800, Train loss: 2.358e+06, Test loss: 5.546e+08, MSE(e): 1.500e-04, MSE(pi1): 3.028e-02, MSE(pi2): 8.507e-05, MSE(pi3): 5.556e-03\n",
      "Epoch 39900, Train loss: 5.619e+06, Test loss: 5.775e+08, MSE(e): 1.219e-04, MSE(pi1): 3.771e-01, MSE(pi2): 5.978e-05, MSE(pi3): 6.287e-03\n",
      "Epoch 40000, Train loss: 3.246e+06, Test loss: 5.787e+08, MSE(e): 1.249e-04, MSE(pi1): 1.417e-01, MSE(pi2): 6.028e-05, MSE(pi3): 5.804e-03\n",
      "Epoch 40100, Train loss: 4.693e+06, Test loss: 5.735e+08, MSE(e): 1.366e-04, MSE(pi1): 2.587e-01, MSE(pi2): 7.519e-05, MSE(pi3): 7.405e-03\n",
      "Epoch 40200, Train loss: 4.072e+06, Test loss: 5.652e+08, MSE(e): 2.435e-04, MSE(pi1): 3.615e-02, MSE(pi2): 1.582e-04, MSE(pi3): 1.276e-02\n",
      "Epoch 40300, Train loss: 6.957e+06, Test loss: 5.792e+08, MSE(e): 1.331e-04, MSE(pi1): 4.305e-01, MSE(pi2): 6.760e-05, MSE(pi3): 1.322e-02\n",
      "Epoch 40400, Train loss: 1.637e+06, Test loss: 5.663e+08, MSE(e): 8.727e-05, MSE(pi1): 1.627e-02, MSE(pi2): 4.799e-05, MSE(pi3): 6.017e-03\n",
      "Epoch 40500, Train loss: 7.044e+06, Test loss: 6.034e+08, MSE(e): 3.146e-04, MSE(pi1): 3.154e-01, MSE(pi2): 1.305e-04, MSE(pi3): 7.439e-03\n",
      "Epoch 40600, Train loss: 2.940e+07, Test loss: 5.392e+08, MSE(e): 1.929e-03, MSE(pi1): 9.272e-01, MSE(pi2): 9.061e-04, MSE(pi3): 8.318e-03\n",
      "Epoch 40700, Train loss: 7.150e+06, Test loss: 6.209e+08, MSE(e): 6.111e-04, MSE(pi1): 4.590e-02, MSE(pi2): 2.922e-04, MSE(pi3): 5.804e-03\n",
      "Epoch 40800, Train loss: 3.665e+07, Test loss: 5.690e+08, MSE(e): 1.507e-03, MSE(pi1): 2.092e+00, MSE(pi2): 6.589e-04, MSE(pi3): 6.690e-03\n",
      "Epoch 40900, Train loss: 2.297e+07, Test loss: 5.303e+08, MSE(e): 1.896e-03, MSE(pi1): 3.458e-01, MSE(pi2): 8.745e-04, MSE(pi3): 5.504e-03\n",
      "Epoch 41000, Train loss: 2.339e+07, Test loss: 6.391e+08, MSE(e): 1.146e-03, MSE(pi1): 1.082e+00, MSE(pi2): 4.673e-04, MSE(pi3): 1.116e-02\n",
      "Epoch 41100, Train loss: 5.080e+06, Test loss: 5.523e+08, MSE(e): 3.490e-04, MSE(pi1): 6.928e-02, MSE(pi2): 1.964e-04, MSE(pi3): 8.975e-03\n",
      "Epoch 41200, Train loss: 1.882e+06, Test loss: 5.776e+08, MSE(e): 8.724e-05, MSE(pi1): 4.005e-02, MSE(pi2): 4.551e-05, MSE(pi3): 6.093e-03\n",
      "Epoch 41300, Train loss: 2.674e+07, Test loss: 6.404e+08, MSE(e): 4.238e-04, MSE(pi1): 2.162e+00, MSE(pi2): 1.966e-04, MSE(pi3): 8.819e-03\n",
      "Epoch 41400, Train loss: 2.757e+06, Test loss: 5.852e+08, MSE(e): 1.274e-04, MSE(pi1): 9.190e-02, MSE(pi2): 6.162e-05, MSE(pi3): 5.643e-03\n",
      "Epoch 41500, Train loss: 5.832e+07, Test loss: 6.697e+08, MSE(e): 6.408e-04, MSE(pi1): 5.074e+00, MSE(pi2): 1.883e-04, MSE(pi3): 1.168e-02\n",
      "Epoch 41600, Train loss: 1.263e+07, Test loss: 6.319e+08, MSE(e): 8.379e-04, MSE(pi1): 3.171e-01, MSE(pi2): 4.493e-04, MSE(pi3): 1.079e-02\n",
      "Epoch 41700, Train loss: 1.947e+07, Test loss: 5.633e+08, MSE(e): 1.596e-03, MSE(pi1): 2.211e-01, MSE(pi2): 7.901e-04, MSE(pi3): 1.300e-02\n",
      "Epoch 41800, Train loss: 1.615e+07, Test loss: 6.153e+08, MSE(e): 1.421e-03, MSE(pi1): 1.029e-01, MSE(pi2): 6.011e-04, MSE(pi3): 9.079e-03\n",
      "Epoch 41900, Train loss: 5.582e+06, Test loss: 6.118e+08, MSE(e): 4.070e-04, MSE(pi1): 8.053e-02, MSE(pi2): 1.813e-04, MSE(pi3): 7.065e-03\n",
      "Epoch 42000, Train loss: 1.909e+06, Test loss: 5.819e+08, MSE(e): 9.860e-05, MSE(pi1): 2.732e-02, MSE(pi2): 5.307e-05, MSE(pi3): 6.500e-03\n",
      "Epoch 42100, Train loss: 2.633e+06, Test loss: 5.747e+08, MSE(e): 1.380e-04, MSE(pi1): 6.866e-02, MSE(pi2): 7.682e-05, MSE(pi3): 5.664e-03\n",
      "Epoch 42200, Train loss: 3.811e+06, Test loss: 5.989e+08, MSE(e): 2.456e-04, MSE(pi1): 7.137e-02, MSE(pi2): 1.111e-04, MSE(pi3): 6.418e-03\n",
      "Epoch 42300, Train loss: 4.416e+06, Test loss: 5.883e+08, MSE(e): 2.967e-04, MSE(pi1): 5.308e-02, MSE(pi2): 1.828e-04, MSE(pi3): 9.188e-03\n",
      "Epoch 42400, Train loss: 1.863e+06, Test loss: 5.765e+08, MSE(e): 8.863e-05, MSE(pi1): 3.574e-02, MSE(pi2): 4.730e-05, MSE(pi3): 6.197e-03\n",
      "Epoch 42500, Train loss: 2.658e+06, Test loss: 5.733e+08, MSE(e): 1.250e-04, MSE(pi1): 8.198e-02, MSE(pi2): 8.042e-05, MSE(pi3): 5.890e-03\n",
      "Epoch 42600, Train loss: 9.713e+06, Test loss: 5.607e+08, MSE(e): 5.722e-04, MSE(pi1): 3.487e-01, MSE(pi2): 3.350e-04, MSE(pi3): 5.041e-03\n",
      "Epoch 42700, Train loss: 2.344e+07, Test loss: 6.440e+08, MSE(e): 5.278e-04, MSE(pi1): 1.744e+00, MSE(pi2): 1.822e-04, MSE(pi3): 7.213e-03\n",
      "Epoch 42800, Train loss: 4.712e+06, Test loss: 5.761e+08, MSE(e): 2.072e-04, MSE(pi1): 2.116e-01, MSE(pi2): 1.105e-04, MSE(pi3): 5.233e-03\n",
      "Epoch 42900, Train loss: 9.424e+07, Test loss: 5.517e+08, MSE(e): 7.915e-03, MSE(pi1): 1.242e+00, MSE(pi2): 3.591e-03, MSE(pi3): 2.677e-02\n",
      "Epoch 43000, Train loss: 3.840e+06, Test loss: 5.778e+08, MSE(e): 1.547e-04, MSE(pi1): 1.739e-01, MSE(pi2): 1.051e-04, MSE(pi3): 5.537e-03\n",
      "Epoch 43100, Train loss: 8.726e+06, Test loss: 6.228e+08, MSE(e): 6.770e-04, MSE(pi1): 1.116e-01, MSE(pi2): 2.843e-04, MSE(pi3): 8.398e-03\n",
      "Epoch 43200, Train loss: 4.205e+07, Test loss: 5.366e+08, MSE(e): 3.866e-03, MSE(pi1): 2.877e-01, MSE(pi2): 1.760e-03, MSE(pi3): 5.060e-03\n",
      "Epoch 43300, Train loss: 6.231e+06, Test loss: 5.932e+08, MSE(e): 1.861e-04, MSE(pi1): 3.461e-01, MSE(pi2): 1.135e-04, MSE(pi3): 9.095e-03\n",
      "Epoch 43400, Train loss: 2.836e+06, Test loss: 5.826e+08, MSE(e): 9.737e-05, MSE(pi1): 1.125e-01, MSE(pi2): 5.655e-05, MSE(pi3): 7.376e-03\n",
      "Epoch 43500, Train loss: 1.144e+07, Test loss: 5.964e+08, MSE(e): 9.252e-04, MSE(pi1): 1.373e-01, MSE(pi2): 4.288e-04, MSE(pi3): 8.123e-03\n",
      "Epoch 43600, Train loss: 7.379e+06, Test loss: 6.394e+08, MSE(e): 6.284e-04, MSE(pi1): 2.974e-02, MSE(pi2): 2.659e-04, MSE(pi3): 7.984e-03\n",
      "Epoch 43700, Train loss: 3.051e+06, Test loss: 6.026e+08, MSE(e): 1.654e-04, MSE(pi1): 7.593e-02, MSE(pi2): 7.883e-05, MSE(pi3): 6.372e-03\n",
      "Epoch 43800, Train loss: 8.162e+06, Test loss: 6.095e+08, MSE(e): 1.408e-04, MSE(pi1): 6.274e-01, MSE(pi2): 5.774e-05, MSE(pi3): 4.802e-03\n",
      "Epoch 43900, Train loss: 6.990e+06, Test loss: 6.274e+08, MSE(e): 3.099e-04, MSE(pi1): 3.070e-01, MSE(pi2): 1.674e-04, MSE(pi3): 8.210e-03\n",
      "Epoch 44000, Train loss: 6.596e+06, Test loss: 6.042e+08, MSE(e): 1.518e-04, MSE(pi1): 4.168e-01, MSE(pi2): 8.490e-05, MSE(pi3): 9.092e-03\n",
      "Epoch 44100, Train loss: 7.117e+06, Test loss: 5.633e+08, MSE(e): 5.322e-04, MSE(pi1): 7.716e-02, MSE(pi2): 2.556e-04, MSE(pi3): 1.023e-02\n",
      "Epoch 44200, Train loss: 6.382e+07, Test loss: 7.646e+08, MSE(e): 6.056e-03, MSE(pi1): 2.189e-01, MSE(pi2): 2.700e-03, MSE(pi3): 1.064e-02\n",
      "Epoch 44300, Train loss: 3.099e+06, Test loss: 5.802e+08, MSE(e): 1.174e-04, MSE(pi1): 1.388e-01, MSE(pi2): 6.916e-05, MSE(pi3): 5.376e-03\n",
      "Epoch 44400, Train loss: 6.601e+06, Test loss: 6.276e+08, MSE(e): 5.515e-04, MSE(pi1): 3.462e-02, MSE(pi2): 2.435e-04, MSE(pi3): 7.401e-03\n",
      "Epoch 44500, Train loss: 2.515e+06, Test loss: 5.993e+08, MSE(e): 1.189e-04, MSE(pi1): 7.275e-02, MSE(pi2): 6.313e-05, MSE(pi3): 5.981e-03\n",
      "Epoch 44600, Train loss: 4.487e+06, Test loss: 5.997e+08, MSE(e): 1.281e-04, MSE(pi1): 2.584e-01, MSE(pi2): 6.000e-05, MSE(pi3): 6.224e-03\n",
      "Epoch 44700, Train loss: 5.841e+07, Test loss: 6.690e+08, MSE(e): 1.081e-03, MSE(pi1): 4.663e+00, MSE(pi2): 4.539e-04, MSE(pi3): 9.790e-03\n",
      "Epoch 44800, Train loss: 5.047e+06, Test loss: 6.047e+08, MSE(e): 3.469e-04, MSE(pi1): 8.878e-02, MSE(pi2): 1.892e-04, MSE(pi3): 6.906e-03\n",
      "Epoch 44900, Train loss: 5.485e+06, Test loss: 5.806e+08, MSE(e): 2.634e-04, MSE(pi1): 1.894e-01, MSE(pi2): 1.608e-04, MSE(pi3): 9.568e-03\n",
      "Epoch 45000, Train loss: 8.359e+06, Test loss: 5.723e+08, MSE(e): 7.250e-04, MSE(pi1): 5.034e-02, MSE(pi2): 5.418e-04, MSE(pi3): 6.056e-03\n",
      "Epoch 45100, Train loss: 6.118e+06, Test loss: 5.709e+08, MSE(e): 4.274e-04, MSE(pi1): 1.289e-01, MSE(pi2): 2.236e-04, MSE(pi3): 5.543e-03\n",
      "Epoch 45200, Train loss: 5.967e+06, Test loss: 5.921e+08, MSE(e): 1.914e-04, MSE(pi1): 3.519e-01, MSE(pi2): 1.178e-04, MSE(pi3): 5.347e-03\n",
      "Epoch 45300, Train loss: 2.068e+07, Test loss: 5.690e+08, MSE(e): 7.060e-04, MSE(pi1): 1.287e+00, MSE(pi2): 4.731e-04, MSE(pi3): 7.498e-03\n",
      "Epoch 45400, Train loss: 2.305e+06, Test loss: 6.056e+08, MSE(e): 1.346e-04, MSE(pi1): 2.419e-02, MSE(pi2): 6.787e-05, MSE(pi3): 7.169e-03\n",
      "Epoch 45500, Train loss: 9.708e+06, Test loss: 6.145e+08, MSE(e): 1.562e-04, MSE(pi1): 7.581e-01, MSE(pi2): 6.074e-05, MSE(pi3): 5.652e-03\n",
      "Epoch 45600, Train loss: 4.213e+06, Test loss: 5.906e+08, MSE(e): 1.415e-04, MSE(pi1): 2.178e-01, MSE(pi2): 9.229e-05, MSE(pi3): 6.194e-03\n",
      "Epoch 45700, Train loss: 1.001e+07, Test loss: 5.905e+08, MSE(e): 2.659e-04, MSE(pi1): 6.856e-01, MSE(pi2): 1.767e-04, MSE(pi3): 4.908e-03\n",
      "Epoch 45800, Train loss: 8.794e+06, Test loss: 6.321e+08, MSE(e): 4.473e-04, MSE(pi1): 3.635e-01, MSE(pi2): 2.127e-04, MSE(pi3): 6.854e-03\n",
      "Epoch 45900, Train loss: 2.357e+06, Test loss: 6.024e+08, MSE(e): 1.097e-04, MSE(pi1): 7.054e-02, MSE(pi2): 5.970e-05, MSE(pi3): 5.548e-03\n",
      "Epoch 46000, Train loss: 4.626e+06, Test loss: 5.983e+08, MSE(e): 1.236e-04, MSE(pi1): 2.459e-01, MSE(pi2): 6.501e-05, MSE(pi3): 9.305e-03\n",
      "Epoch 46100, Train loss: 1.624e+07, Test loss: 5.614e+08, MSE(e): 1.511e-03, MSE(pi1): 2.847e-02, MSE(pi2): 6.932e-04, MSE(pi3): 8.475e-03\n",
      "Epoch 46200, Train loss: 5.445e+06, Test loss: 6.175e+08, MSE(e): 2.589e-04, MSE(pi1): 2.028e-01, MSE(pi2): 1.775e-04, MSE(pi3): 8.278e-03\n",
      "Epoch 46300, Train loss: 2.309e+07, Test loss: 5.564e+08, MSE(e): 1.886e-03, MSE(pi1): 3.734e-01, MSE(pi2): 9.331e-04, MSE(pi3): 4.935e-03\n",
      "Epoch 46400, Train loss: 4.123e+06, Test loss: 5.983e+08, MSE(e): 1.019e-04, MSE(pi1): 2.538e-01, MSE(pi2): 5.025e-05, MSE(pi3): 5.668e-03\n",
      "Epoch 46500, Train loss: 2.740e+06, Test loss: 5.992e+08, MSE(e): 8.658e-05, MSE(pi1): 1.227e-01, MSE(pi2): 4.550e-05, MSE(pi3): 6.470e-03\n",
      "Epoch 46600, Train loss: 2.855e+06, Test loss: 5.905e+08, MSE(e): 9.512e-05, MSE(pi1): 1.320e-01, MSE(pi2): 5.193e-05, MSE(pi3): 5.840e-03\n",
      "Epoch 46700, Train loss: 7.905e+06, Test loss: 6.122e+08, MSE(e): 2.838e-04, MSE(pi1): 4.304e-01, MSE(pi2): 1.900e-04, MSE(pi3): 7.640e-03\n",
      "Epoch 46800, Train loss: 4.093e+06, Test loss: 5.886e+08, MSE(e): 1.736e-04, MSE(pi1): 1.810e-01, MSE(pi2): 9.607e-05, MSE(pi3): 5.474e-03\n",
      "Epoch 46900, Train loss: 2.877e+06, Test loss: 5.988e+08, MSE(e): 9.222e-05, MSE(pi1): 1.105e-01, MSE(pi2): 4.787e-05, MSE(pi3): 8.491e-03\n",
      "Epoch 47000, Train loss: 1.166e+07, Test loss: 6.459e+08, MSE(e): 4.454e-04, MSE(pi1): 6.098e-01, MSE(pi2): 2.477e-04, MSE(pi3): 1.109e-02\n",
      "Epoch 47100, Train loss: 2.116e+07, Test loss: 6.052e+08, MSE(e): 1.933e-03, MSE(pi1): 1.048e-01, MSE(pi2): 1.201e-03, MSE(pi3): 7.853e-03\n",
      "Epoch 47200, Train loss: 2.776e+06, Test loss: 6.183e+08, MSE(e): 1.830e-04, MSE(pi1): 7.049e-03, MSE(pi2): 8.192e-05, MSE(pi3): 8.754e-03\n",
      "Epoch 47300, Train loss: 2.368e+06, Test loss: 6.083e+08, MSE(e): 1.411e-04, MSE(pi1): 3.882e-02, MSE(pi2): 6.998e-05, MSE(pi3): 5.693e-03\n",
      "Epoch 47400, Train loss: 7.184e+06, Test loss: 6.194e+08, MSE(e): 3.128e-04, MSE(pi1): 3.480e-01, MSE(pi2): 1.995e-04, MSE(pi3): 5.767e-03\n",
      "Epoch 47500, Train loss: 4.687e+06, Test loss: 5.811e+08, MSE(e): 2.737e-04, MSE(pi1): 1.399e-01, MSE(pi2): 1.272e-04, MSE(pi3): 5.509e-03\n",
      "Epoch 47600, Train loss: 3.203e+06, Test loss: 6.182e+08, MSE(e): 1.846e-04, MSE(pi1): 4.080e-02, MSE(pi2): 9.696e-05, MSE(pi3): 9.491e-03\n",
      "Epoch 47700, Train loss: 3.839e+06, Test loss: 6.265e+08, MSE(e): 2.532e-04, MSE(pi1): 5.366e-02, MSE(pi2): 1.248e-04, MSE(pi3): 7.702e-03\n",
      "Epoch 47800, Train loss: 2.002e+06, Test loss: 5.986e+08, MSE(e): 9.803e-05, MSE(pi1): 4.313e-02, MSE(pi2): 5.235e-05, MSE(pi3): 5.900e-03\n",
      "Epoch 47900, Train loss: 5.992e+06, Test loss: 5.760e+08, MSE(e): 4.781e-04, MSE(pi1): 3.236e-02, MSE(pi2): 2.557e-04, MSE(pi3): 8.876e-03\n",
      "Epoch 48000, Train loss: 4.027e+06, Test loss: 6.080e+08, MSE(e): 1.660e-04, MSE(pi1): 1.609e-01, MSE(pi2): 7.550e-05, MSE(pi3): 7.586e-03\n",
      "Epoch 48100, Train loss: 3.146e+07, Test loss: 5.615e+08, MSE(e): 2.919e-03, MSE(pi1): 1.774e-01, MSE(pi2): 1.379e-03, MSE(pi3): 4.933e-03\n",
      "Epoch 48200, Train loss: 3.867e+07, Test loss: 7.147e+08, MSE(e): 3.025e-03, MSE(pi1): 7.273e-01, MSE(pi2): 1.360e-03, MSE(pi3): 1.144e-02\n",
      "Epoch 48300, Train loss: 9.131e+06, Test loss: 5.871e+08, MSE(e): 4.701e-04, MSE(pi1): 3.519e-01, MSE(pi2): 2.491e-04, MSE(pi3): 9.118e-03\n",
      "Epoch 48400, Train loss: 3.670e+06, Test loss: 5.944e+08, MSE(e): 1.684e-04, MSE(pi1): 1.372e-01, MSE(pi2): 9.432e-05, MSE(pi3): 6.144e-03\n",
      "Epoch 48500, Train loss: 1.275e+07, Test loss: 6.554e+08, MSE(e): 1.135e-03, MSE(pi1): 7.372e-02, MSE(pi2): 4.983e-04, MSE(pi3): 6.622e-03\n",
      "Epoch 48600, Train loss: 4.150e+06, Test loss: 5.855e+08, MSE(e): 3.345e-04, MSE(pi1): 1.967e-02, MSE(pi2): 1.661e-04, MSE(pi3): 6.082e-03\n",
      "Epoch 48700, Train loss: 9.665e+06, Test loss: 5.756e+08, MSE(e): 8.663e-04, MSE(pi1): 4.580e-02, MSE(pi2): 4.083e-04, MSE(pi3): 5.442e-03\n",
      "Epoch 48800, Train loss: 1.871e+06, Test loss: 6.022e+08, MSE(e): 1.049e-04, MSE(pi1): 1.024e-02, MSE(pi2): 5.722e-05, MSE(pi3): 7.203e-03\n",
      "Epoch 48900, Train loss: 4.499e+06, Test loss: 6.160e+08, MSE(e): 1.259e-04, MSE(pi1): 2.526e-01, MSE(pi2): 6.409e-05, MSE(pi3): 7.134e-03\n",
      "Epoch 49000, Train loss: 1.978e+06, Test loss: 6.059e+08, MSE(e): 7.747e-05, MSE(pi1): 6.248e-02, MSE(pi2): 4.154e-05, MSE(pi3): 5.784e-03\n",
      "Epoch 49100, Train loss: 1.211e+07, Test loss: 6.136e+08, MSE(e): 1.928e-04, MSE(pi1): 9.301e-01, MSE(pi2): 9.721e-05, MSE(pi3): 8.774e-03\n",
      "Epoch 49200, Train loss: 4.167e+07, Test loss: 6.816e+08, MSE(e): 3.720e-03, MSE(pi1): 3.921e-01, MSE(pi2): 1.595e-03, MSE(pi3): 5.522e-03\n",
      "Epoch 49300, Train loss: 1.602e+06, Test loss: 6.056e+08, MSE(e): 7.841e-05, MSE(pi1): 1.772e-02, MSE(pi2): 4.509e-05, MSE(pi3): 6.407e-03\n",
      "Epoch 49400, Train loss: 8.242e+06, Test loss: 6.605e+08, MSE(e): 6.770e-04, MSE(pi1): 3.758e-02, MSE(pi2): 3.057e-04, MSE(pi3): 1.096e-02\n",
      "Epoch 49500, Train loss: 4.221e+06, Test loss: 6.270e+08, MSE(e): 2.026e-04, MSE(pi1): 1.480e-01, MSE(pi2): 9.971e-05, MSE(pi3): 7.155e-03\n",
      "Epoch 49600, Train loss: 7.429e+06, Test loss: 6.493e+08, MSE(e): 2.617e-04, MSE(pi1): 4.111e-01, MSE(pi2): 1.195e-04, MSE(pi3): 7.005e-03\n",
      "Epoch 49700, Train loss: 6.466e+07, Test loss: 6.127e+08, MSE(e): 6.233e-03, MSE(pi1): 9.757e-02, MSE(pi2): 3.154e-03, MSE(pi3): 1.357e-02\n",
      "Epoch 49800, Train loss: 1.587e+06, Test loss: 6.103e+08, MSE(e): 7.381e-05, MSE(pi1): 2.212e-02, MSE(pi2): 4.057e-05, MSE(pi3): 6.274e-03\n",
      "Epoch 49900, Train loss: 3.977e+06, Test loss: 6.197e+08, MSE(e): 9.905e-05, MSE(pi1): 2.276e-01, MSE(pi2): 4.791e-05, MSE(pi3): 7.107e-03\n",
      "Epoch 50000, Train loss: 2.093e+06, Test loss: 6.031e+08, MSE(e): 1.148e-04, MSE(pi1): 3.580e-02, MSE(pi2): 6.216e-05, MSE(pi3): 5.874e-03\n",
      "Epoch 50100, Train loss: 3.108e+06, Test loss: 6.330e+08, MSE(e): 2.191e-04, MSE(pi1): 1.952e-02, MSE(pi2): 1.035e-04, MSE(pi3): 7.208e-03\n",
      "Epoch 50200, Train loss: 9.225e+06, Test loss: 6.234e+08, MSE(e): 1.519e-04, MSE(pi1): 6.874e-01, MSE(pi2): 6.663e-05, MSE(pi3): 8.316e-03\n",
      "Epoch 50300, Train loss: 9.471e+06, Test loss: 5.959e+08, MSE(e): 7.177e-04, MSE(pi1): 1.257e-01, MSE(pi2): 3.685e-04, MSE(pi3): 1.037e-02\n",
      "Epoch 50400, Train loss: 5.368e+06, Test loss: 6.048e+08, MSE(e): 4.007e-04, MSE(pi1): 7.487e-02, MSE(pi2): 2.161e-04, MSE(pi3): 6.123e-03\n",
      "Epoch 50500, Train loss: 3.521e+07, Test loss: 5.794e+08, MSE(e): 3.058e-03, MSE(pi1): 4.150e-01, MSE(pi2): 1.439e-03, MSE(pi3): 4.782e-03\n",
      "Epoch 50600, Train loss: 5.243e+06, Test loss: 6.001e+08, MSE(e): 3.482e-04, MSE(pi1): 9.805e-02, MSE(pi2): 1.770e-04, MSE(pi3): 7.803e-03\n",
      "Epoch 50700, Train loss: 4.614e+06, Test loss: 6.239e+08, MSE(e): 1.088e-04, MSE(pi1): 2.902e-01, MSE(pi2): 5.298e-05, MSE(pi3): 6.230e-03\n",
      "Epoch 50800, Train loss: 1.328e+07, Test loss: 5.811e+08, MSE(e): 1.159e-03, MSE(pi1): 1.074e-01, MSE(pi2): 5.715e-04, MSE(pi3): 6.188e-03\n",
      "Epoch 50900, Train loss: 2.275e+06, Test loss: 6.130e+08, MSE(e): 1.450e-04, MSE(pi1): 2.153e-02, MSE(pi2): 8.199e-05, MSE(pi3): 6.093e-03\n",
      "Epoch 51000, Train loss: 1.658e+07, Test loss: 6.311e+08, MSE(e): 3.282e-04, MSE(pi1): 1.281e+00, MSE(pi2): 1.976e-04, MSE(pi3): 4.947e-03\n",
      "Epoch 51100, Train loss: 4.235e+06, Test loss: 6.319e+08, MSE(e): 1.018e-04, MSE(pi1): 2.063e-01, MSE(pi2): 5.367e-05, MSE(pi3): 1.154e-02\n",
      "Epoch 51200, Train loss: 3.935e+06, Test loss: 6.103e+08, MSE(e): 1.733e-04, MSE(pi1): 1.734e-01, MSE(pi2): 9.755e-05, MSE(pi3): 4.678e-03\n",
      "Epoch 51300, Train loss: 2.486e+07, Test loss: 5.878e+08, MSE(e): 1.870e-03, MSE(pi1): 4.869e-01, MSE(pi2): 9.186e-04, MSE(pi3): 1.292e-02\n",
      "Epoch 51400, Train loss: 2.997e+06, Test loss: 6.261e+08, MSE(e): 1.086e-04, MSE(pi1): 1.077e-01, MSE(pi2): 6.195e-05, MSE(pi3): 8.336e-03\n",
      "Epoch 51500, Train loss: 2.688e+07, Test loss: 6.679e+08, MSE(e): 2.454e-03, MSE(pi1): 1.672e-01, MSE(pi2): 1.231e-03, MSE(pi3): 6.671e-03\n",
      "Epoch 51600, Train loss: 1.296e+07, Test loss: 5.841e+08, MSE(e): 1.180e-03, MSE(pi1): 6.670e-02, MSE(pi2): 5.308e-04, MSE(pi3): 4.911e-03\n",
      "Epoch 51700, Train loss: 6.807e+06, Test loss: 6.418e+08, MSE(e): 4.947e-04, MSE(pi1): 8.888e-02, MSE(pi2): 1.960e-04, MSE(pi3): 9.718e-03\n",
      "Epoch 51800, Train loss: 4.215e+06, Test loss: 6.511e+08, MSE(e): 2.694e-04, MSE(pi1): 7.153e-02, MSE(pi2): 1.312e-04, MSE(pi3): 8.055e-03\n",
      "Epoch 51900, Train loss: 1.877e+06, Test loss: 6.183e+08, MSE(e): 7.541e-05, MSE(pi1): 5.389e-02, MSE(pi2): 4.101e-05, MSE(pi3): 5.840e-03\n",
      "Epoch 52000, Train loss: 3.901e+06, Test loss: 6.180e+08, MSE(e): 1.333e-04, MSE(pi1): 1.982e-01, MSE(pi2): 8.671e-05, MSE(pi3): 5.856e-03\n",
      "Epoch 52100, Train loss: 8.242e+06, Test loss: 6.524e+08, MSE(e): 2.914e-04, MSE(pi1): 3.560e-01, MSE(pi2): 1.230e-04, MSE(pi3): 1.767e-02\n",
      "Epoch 52200, Train loss: 3.188e+06, Test loss: 6.057e+08, MSE(e): 2.257e-04, MSE(pi1): 3.267e-02, MSE(pi2): 1.077e-04, MSE(pi3): 6.040e-03\n",
      "Epoch 52300, Train loss: 8.798e+06, Test loss: 6.579e+08, MSE(e): 3.033e-04, MSE(pi1): 4.991e-01, MSE(pi2): 1.383e-04, MSE(pi3): 7.739e-03\n",
      "Epoch 52400, Train loss: 5.170e+06, Test loss: 6.232e+08, MSE(e): 1.364e-04, MSE(pi1): 2.809e-01, MSE(pi2): 8.444e-05, MSE(pi3): 9.977e-03\n",
      "Epoch 52500, Train loss: 2.441e+06, Test loss: 6.165e+08, MSE(e): 1.449e-04, MSE(pi1): 3.877e-02, MSE(pi2): 7.053e-05, MSE(pi3): 6.040e-03\n",
      "Epoch 52600, Train loss: 6.382e+06, Test loss: 6.161e+08, MSE(e): 1.259e-04, MSE(pi1): 3.499e-01, MSE(pi2): 7.942e-05, MSE(pi3): 1.625e-02\n",
      "Epoch 52700, Train loss: 1.719e+07, Test loss: 6.369e+08, MSE(e): 2.645e-04, MSE(pi1): 1.394e+00, MSE(pi2): 1.302e-04, MSE(pi3): 6.144e-03\n",
      "Epoch 52800, Train loss: 2.189e+06, Test loss: 6.292e+08, MSE(e): 9.530e-05, MSE(pi1): 4.492e-02, MSE(pi2): 4.767e-05, MSE(pi3): 7.864e-03\n",
      "Epoch 52900, Train loss: 1.869e+06, Test loss: 6.245e+08, MSE(e): 8.030e-05, MSE(pi1): 4.786e-02, MSE(pi2): 4.504e-05, MSE(pi3): 5.877e-03\n",
      "Epoch 53000, Train loss: 4.730e+06, Test loss: 6.291e+08, MSE(e): 1.216e-04, MSE(pi1): 2.991e-01, MSE(pi2): 6.605e-05, MSE(pi3): 5.233e-03\n",
      "Epoch 53100, Train loss: 4.791e+07, Test loss: 7.520e+08, MSE(e): 4.450e-03, MSE(pi1): 2.351e-01, MSE(pi2): 1.995e-03, MSE(pi3): 1.064e-02\n",
      "Epoch 53200, Train loss: 2.003e+07, Test loss: 6.635e+08, MSE(e): 7.652e-04, MSE(pi1): 1.135e+00, MSE(pi2): 3.776e-04, MSE(pi3): 1.028e-02\n",
      "Epoch 53300, Train loss: 1.344e+07, Test loss: 6.594e+08, MSE(e): 2.870e-04, MSE(pi1): 1.004e+00, MSE(pi2): 1.236e-04, MSE(pi3): 5.321e-03\n",
      "Epoch 53400, Train loss: 1.699e+07, Test loss: 5.936e+08, MSE(e): 1.529e-03, MSE(pi1): 8.429e-02, MSE(pi2): 7.191e-04, MSE(pi3): 8.563e-03\n",
      "Epoch 53500, Train loss: 5.421e+06, Test loss: 6.420e+08, MSE(e): 3.411e-04, MSE(pi1): 1.280e-01, MSE(pi2): 2.029e-04, MSE(pi3): 7.302e-03\n",
      "Epoch 53600, Train loss: 3.393e+06, Test loss: 6.092e+08, MSE(e): 2.601e-04, MSE(pi1): 1.594e-02, MSE(pi2): 1.331e-04, MSE(pi3): 6.326e-03\n",
      "Epoch 53700, Train loss: 7.140e+06, Test loss: 6.677e+08, MSE(e): 4.851e-04, MSE(pi1): 1.393e-01, MSE(pi2): 2.424e-04, MSE(pi3): 8.950e-03\n",
      "Epoch 53800, Train loss: 3.012e+07, Test loss: 6.083e+08, MSE(e): 2.829e-03, MSE(pi1): 9.140e-02, MSE(pi2): 1.375e-03, MSE(pi3): 9.133e-03\n",
      "Epoch 53900, Train loss: 2.076e+07, Test loss: 6.060e+08, MSE(e): 1.229e-03, MSE(pi1): 7.978e-01, MSE(pi2): 6.572e-04, MSE(pi3): 4.884e-03\n",
      "Epoch 54000, Train loss: 1.141e+07, Test loss: 6.747e+08, MSE(e): 3.732e-04, MSE(pi1): 6.727e-01, MSE(pi2): 1.818e-04, MSE(pi3): 9.532e-03\n",
      "Epoch 54100, Train loss: 1.017e+07, Test loss: 5.963e+08, MSE(e): 8.726e-04, MSE(pi1): 8.214e-02, MSE(pi2): 4.310e-04, MSE(pi3): 6.220e-03\n",
      "Epoch 54200, Train loss: 1.183e+07, Test loss: 6.815e+08, MSE(e): 8.975e-04, MSE(pi1): 2.216e-01, MSE(pi2): 4.141e-04, MSE(pi3): 6.379e-03\n",
      "Epoch 54300, Train loss: 6.680e+06, Test loss: 6.149e+08, MSE(e): 3.491e-04, MSE(pi1): 2.607e-01, MSE(pi2): 1.473e-04, MSE(pi3): 5.811e-03\n",
      "Epoch 54400, Train loss: 4.351e+06, Test loss: 6.405e+08, MSE(e): 1.032e-04, MSE(pi1): 2.576e-01, MSE(pi2): 4.698e-05, MSE(pi3): 7.431e-03\n",
      "Epoch 54500, Train loss: 8.731e+06, Test loss: 6.352e+08, MSE(e): 2.005e-04, MSE(pi1): 5.953e-01, MSE(pi2): 9.588e-05, MSE(pi3): 7.726e-03\n",
      "Epoch 54600, Train loss: 2.589e+06, Test loss: 6.322e+08, MSE(e): 8.053e-05, MSE(pi1): 1.213e-01, MSE(pi2): 4.114e-05, MSE(pi3): 5.705e-03\n",
      "Epoch 54700, Train loss: 2.130e+07, Test loss: 6.542e+08, MSE(e): 3.062e-04, MSE(pi1): 1.735e+00, MSE(pi2): 1.367e-04, MSE(pi3): 8.808e-03\n",
      "Epoch 54800, Train loss: 3.309e+06, Test loss: 6.298e+08, MSE(e): 1.527e-04, MSE(pi1): 1.218e-01, MSE(pi2): 9.253e-05, MSE(pi3): 5.644e-03\n",
      "Epoch 54900, Train loss: 4.900e+06, Test loss: 6.248e+08, MSE(e): 3.111e-04, MSE(pi1): 1.081e-01, MSE(pi2): 1.427e-04, MSE(pi3): 7.080e-03\n",
      "Epoch 55000, Train loss: 1.798e+06, Test loss: 6.288e+08, MSE(e): 8.772e-05, MSE(pi1): 1.329e-02, MSE(pi2): 4.878e-05, MSE(pi3): 7.883e-03\n",
      "Epoch 55100, Train loss: 2.538e+06, Test loss: 6.396e+08, MSE(e): 9.464e-05, MSE(pi1): 9.068e-02, MSE(pi2): 5.420e-05, MSE(pi3): 6.850e-03\n",
      "Epoch 55200, Train loss: 3.270e+06, Test loss: 6.330e+08, MSE(e): 9.093e-05, MSE(pi1): 1.783e-01, MSE(pi2): 4.481e-05, MSE(pi3): 5.781e-03\n",
      "Epoch 55300, Train loss: 2.740e+06, Test loss: 6.422e+08, MSE(e): 1.183e-04, MSE(pi1): 7.143e-02, MSE(pi2): 7.048e-05, MSE(pi3): 8.424e-03\n",
      "Epoch 55400, Train loss: 6.499e+06, Test loss: 6.378e+08, MSE(e): 1.644e-04, MSE(pi1): 4.093e-01, MSE(pi2): 8.403e-05, MSE(pi3): 7.624e-03\n",
      "Epoch 55500, Train loss: 1.133e+07, Test loss: 6.883e+08, MSE(e): 9.514e-04, MSE(pi1): 4.972e-02, MSE(pi2): 4.921e-04, MSE(pi3): 1.321e-02\n",
      "Epoch 55600, Train loss: 4.137e+06, Test loss: 6.238e+08, MSE(e): 1.856e-04, MSE(pi1): 1.779e-01, MSE(pi2): 1.146e-04, MSE(pi3): 5.019e-03\n",
      "Epoch 55700, Train loss: 5.467e+06, Test loss: 6.470e+08, MSE(e): 2.184e-04, MSE(pi1): 2.278e-01, MSE(pi2): 1.455e-04, MSE(pi3): 1.005e-02\n",
      "Epoch 55800, Train loss: 2.752e+06, Test loss: 6.392e+08, MSE(e): 1.020e-04, MSE(pi1): 9.700e-02, MSE(pi2): 5.748e-05, MSE(pi3): 7.622e-03\n",
      "Epoch 55900, Train loss: 2.680e+07, Test loss: 7.270e+08, MSE(e): 2.212e-03, MSE(pi1): 3.711e-01, MSE(pi2): 1.015e-03, MSE(pi3): 9.719e-03\n",
      "Epoch 56000, Train loss: 3.143e+06, Test loss: 6.309e+08, MSE(e): 1.305e-04, MSE(pi1): 5.330e-02, MSE(pi2): 9.058e-05, MSE(pi3): 1.305e-02\n",
      "Epoch 56100, Train loss: 3.264e+06, Test loss: 6.237e+08, MSE(e): 1.831e-04, MSE(pi1): 8.682e-02, MSE(pi2): 1.205e-04, MSE(pi3): 5.655e-03\n",
      "Epoch 56200, Train loss: 5.283e+06, Test loss: 6.113e+08, MSE(e): 4.037e-04, MSE(pi1): 7.916e-02, MSE(pi2): 2.157e-04, MSE(pi3): 4.545e-03\n",
      "Epoch 56300, Train loss: 7.383e+06, Test loss: 6.464e+08, MSE(e): 1.360e-04, MSE(pi1): 4.657e-01, MSE(pi2): 6.990e-05, MSE(pi3): 1.366e-02\n",
      "Epoch 56400, Train loss: 3.024e+06, Test loss: 6.277e+08, MSE(e): 1.542e-04, MSE(pi1): 9.732e-02, MSE(pi2): 8.054e-05, MSE(pi3): 5.081e-03\n",
      "Epoch 56500, Train loss: 5.955e+06, Test loss: 6.518e+08, MSE(e): 1.839e-04, MSE(pi1): 3.505e-01, MSE(pi2): 8.247e-05, MSE(pi3): 6.114e-03\n",
      "Epoch 56600, Train loss: 2.243e+06, Test loss: 6.336e+08, MSE(e): 1.338e-04, MSE(pi1): 8.934e-03, MSE(pi2): 7.762e-05, MSE(pi3): 8.157e-03\n",
      "Epoch 56700, Train loss: 7.591e+06, Test loss: 6.521e+08, MSE(e): 2.383e-04, MSE(pi1): 4.645e-01, MSE(pi2): 1.380e-04, MSE(pi3): 5.626e-03\n",
      "Epoch 56800, Train loss: 1.051e+07, Test loss: 6.736e+08, MSE(e): 7.755e-04, MSE(pi1): 2.166e-01, MSE(pi2): 4.149e-04, MSE(pi3): 5.909e-03\n",
      "Epoch 56900, Train loss: 9.183e+06, Test loss: 6.651e+08, MSE(e): 1.840e-04, MSE(pi1): 6.428e-01, MSE(pi2): 9.478e-05, MSE(pi3): 9.155e-03\n",
      "Epoch 57000, Train loss: 1.503e+07, Test loss: 6.045e+08, MSE(e): 1.422e-03, MSE(pi1): 2.498e-02, MSE(pi2): 6.483e-04, MSE(pi3): 5.649e-03\n",
      "Epoch 57100, Train loss: 6.472e+06, Test loss: 6.596e+08, MSE(e): 1.253e-04, MSE(pi1): 4.460e-01, MSE(pi2): 6.022e-05, MSE(pi3): 7.595e-03\n",
      "Epoch 57200, Train loss: 4.575e+06, Test loss: 6.536e+08, MSE(e): 1.247e-04, MSE(pi1): 2.511e-01, MSE(pi2): 7.365e-05, MSE(pi3): 8.167e-03\n",
      "Epoch 57300, Train loss: 2.869e+06, Test loss: 6.407e+08, MSE(e): 9.149e-05, MSE(pi1): 1.260e-01, MSE(pi2): 4.908e-05, MSE(pi3): 6.936e-03\n",
      "Epoch 57400, Train loss: 2.136e+07, Test loss: 6.135e+08, MSE(e): 1.820e-03, MSE(pi1): 2.658e-01, MSE(pi2): 8.333e-04, MSE(pi3): 5.003e-03\n",
      "Epoch 57500, Train loss: 1.855e+07, Test loss: 6.873e+08, MSE(e): 7.424e-04, MSE(pi1): 1.014e+00, MSE(pi2): 3.596e-04, MSE(pi3): 9.803e-03\n",
      "Epoch 57600, Train loss: 8.812e+06, Test loss: 6.344e+08, MSE(e): 4.183e-04, MSE(pi1): 3.858e-01, MSE(pi2): 1.938e-04, MSE(pi3): 7.705e-03\n",
      "Epoch 57700, Train loss: 8.678e+06, Test loss: 6.886e+08, MSE(e): 4.078e-04, MSE(pi1): 3.993e-01, MSE(pi2): 1.863e-04, MSE(pi3): 6.074e-03\n",
      "Epoch 57800, Train loss: 1.206e+07, Test loss: 6.353e+08, MSE(e): 4.590e-04, MSE(pi1): 6.974e-01, MSE(pi2): 2.701e-04, MSE(pi3): 4.985e-03\n",
      "Epoch 57900, Train loss: 2.874e+06, Test loss: 6.640e+08, MSE(e): 1.794e-04, MSE(pi1): 4.346e-02, MSE(pi2): 8.196e-05, MSE(pi3): 6.455e-03\n",
      "Epoch 58000, Train loss: 5.235e+06, Test loss: 6.452e+08, MSE(e): 1.126e-04, MSE(pi1): 3.326e-01, MSE(pi2): 5.397e-05, MSE(pi3): 7.834e-03\n",
      "Epoch 58100, Train loss: 1.657e+06, Test loss: 6.439e+08, MSE(e): 7.416e-05, MSE(pi1): 3.316e-02, MSE(pi2): 4.227e-05, MSE(pi3): 5.843e-03\n",
      "Epoch 58200, Train loss: 2.605e+07, Test loss: 6.929e+08, MSE(e): 3.160e-04, MSE(pi1): 2.174e+00, MSE(pi2): 1.167e-04, MSE(pi3): 1.151e-02\n",
      "Epoch 58300, Train loss: 2.166e+06, Test loss: 6.542e+08, MSE(e): 1.081e-04, MSE(pi1): 5.228e-02, MSE(pi2): 5.737e-05, MSE(pi3): 5.621e-03\n",
      "Epoch 58400, Train loss: 1.013e+07, Test loss: 6.437e+08, MSE(e): 2.336e-04, MSE(pi1): 7.286e-01, MSE(pi2): 1.440e-04, MSE(pi3): 5.093e-03\n",
      "Epoch 58500, Train loss: 1.515e+06, Test loss: 6.502e+08, MSE(e): 7.502e-05, MSE(pi1): 1.282e-02, MSE(pi2): 4.022e-05, MSE(pi3): 6.370e-03\n",
      "Epoch 58600, Train loss: 3.236e+06, Test loss: 6.502e+08, MSE(e): 1.166e-04, MSE(pi1): 1.306e-01, MSE(pi2): 6.797e-05, MSE(pi3): 7.646e-03\n",
      "Epoch 58700, Train loss: 7.584e+06, Test loss: 6.577e+08, MSE(e): 1.284e-04, MSE(pi1): 5.004e-01, MSE(pi2): 6.809e-05, MSE(pi3): 1.297e-02\n",
      "Epoch 58800, Train loss: 1.924e+06, Test loss: 6.415e+08, MSE(e): 9.256e-05, MSE(pi1): 4.323e-02, MSE(pi2): 5.175e-05, MSE(pi3): 5.656e-03\n",
      "Epoch 58900, Train loss: 4.804e+07, Test loss: 8.028e+08, MSE(e): 4.282e-03, MSE(pi1): 4.207e-01, MSE(pi2): 1.950e-03, MSE(pi3): 1.016e-02\n",
      "Epoch 59000, Train loss: 8.477e+06, Test loss: 6.573e+08, MSE(e): 1.628e-04, MSE(pi1): 6.313e-01, MSE(pi2): 8.275e-05, MSE(pi3): 5.356e-03\n",
      "Epoch 59100, Train loss: 6.760e+06, Test loss: 6.636e+08, MSE(e): 1.185e-04, MSE(pi1): 4.773e-01, MSE(pi2): 5.219e-05, MSE(pi3): 8.026e-03\n",
      "Epoch 59200, Train loss: 9.830e+06, Test loss: 6.971e+08, MSE(e): 6.371e-04, MSE(pi1): 2.338e-01, MSE(pi2): 3.141e-04, MSE(pi3): 1.121e-02\n",
      "Epoch 59300, Train loss: 1.336e+07, Test loss: 6.471e+08, MSE(e): 3.755e-04, MSE(pi1): 9.114e-01, MSE(pi2): 1.743e-04, MSE(pi3): 4.949e-03\n",
      "Epoch 59400, Train loss: 6.838e+06, Test loss: 6.373e+08, MSE(e): 3.679e-04, MSE(pi1): 2.383e-01, MSE(pi2): 1.713e-04, MSE(pi3): 7.766e-03\n",
      "Epoch 59500, Train loss: 2.098e+07, Test loss: 7.258e+08, MSE(e): 1.867e-03, MSE(pi1): 1.518e-01, MSE(pi2): 8.122e-04, MSE(pi3): 7.880e-03\n",
      "Epoch 59600, Train loss: 5.011e+06, Test loss: 6.429e+08, MSE(e): 1.685e-04, MSE(pi1): 2.832e-01, MSE(pi2): 9.193e-05, MSE(pi3): 4.938e-03\n",
      "Epoch 59700, Train loss: 3.962e+06, Test loss: 6.574e+08, MSE(e): 1.118e-04, MSE(pi1): 1.951e-01, MSE(pi2): 6.473e-05, MSE(pi3): 8.921e-03\n",
      "Epoch 59800, Train loss: 5.428e+06, Test loss: 6.604e+08, MSE(e): 3.517e-04, MSE(pi1): 1.163e-01, MSE(pi2): 2.166e-04, MSE(pi3): 7.484e-03\n",
      "Epoch 59900, Train loss: 1.347e+07, Test loss: 6.907e+08, MSE(e): 3.411e-04, MSE(pi1): 9.380e-01, MSE(pi2): 1.361e-04, MSE(pi3): 6.807e-03\n",
      "Epoch 60000, Train loss: 6.710e+06, Test loss: 6.476e+08, MSE(e): 3.053e-04, MSE(pi1): 3.016e-01, MSE(pi2): 1.301e-04, MSE(pi3): 6.408e-03\n",
      "Epoch 60100, Train loss: 6.575e+06, Test loss: 6.627e+08, MSE(e): 5.100e-04, MSE(pi1): 7.002e-02, MSE(pi2): 3.196e-04, MSE(pi3): 7.751e-03\n",
      "Epoch 60200, Train loss: 3.285e+06, Test loss: 6.453e+08, MSE(e): 1.345e-04, MSE(pi1): 1.341e-01, MSE(pi2): 6.732e-05, MSE(pi3): 5.981e-03\n",
      "Epoch 60300, Train loss: 1.299e+07, Test loss: 6.379e+08, MSE(e): 7.695e-04, MSE(pi1): 3.996e-01, MSE(pi2): 4.011e-04, MSE(pi3): 1.299e-02\n",
      "Epoch 60400, Train loss: 4.645e+06, Test loss: 6.419e+08, MSE(e): 2.426e-04, MSE(pi1): 1.533e-01, MSE(pi2): 1.684e-04, MSE(pi3): 6.853e-03\n",
      "Epoch 60500, Train loss: 3.885e+06, Test loss: 6.636e+08, MSE(e): 1.541e-04, MSE(pi1): 1.546e-01, MSE(pi2): 7.476e-05, MSE(pi3): 7.989e-03\n",
      "Epoch 60600, Train loss: 4.758e+06, Test loss: 6.700e+08, MSE(e): 2.707e-04, MSE(pi1): 1.199e-01, MSE(pi2): 1.354e-04, MSE(pi3): 8.517e-03\n",
      "Epoch 60700, Train loss: 2.604e+06, Test loss: 6.599e+08, MSE(e): 1.088e-04, MSE(pi1): 7.936e-02, MSE(pi2): 4.962e-05, MSE(pi3): 7.227e-03\n",
      "Epoch 60800, Train loss: 1.443e+07, Test loss: 7.246e+08, MSE(e): 1.324e-03, MSE(pi1): 3.812e-02, MSE(pi2): 5.843e-04, MSE(pi3): 8.147e-03\n",
      "Epoch 60900, Train loss: 7.285e+06, Test loss: 6.376e+08, MSE(e): 4.094e-04, MSE(pi1): 2.568e-01, MSE(pi2): 2.040e-04, MSE(pi3): 6.229e-03\n",
      "Epoch 61000, Train loss: 8.936e+06, Test loss: 6.756e+08, MSE(e): 5.102e-04, MSE(pi1): 2.515e-01, MSE(pi2): 2.786e-04, MSE(pi3): 1.319e-02\n",
      "Epoch 61100, Train loss: 7.607e+06, Test loss: 6.606e+08, MSE(e): 1.488e-04, MSE(pi1): 4.971e-01, MSE(pi2): 6.657e-05, MSE(pi3): 1.149e-02\n",
      "Epoch 61200, Train loss: 4.645e+06, Test loss: 6.739e+08, MSE(e): 1.662e-04, MSE(pi1): 2.138e-01, MSE(pi2): 9.518e-05, MSE(pi3): 8.453e-03\n",
      "Epoch 61300, Train loss: 5.571e+06, Test loss: 6.713e+08, MSE(e): 3.480e-04, MSE(pi1): 9.169e-02, MSE(pi2): 2.217e-04, MSE(pi3): 1.175e-02\n",
      "Epoch 61400, Train loss: 1.075e+07, Test loss: 6.351e+08, MSE(e): 9.161e-04, MSE(pi1): 9.112e-02, MSE(pi2): 4.293e-04, MSE(pi3): 6.746e-03\n",
      "Epoch 61500, Train loss: 7.675e+06, Test loss: 6.750e+08, MSE(e): 1.461e-04, MSE(pi1): 5.470e-01, MSE(pi2): 6.226e-05, MSE(pi3): 7.442e-03\n",
      "Epoch 61600, Train loss: 4.319e+06, Test loss: 6.635e+08, MSE(e): 1.004e-04, MSE(pi1): 2.793e-01, MSE(pi2): 4.352e-05, MSE(pi3): 5.224e-03\n",
      "Epoch 61700, Train loss: 1.089e+07, Test loss: 6.732e+08, MSE(e): 4.172e-04, MSE(pi1): 5.125e-01, MSE(pi2): 2.782e-04, MSE(pi3): 1.589e-02\n",
      "Epoch 61800, Train loss: 5.159e+06, Test loss: 6.487e+08, MSE(e): 4.112e-04, MSE(pi1): 3.846e-02, MSE(pi2): 3.068e-04, MSE(pi3): 6.620e-03\n",
      "Epoch 61900, Train loss: 4.947e+06, Test loss: 6.392e+08, MSE(e): 3.491e-04, MSE(pi1): 9.023e-02, MSE(pi2): 1.875e-04, MSE(pi3): 5.532e-03\n",
      "Epoch 62000, Train loss: 3.219e+06, Test loss: 6.658e+08, MSE(e): 1.290e-04, MSE(pi1): 1.052e-01, MSE(pi2): 7.548e-05, MSE(pi3): 8.765e-03\n",
      "Epoch 62100, Train loss: 5.538e+06, Test loss: 6.635e+08, MSE(e): 1.620e-04, MSE(pi1): 3.401e-01, MSE(pi2): 1.022e-04, MSE(pi3): 5.160e-03\n",
      "Epoch 62200, Train loss: 2.039e+06, Test loss: 6.673e+08, MSE(e): 9.578e-05, MSE(pi1): 2.767e-02, MSE(pi2): 4.912e-05, MSE(pi3): 8.046e-03\n",
      "Epoch 62300, Train loss: 1.237e+07, Test loss: 6.680e+08, MSE(e): 2.259e-04, MSE(pi1): 9.533e-01, MSE(pi2): 1.330e-04, MSE(pi3): 5.768e-03\n",
      "Epoch 62400, Train loss: 2.573e+06, Test loss: 6.518e+08, MSE(e): 1.509e-04, MSE(pi1): 4.252e-02, MSE(pi2): 1.007e-04, MSE(pi3): 6.384e-03\n",
      "Epoch 62500, Train loss: 1.244e+07, Test loss: 6.998e+08, MSE(e): 2.768e-04, MSE(pi1): 8.802e-01, MSE(pi2): 1.379e-04, MSE(pi3): 8.713e-03\n",
      "Epoch 62600, Train loss: 3.054e+07, Test loss: 6.317e+08, MSE(e): 2.924e-03, MSE(pi1): 5.380e-02, MSE(pi2): 1.468e-03, MSE(pi3): 7.641e-03\n",
      "Epoch 62700, Train loss: 6.812e+06, Test loss: 6.650e+08, MSE(e): 2.002e-04, MSE(pi1): 4.291e-01, MSE(pi2): 1.327e-04, MSE(pi3): 5.187e-03\n",
      "Epoch 62800, Train loss: 1.179e+07, Test loss: 7.151e+08, MSE(e): 7.041e-04, MSE(pi1): 3.624e-01, MSE(pi2): 3.525e-04, MSE(pi3): 1.130e-02\n",
      "Epoch 62900, Train loss: 1.153e+07, Test loss: 6.368e+08, MSE(e): 8.596e-04, MSE(pi1): 2.499e-01, MSE(pi2): 4.634e-04, MSE(pi3): 4.348e-03\n",
      "Epoch 63000, Train loss: 1.660e+06, Test loss: 6.628e+08, MSE(e): 7.036e-05, MSE(pi1): 3.746e-02, MSE(pi2): 3.967e-05, MSE(pi3): 5.820e-03\n",
      "Epoch 63100, Train loss: 2.358e+07, Test loss: 6.468e+08, MSE(e): 1.642e-03, MSE(pi1): 6.131e-01, MSE(pi2): 7.304e-04, MSE(pi3): 1.031e-02\n",
      "Epoch 63200, Train loss: 4.362e+06, Test loss: 6.818e+08, MSE(e): 3.276e-04, MSE(pi1): 5.040e-02, MSE(pi2): 1.468e-04, MSE(pi3): 5.819e-03\n",
      "Epoch 63300, Train loss: 2.240e+07, Test loss: 6.429e+08, MSE(e): 1.311e-03, MSE(pi1): 8.774e-01, MSE(pi2): 6.291e-04, MSE(pi3): 5.243e-03\n",
      "Epoch 63400, Train loss: 2.270e+06, Test loss: 6.683e+08, MSE(e): 9.390e-05, MSE(pi1): 5.260e-02, MSE(pi2): 4.669e-05, MSE(pi3): 8.048e-03\n",
      "Epoch 63500, Train loss: 2.058e+06, Test loss: 6.602e+08, MSE(e): 9.565e-05, MSE(pi1): 3.540e-02, MSE(pi2): 5.122e-05, MSE(pi3): 7.476e-03\n",
      "Epoch 63600, Train loss: 5.917e+06, Test loss: 6.581e+08, MSE(e): 1.523e-04, MSE(pi1): 3.758e-01, MSE(pi2): 8.944e-05, MSE(pi3): 6.358e-03\n",
      "Epoch 63700, Train loss: 2.651e+06, Test loss: 6.731e+08, MSE(e): 1.163e-04, MSE(pi1): 9.593e-02, MSE(pi2): 5.725e-05, MSE(pi3): 5.290e-03\n",
      "Epoch 63800, Train loss: 2.115e+06, Test loss: 6.686e+08, MSE(e): 7.727e-05, MSE(pi1): 7.510e-02, MSE(pi2): 4.263e-05, MSE(pi3): 5.915e-03\n",
      "Epoch 63900, Train loss: 4.962e+06, Test loss: 6.655e+08, MSE(e): 1.268e-04, MSE(pi1): 3.193e-01, MSE(pi2): 7.806e-05, MSE(pi3): 5.023e-03\n",
      "Epoch 64000, Train loss: 1.011e+07, Test loss: 6.684e+08, MSE(e): 4.543e-04, MSE(pi1): 5.100e-01, MSE(pi2): 2.943e-04, MSE(pi3): 4.671e-03\n",
      "Epoch 64100, Train loss: 1.768e+06, Test loss: 6.677e+08, MSE(e): 8.438e-05, MSE(pi1): 3.299e-02, MSE(pi2): 4.695e-05, MSE(pi3): 5.947e-03\n",
      "Epoch 64200, Train loss: 7.435e+06, Test loss: 6.581e+08, MSE(e): 2.875e-04, MSE(pi1): 4.037e-01, MSE(pi2): 2.100e-04, MSE(pi3): 5.224e-03\n",
      "Epoch 64300, Train loss: 8.385e+06, Test loss: 6.423e+08, MSE(e): 6.480e-04, MSE(pi1): 1.394e-01, MSE(pi2): 3.320e-04, MSE(pi3): 5.113e-03\n",
      "Epoch 64400, Train loss: 1.170e+07, Test loss: 6.478e+08, MSE(e): 7.938e-04, MSE(pi1): 3.233e-01, MSE(pi2): 3.998e-04, MSE(pi3): 5.319e-03\n",
      "Epoch 64500, Train loss: 6.417e+06, Test loss: 6.488e+08, MSE(e): 4.070e-04, MSE(pi1): 1.742e-01, MSE(pi2): 1.986e-04, MSE(pi3): 6.050e-03\n",
      "Epoch 64600, Train loss: 2.522e+06, Test loss: 6.800e+08, MSE(e): 1.308e-04, MSE(pi1): 3.966e-02, MSE(pi2): 6.102e-05, MSE(pi3): 8.173e-03\n",
      "Epoch 64700, Train loss: 6.003e+06, Test loss: 6.791e+08, MSE(e): 3.430e-04, MSE(pi1): 1.999e-01, MSE(pi2): 2.095e-04, MSE(pi3): 5.740e-03\n",
      "Epoch 64800, Train loss: 2.485e+06, Test loss: 6.671e+08, MSE(e): 1.091e-04, MSE(pi1): 2.045e-02, MSE(pi2): 5.996e-05, MSE(pi3): 1.189e-02\n",
      "Epoch 64900, Train loss: 1.302e+07, Test loss: 7.118e+08, MSE(e): 7.827e-04, MSE(pi1): 3.616e-01, MSE(pi2): 3.043e-04, MSE(pi3): 1.573e-02\n",
      "Epoch 65000, Train loss: 2.867e+06, Test loss: 6.627e+08, MSE(e): 1.138e-04, MSE(pi1): 1.188e-01, MSE(pi2): 6.546e-05, MSE(pi3): 5.402e-03\n",
      "Epoch 65100, Train loss: 2.166e+07, Test loss: 7.455e+08, MSE(e): 1.690e-03, MSE(pi1): 3.471e-01, MSE(pi2): 7.129e-04, MSE(pi3): 1.288e-02\n",
      "Epoch 65200, Train loss: 6.284e+06, Test loss: 6.656e+08, MSE(e): 4.554e-04, MSE(pi1): 7.964e-02, MSE(pi2): 2.547e-04, MSE(pi3): 9.330e-03\n",
      "Epoch 65300, Train loss: 2.608e+07, Test loss: 6.350e+08, MSE(e): 2.414e-03, MSE(pi1): 1.507e-01, MSE(pi2): 1.126e-03, MSE(pi3): 4.348e-03\n",
      "Epoch 65400, Train loss: 1.017e+07, Test loss: 7.062e+08, MSE(e): 8.078e-04, MSE(pi1): 1.362e-01, MSE(pi2): 3.428e-04, MSE(pi3): 7.267e-03\n",
      "Epoch 65500, Train loss: 2.974e+07, Test loss: 7.580e+08, MSE(e): 2.677e-03, MSE(pi1): 5.099e-02, MSE(pi2): 1.202e-03, MSE(pi3): 2.463e-02\n",
      "Epoch 65600, Train loss: 5.350e+07, Test loss: 6.523e+08, MSE(e): 4.777e-03, MSE(pi1): 5.240e-01, MSE(pi2): 2.252e-03, MSE(pi3): 4.839e-03\n",
      "Epoch 65700, Train loss: 9.495e+06, Test loss: 6.787e+08, MSE(e): 4.747e-04, MSE(pi1): 3.241e-01, MSE(pi2): 3.152e-04, MSE(pi3): 1.506e-02\n",
      "Epoch 65800, Train loss: 3.281e+06, Test loss: 6.673e+08, MSE(e): 1.900e-04, MSE(pi1): 8.129e-02, MSE(pi2): 9.617e-05, MSE(pi3): 5.679e-03\n",
      "Epoch 65900, Train loss: 2.835e+06, Test loss: 6.642e+08, MSE(e): 1.514e-04, MSE(pi1): 6.574e-02, MSE(pi2): 7.374e-05, MSE(pi3): 6.638e-03\n",
      "Epoch 66000, Train loss: 9.589e+06, Test loss: 7.135e+08, MSE(e): 5.682e-04, MSE(pi1): 3.354e-01, MSE(pi2): 2.620e-04, MSE(pi3): 5.532e-03\n",
      "Epoch 66100, Train loss: 5.767e+06, Test loss: 6.981e+08, MSE(e): 3.574e-04, MSE(pi1): 1.232e-01, MSE(pi2): 1.905e-04, MSE(pi3): 9.615e-03\n",
      "Epoch 66200, Train loss: 2.632e+06, Test loss: 6.829e+08, MSE(e): 1.763e-04, MSE(pi1): 1.932e-02, MSE(pi2): 7.896e-05, MSE(pi3): 6.766e-03\n",
      "Epoch 66300, Train loss: 8.183e+06, Test loss: 6.850e+08, MSE(e): 1.355e-04, MSE(pi1): 6.348e-01, MSE(pi2): 6.298e-05, MSE(pi3): 4.801e-03\n",
      "Epoch 66400, Train loss: 1.747e+06, Test loss: 6.726e+08, MSE(e): 7.714e-05, MSE(pi1): 3.103e-02, MSE(pi2): 4.197e-05, MSE(pi3): 6.657e-03\n",
      "Epoch 66500, Train loss: 5.591e+06, Test loss: 6.684e+08, MSE(e): 1.618e-04, MSE(pi1): 3.265e-01, MSE(pi2): 7.982e-05, MSE(pi3): 7.076e-03\n",
      "Epoch 66600, Train loss: 5.990e+06, Test loss: 6.797e+08, MSE(e): 1.688e-04, MSE(pi1): 3.482e-01, MSE(pi2): 9.133e-05, MSE(pi3): 8.202e-03\n",
      "Epoch 66700, Train loss: 2.470e+06, Test loss: 6.839e+08, MSE(e): 9.033e-05, MSE(pi1): 8.584e-02, MSE(pi2): 5.169e-05, MSE(pi3): 7.088e-03\n",
      "Epoch 66800, Train loss: 7.901e+06, Test loss: 6.609e+08, MSE(e): 3.845e-04, MSE(pi1): 3.468e-01, MSE(pi2): 1.824e-04, MSE(pi3): 5.870e-03\n",
      "Epoch 66900, Train loss: 4.278e+06, Test loss: 6.838e+08, MSE(e): 1.284e-04, MSE(pi1): 1.804e-01, MSE(pi2): 7.089e-05, MSE(pi3): 1.190e-02\n",
      "Epoch 67000, Train loss: 4.971e+06, Test loss: 6.570e+08, MSE(e): 4.032e-04, MSE(pi1): 3.690e-02, MSE(pi2): 1.839e-04, MSE(pi3): 5.699e-03\n",
      "Epoch 67100, Train loss: 4.930e+06, Test loss: 7.004e+08, MSE(e): 2.400e-04, MSE(pi1): 1.698e-01, MSE(pi2): 1.354e-04, MSE(pi3): 8.319e-03\n",
      "Epoch 67200, Train loss: 3.062e+06, Test loss: 6.655e+08, MSE(e): 2.067e-04, MSE(pi1): 3.762e-02, MSE(pi2): 1.010e-04, MSE(pi3): 6.188e-03\n",
      "Epoch 67300, Train loss: 5.282e+06, Test loss: 6.740e+08, MSE(e): 1.480e-04, MSE(pi1): 3.290e-01, MSE(pi2): 8.948e-05, MSE(pi3): 5.117e-03\n",
      "Epoch 67400, Train loss: 5.393e+06, Test loss: 6.956e+08, MSE(e): 2.532e-04, MSE(pi1): 2.312e-01, MSE(pi2): 1.141e-04, MSE(pi3): 5.489e-03\n",
      "Epoch 67500, Train loss: 6.943e+06, Test loss: 6.638e+08, MSE(e): 4.754e-04, MSE(pi1): 1.117e-01, MSE(pi2): 2.444e-04, MSE(pi3): 1.073e-02\n",
      "Epoch 67600, Train loss: 6.735e+07, Test loss: 8.326e+08, MSE(e): 6.434e-03, MSE(pi1): 1.938e-01, MSE(pi2): 2.861e-03, MSE(pi3): 1.074e-02\n",
      "Epoch 67700, Train loss: 5.987e+06, Test loss: 6.713e+08, MSE(e): 2.233e-04, MSE(pi1): 3.200e-01, MSE(pi2): 1.567e-04, MSE(pi3): 5.534e-03\n",
      "Epoch 67800, Train loss: 3.590e+06, Test loss: 6.948e+08, MSE(e): 1.662e-04, MSE(pi1): 1.331e-01, MSE(pi2): 7.672e-05, MSE(pi3): 5.973e-03\n",
      "Epoch 67900, Train loss: 5.461e+06, Test loss: 6.736e+08, MSE(e): 4.057e-04, MSE(pi1): 5.830e-02, MSE(pi2): 2.451e-04, MSE(pi3): 8.216e-03\n",
      "Epoch 68000, Train loss: 5.268e+06, Test loss: 7.044e+08, MSE(e): 2.210e-04, MSE(pi1): 2.226e-01, MSE(pi2): 1.247e-04, MSE(pi3): 8.317e-03\n",
      "Epoch 68100, Train loss: 7.139e+06, Test loss: 7.128e+08, MSE(e): 4.237e-04, MSE(pi1): 1.954e-01, MSE(pi2): 2.152e-04, MSE(pi3): 9.477e-03\n",
      "Epoch 68200, Train loss: 7.663e+06, Test loss: 7.138e+08, MSE(e): 5.711e-04, MSE(pi1): 6.230e-02, MSE(pi2): 3.166e-04, MSE(pi3): 1.329e-02\n",
      "Epoch 68300, Train loss: 3.099e+06, Test loss: 6.845e+08, MSE(e): 8.729e-05, MSE(pi1): 1.505e-01, MSE(pi2): 4.693e-05, MSE(pi3): 7.217e-03\n",
      "Epoch 68400, Train loss: 5.941e+07, Test loss: 8.360e+08, MSE(e): 5.353e-03, MSE(pi1): 4.685e-01, MSE(pi2): 2.443e-03, MSE(pi3): 1.192e-02\n",
      "Epoch 68500, Train loss: 4.681e+06, Test loss: 6.937e+08, MSE(e): 2.528e-04, MSE(pi1): 1.143e-01, MSE(pi2): 1.705e-04, MSE(pi3): 1.011e-02\n",
      "Epoch 68600, Train loss: 6.934e+06, Test loss: 6.927e+08, MSE(e): 2.973e-04, MSE(pi1): 3.522e-01, MSE(pi2): 1.905e-04, MSE(pi3): 4.390e-03\n",
      "Epoch 68700, Train loss: 8.881e+06, Test loss: 6.623e+08, MSE(e): 7.080e-04, MSE(pi1): 1.086e-01, MSE(pi2): 3.353e-04, MSE(pi3): 7.145e-03\n",
      "Epoch 68800, Train loss: 4.706e+06, Test loss: 6.644e+08, MSE(e): 3.511e-04, MSE(pi1): 6.089e-02, MSE(pi2): 1.856e-04, MSE(pi3): 5.866e-03\n",
      "Epoch 68900, Train loss: 7.170e+06, Test loss: 6.883e+08, MSE(e): 3.002e-04, MSE(pi1): 3.032e-01, MSE(pi2): 1.744e-04, MSE(pi3): 1.135e-02\n",
      "Epoch 69000, Train loss: 2.402e+07, Test loss: 7.698e+08, MSE(e): 2.120e-03, MSE(pi1): 2.107e-01, MSE(pi2): 9.489e-04, MSE(pi3): 7.095e-03\n",
      "Epoch 69100, Train loss: 1.076e+07, Test loss: 6.710e+08, MSE(e): 9.616e-04, MSE(pi1): 5.122e-02, MSE(pi2): 4.505e-04, MSE(pi3): 6.268e-03\n",
      "Epoch 69200, Train loss: 8.557e+06, Test loss: 6.615e+08, MSE(e): 6.909e-04, MSE(pi1): 1.056e-01, MSE(pi2): 4.551e-04, MSE(pi3): 5.927e-03\n",
      "Epoch 69300, Train loss: 1.228e+07, Test loss: 7.205e+08, MSE(e): 7.236e-04, MSE(pi1): 4.367e-01, MSE(pi2): 3.066e-04, MSE(pi3): 6.803e-03\n",
      "Epoch 69400, Train loss: 6.810e+06, Test loss: 7.019e+08, MSE(e): 3.982e-04, MSE(pi1): 2.143e-01, MSE(pi2): 1.935e-04, MSE(pi3): 6.845e-03\n",
      "Epoch 69500, Train loss: 4.327e+06, Test loss: 6.781e+08, MSE(e): 1.977e-04, MSE(pi1): 1.560e-01, MSE(pi2): 1.001e-04, MSE(pi3): 7.911e-03\n",
      "Epoch 69600, Train loss: 5.618e+06, Test loss: 6.989e+08, MSE(e): 1.228e-04, MSE(pi1): 3.798e-01, MSE(pi2): 5.288e-05, MSE(pi3): 5.928e-03\n",
      "Epoch 69700, Train loss: 5.790e+06, Test loss: 6.711e+08, MSE(e): 3.320e-04, MSE(pi1): 1.850e-01, MSE(pi2): 1.801e-04, MSE(pi3): 6.196e-03\n",
      "Epoch 69800, Train loss: 5.400e+06, Test loss: 6.744e+08, MSE(e): 3.337e-04, MSE(pi1): 1.496e-01, MSE(pi2): 1.917e-04, MSE(pi3): 5.676e-03\n",
      "Epoch 69900, Train loss: 4.742e+06, Test loss: 6.796e+08, MSE(e): 2.573e-04, MSE(pi1): 1.576e-01, MSE(pi2): 1.294e-04, MSE(pi3): 5.925e-03\n",
      "Epoch 70000, Train loss: 8.057e+06, Test loss: 6.641e+08, MSE(e): 6.215e-04, MSE(pi1): 8.788e-02, MSE(pi2): 3.109e-04, MSE(pi3): 9.631e-03\n",
      "Epoch 70100, Train loss: 4.291e+06, Test loss: 7.021e+08, MSE(e): 1.145e-04, MSE(pi1): 2.359e-01, MSE(pi2): 6.653e-05, MSE(pi3): 7.875e-03\n",
      "Epoch 70200, Train loss: 1.894e+07, Test loss: 6.590e+08, MSE(e): 1.793e-03, MSE(pi1): 3.812e-02, MSE(pi2): 8.351e-04, MSE(pi3): 6.264e-03\n",
      "Epoch 70300, Train loss: 8.610e+06, Test loss: 7.274e+08, MSE(e): 6.804e-04, MSE(pi1): 3.898e-02, MSE(pi2): 3.073e-04, MSE(pi3): 1.416e-02\n",
      "Epoch 70400, Train loss: 3.417e+06, Test loss: 6.836e+08, MSE(e): 9.991e-05, MSE(pi1): 1.899e-01, MSE(pi2): 6.059e-05, MSE(pi3): 5.189e-03\n",
      "Epoch 70500, Train loss: 2.677e+06, Test loss: 6.900e+08, MSE(e): 8.267e-05, MSE(pi1): 1.087e-01, MSE(pi2): 4.481e-05, MSE(pi3): 7.636e-03\n",
      "Epoch 70600, Train loss: 5.265e+06, Test loss: 6.814e+08, MSE(e): 1.860e-04, MSE(pi1): 2.892e-01, MSE(pi2): 1.213e-04, MSE(pi3): 5.129e-03\n",
      "Epoch 70700, Train loss: 2.749e+06, Test loss: 6.981e+08, MSE(e): 1.086e-04, MSE(pi1): 8.853e-02, MSE(pi2): 6.508e-05, MSE(pi3): 7.771e-03\n",
      "Epoch 70800, Train loss: 2.231e+06, Test loss: 6.880e+08, MSE(e): 8.463e-05, MSE(pi1): 7.223e-02, MSE(pi2): 5.066e-05, MSE(pi3): 6.628e-03\n",
      "Epoch 70900, Train loss: 1.975e+06, Test loss: 6.959e+08, MSE(e): 8.181e-05, MSE(pi1): 4.399e-02, MSE(pi2): 4.198e-05, MSE(pi3): 7.174e-03\n",
      "Epoch 71000, Train loss: 1.698e+06, Test loss: 6.875e+08, MSE(e): 8.788e-05, MSE(pi1): 1.550e-02, MSE(pi2): 5.026e-05, MSE(pi3): 6.638e-03\n",
      "Epoch 71100, Train loss: 1.823e+06, Test loss: 6.848e+08, MSE(e): 7.592e-05, MSE(pi1): 4.436e-02, MSE(pi2): 4.197e-05, MSE(pi3): 6.204e-03\n",
      "Epoch 71200, Train loss: 5.191e+06, Test loss: 7.029e+08, MSE(e): 1.139e-04, MSE(pi1): 3.123e-01, MSE(pi2): 6.171e-05, MSE(pi3): 9.292e-03\n",
      "Epoch 71300, Train loss: 4.728e+06, Test loss: 6.900e+08, MSE(e): 1.830e-04, MSE(pi1): 2.161e-01, MSE(pi2): 8.428e-05, MSE(pi3): 7.368e-03\n",
      "Epoch 71400, Train loss: 2.330e+06, Test loss: 6.898e+08, MSE(e): 8.717e-05, MSE(pi1): 8.440e-02, MSE(pi2): 5.283e-05, MSE(pi3): 6.144e-03\n",
      "Epoch 71500, Train loss: 3.070e+06, Test loss: 6.971e+08, MSE(e): 7.888e-05, MSE(pi1): 1.492e-01, MSE(pi2): 4.068e-05, MSE(pi3): 7.895e-03\n",
      "Epoch 71600, Train loss: 2.425e+06, Test loss: 6.896e+08, MSE(e): 1.142e-04, MSE(pi1): 5.958e-02, MSE(pi2): 6.407e-05, MSE(pi3): 6.871e-03\n",
      "Epoch 71700, Train loss: 6.561e+06, Test loss: 6.891e+08, MSE(e): 1.785e-04, MSE(pi1): 4.292e-01, MSE(pi2): 9.936e-05, MSE(pi3): 4.831e-03\n",
      "Epoch 71800, Train loss: 6.882e+06, Test loss: 7.131e+08, MSE(e): 4.942e-04, MSE(pi1): 1.210e-01, MSE(pi2): 2.343e-04, MSE(pi3): 7.307e-03\n",
      "Epoch 71900, Train loss: 8.516e+06, Test loss: 7.121e+08, MSE(e): 6.384e-04, MSE(pi1): 1.277e-01, MSE(pi2): 2.658e-04, MSE(pi3): 8.554e-03\n",
      "Epoch 72000, Train loss: 4.918e+06, Test loss: 6.761e+08, MSE(e): 3.358e-04, MSE(pi1): 9.281e-02, MSE(pi2): 1.902e-04, MSE(pi3): 6.325e-03\n",
      "Epoch 72100, Train loss: 7.320e+06, Test loss: 6.944e+08, MSE(e): 1.465e-04, MSE(pi1): 5.309e-01, MSE(pi2): 8.171e-05, MSE(pi3): 5.453e-03\n",
      "Epoch 72200, Train loss: 2.751e+06, Test loss: 6.935e+08, MSE(e): 1.356e-04, MSE(pi1): 8.066e-02, MSE(pi2): 9.272e-05, MSE(pi3): 5.884e-03\n",
      "Epoch 72300, Train loss: 2.588e+06, Test loss: 6.858e+08, MSE(e): 1.004e-04, MSE(pi1): 1.030e-01, MSE(pi2): 5.530e-05, MSE(pi3): 5.538e-03\n",
      "Epoch 72400, Train loss: 2.263e+06, Test loss: 7.031e+08, MSE(e): 1.177e-04, MSE(pi1): 3.162e-02, MSE(pi2): 5.020e-05, MSE(pi3): 7.698e-03\n",
      "Epoch 72500, Train loss: 9.199e+06, Test loss: 7.106e+08, MSE(e): 7.058e-04, MSE(pi1): 1.274e-01, MSE(pi2): 4.105e-04, MSE(pi3): 8.665e-03\n",
      "Epoch 72600, Train loss: 9.687e+06, Test loss: 6.853e+08, MSE(e): 7.090e-04, MSE(pi1): 1.899e-01, MSE(pi2): 3.649e-04, MSE(pi3): 6.977e-03\n",
      "Epoch 72700, Train loss: 1.950e+06, Test loss: 7.029e+08, MSE(e): 8.100e-05, MSE(pi1): 2.634e-02, MSE(pi2): 4.173e-05, MSE(pi3): 8.763e-03\n",
      "Epoch 72800, Train loss: 8.893e+06, Test loss: 6.725e+08, MSE(e): 7.828e-04, MSE(pi1): 4.915e-02, MSE(pi2): 3.550e-04, MSE(pi3): 5.735e-03\n",
      "Epoch 72900, Train loss: 9.917e+06, Test loss: 7.408e+08, MSE(e): 7.794e-04, MSE(pi1): 1.367e-01, MSE(pi2): 3.639e-04, MSE(pi3): 7.555e-03\n",
      "Epoch 73000, Train loss: 4.891e+06, Test loss: 7.066e+08, MSE(e): 1.086e-04, MSE(pi1): 3.236e-01, MSE(pi2): 5.474e-05, MSE(pi3): 5.690e-03\n",
      "Epoch 73100, Train loss: 5.131e+06, Test loss: 6.855e+08, MSE(e): 3.464e-04, MSE(pi1): 1.125e-01, MSE(pi2): 1.700e-04, MSE(pi3): 5.413e-03\n",
      "Epoch 73200, Train loss: 2.140e+06, Test loss: 7.079e+08, MSE(e): 1.320e-04, MSE(pi1): 1.517e-02, MSE(pi2): 6.194e-05, MSE(pi3): 6.683e-03\n",
      "Epoch 73300, Train loss: 1.441e+07, Test loss: 6.763e+08, MSE(e): 1.280e-03, MSE(pi1): 8.660e-02, MSE(pi2): 5.977e-04, MSE(pi3): 7.463e-03\n",
      "Epoch 73400, Train loss: 9.009e+06, Test loss: 7.284e+08, MSE(e): 5.404e-04, MSE(pi1): 3.019e-01, MSE(pi2): 2.339e-04, MSE(pi3): 5.858e-03\n",
      "Epoch 73500, Train loss: 1.044e+07, Test loss: 6.926e+08, MSE(e): 4.393e-04, MSE(pi1): 5.575e-01, MSE(pi2): 2.464e-04, MSE(pi3): 4.739e-03\n",
      "Epoch 73600, Train loss: 5.195e+06, Test loss: 7.042e+08, MSE(e): 2.081e-04, MSE(pi1): 2.287e-01, MSE(pi2): 1.207e-04, MSE(pi3): 8.270e-03\n",
      "Epoch 73700, Train loss: 5.108e+06, Test loss: 7.090e+08, MSE(e): 1.155e-04, MSE(pi1): 3.064e-01, MSE(pi2): 6.077e-05, MSE(pi3): 8.889e-03\n",
      "Epoch 73800, Train loss: 1.692e+07, Test loss: 7.631e+08, MSE(e): 7.553e-04, MSE(pi1): 8.516e-01, MSE(pi2): 3.300e-04, MSE(pi3): 8.526e-03\n",
      "Epoch 73900, Train loss: 1.738e+06, Test loss: 6.973e+08, MSE(e): 6.603e-05, MSE(pi1): 4.935e-02, MSE(pi2): 3.684e-05, MSE(pi3): 5.847e-03\n",
      "Epoch 74000, Train loss: 1.498e+07, Test loss: 7.062e+08, MSE(e): 4.765e-04, MSE(pi1): 9.534e-01, MSE(pi2): 3.219e-04, MSE(pi3): 6.815e-03\n",
      "Epoch 74100, Train loss: 6.578e+06, Test loss: 7.279e+08, MSE(e): 4.752e-04, MSE(pi1): 1.096e-01, MSE(pi2): 2.101e-04, MSE(pi3): 7.295e-03\n",
      "Epoch 74200, Train loss: 4.294e+06, Test loss: 7.004e+08, MSE(e): 1.885e-04, MSE(pi1): 1.841e-01, MSE(pi2): 1.365e-04, MSE(pi3): 5.670e-03\n",
      "Epoch 74300, Train loss: 5.118e+06, Test loss: 6.979e+08, MSE(e): 1.466e-04, MSE(pi1): 2.795e-01, MSE(pi2): 8.484e-05, MSE(pi3): 8.569e-03\n",
      "Epoch 74400, Train loss: 2.587e+06, Test loss: 7.060e+08, MSE(e): 8.038e-05, MSE(pi1): 1.077e-01, MSE(pi2): 4.421e-05, MSE(pi3): 7.062e-03\n",
      "Epoch 74500, Train loss: 2.324e+07, Test loss: 7.895e+08, MSE(e): 1.646e-03, MSE(pi1): 5.963e-01, MSE(pi2): 7.483e-04, MSE(pi3): 8.129e-03\n",
      "Epoch 74600, Train loss: 4.596e+06, Test loss: 7.029e+08, MSE(e): 1.028e-04, MSE(pi1): 3.060e-01, MSE(pi2): 5.091e-05, MSE(pi3): 5.081e-03\n",
      "Epoch 74700, Train loss: 4.061e+06, Test loss: 6.994e+08, MSE(e): 1.547e-04, MSE(pi1): 2.021e-01, MSE(pi2): 1.023e-04, MSE(pi3): 4.934e-03\n",
      "Epoch 74800, Train loss: 3.622e+06, Test loss: 6.925e+08, MSE(e): 2.143e-04, MSE(pi1): 8.501e-02, MSE(pi2): 1.041e-04, MSE(pi3): 6.291e-03\n",
      "Epoch 74900, Train loss: 4.920e+06, Test loss: 7.177e+08, MSE(e): 2.778e-04, MSE(pi1): 1.147e-01, MSE(pi2): 1.854e-04, MSE(pi3): 9.944e-03\n",
      "Epoch 75000, Train loss: 2.364e+06, Test loss: 7.103e+08, MSE(e): 1.085e-04, MSE(pi1): 7.186e-02, MSE(pi2): 5.599e-05, MSE(pi3): 5.598e-03\n",
      "Epoch 75100, Train loss: 1.844e+06, Test loss: 7.176e+08, MSE(e): 7.976e-05, MSE(pi1): 2.942e-02, MSE(pi2): 4.485e-05, MSE(pi3): 7.520e-03\n",
      "Epoch 75200, Train loss: 3.829e+06, Test loss: 7.062e+08, MSE(e): 1.531e-04, MSE(pi1): 1.515e-01, MSE(pi2): 8.274e-05, MSE(pi3): 7.823e-03\n",
      "Epoch 75300, Train loss: 7.634e+06, Test loss: 6.835e+08, MSE(e): 4.862e-04, MSE(pi1): 2.211e-01, MSE(pi2): 3.104e-04, MSE(pi3): 5.617e-03\n",
      "Epoch 75400, Train loss: 3.650e+07, Test loss: 7.164e+08, MSE(e): 1.829e-03, MSE(pi1): 1.771e+00, MSE(pi2): 1.055e-03, MSE(pi3): 5.059e-03\n",
      "Epoch 75500, Train loss: 4.039e+06, Test loss: 7.331e+08, MSE(e): 2.602e-04, MSE(pi1): 8.246e-02, MSE(pi2): 1.316e-04, MSE(pi3): 6.128e-03\n",
      "Epoch 75600, Train loss: 5.754e+06, Test loss: 7.286e+08, MSE(e): 2.021e-04, MSE(pi1): 2.715e-01, MSE(pi2): 1.202e-04, MSE(pi3): 1.018e-02\n",
      "Epoch 75700, Train loss: 3.110e+06, Test loss: 7.288e+08, MSE(e): 1.702e-04, MSE(pi1): 7.329e-02, MSE(pi2): 7.785e-05, MSE(pi3): 6.756e-03\n",
      "Epoch 75800, Train loss: 1.415e+07, Test loss: 6.972e+08, MSE(e): 3.315e-04, MSE(pi1): 9.396e-01, MSE(pi2): 1.628e-04, MSE(pi3): 1.440e-02\n",
      "Epoch 75900, Train loss: 4.106e+06, Test loss: 7.219e+08, MSE(e): 1.150e-04, MSE(pi1): 2.435e-01, MSE(pi2): 4.682e-05, MSE(pi3): 5.208e-03\n",
      "Epoch 76000, Train loss: 5.348e+06, Test loss: 7.062e+08, MSE(e): 1.359e-04, MSE(pi1): 3.488e-01, MSE(pi2): 8.476e-05, MSE(pi3): 5.006e-03\n",
      "Epoch 76100, Train loss: 3.125e+06, Test loss: 7.174e+08, MSE(e): 1.236e-04, MSE(pi1): 9.972e-02, MSE(pi2): 7.102e-05, MSE(pi3): 8.916e-03\n",
      "Epoch 76200, Train loss: 4.300e+06, Test loss: 6.978e+08, MSE(e): 2.257e-04, MSE(pi1): 1.417e-01, MSE(pi2): 1.652e-04, MSE(pi3): 6.256e-03\n",
      "Epoch 76300, Train loss: 1.915e+06, Test loss: 7.074e+08, MSE(e): 6.668e-05, MSE(pi1): 6.265e-02, MSE(pi2): 3.583e-05, MSE(pi3): 6.217e-03\n",
      "Epoch 76400, Train loss: 1.964e+06, Test loss: 7.095e+08, MSE(e): 8.607e-05, MSE(pi1): 3.077e-02, MSE(pi2): 5.123e-05, MSE(pi3): 7.956e-03\n",
      "Epoch 76500, Train loss: 3.121e+06, Test loss: 7.150e+08, MSE(e): 8.506e-05, MSE(pi1): 1.527e-01, MSE(pi2): 4.126e-05, MSE(pi3): 7.430e-03\n",
      "Epoch 76600, Train loss: 3.188e+06, Test loss: 7.184e+08, MSE(e): 1.103e-04, MSE(pi1): 1.406e-01, MSE(pi2): 5.190e-05, MSE(pi3): 6.790e-03\n",
      "Epoch 76700, Train loss: 2.029e+06, Test loss: 7.102e+08, MSE(e): 7.010e-05, MSE(pi1): 7.563e-02, MSE(pi2): 3.935e-05, MSE(pi3): 5.717e-03\n",
      "Epoch 76800, Train loss: 5.182e+06, Test loss: 7.316e+08, MSE(e): 2.172e-04, MSE(pi1): 2.106e-01, MSE(pi2): 1.025e-04, MSE(pi3): 9.044e-03\n",
      "Epoch 76900, Train loss: 5.729e+06, Test loss: 7.162e+08, MSE(e): 3.290e-04, MSE(pi1): 1.885e-01, MSE(pi2): 2.030e-04, MSE(pi3): 5.542e-03\n",
      "Epoch 77000, Train loss: 1.333e+07, Test loss: 6.862e+08, MSE(e): 9.651e-04, MSE(pi1): 2.940e-01, MSE(pi2): 4.934e-04, MSE(pi3): 7.356e-03\n",
      "Epoch 77100, Train loss: 8.249e+06, Test loss: 7.252e+08, MSE(e): 4.221e-04, MSE(pi1): 2.272e-01, MSE(pi2): 2.980e-04, MSE(pi3): 1.756e-02\n",
      "Epoch 77200, Train loss: 3.581e+06, Test loss: 7.239e+08, MSE(e): 1.283e-04, MSE(pi1): 1.256e-01, MSE(pi2): 6.287e-05, MSE(pi3): 1.043e-02\n",
      "Epoch 77300, Train loss: 2.148e+07, Test loss: 7.884e+08, MSE(e): 1.939e-03, MSE(pi1): 1.128e-01, MSE(pi2): 8.500e-04, MSE(pi3): 9.593e-03\n",
      "Epoch 77400, Train loss: 1.734e+07, Test loss: 7.204e+08, MSE(e): 7.944e-04, MSE(pi1): 8.933e-01, MSE(pi2): 5.038e-04, MSE(pi3): 4.605e-03\n",
      "Epoch 77500, Train loss: 3.301e+06, Test loss: 7.279e+08, MSE(e): 1.912e-04, MSE(pi1): 6.201e-02, MSE(pi2): 9.432e-05, MSE(pi3): 7.686e-03\n",
      "Epoch 77600, Train loss: 1.864e+06, Test loss: 7.083e+08, MSE(e): 8.345e-05, MSE(pi1): 4.028e-02, MSE(pi2): 4.640e-05, MSE(pi3): 6.262e-03\n",
      "Epoch 77700, Train loss: 5.781e+06, Test loss: 7.130e+08, MSE(e): 2.139e-04, MSE(pi1): 2.826e-01, MSE(pi2): 1.040e-04, MSE(pi3): 8.158e-03\n",
      "Epoch 77800, Train loss: 9.052e+06, Test loss: 6.959e+08, MSE(e): 5.846e-04, MSE(pi1): 2.667e-01, MSE(pi2): 3.592e-04, MSE(pi3): 5.388e-03\n",
      "Epoch 77900, Train loss: 1.236e+07, Test loss: 7.685e+08, MSE(e): 1.118e-03, MSE(pi1): 4.342e-02, MSE(pi2): 4.801e-04, MSE(pi3): 7.413e-03\n",
      "Epoch 78000, Train loss: 6.027e+06, Test loss: 7.080e+08, MSE(e): 2.176e-04, MSE(pi1): 3.301e-01, MSE(pi2): 1.576e-04, MSE(pi3): 5.502e-03\n",
      "Epoch 78100, Train loss: 2.238e+06, Test loss: 6.999e+08, MSE(e): 1.086e-04, MSE(pi1): 5.664e-02, MSE(pi2): 7.591e-05, MSE(pi3): 5.862e-03\n",
      "Epoch 78200, Train loss: 7.496e+06, Test loss: 7.064e+08, MSE(e): 5.580e-04, MSE(pi1): 1.092e-01, MSE(pi2): 2.500e-04, MSE(pi3): 8.243e-03\n",
      "Epoch 78300, Train loss: 6.182e+06, Test loss: 7.256e+08, MSE(e): 2.363e-04, MSE(pi1): 3.214e-01, MSE(pi2): 1.123e-04, MSE(pi3): 6.056e-03\n",
      "Epoch 78400, Train loss: 2.805e+06, Test loss: 7.069e+08, MSE(e): 1.791e-04, MSE(pi1): 3.477e-02, MSE(pi2): 8.757e-05, MSE(pi3): 6.657e-03\n",
      "Epoch 78500, Train loss: 5.370e+06, Test loss: 7.137e+08, MSE(e): 1.465e-04, MSE(pi1): 3.393e-01, MSE(pi2): 9.431e-05, MSE(pi3): 5.128e-03\n",
      "Epoch 78600, Train loss: 4.581e+06, Test loss: 7.204e+08, MSE(e): 2.258e-04, MSE(pi1): 1.530e-01, MSE(pi2): 1.327e-04, MSE(pi3): 7.931e-03\n",
      "Epoch 78700, Train loss: 4.044e+06, Test loss: 7.103e+08, MSE(e): 1.434e-04, MSE(pi1): 1.919e-01, MSE(pi2): 9.586e-05, MSE(pi3): 6.919e-03\n",
      "Epoch 78800, Train loss: 2.436e+06, Test loss: 7.297e+08, MSE(e): 1.579e-04, MSE(pi1): 1.221e-02, MSE(pi2): 7.425e-05, MSE(pi3): 7.346e-03\n",
      "Epoch 78900, Train loss: 1.154e+07, Test loss: 7.077e+08, MSE(e): 8.154e-04, MSE(pi1): 2.269e-01, MSE(pi2): 4.174e-04, MSE(pi3): 1.119e-02\n",
      "Epoch 79000, Train loss: 5.884e+06, Test loss: 7.295e+08, MSE(e): 1.327e-04, MSE(pi1): 3.745e-01, MSE(pi2): 7.216e-05, MSE(pi3): 8.117e-03\n",
      "Epoch 79100, Train loss: 4.190e+06, Test loss: 7.354e+08, MSE(e): 2.496e-04, MSE(pi1): 9.481e-02, MSE(pi2): 1.278e-04, MSE(pi3): 7.459e-03\n",
      "Epoch 79200, Train loss: 9.246e+06, Test loss: 7.466e+08, MSE(e): 6.768e-04, MSE(pi1): 1.620e-01, MSE(pi2): 2.771e-04, MSE(pi3): 8.583e-03\n",
      "Epoch 79300, Train loss: 1.278e+07, Test loss: 7.465e+08, MSE(e): 1.086e-03, MSE(pi1): 1.001e-01, MSE(pi2): 4.738e-04, MSE(pi3): 9.153e-03\n",
      "Epoch 79400, Train loss: 4.185e+06, Test loss: 7.273e+08, MSE(e): 8.984e-05, MSE(pi1): 2.499e-01, MSE(pi2): 4.355e-05, MSE(pi3): 7.871e-03\n",
      "Epoch 79500, Train loss: 5.961e+06, Test loss: 7.124e+08, MSE(e): 2.753e-04, MSE(pi1): 2.460e-01, MSE(pi2): 1.226e-04, MSE(pi3): 7.478e-03\n",
      "Epoch 79600, Train loss: 5.951e+06, Test loss: 7.279e+08, MSE(e): 4.161e-04, MSE(pi1): 1.054e-01, MSE(pi2): 2.642e-04, MSE(pi3): 7.360e-03\n",
      "Epoch 79700, Train loss: 3.552e+07, Test loss: 7.981e+08, MSE(e): 3.338e-03, MSE(pi1): 1.528e-01, MSE(pi2): 1.447e-03, MSE(pi3): 6.107e-03\n",
      "Epoch 79800, Train loss: 6.002e+06, Test loss: 7.053e+08, MSE(e): 4.671e-04, MSE(pi1): 6.764e-02, MSE(pi2): 2.275e-04, MSE(pi3): 6.548e-03\n",
      "Epoch 79900, Train loss: 6.704e+06, Test loss: 7.143e+08, MSE(e): 2.268e-04, MSE(pi1): 3.900e-01, MSE(pi2): 1.258e-04, MSE(pi3): 5.355e-03\n",
      "Epoch 80000, Train loss: 4.914e+06, Test loss: 7.152e+08, MSE(e): 1.677e-04, MSE(pi1): 2.693e-01, MSE(pi2): 1.165e-04, MSE(pi3): 5.450e-03\n",
      "Epoch 80100, Train loss: 6.511e+06, Test loss: 7.153e+08, MSE(e): 4.165e-04, MSE(pi1): 1.409e-01, MSE(pi2): 2.201e-04, MSE(pi3): 9.370e-03\n",
      "Epoch 80200, Train loss: 7.099e+06, Test loss: 7.901e+08, MSE(e): 5.155e-04, MSE(pi1): 1.051e-01, MSE(pi2): 2.276e-04, MSE(pi3): 8.926e-03\n",
      "Epoch 80300, Train loss: 1.889e+06, Test loss: 7.203e+08, MSE(e): 7.731e-05, MSE(pi1): 2.358e-02, MSE(pi2): 4.634e-05, MSE(pi3): 8.800e-03\n",
      "Epoch 80400, Train loss: 4.225e+07, Test loss: 8.316e+08, MSE(e): 3.670e-03, MSE(pi1): 3.763e-01, MSE(pi2): 1.771e-03, MSE(pi3): 1.789e-02\n",
      "Epoch 80500, Train loss: 4.728e+06, Test loss: 7.319e+08, MSE(e): 1.151e-04, MSE(pi1): 2.944e-01, MSE(pi2): 5.088e-05, MSE(pi3): 6.336e-03\n",
      "Epoch 80600, Train loss: 1.550e+06, Test loss: 7.217e+08, MSE(e): 6.471e-05, MSE(pi1): 2.009e-02, MSE(pi2): 3.722e-05, MSE(pi3): 7.017e-03\n",
      "Epoch 80700, Train loss: 8.465e+06, Test loss: 7.491e+08, MSE(e): 5.258e-04, MSE(pi1): 2.368e-01, MSE(pi2): 2.616e-04, MSE(pi3): 8.394e-03\n",
      "Epoch 80800, Train loss: 1.830e+06, Test loss: 7.189e+08, MSE(e): 7.715e-05, MSE(pi1): 4.356e-02, MSE(pi2): 4.128e-05, MSE(pi3): 6.225e-03\n",
      "Epoch 80900, Train loss: 1.017e+07, Test loss: 7.053e+08, MSE(e): 6.640e-04, MSE(pi1): 2.896e-01, MSE(pi2): 3.123e-04, MSE(pi3): 6.298e-03\n",
      "Epoch 81000, Train loss: 8.169e+06, Test loss: 7.497e+08, MSE(e): 4.220e-04, MSE(pi1): 3.339e-01, MSE(pi2): 1.932e-04, MSE(pi3): 6.099e-03\n",
      "Epoch 81100, Train loss: 3.842e+06, Test loss: 7.336e+08, MSE(e): 1.108e-04, MSE(pi1): 2.058e-01, MSE(pi2): 5.412e-05, MSE(pi3): 6.758e-03\n",
      "Epoch 81200, Train loss: 4.119e+06, Test loss: 7.283e+08, MSE(e): 9.920e-05, MSE(pi1): 2.304e-01, MSE(pi2): 5.614e-05, MSE(pi3): 8.228e-03\n",
      "Epoch 81300, Train loss: 2.247e+06, Test loss: 7.307e+08, MSE(e): 9.521e-05, MSE(pi1): 4.030e-02, MSE(pi2): 5.507e-05, MSE(pi3): 8.921e-03\n",
      "Epoch 81400, Train loss: 5.999e+06, Test loss: 7.566e+08, MSE(e): 2.552e-04, MSE(pi1): 1.597e-01, MSE(pi2): 9.917e-05, MSE(pi3): 1.851e-02\n",
      "Epoch 81500, Train loss: 5.980e+06, Test loss: 7.317e+08, MSE(e): 1.327e-04, MSE(pi1): 3.859e-01, MSE(pi2): 6.632e-05, MSE(pi3): 7.936e-03\n",
      "Epoch 81600, Train loss: 3.452e+07, Test loss: 7.849e+08, MSE(e): 3.167e-03, MSE(pi1): 1.832e-01, MSE(pi2): 1.428e-03, MSE(pi3): 1.023e-02\n",
      "Epoch 81700, Train loss: 2.699e+06, Test loss: 7.250e+08, MSE(e): 8.525e-05, MSE(pi1): 1.215e-01, MSE(pi2): 4.028e-05, MSE(pi3): 6.318e-03\n",
      "Epoch 81800, Train loss: 1.497e+06, Test loss: 7.287e+08, MSE(e): 6.959e-05, MSE(pi1): 1.741e-02, MSE(pi2): 3.946e-05, MSE(pi3): 6.266e-03\n",
      "Epoch 81900, Train loss: 5.999e+06, Test loss: 7.239e+08, MSE(e): 1.513e-04, MSE(pi1): 3.969e-01, MSE(pi2): 9.299e-05, MSE(pi3): 5.171e-03\n",
      "Epoch 82000, Train loss: 5.940e+06, Test loss: 7.292e+08, MSE(e): 1.830e-04, MSE(pi1): 3.182e-01, MSE(pi2): 9.390e-05, MSE(pi3): 9.281e-03\n",
      "Epoch 82100, Train loss: 1.592e+06, Test loss: 7.304e+08, MSE(e): 6.169e-05, MSE(pi1): 2.468e-02, MSE(pi2): 3.336e-05, MSE(pi3): 7.287e-03\n",
      "Epoch 82200, Train loss: 3.311e+07, Test loss: 8.181e+08, MSE(e): 3.135e-03, MSE(pi1): 6.278e-02, MSE(pi2): 1.381e-03, MSE(pi3): 1.131e-02\n",
      "Epoch 82300, Train loss: 2.066e+07, Test loss: 7.976e+08, MSE(e): 1.691e-03, MSE(pi1): 2.733e-01, MSE(pi2): 7.786e-04, MSE(pi3): 1.013e-02\n",
      "Epoch 82400, Train loss: 1.414e+07, Test loss: 7.604e+08, MSE(e): 9.625e-04, MSE(pi1): 3.964e-01, MSE(pi2): 5.014e-04, MSE(pi3): 5.553e-03\n",
      "Epoch 82500, Train loss: 2.377e+07, Test loss: 7.071e+08, MSE(e): 1.866e-03, MSE(pi1): 4.642e-01, MSE(pi2): 9.361e-04, MSE(pi3): 4.667e-03\n",
      "Epoch 82600, Train loss: 1.779e+07, Test loss: 7.182e+08, MSE(e): 1.064e-03, MSE(pi1): 6.640e-01, MSE(pi2): 5.435e-04, MSE(pi3): 5.171e-03\n",
      "Epoch 82700, Train loss: 2.840e+06, Test loss: 7.217e+08, MSE(e): 1.629e-04, MSE(pi1): 3.925e-02, MSE(pi2): 9.016e-05, MSE(pi3): 8.183e-03\n",
      "Epoch 82800, Train loss: 2.315e+06, Test loss: 7.256e+08, MSE(e): 1.097e-04, MSE(pi1): 5.760e-02, MSE(pi2): 5.681e-05, MSE(pi3): 6.420e-03\n",
      "Epoch 82900, Train loss: 1.447e+07, Test loss: 7.649e+08, MSE(e): 1.237e-03, MSE(pi1): 1.404e-01, MSE(pi2): 5.215e-04, MSE(pi3): 6.991e-03\n",
      "Epoch 83000, Train loss: 5.790e+06, Test loss: 7.136e+08, MSE(e): 3.967e-04, MSE(pi1): 1.239e-01, MSE(pi2): 2.224e-04, MSE(pi3): 5.836e-03\n",
      "Epoch 83100, Train loss: 7.643e+06, Test loss: 7.309e+08, MSE(e): 1.763e-04, MSE(pi1): 5.150e-01, MSE(pi2): 8.447e-05, MSE(pi3): 7.292e-03\n",
      "Epoch 83200, Train loss: 4.534e+06, Test loss: 7.274e+08, MSE(e): 1.171e-04, MSE(pi1): 2.827e-01, MSE(pi2): 7.147e-05, MSE(pi3): 5.363e-03\n",
      "Epoch 83300, Train loss: 3.482e+06, Test loss: 7.186e+08, MSE(e): 2.681e-04, MSE(pi1): 2.252e-02, MSE(pi2): 1.274e-04, MSE(pi3): 5.759e-03\n",
      "Epoch 83400, Train loss: 4.607e+06, Test loss: 7.260e+08, MSE(e): 1.864e-04, MSE(pi1): 2.059e-01, MSE(pi2): 9.665e-05, MSE(pi3): 6.837e-03\n",
      "Epoch 83500, Train loss: 6.330e+06, Test loss: 7.486e+08, MSE(e): 4.506e-04, MSE(pi1): 1.265e-01, MSE(pi2): 2.201e-04, MSE(pi3): 5.584e-03\n",
      "Epoch 83600, Train loss: 1.386e+07, Test loss: 7.154e+08, MSE(e): 1.165e-03, MSE(pi1): 1.504e-01, MSE(pi2): 5.577e-04, MSE(pi3): 7.005e-03\n",
      "Epoch 83700, Train loss: 7.840e+06, Test loss: 7.441e+08, MSE(e): 3.170e-04, MSE(pi1): 3.925e-01, MSE(pi2): 1.394e-04, MSE(pi3): 7.453e-03\n",
      "Epoch 83800, Train loss: 7.169e+06, Test loss: 7.302e+08, MSE(e): 1.533e-04, MSE(pi1): 4.500e-01, MSE(pi2): 8.223e-05, MSE(pi3): 1.136e-02\n",
      "Epoch 83900, Train loss: 6.324e+06, Test loss: 7.479e+08, MSE(e): 2.350e-04, MSE(pi1): 3.123e-01, MSE(pi2): 1.557e-04, MSE(pi3): 8.504e-03\n",
      "Epoch 84000, Train loss: 3.142e+06, Test loss: 7.320e+08, MSE(e): 2.188e-04, MSE(pi1): 3.554e-02, MSE(pi2): 1.487e-04, MSE(pi3): 5.988e-03\n",
      "Epoch 84100, Train loss: 2.035e+06, Test loss: 7.306e+08, MSE(e): 7.880e-05, MSE(pi1): 6.373e-02, MSE(pi2): 4.726e-05, MSE(pi3): 6.099e-03\n",
      "Epoch 84200, Train loss: 2.088e+06, Test loss: 7.315e+08, MSE(e): 7.490e-05, MSE(pi1): 7.585e-02, MSE(pi2): 3.958e-05, MSE(pi3): 5.807e-03\n",
      "Epoch 84300, Train loss: 3.359e+06, Test loss: 7.254e+08, MSE(e): 1.513e-04, MSE(pi1): 1.239e-01, MSE(pi2): 8.430e-05, MSE(pi3): 6.073e-03\n",
      "Epoch 84400, Train loss: 4.011e+06, Test loss: 7.405e+08, MSE(e): 2.551e-04, MSE(pi1): 7.061e-02, MSE(pi2): 1.398e-04, MSE(pi3): 7.538e-03\n",
      "Epoch 84500, Train loss: 7.189e+06, Test loss: 7.260e+08, MSE(e): 4.448e-04, MSE(pi1): 2.121e-01, MSE(pi2): 1.935e-04, MSE(pi3): 6.209e-03\n",
      "Epoch 84600, Train loss: 1.452e+07, Test loss: 7.435e+08, MSE(e): 1.320e-03, MSE(pi1): 2.509e-02, MSE(pi2): 7.898e-04, MSE(pi3): 1.064e-02\n",
      "Epoch 84700, Train loss: 2.808e+06, Test loss: 7.232e+08, MSE(e): 1.945e-04, MSE(pi1): 2.514e-02, MSE(pi2): 9.627e-05, MSE(pi3): 6.119e-03\n",
      "Epoch 84800, Train loss: 8.572e+06, Test loss: 7.605e+08, MSE(e): 2.606e-04, MSE(pi1): 5.377e-01, MSE(pi2): 1.044e-04, MSE(pi3): 5.895e-03\n",
      "Epoch 84900, Train loss: 1.138e+07, Test loss: 7.381e+08, MSE(e): 7.079e-04, MSE(pi1): 3.527e-01, MSE(pi2): 3.679e-04, MSE(pi3): 7.709e-03\n",
      "Epoch 85000, Train loss: 2.949e+06, Test loss: 7.309e+08, MSE(e): 7.476e-05, MSE(pi1): 1.590e-01, MSE(pi2): 3.662e-05, MSE(pi3): 6.117e-03\n",
      "Epoch 85100, Train loss: 3.771e+06, Test loss: 7.439e+08, MSE(e): 8.338e-05, MSE(pi1): 1.916e-01, MSE(pi2): 4.354e-05, MSE(pi3): 1.021e-02\n",
      "Epoch 85200, Train loss: 3.947e+06, Test loss: 7.330e+08, MSE(e): 1.325e-04, MSE(pi1): 2.001e-01, MSE(pi2): 9.291e-05, MSE(pi3): 6.205e-03\n",
      "Epoch 85300, Train loss: 1.603e+07, Test loss: 7.271e+08, MSE(e): 8.391e-04, MSE(pi1): 6.863e-01, MSE(pi2): 4.558e-04, MSE(pi3): 7.758e-03\n",
      "Epoch 85400, Train loss: 1.804e+06, Test loss: 7.384e+08, MSE(e): 5.957e-05, MSE(pi1): 5.475e-02, MSE(pi2): 3.207e-05, MSE(pi3): 6.606e-03\n",
      "Epoch 85500, Train loss: 2.498e+06, Test loss: 7.491e+08, MSE(e): 1.313e-04, MSE(pi1): 5.779e-02, MSE(pi2): 6.031e-05, MSE(pi3): 6.065e-03\n",
      "Epoch 85600, Train loss: 5.698e+06, Test loss: 7.658e+08, MSE(e): 4.077e-04, MSE(pi1): 9.943e-02, MSE(pi2): 1.901e-04, MSE(pi3): 6.276e-03\n",
      "Epoch 85700, Train loss: 6.100e+06, Test loss: 7.357e+08, MSE(e): 2.361e-04, MSE(pi1): 3.052e-01, MSE(pi2): 1.156e-04, MSE(pi3): 6.874e-03\n",
      "Epoch 85800, Train loss: 1.071e+07, Test loss: 7.717e+08, MSE(e): 8.799e-04, MSE(pi1): 1.359e-01, MSE(pi2): 4.248e-04, MSE(pi3): 5.519e-03\n",
      "Epoch 85900, Train loss: 3.742e+06, Test loss: 7.469e+08, MSE(e): 8.104e-05, MSE(pi1): 2.104e-01, MSE(pi2): 3.614e-05, MSE(pi3): 8.282e-03\n",
      "Epoch 86000, Train loss: 1.999e+06, Test loss: 7.457e+08, MSE(e): 9.291e-05, MSE(pi1): 2.312e-02, MSE(pi2): 5.477e-05, MSE(pi3): 8.385e-03\n",
      "Epoch 86100, Train loss: 7.947e+06, Test loss: 7.607e+08, MSE(e): 1.664e-04, MSE(pi1): 5.382e-01, MSE(pi2): 8.482e-05, MSE(pi3): 9.017e-03\n",
      "Epoch 86200, Train loss: 4.270e+06, Test loss: 7.448e+08, MSE(e): 1.509e-04, MSE(pi1): 1.927e-01, MSE(pi2): 8.025e-05, MSE(pi3): 8.339e-03\n",
      "Epoch 86300, Train loss: 2.884e+06, Test loss: 7.359e+08, MSE(e): 1.808e-04, MSE(pi1): 3.513e-02, MSE(pi2): 9.125e-05, MSE(pi3): 7.254e-03\n",
      "Epoch 86400, Train loss: 1.980e+07, Test loss: 7.188e+08, MSE(e): 1.778e-03, MSE(pi1): 1.432e-01, MSE(pi2): 8.459e-04, MSE(pi3): 5.895e-03\n",
      "Epoch 86500, Train loss: 4.582e+06, Test loss: 7.401e+08, MSE(e): 1.057e-04, MSE(pi1): 2.888e-01, MSE(pi2): 5.432e-05, MSE(pi3): 6.374e-03\n",
      "Epoch 86600, Train loss: 3.655e+06, Test loss: 7.479e+08, MSE(e): 8.083e-05, MSE(pi1): 2.207e-01, MSE(pi2): 3.891e-05, MSE(pi3): 6.388e-03\n",
      "Epoch 86700, Train loss: 1.723e+06, Test loss: 7.436e+08, MSE(e): 6.944e-05, MSE(pi1): 4.280e-02, MSE(pi2): 3.925e-05, MSE(pi3): 6.006e-03\n",
      "Epoch 86800, Train loss: 1.697e+06, Test loss: 7.402e+08, MSE(e): 6.497e-05, MSE(pi1): 3.723e-02, MSE(pi2): 3.421e-05, MSE(pi3): 6.749e-03\n",
      "Epoch 86900, Train loss: 4.808e+06, Test loss: 7.374e+08, MSE(e): 1.498e-04, MSE(pi1): 2.358e-01, MSE(pi2): 9.028e-05, MSE(pi3): 9.527e-03\n",
      "Epoch 87000, Train loss: 1.677e+06, Test loss: 7.450e+08, MSE(e): 5.681e-05, MSE(pi1): 4.974e-02, MSE(pi2): 3.087e-05, MSE(pi3): 6.113e-03\n",
      "Epoch 87100, Train loss: 1.978e+06, Test loss: 7.368e+08, MSE(e): 1.230e-04, MSE(pi1): 1.386e-02, MSE(pi2): 6.190e-05, MSE(pi3): 6.099e-03\n",
      "Epoch 87200, Train loss: 9.365e+06, Test loss: 7.257e+08, MSE(e): 7.957e-04, MSE(pi1): 8.666e-02, MSE(pi2): 3.843e-04, MSE(pi3): 5.423e-03\n",
      "Epoch 87300, Train loss: 2.436e+06, Test loss: 7.447e+08, MSE(e): 8.221e-05, MSE(pi1): 9.655e-02, MSE(pi2): 4.478e-05, MSE(pi3): 6.481e-03\n",
      "Epoch 87400, Train loss: 2.357e+06, Test loss: 7.505e+08, MSE(e): 1.009e-04, MSE(pi1): 5.906e-02, MSE(pi2): 5.479e-05, MSE(pi3): 7.575e-03\n",
      "Epoch 87500, Train loss: 4.079e+06, Test loss: 7.540e+08, MSE(e): 1.378e-04, MSE(pi1): 1.831e-01, MSE(pi2): 8.199e-05, MSE(pi3): 8.699e-03\n",
      "Epoch 87600, Train loss: 7.943e+06, Test loss: 7.661e+08, MSE(e): 4.581e-04, MSE(pi1): 2.849e-01, MSE(pi2): 2.001e-04, MSE(pi3): 5.137e-03\n",
      "Epoch 87700, Train loss: 9.775e+06, Test loss: 7.388e+08, MSE(e): 7.583e-04, MSE(pi1): 1.194e-01, MSE(pi2): 3.637e-04, MSE(pi3): 9.990e-03\n",
      "Epoch 87800, Train loss: 2.007e+06, Test loss: 7.454e+08, MSE(e): 6.315e-05, MSE(pi1): 7.053e-02, MSE(pi2): 3.345e-05, MSE(pi3): 6.702e-03\n",
      "Epoch 87900, Train loss: 6.312e+06, Test loss: 7.768e+08, MSE(e): 4.199e-04, MSE(pi1): 1.456e-01, MSE(pi2): 2.100e-04, MSE(pi3): 6.581e-03\n",
      "Epoch 88000, Train loss: 4.574e+06, Test loss: 7.428e+08, MSE(e): 1.395e-04, MSE(pi1): 2.451e-01, MSE(pi2): 6.862e-05, MSE(pi3): 7.274e-03\n",
      "Epoch 88100, Train loss: 5.441e+07, Test loss: 7.518e+08, MSE(e): 4.984e-03, MSE(pi1): 3.858e-01, MSE(pi2): 2.229e-03, MSE(pi3): 7.125e-03\n",
      "Epoch 88200, Train loss: 2.365e+06, Test loss: 7.527e+08, MSE(e): 7.109e-05, MSE(pi1): 9.938e-02, MSE(pi2): 3.993e-05, MSE(pi3): 6.599e-03\n",
      "Epoch 88300, Train loss: 5.138e+06, Test loss: 7.661e+08, MSE(e): 2.842e-04, MSE(pi1): 1.402e-01, MSE(pi2): 1.373e-04, MSE(pi3): 8.940e-03\n",
      "Epoch 88400, Train loss: 3.571e+06, Test loss: 7.479e+08, MSE(e): 1.628e-04, MSE(pi1): 1.253e-01, MSE(pi2): 8.190e-05, MSE(pi3): 6.908e-03\n",
      "Epoch 88500, Train loss: 3.856e+06, Test loss: 7.620e+08, MSE(e): 1.691e-04, MSE(pi1): 1.541e-01, MSE(pi2): 7.084e-05, MSE(pi3): 6.234e-03\n",
      "Epoch 88600, Train loss: 4.005e+06, Test loss: 7.598e+08, MSE(e): 3.029e-04, MSE(pi1): 4.221e-02, MSE(pi2): 1.719e-04, MSE(pi3): 5.540e-03\n",
      "Epoch 88700, Train loss: 5.562e+06, Test loss: 7.571e+08, MSE(e): 2.867e-04, MSE(pi1): 1.801e-01, MSE(pi2): 1.850e-04, MSE(pi3): 8.940e-03\n",
      "Epoch 88800, Train loss: 5.862e+06, Test loss: 7.616e+08, MSE(e): 4.148e-04, MSE(pi1): 7.775e-02, MSE(pi2): 2.781e-04, MSE(pi3): 9.355e-03\n",
      "Epoch 88900, Train loss: 8.273e+06, Test loss: 7.692e+08, MSE(e): 1.734e-04, MSE(pi1): 5.808e-01, MSE(pi2): 8.612e-05, MSE(pi3): 7.317e-03\n",
      "Epoch 89000, Train loss: 5.308e+07, Test loss: 8.542e+08, MSE(e): 5.008e-03, MSE(pi1): 1.534e-01, MSE(pi2): 2.189e-03, MSE(pi3): 1.467e-02\n",
      "Epoch 89100, Train loss: 5.512e+06, Test loss: 7.716e+08, MSE(e): 1.850e-04, MSE(pi1): 2.490e-01, MSE(pi2): 9.964e-05, MSE(pi3): 1.171e-02\n",
      "Epoch 89200, Train loss: 7.484e+06, Test loss: 7.853e+08, MSE(e): 5.621e-04, MSE(pi1): 1.115e-01, MSE(pi2): 2.574e-04, MSE(pi3): 7.488e-03\n",
      "Epoch 89300, Train loss: 4.968e+07, Test loss: 7.447e+08, MSE(e): 4.703e-03, MSE(pi1): 2.111e-01, MSE(pi2): 2.227e-03, MSE(pi3): 5.413e-03\n",
      "Epoch 89400, Train loss: 9.603e+06, Test loss: 7.623e+08, MSE(e): 1.560e-04, MSE(pi1): 7.306e-01, MSE(pi2): 6.311e-05, MSE(pi3): 7.365e-03\n",
      "Epoch 89500, Train loss: 3.813e+06, Test loss: 7.440e+08, MSE(e): 1.484e-04, MSE(pi1): 1.661e-01, MSE(pi2): 9.524e-05, MSE(pi3): 6.682e-03\n",
      "Epoch 89600, Train loss: 3.000e+07, Test loss: 7.380e+08, MSE(e): 2.684e-03, MSE(pi1): 2.556e-01, MSE(pi2): 1.236e-03, MSE(pi3): 6.103e-03\n",
      "Epoch 89700, Train loss: 2.982e+07, Test loss: 7.434e+08, MSE(e): 2.775e-03, MSE(pi1): 1.529e-01, MSE(pi2): 1.248e-03, MSE(pi3): 5.446e-03\n",
      "Epoch 89800, Train loss: 3.853e+06, Test loss: 7.539e+08, MSE(e): 8.924e-05, MSE(pi1): 2.421e-01, MSE(pi2): 4.820e-05, MSE(pi3): 5.391e-03\n",
      "Epoch 89900, Train loss: 5.017e+06, Test loss: 7.649e+08, MSE(e): 9.998e-05, MSE(pi1): 3.310e-01, MSE(pi2): 4.071e-05, MSE(pi3): 7.075e-03\n",
      "Epoch 90000, Train loss: 3.802e+06, Test loss: 7.607e+08, MSE(e): 8.872e-05, MSE(pi1): 2.150e-01, MSE(pi2): 4.854e-05, MSE(pi3): 7.645e-03\n",
      "Epoch 90100, Train loss: 1.162e+07, Test loss: 8.102e+08, MSE(e): 4.403e-04, MSE(pi1): 6.509e-01, MSE(pi2): 1.917e-04, MSE(pi3): 7.082e-03\n",
      "Epoch 90200, Train loss: 4.735e+06, Test loss: 7.653e+08, MSE(e): 8.838e-05, MSE(pi1): 3.170e-01, MSE(pi2): 4.033e-05, MSE(pi3): 6.817e-03\n",
      "Epoch 90300, Train loss: 5.188e+06, Test loss: 7.685e+08, MSE(e): 2.643e-04, MSE(pi1): 2.067e-01, MSE(pi2): 1.432e-04, MSE(pi3): 4.771e-03\n",
      "Epoch 90400, Train loss: 1.603e+06, Test loss: 7.531e+08, MSE(e): 7.318e-05, MSE(pi1): 2.294e-02, MSE(pi2): 3.958e-05, MSE(pi3): 6.422e-03\n",
      "Epoch 90500, Train loss: 2.329e+06, Test loss: 7.468e+08, MSE(e): 1.521e-04, MSE(pi1): 2.058e-02, MSE(pi2): 7.597e-05, MSE(pi3): 6.021e-03\n",
      "Epoch 90600, Train loss: 3.951e+06, Test loss: 7.676e+08, MSE(e): 1.048e-04, MSE(pi1): 2.204e-01, MSE(pi2): 4.712e-05, MSE(pi3): 6.990e-03\n",
      "Epoch 90700, Train loss: 1.676e+06, Test loss: 7.598e+08, MSE(e): 7.463e-05, MSE(pi1): 3.346e-02, MSE(pi2): 4.072e-05, MSE(pi3): 5.950e-03\n",
      "Epoch 90800, Train loss: 2.600e+06, Test loss: 7.582e+08, MSE(e): 8.613e-05, MSE(pi1): 1.148e-01, MSE(pi2): 5.114e-05, MSE(pi3): 5.902e-03\n",
      "Epoch 90900, Train loss: 2.771e+06, Test loss: 7.592e+08, MSE(e): 9.458e-05, MSE(pi1): 1.210e-01, MSE(pi2): 5.307e-05, MSE(pi3): 6.154e-03\n",
      "Epoch 91000, Train loss: 1.678e+07, Test loss: 8.076e+08, MSE(e): 1.103e-03, MSE(pi1): 4.756e-01, MSE(pi2): 5.121e-04, MSE(pi3): 9.955e-03\n",
      "Epoch 91100, Train loss: 6.878e+06, Test loss: 7.516e+08, MSE(e): 3.185e-04, MSE(pi1): 3.218e-01, MSE(pi2): 2.217e-04, MSE(pi3): 4.755e-03\n",
      "Epoch 91200, Train loss: 2.057e+07, Test loss: 8.333e+08, MSE(e): 1.691e-03, MSE(pi1): 2.482e-01, MSE(pi2): 8.017e-04, MSE(pi3): 1.172e-02\n",
      "Epoch 91300, Train loss: 6.267e+06, Test loss: 7.621e+08, MSE(e): 4.573e-04, MSE(pi1): 6.407e-02, MSE(pi2): 3.392e-04, MSE(pi3): 1.053e-02\n",
      "Epoch 91400, Train loss: 3.392e+06, Test loss: 7.583e+08, MSE(e): 8.789e-05, MSE(pi1): 1.944e-01, MSE(pi2): 5.192e-05, MSE(pi3): 5.694e-03\n",
      "Epoch 91500, Train loss: 1.585e+07, Test loss: 7.924e+08, MSE(e): 6.138e-04, MSE(pi1): 8.226e-01, MSE(pi2): 3.835e-04, MSE(pi3): 1.486e-02\n",
      "Epoch 91600, Train loss: 1.697e+07, Test loss: 7.404e+08, MSE(e): 1.266e-03, MSE(pi1): 3.771e-01, MSE(pi2): 6.699e-04, MSE(pi3): 5.322e-03\n",
      "Epoch 91700, Train loss: 7.323e+06, Test loss: 7.487e+08, MSE(e): 4.069e-04, MSE(pi1): 2.741e-01, MSE(pi2): 1.862e-04, MSE(pi3): 5.124e-03\n",
      "Epoch 91800, Train loss: 3.177e+06, Test loss: 7.687e+08, MSE(e): 1.073e-04, MSE(pi1): 1.383e-01, MSE(pi2): 6.758e-05, MSE(pi3): 7.212e-03\n",
      "Epoch 91900, Train loss: 4.561e+06, Test loss: 7.725e+08, MSE(e): 1.581e-04, MSE(pi1): 2.466e-01, MSE(pi2): 9.873e-05, MSE(pi3): 5.130e-03\n",
      "Epoch 92000, Train loss: 4.827e+06, Test loss: 7.801e+08, MSE(e): 3.266e-04, MSE(pi1): 8.727e-02, MSE(pi2): 1.596e-04, MSE(pi3): 6.882e-03\n",
      "Epoch 92100, Train loss: 2.337e+06, Test loss: 7.585e+08, MSE(e): 1.146e-04, MSE(pi1): 5.395e-02, MSE(pi2): 6.113e-05, MSE(pi3): 6.517e-03\n",
      "Epoch 92200, Train loss: 5.164e+06, Test loss: 7.697e+08, MSE(e): 8.836e-05, MSE(pi1): 3.613e-01, MSE(pi2): 4.011e-05, MSE(pi3): 6.667e-03\n",
      "Epoch 92300, Train loss: 4.967e+06, Test loss: 7.660e+08, MSE(e): 1.193e-04, MSE(pi1): 3.260e-01, MSE(pi2): 7.438e-05, MSE(pi3): 5.141e-03\n",
      "Epoch 92400, Train loss: 1.940e+07, Test loss: 7.544e+08, MSE(e): 1.406e-03, MSE(pi1): 4.718e-01, MSE(pi2): 6.362e-04, MSE(pi3): 6.231e-03\n",
      "Epoch 92500, Train loss: 2.724e+06, Test loss: 7.730e+08, MSE(e): 1.432e-04, MSE(pi1): 6.355e-02, MSE(pi2): 6.495e-05, MSE(pi3): 6.562e-03\n",
      "Epoch 92600, Train loss: 2.737e+06, Test loss: 7.574e+08, MSE(e): 1.250e-04, MSE(pi1): 7.331e-02, MSE(pi2): 7.437e-05, MSE(pi3): 7.535e-03\n",
      "Epoch 92700, Train loss: 2.352e+06, Test loss: 7.566e+08, MSE(e): 1.526e-04, MSE(pi1): 2.690e-02, MSE(pi2): 8.153e-05, MSE(pi3): 5.573e-03\n",
      "Epoch 92800, Train loss: 3.311e+07, Test loss: 8.577e+08, MSE(e): 2.280e-03, MSE(pi1): 8.405e-01, MSE(pi2): 1.094e-03, MSE(pi3): 1.898e-02\n",
      "Epoch 92900, Train loss: 2.936e+06, Test loss: 7.707e+08, MSE(e): 7.242e-05, MSE(pi1): 1.498e-01, MSE(pi2): 3.592e-05, MSE(pi3): 7.135e-03\n",
      "Epoch 93000, Train loss: 2.568e+06, Test loss: 7.592e+08, MSE(e): 1.511e-04, MSE(pi1): 4.159e-02, MSE(pi2): 7.594e-05, MSE(pi3): 6.411e-03\n",
      "Epoch 93100, Train loss: 4.153e+06, Test loss: 7.559e+08, MSE(e): 2.850e-04, MSE(pi1): 6.190e-02, MSE(pi2): 1.996e-04, MSE(pi3): 6.841e-03\n",
      "Epoch 93200, Train loss: 3.430e+06, Test loss: 7.652e+08, MSE(e): 8.957e-05, MSE(pi1): 2.000e-01, MSE(pi2): 4.962e-05, MSE(pi3): 5.341e-03\n",
      "Epoch 93300, Train loss: 3.338e+06, Test loss: 7.785e+08, MSE(e): 2.152e-04, MSE(pi1): 5.120e-02, MSE(pi2): 1.024e-04, MSE(pi3): 6.731e-03\n",
      "Epoch 93400, Train loss: 1.193e+07, Test loss: 8.026e+08, MSE(e): 7.157e-04, MSE(pi1): 3.502e-01, MSE(pi2): 3.351e-04, MSE(pi3): 1.276e-02\n",
      "Epoch 93500, Train loss: 9.144e+06, Test loss: 7.812e+08, MSE(e): 1.509e-04, MSE(pi1): 6.555e-01, MSE(pi2): 7.802e-05, MSE(pi3): 1.079e-02\n",
      "Epoch 93600, Train loss: 1.689e+06, Test loss: 7.621e+08, MSE(e): 9.311e-05, MSE(pi1): 1.589e-02, MSE(pi2): 4.954e-05, MSE(pi3): 5.995e-03\n",
      "Epoch 93700, Train loss: 5.187e+06, Test loss: 7.872e+08, MSE(e): 2.819e-04, MSE(pi1): 1.539e-01, MSE(pi2): 1.239e-04, MSE(pi3): 8.294e-03\n",
      "Epoch 93800, Train loss: 2.785e+06, Test loss: 7.765e+08, MSE(e): 8.215e-05, MSE(pi1): 1.167e-01, MSE(pi2): 4.796e-05, MSE(pi3): 7.967e-03\n",
      "Epoch 93900, Train loss: 1.630e+07, Test loss: 7.940e+08, MSE(e): 4.841e-04, MSE(pi1): 1.024e+00, MSE(pi2): 2.774e-04, MSE(pi3): 1.219e-02\n",
      "Epoch 94000, Train loss: 1.663e+06, Test loss: 7.677e+08, MSE(e): 6.260e-05, MSE(pi1): 4.673e-02, MSE(pi2): 3.532e-05, MSE(pi3): 5.693e-03\n",
      "Epoch 94100, Train loss: 3.184e+06, Test loss: 7.644e+08, MSE(e): 1.734e-04, MSE(pi1): 7.760e-02, MSE(pi2): 9.259e-05, MSE(pi3): 6.739e-03\n",
      "Epoch 94200, Train loss: 3.431e+06, Test loss: 7.650e+08, MSE(e): 1.078e-04, MSE(pi1): 1.803e-01, MSE(pi2): 6.844e-05, MSE(pi3): 5.502e-03\n",
      "Epoch 94300, Train loss: 1.873e+06, Test loss: 7.718e+08, MSE(e): 5.687e-05, MSE(pi1): 6.570e-02, MSE(pi2): 2.978e-05, MSE(pi3): 6.471e-03\n",
      "Epoch 94400, Train loss: 9.414e+06, Test loss: 7.609e+08, MSE(e): 5.633e-04, MSE(pi1): 3.122e-01, MSE(pi2): 2.735e-04, MSE(pi3): 6.593e-03\n",
      "Epoch 94500, Train loss: 3.268e+06, Test loss: 7.648e+08, MSE(e): 2.065e-04, MSE(pi1): 6.380e-02, MSE(pi2): 1.014e-04, MSE(pi3): 5.657e-03\n",
      "Epoch 94600, Train loss: 3.702e+06, Test loss: 7.724e+08, MSE(e): 8.011e-05, MSE(pi1): 2.111e-01, MSE(pi2): 3.510e-05, MSE(pi3): 7.903e-03\n",
      "Epoch 94700, Train loss: 1.524e+07, Test loss: 7.926e+08, MSE(e): 2.460e-04, MSE(pi1): 1.160e+00, MSE(pi2): 1.327e-04, MSE(pi3): 1.185e-02\n",
      "Epoch 94800, Train loss: 1.646e+06, Test loss: 7.772e+08, MSE(e): 7.216e-05, MSE(pi1): 2.718e-02, MSE(pi2): 3.529e-05, MSE(pi3): 6.524e-03\n",
      "Epoch 94900, Train loss: 2.601e+06, Test loss: 7.827e+08, MSE(e): 1.194e-04, MSE(pi1): 4.531e-02, MSE(pi2): 6.502e-05, MSE(pi3): 9.531e-03\n",
      "Epoch 95000, Train loss: 3.506e+06, Test loss: 7.854e+08, MSE(e): 1.257e-04, MSE(pi1): 1.446e-01, MSE(pi2): 6.530e-05, MSE(pi3): 8.034e-03\n",
      "Epoch 95100, Train loss: 5.172e+06, Test loss: 7.667e+08, MSE(e): 1.825e-04, MSE(pi1): 2.507e-01, MSE(pi2): 1.132e-04, MSE(pi3): 8.401e-03\n",
      "Epoch 95200, Train loss: 1.755e+06, Test loss: 7.679e+08, MSE(e): 8.431e-05, MSE(pi1): 3.185e-02, MSE(pi2): 4.430e-05, MSE(pi3): 5.935e-03\n",
      "Epoch 95300, Train loss: 1.908e+06, Test loss: 7.813e+08, MSE(e): 8.926e-05, MSE(pi1): 1.409e-02, MSE(pi2): 4.166e-05, MSE(pi3): 8.748e-03\n",
      "Epoch 95400, Train loss: 2.311e+06, Test loss: 7.698e+08, MSE(e): 7.574e-05, MSE(pi1): 9.524e-02, MSE(pi2): 3.806e-05, MSE(pi3): 6.009e-03\n",
      "Epoch 95500, Train loss: 1.687e+06, Test loss: 7.813e+08, MSE(e): 8.574e-05, MSE(pi1): 1.739e-02, MSE(pi2): 4.348e-05, MSE(pi3): 6.561e-03\n",
      "Epoch 95600, Train loss: 2.619e+06, Test loss: 7.847e+08, MSE(e): 1.394e-04, MSE(pi1): 4.406e-02, MSE(pi2): 6.688e-05, MSE(pi3): 7.845e-03\n",
      "Epoch 95700, Train loss: 1.696e+06, Test loss: 7.752e+08, MSE(e): 5.965e-05, MSE(pi1): 5.194e-02, MSE(pi2): 3.526e-05, MSE(pi3): 5.804e-03\n",
      "Epoch 95800, Train loss: 6.355e+06, Test loss: 7.786e+08, MSE(e): 1.374e-04, MSE(pi1): 4.370e-01, MSE(pi2): 8.218e-05, MSE(pi3): 6.111e-03\n",
      "Epoch 95900, Train loss: 2.504e+06, Test loss: 7.831e+08, MSE(e): 6.800e-05, MSE(pi1): 1.166e-01, MSE(pi2): 3.464e-05, MSE(pi3): 6.575e-03\n",
      "Epoch 96000, Train loss: 2.432e+06, Test loss: 7.900e+08, MSE(e): 1.365e-04, MSE(pi1): 3.973e-02, MSE(pi2): 7.116e-05, MSE(pi3): 6.703e-03\n",
      "Epoch 96100, Train loss: 5.537e+06, Test loss: 7.693e+08, MSE(e): 2.120e-04, MSE(pi1): 2.604e-01, MSE(pi2): 9.929e-05, MSE(pi3): 8.125e-03\n",
      "Epoch 96200, Train loss: 2.989e+06, Test loss: 7.912e+08, MSE(e): 2.031e-04, MSE(pi1): 3.521e-02, MSE(pi2): 8.803e-05, MSE(pi3): 6.062e-03\n",
      "Epoch 96300, Train loss: 3.469e+06, Test loss: 7.665e+08, MSE(e): 2.312e-04, MSE(pi1): 6.535e-02, MSE(pi2): 1.352e-04, MSE(pi3): 5.035e-03\n",
      "Epoch 96400, Train loss: 7.252e+06, Test loss: 7.863e+08, MSE(e): 2.069e-04, MSE(pi1): 4.502e-01, MSE(pi2): 1.078e-04, MSE(pi3): 6.819e-03\n",
      "Epoch 96500, Train loss: 1.048e+07, Test loss: 7.683e+08, MSE(e): 8.753e-04, MSE(pi1): 1.171e-01, MSE(pi2): 4.117e-04, MSE(pi3): 5.542e-03\n",
      "Epoch 96600, Train loss: 7.361e+06, Test loss: 7.645e+08, MSE(e): 5.612e-04, MSE(pi1): 1.309e-01, MSE(pi2): 2.917e-04, MSE(pi3): 4.401e-03\n",
      "Epoch 96700, Train loss: 1.616e+07, Test loss: 8.089e+08, MSE(e): 1.327e-03, MSE(pi1): 2.319e-01, MSE(pi2): 5.984e-04, MSE(pi3): 5.706e-03\n",
      "Epoch 96800, Train loss: 8.384e+06, Test loss: 7.709e+08, MSE(e): 5.731e-04, MSE(pi1): 2.104e-01, MSE(pi2): 2.717e-04, MSE(pi3): 5.493e-03\n",
      "Epoch 96900, Train loss: 6.108e+06, Test loss: 7.799e+08, MSE(e): 2.889e-04, MSE(pi1): 2.419e-01, MSE(pi2): 1.351e-04, MSE(pi3): 8.000e-03\n",
      "Epoch 97000, Train loss: 5.244e+06, Test loss: 7.907e+08, MSE(e): 1.019e-04, MSE(pi1): 3.600e-01, MSE(pi2): 4.781e-05, MSE(pi3): 6.246e-03\n",
      "Epoch 97100, Train loss: 3.459e+07, Test loss: 7.647e+08, MSE(e): 3.182e-03, MSE(pi1): 2.178e-01, MSE(pi2): 1.517e-03, MSE(pi3): 5.862e-03\n",
      "Epoch 97200, Train loss: 2.947e+06, Test loss: 7.854e+08, MSE(e): 7.537e-05, MSE(pi1): 1.505e-01, MSE(pi2): 4.319e-05, MSE(pi3): 6.877e-03\n",
      "Epoch 97300, Train loss: 3.850e+06, Test loss: 7.843e+08, MSE(e): 9.383e-05, MSE(pi1): 2.359e-01, MSE(pi2): 5.360e-05, MSE(pi3): 5.530e-03\n",
      "Epoch 97400, Train loss: 2.980e+06, Test loss: 8.052e+08, MSE(e): 1.572e-04, MSE(pi1): 8.033e-02, MSE(pi2): 7.228e-05, MSE(pi3): 6.043e-03\n",
      "Epoch 97500, Train loss: 6.249e+06, Test loss: 7.846e+08, MSE(e): 3.996e-04, MSE(pi1): 1.450e-01, MSE(pi2): 2.296e-04, MSE(pi3): 8.019e-03\n",
      "Epoch 97600, Train loss: 2.886e+06, Test loss: 7.910e+08, MSE(e): 2.013e-04, MSE(pi1): 2.471e-02, MSE(pi2): 1.002e-04, MSE(pi3): 6.264e-03\n",
      "Epoch 97700, Train loss: 2.360e+06, Test loss: 7.820e+08, MSE(e): 7.680e-05, MSE(pi1): 9.135e-02, MSE(pi2): 4.289e-05, MSE(pi3): 6.784e-03\n",
      "Epoch 97800, Train loss: 7.591e+06, Test loss: 7.879e+08, MSE(e): 1.660e-04, MSE(pi1): 5.331e-01, MSE(pi2): 9.984e-05, MSE(pi3): 6.004e-03\n",
      "Epoch 97900, Train loss: 6.249e+06, Test loss: 7.789e+08, MSE(e): 1.573e-04, MSE(pi1): 3.887e-01, MSE(pi2): 1.100e-04, MSE(pi3): 7.890e-03\n",
      "Epoch 98000, Train loss: 1.156e+07, Test loss: 7.716e+08, MSE(e): 9.218e-04, MSE(pi1): 1.771e-01, MSE(pi2): 4.172e-04, MSE(pi3): 5.688e-03\n",
      "Epoch 98100, Train loss: 3.766e+07, Test loss: 8.894e+08, MSE(e): 3.442e-03, MSE(pi1): 2.489e-01, MSE(pi2): 1.515e-03, MSE(pi3): 7.492e-03\n",
      "Epoch 98200, Train loss: 3.895e+06, Test loss: 7.876e+08, MSE(e): 1.567e-04, MSE(pi1): 1.789e-01, MSE(pi2): 1.002e-04, MSE(pi3): 5.393e-03\n",
      "Epoch 98300, Train loss: 1.156e+07, Test loss: 7.700e+08, MSE(e): 1.071e-03, MSE(pi1): 2.969e-02, MSE(pi2): 4.860e-04, MSE(pi3): 5.532e-03\n",
      "Epoch 98400, Train loss: 1.122e+07, Test loss: 8.184e+08, MSE(e): 9.242e-04, MSE(pi1): 1.325e-01, MSE(pi2): 4.012e-04, MSE(pi3): 6.513e-03\n",
      "Epoch 98500, Train loss: 3.540e+06, Test loss: 7.846e+08, MSE(e): 1.365e-04, MSE(pi1): 1.691e-01, MSE(pi2): 9.506e-05, MSE(pi3): 4.844e-03\n",
      "Epoch 98600, Train loss: 5.803e+06, Test loss: 8.035e+08, MSE(e): 2.181e-04, MSE(pi1): 2.774e-01, MSE(pi2): 1.265e-04, MSE(pi3): 8.486e-03\n",
      "Epoch 98700, Train loss: 4.478e+06, Test loss: 8.011e+08, MSE(e): 2.198e-04, MSE(pi1): 1.560e-01, MSE(pi2): 1.066e-04, MSE(pi3): 7.206e-03\n",
      "Epoch 98800, Train loss: 2.278e+06, Test loss: 7.855e+08, MSE(e): 6.793e-05, MSE(pi1): 1.034e-01, MSE(pi2): 3.559e-05, MSE(pi3): 5.648e-03\n",
      "Epoch 98900, Train loss: 2.236e+06, Test loss: 7.930e+08, MSE(e): 8.125e-05, MSE(pi1): 7.957e-02, MSE(pi2): 4.400e-05, MSE(pi3): 6.282e-03\n",
      "Epoch 99000, Train loss: 3.390e+06, Test loss: 7.946e+08, MSE(e): 1.743e-04, MSE(pi1): 1.053e-01, MSE(pi2): 8.795e-05, MSE(pi3): 5.939e-03\n",
      "Epoch 99100, Train loss: 3.078e+06, Test loss: 7.854e+08, MSE(e): 1.039e-04, MSE(pi1): 1.354e-01, MSE(pi2): 5.269e-05, MSE(pi3): 6.860e-03\n",
      "Epoch 99200, Train loss: 2.606e+06, Test loss: 7.921e+08, MSE(e): 1.166e-04, MSE(pi1): 6.585e-02, MSE(pi2): 6.380e-05, MSE(pi3): 7.814e-03\n",
      "Epoch 99300, Train loss: 4.091e+06, Test loss: 7.975e+08, MSE(e): 1.091e-04, MSE(pi1): 2.209e-01, MSE(pi2): 5.710e-05, MSE(pi3): 7.915e-03\n",
      "Epoch 99400, Train loss: 3.510e+06, Test loss: 7.824e+08, MSE(e): 2.383e-04, MSE(pi1): 3.927e-02, MSE(pi2): 1.402e-04, MSE(pi3): 7.343e-03\n",
      "Epoch 99500, Train loss: 2.241e+06, Test loss: 7.933e+08, MSE(e): 7.859e-05, MSE(pi1): 5.591e-02, MSE(pi2): 4.492e-05, MSE(pi3): 8.959e-03\n",
      "Epoch 99600, Train loss: 6.132e+06, Test loss: 7.838e+08, MSE(e): 3.840e-04, MSE(pi1): 1.640e-01, MSE(pi2): 2.970e-04, MSE(pi3): 6.520e-03\n",
      "Epoch 99700, Train loss: 3.725e+07, Test loss: 7.905e+08, MSE(e): 3.382e-03, MSE(pi1): 2.665e-01, MSE(pi2): 1.534e-03, MSE(pi3): 7.667e-03\n",
      "Epoch 99800, Train loss: 6.264e+06, Test loss: 7.916e+08, MSE(e): 2.013e-04, MSE(pi1): 3.410e-01, MSE(pi2): 1.058e-04, MSE(pi3): 8.414e-03\n",
      "Epoch 99900, Train loss: 2.736e+06, Test loss: 7.853e+08, MSE(e): 1.280e-04, MSE(pi1): 4.918e-02, MSE(pi2): 8.893e-05, MSE(pi3): 9.647e-03\n",
      "\n",
      "Training process finished after 100000 epochs\n",
      "\n"
     ]
    }
   ],
   "source": [
    "start_epoch = 18000\n",
    "n_epochs = 100000\n",
    "\n",
    "batch_size = 64 \n",
    "n_checkpoints = 10\n",
    "\n",
    "second_lr = 3e-4\n",
    "\n",
    "train_loop(model, optimizer, X_train, y_train, f_train, X_test, y_test, f_test,\n",
    "           D, n_checkpoints, start_epoch=start_epoch, n_epochs=n_epochs, batch_size=batch_size, \n",
    "           model_results_path=MODEL_RESULTS_TRANSFERLEARNING_PATH, device=DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x7f4871026ed0>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.scatter(Mx(My(TensOps(model(X_train)[0], space_dimension=2, contravariance=0, covariance=0))).values.cpu().detach().numpy().flatten(), \n",
    "            model(X_train)[1].cpu().detach().numpy().flatten())\n",
    "\n",
    "plt.scatter(y_train.values.cpu().detach().numpy().flatten(), \n",
    "           K_train.values.cpu().detach().numpy().flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Parametros de entrenamiento\n",
    "# start_epoch = 9000\n",
    "# n_epochs = 100000\n",
    "\n",
    "# batch_size = 64 \n",
    "# n_checkpoints = 100\n",
    "\n",
    "# second_lr = 1e-4\n",
    "\n",
    "# train_loop(model, optimizer, X_train_NN, y_train_NN, f_train_NN, X_test_NN, y_test_NN, f_test_NN,\n",
    "#            D, n_checkpoints, start_epoch=start_epoch, n_epochs=n_epochs, batch_size=batch_size, \n",
    "#            model_results_path=MODEL_RESULTS_TRANSFERLEARNING_PATH, device=DEVICE, new_lr=second_lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SciML_test_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
